{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the GROQ API key is loaded\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found. Make sure it's set in your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a single PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from all PDFs in a folder\n",
    "def extract_text_from_folder(folder_path):\n",
    "    all_text = {}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            all_text[file] = extract_text_from_pdf(pdf_path)\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'Page\\s+\\d+\\s+(of\\s+\\d+)?', '', text, flags=re.IGNORECASE)  # Remove page numbers\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remove multiple newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove excess spaces\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [line for line in lines if len(line.strip()) > 10]  # Remove short lines\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean all texts\n",
    "def clean_all_texts(pdf_texts):\n",
    "    cleaned_texts = {}\n",
    "    for pdf_name, text in pdf_texts.items():\n",
    "        print(f\"Cleaning text for: {pdf_name}\")\n",
    "        cleaned_texts[pdf_name] = clean_text(text)\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text into smaller sections\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_token + len(word) + 1 > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_token = 0\n",
    "        current_chunk.append(word)\n",
    "        current_token += len(word) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk all cleaned texts\n",
    "def chunk_all_texts(cleaned_texts, max_length=500):\n",
    "    chunked_texts = {}\n",
    "    for pdf_name, text in cleaned_texts.items():\n",
    "        print(f\"Chunking text for: {pdf_name}\")\n",
    "        chunked_texts[pdf_name] = chunk_text(text, max_length)\n",
    "    return chunked_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1501.05039v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1602.00203v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1607.00858v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1705.03921v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1711.03577v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.03551v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.04825v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.08355v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1806.01756v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1812.05448v4.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.02354v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.04195v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.09388v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1908.02130v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2002.05658v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2007.03606v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2010.05125v2.pdf\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2106.00120v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2108.01468v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2108.11510v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2112.01590v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2201.05852v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2201.05867v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2303.01980v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2303.02715v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2306.13586v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2306.16177v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2308.04896v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2403.00776v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2403.03387v2.pdf\n",
      "Cleaning text for: 1501.05039v1.pdf\n",
      "Cleaning text for: 1602.00203v1.pdf\n",
      "Cleaning text for: 1607.00858v1.pdf\n",
      "Cleaning text for: 1705.03921v1.pdf\n",
      "Cleaning text for: 1711.03577v1.pdf\n",
      "Cleaning text for: 1805.03551v2.pdf\n",
      "Cleaning text for: 1805.04825v1.pdf\n",
      "Cleaning text for: 1805.08355v1.pdf\n",
      "Cleaning text for: 1806.01756v1.pdf\n",
      "Cleaning text for: 1812.05448v4.pdf\n",
      "Cleaning text for: 1901.02354v2.pdf\n",
      "Cleaning text for: 1901.04195v1.pdf\n",
      "Cleaning text for: 1901.09388v2.pdf\n",
      "Cleaning text for: 1908.02130v1.pdf\n",
      "Cleaning text for: 2002.05658v1.pdf\n",
      "Cleaning text for: 2007.03606v1.pdf\n",
      "Cleaning text for: 2010.05125v2.pdf\n",
      "Cleaning text for: 2106.00120v3.pdf\n",
      "Cleaning text for: 2108.01468v1.pdf\n",
      "Cleaning text for: 2108.11510v1.pdf\n",
      "Cleaning text for: 2112.01590v3.pdf\n",
      "Cleaning text for: 2201.05852v1.pdf\n",
      "Cleaning text for: 2201.05867v1.pdf\n",
      "Cleaning text for: 2303.01980v1.pdf\n",
      "Cleaning text for: 2303.02715v1.pdf\n",
      "Cleaning text for: 2306.13586v1.pdf\n",
      "Cleaning text for: 2306.16177v3.pdf\n",
      "Cleaning text for: 2308.04896v1.pdf\n",
      "Cleaning text for: 2403.00776v1.pdf\n",
      "Cleaning text for: 2403.03387v2.pdf\n",
      "Chunking text for: 1501.05039v1.pdf\n",
      "Chunking text for: 1602.00203v1.pdf\n",
      "Chunking text for: 1607.00858v1.pdf\n",
      "Chunking text for: 1705.03921v1.pdf\n",
      "Chunking text for: 1711.03577v1.pdf\n",
      "Chunking text for: 1805.03551v2.pdf\n",
      "Chunking text for: 1805.04825v1.pdf\n",
      "Chunking text for: 1805.08355v1.pdf\n",
      "Chunking text for: 1806.01756v1.pdf\n",
      "Chunking text for: 1812.05448v4.pdf\n",
      "Chunking text for: 1901.02354v2.pdf\n",
      "Chunking text for: 1901.04195v1.pdf\n",
      "Chunking text for: 1901.09388v2.pdf\n",
      "Chunking text for: 1908.02130v1.pdf\n",
      "Chunking text for: 2002.05658v1.pdf\n",
      "Chunking text for: 2007.03606v1.pdf\n",
      "Chunking text for: 2010.05125v2.pdf\n",
      "Chunking text for: 2106.00120v3.pdf\n",
      "Chunking text for: 2108.01468v1.pdf\n",
      "Chunking text for: 2108.11510v1.pdf\n",
      "Chunking text for: 2112.01590v3.pdf\n",
      "Chunking text for: 2201.05852v1.pdf\n",
      "Chunking text for: 2201.05867v1.pdf\n",
      "Chunking text for: 2303.01980v1.pdf\n",
      "Chunking text for: 2303.02715v1.pdf\n",
      "Chunking text for: 2306.13586v1.pdf\n",
      "Chunking text for: 2306.16177v3.pdf\n",
      "Chunking text for: 2308.04896v1.pdf\n",
      "Chunking text for: 2403.00776v1.pdf\n",
      "Chunking text for: 2403.03387v2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load and process PDF folder\n",
    "folder_path = r\"C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\"\n",
    "pdf_texts = extract_text_from_folder(folder_path)\n",
    "cleaned_pdf_texts = clean_all_texts(pdf_texts)\n",
    "chunked_texts = chunk_all_texts(cleaned_pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3703\n"
     ]
    }
   ],
   "source": [
    "# Flatten the chunked texts\n",
    "flat_chunks = [chunk for pdf_chunks in chunked_texts.values() for chunk in pdf_chunks]\n",
    "print(f\"Total chunks created: {len(flat_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 116/116 [01:04<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 3703 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(flat_chunks, show_progress_bar=True)\n",
    "print(f\"Generated embeddings for {len(flat_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and populated with embeddings.\n"
     ]
    }
   ],
   "source": [
    "# FAISS Index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "print(\"FAISS index created and populated with embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant chunks\n",
    "def retrieve_relevant_chunks(query, embedding_model, index, chunks, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k=top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROQ LLM initialization\n",
    "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine chunks\n",
    "def combine_chunks(relevant_chunks, max_length=3000):\n",
    "    combined_text = \"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if len(combined_text) + len(chunk) <= max_length:\n",
    "            combined_text += chunk + \"\\n\"\n",
    "        else:\n",
    "            break\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response using GROQ LLM\n",
    "def generate_response(query, context):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Use the context provided to answer the question accurately. \n",
    "    If you do not have information to answer the question, say 'I don't have enough information to answer this question'.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = llm.invoke(input=prompt, max_tokens=300)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full RAG pipeline\n",
    "def query_rag_system(query, embedding_model, llm, max_context_length=3000):\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, embedding_model, index, flat_chunks, top_k=5)\n",
    "    context = combine_chunks(relevant_chunks, max_length=max_context_length)\n",
    "    response = generate_response(query, context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "The main components of the Transformer architecture include:\n",
      "\n",
      "1. Attention Mechanism: This is the core component of the Transformer model, which calculates the attentions from the input sequence to determine the aggregation.\n",
      "\n",
      "2. Positional Embedding: This component is used to encode the token positions in the input sequence, as Transformers make least assumptions on the structural information of data.\n",
      "\n",
      "3. Linear Embedding: In the context of Vision Transformer (ViT), each image is split into fixed-size patches, and then each patch is linearly embedded.\n",
      "\n",
      "4. Transformer Encoder: The resulting sequence of vectors from the above components is fed into a standard Transformer encoder for sequence modeling.\n",
      "\n",
      "5. Self-Attention Mechanism: This dynamically computes the connection weights between every two tokens in the input sequence.\n",
      "\n",
      "6. Feature Dimension: This measures the number of parameters required to closely approximate the optimization problem.\n",
      "\n",
      "7. Intrinsic-Dimension Generalization Bound: This indicates that the pre-trained parameters implicitly affect the inductive bias of models, and a larger pre-trained model might correspond to a smaller allowed hypothesis space during fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main components of the transformer architecture?\"\n",
    "response = query_rag_system(query, embedding_model, llm)\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
