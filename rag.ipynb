{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soura\\anaconda3\\envs\\llmpy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the GROQ API key is loaded\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found. Make sure it's set in your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a single PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from all PDFs in a folder\n",
    "def extract_text_from_folder(folder_path):\n",
    "    all_text = {}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            all_text[file] = extract_text_from_pdf(pdf_path)\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'Page\\s+\\d+\\s+(of\\s+\\d+)?', '', text, flags=re.IGNORECASE)  # Remove page numbers\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remove multiple newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove excess spaces\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [line for line in lines if len(line.strip()) > 10]  # Remove short lines\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean all texts\n",
    "def clean_all_texts(pdf_texts):\n",
    "    cleaned_texts = {}\n",
    "    for pdf_name, text in pdf_texts.items():\n",
    "        print(f\"Cleaning text for: {pdf_name}\")\n",
    "        cleaned_texts[pdf_name] = clean_text(text)\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk text into smaller sections\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_token + len(word) + 1 > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_token = 0\n",
    "        current_chunk.append(word)\n",
    "        current_token += len(word) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chunk all cleaned texts\n",
    "def chunk_all_texts(cleaned_texts, max_length=500):\n",
    "    chunked_texts = {}\n",
    "    for pdf_name, text in cleaned_texts.items():\n",
    "        print(f\"Chunking text for: {pdf_name}\")\n",
    "        chunked_texts[pdf_name] = chunk_text(text, max_length)\n",
    "    return chunked_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1501.05039v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1602.00203v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1607.00858v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1705.03921v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1711.03577v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.03551v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.04825v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.08355v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1806.01756v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1812.05448v4.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.02354v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.04195v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.09388v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1908.02130v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2002.05658v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2007.03606v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2010.05125v2.pdf\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2106.00120v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2108.01468v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2108.11510v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2112.01590v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2201.05852v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2201.05867v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2303.01980v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2303.02715v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2306.13586v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2306.16177v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2308.04896v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2403.00776v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2403.03387v2.pdf\n",
      "Cleaning text for: 1501.05039v1.pdf\n",
      "Cleaning text for: 1602.00203v1.pdf\n",
      "Cleaning text for: 1607.00858v1.pdf\n",
      "Cleaning text for: 1705.03921v1.pdf\n",
      "Cleaning text for: 1711.03577v1.pdf\n",
      "Cleaning text for: 1805.03551v2.pdf\n",
      "Cleaning text for: 1805.04825v1.pdf\n",
      "Cleaning text for: 1805.08355v1.pdf\n",
      "Cleaning text for: 1806.01756v1.pdf\n",
      "Cleaning text for: 1812.05448v4.pdf\n",
      "Cleaning text for: 1901.02354v2.pdf\n",
      "Cleaning text for: 1901.04195v1.pdf\n",
      "Cleaning text for: 1901.09388v2.pdf\n",
      "Cleaning text for: 1908.02130v1.pdf\n",
      "Cleaning text for: 2002.05658v1.pdf\n",
      "Cleaning text for: 2007.03606v1.pdf\n",
      "Cleaning text for: 2010.05125v2.pdf\n",
      "Cleaning text for: 2106.00120v3.pdf\n",
      "Cleaning text for: 2108.01468v1.pdf\n",
      "Cleaning text for: 2108.11510v1.pdf\n",
      "Cleaning text for: 2112.01590v3.pdf\n",
      "Cleaning text for: 2201.05852v1.pdf\n",
      "Cleaning text for: 2201.05867v1.pdf\n",
      "Cleaning text for: 2303.01980v1.pdf\n",
      "Cleaning text for: 2303.02715v1.pdf\n",
      "Cleaning text for: 2306.13586v1.pdf\n",
      "Cleaning text for: 2306.16177v3.pdf\n",
      "Cleaning text for: 2308.04896v1.pdf\n",
      "Cleaning text for: 2403.00776v1.pdf\n",
      "Cleaning text for: 2403.03387v2.pdf\n",
      "Chunking text for: 1501.05039v1.pdf\n",
      "Chunking text for: 1602.00203v1.pdf\n",
      "Chunking text for: 1607.00858v1.pdf\n",
      "Chunking text for: 1705.03921v1.pdf\n",
      "Chunking text for: 1711.03577v1.pdf\n",
      "Chunking text for: 1805.03551v2.pdf\n",
      "Chunking text for: 1805.04825v1.pdf\n",
      "Chunking text for: 1805.08355v1.pdf\n",
      "Chunking text for: 1806.01756v1.pdf\n",
      "Chunking text for: 1812.05448v4.pdf\n",
      "Chunking text for: 1901.02354v2.pdf\n",
      "Chunking text for: 1901.04195v1.pdf\n",
      "Chunking text for: 1901.09388v2.pdf\n",
      "Chunking text for: 1908.02130v1.pdf\n",
      "Chunking text for: 2002.05658v1.pdf\n",
      "Chunking text for: 2007.03606v1.pdf\n",
      "Chunking text for: 2010.05125v2.pdf\n",
      "Chunking text for: 2106.00120v3.pdf\n",
      "Chunking text for: 2108.01468v1.pdf\n",
      "Chunking text for: 2108.11510v1.pdf\n",
      "Chunking text for: 2112.01590v3.pdf\n",
      "Chunking text for: 2201.05852v1.pdf\n",
      "Chunking text for: 2201.05867v1.pdf\n",
      "Chunking text for: 2303.01980v1.pdf\n",
      "Chunking text for: 2303.02715v1.pdf\n",
      "Chunking text for: 2306.13586v1.pdf\n",
      "Chunking text for: 2306.16177v3.pdf\n",
      "Chunking text for: 2308.04896v1.pdf\n",
      "Chunking text for: 2403.00776v1.pdf\n",
      "Chunking text for: 2403.03387v2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load and process PDF folder\n",
    "folder_path = r\"C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\"\n",
    "pdf_texts = extract_text_from_folder(folder_path)\n",
    "cleaned_pdf_texts = clean_all_texts(pdf_texts)\n",
    "chunked_texts = chunk_all_texts(cleaned_pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3703\n"
     ]
    }
   ],
   "source": [
    "# Flatten the chunked texts\n",
    "flat_chunks = [chunk for pdf_chunks in chunked_texts.values() for chunk in pdf_chunks]\n",
    "print(f\"Total chunks created: {len(flat_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 116/116 [01:02<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 3703 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(flat_chunks, show_progress_bar=True)\n",
    "print(f\"Generated embeddings for {len(flat_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and populated with embeddings.\n"
     ]
    }
   ],
   "source": [
    "# FAISS Index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "print(\"FAISS index created and populated with embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant chunks\n",
    "def retrieve_relevant_chunks(query, embedding_model, index, chunks, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k=top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROQ LLM initialization\n",
    "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine chunks\n",
    "def combine_chunks(relevant_chunks, max_length=3000):\n",
    "    combined_text = \"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if len(combined_text) + len(chunk) <= max_length:\n",
    "            combined_text += chunk + \"\\n\"\n",
    "        else:\n",
    "            break\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response using GROQ LLM\n",
    "def generate_response(query, context):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Use the context provided to answer the question accurately. \n",
    "    If you do not have information to answer the question, say 'I don't have enough information to answer this question'.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = llm.invoke(input=prompt, max_tokens=300)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full RAG pipeline\n",
    "def query_rag_system(query, embedding_model, llm, max_context_length=3000):\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, embedding_model, index, flat_chunks, top_k=5)\n",
    "    context = combine_chunks(relevant_chunks, max_length=max_context_length)\n",
    "    response = generate_response(query, context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "The Transformer architecture is a powerful model for sequence modeling, particularly in natural language processing. It was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. Transformer makes the least assumptions about the structural information of data, making it an expressive architecture for storing transferable knowledge extracted by pre-training on large amounts of training data.\n",
      "\n",
      "The Transformer model consists of an encoder and a decoder, both of which are composed of multiple identical layers stacked on top of each other. Each layer contains a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows the model to weigh the importance of different words in the input sequence when encoding or decoding, while the feed-forward network applies a non-linear transformation to the output of the self-attention mechanism.\n",
      "\n",
      "One of the key features of the Transformer architecture is the use of positional embeddings to encode the position of tokens in the input sequence. This is because the model does not have any recurrence or convolution, so it needs a way to keep track of the order of the tokens. Positional embeddings are added to the token embeddings before they are fed into the Transformer model.\n",
      "\n",
      "Another important aspect of the Transformer architecture is the removal of the local connectivity assumption. Unlike traditional recurrent neural networks (RNN\n"
     ]
    }
   ],
   "source": [
    "query = \"explain the concept of transformer architecture\"\n",
    "response = query_rag_system(query, embedding_model, llm)\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
