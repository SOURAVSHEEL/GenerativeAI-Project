{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "# from langchain.embeddings import \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load enviroment variables from the .env files\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the api key from the .env file\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrct_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_folder(folder_path):\n",
    "    all_text = {}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, file)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            all_text[file] = extrct_text_from_pdf(pdf_path)\n",
    "    return all_text            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1501.05039v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1602.00203v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1607.00858v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1705.03921v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1711.03577v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.03551v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.04825v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1805.08355v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1806.01756v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1812.05448v4.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.02354v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.04195v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1901.09388v2.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\1908.02130v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2002.05658v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2007.03606v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2010.05125v2.pdf\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'times-minus'\n",
      "\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2106.00120v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2108.01468v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2108.11510v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2112.01590v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2201.05852v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2201.05867v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2303.01980v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2303.02715v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2306.13586v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2306.16177v3.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2308.04896v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2403.00776v1.pdf\n",
      "Processing: C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\\2403.03387v2.pdf\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\soura\\OneDrive\\Desktop\\Projects\\GenerativeAI-Projects\\ResearchPaper-Query-RAG\\ResearchPaper-Data\"\n",
    "pdf_texts = extract_text_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1501.05039v1.pdf': \"Defining Data Science \\nBeyond the study of the rules of the natural world as reflected by data \\n \\nYangyong Zhu and Yun Xiong \\nSchool of Computer Science, Fudan University, Shanghai, China  \\nShanghai Key Laboratory of Data Science, Fudan University, China. \\n{yyzhu, yunx}@fudan.edu.cn \\n \\nData science has received widespread attention in academic and industrial circles. New data \\nscience research institutes and organizations have continued to emerge on the scene, such as the \\nColumbia University Institute for Data Sciences and Engineering and New York University \\nCenter for Data Science. The University of California at Berkeley, Columbia University, Fudan \\nUniversity, and other universities have launched data science courses and degree programs. \\nCleveland and Smith proposed that data science should be considered an independent \\ndiscipline2, 8. Facebook, Google, EMC, IBM, and other companies have established employment \\npositions for data scientists. According to Harvard Business Review, the data scientist is “the \\nsexiest job of the 21st century.” Currently, there are several viewpoints regarding the definition \\nof data science (see page 2). However, there is no consensus definition. We believe that, as a \\nnew science, the research objectives of data science are different from those of other, more \\nestablished branches of science. In addition, the scientific issues that data science addresses are \\nnot studied by natural or social sciences.  \\nOur team has worked on data technology and research projects funded by Chinese \\ngovernment since 1998, and we have applied our work to life science, healthcare, finance, \\ntransportation, and other fields (Table 1). Over the years, we have noticed a number of common \\nissues related to data in scientific research and industrial applications, most notably the similarity \\nof data objects. We have come to realize that there is a considerable need to conduct research \\nspecifically on the data itself, and we started to explore concept of data science in 20099. Since \\n2010, we have hosted the annual International Symposium on Data Science and Dataology \\n(iwdds.fudan.edu.cn). The symposium provides us with forum for the discussion of data science \\nissues with scientists involved in computer science, life sciences, astronomy, and other fields. \\nOver the past 16 years, our understanding of data science has taken more solid shape. We believe \\nthat data in cyberspace have formed what we call datanature9, 10. Data science is the scientific \\nresearch of datanature.   \\nThere are several current viewpoints on data science.  \\nVP1: Data science is the science of studying scientific data.  \\nThe Committee on Data for Science and Technology (CODATA) launched the Data Science \\nJournal (codata.org/dsj/) in 2002. CODATA regards data science as the methods and \\ntechnologies used to conduct scientific research through management and utilization of scientific \\ndata. As scientific data have become more accessible, data science has been used to better \\ncharacterize the data-intensive nature of today’s science and engineering. Many disciplines use \\ndata technology to deal with scientific data from their respective areas. From this, X-informatics \\nemerged, including bioinformatics, neuroinformatics, and social informatics. \\nFor example, researchers in NuMedii, Inc., a big-data company in Silicon Valley, predicted \\nwhether existing drugs could be used to treat ovarian cancer by examining gene expression data \\nfrom over 2,500 ovarian tumor samples6.  \\nAs another example, mathematicians from Harvard University Aiden and Michel studied \\nAmerican history using Ngrams on Google1. They used Ngrams to search for the usage \\nfrequencies of two phrases: “United States are” and “United States is.” The search results \\nshowed that before the American Civil War, the two phrases were used at roughly equal \\nfrequency, but after the Civil War, the latter became far more common than the former. This is \\nseen as indicative of the levels of acceptance by the public of the United States as a unified \\nnation before and after the Civil War. \\nFrom this point of view, data mainly refer to data generated and used in scientific studies. This \\nemphasizes that data science is the management, processing, and use of scientific data to support \\nscientific research, i.e., the currently commonly known data-intensive scientific research or \\nfourth paradigm of scientific research4.  \\nVP2: Data science is the science of studying business data.  \\nIn 2010, Loukides discussed what data science is, arguing that data science should enable the \\ncreation of data products rather than working as a simple application with data5. In 2013, Provost \\net al. pointed out, “extracting knowledge from data to solve business problems” is one of the \\nfundamental concepts of data science7.  \\nProviding support for BI methodology research makes up a significant portion of the work \\nperformed by many data scientists. To effect this, a large proportion of BI practitioners were \\ntransitioned into data scientists. Amazon, Google, LinkedIn, Facebook, and other internet \\ncompanies opened job positions for data scientists and established data science teams. These \\ndata scientists study and analyze business data to provide services for management decision \\nmaking. For example, Amazon uses collaborative filtering to generate high-quality product \\nrecommendations, and Facebook uses a “People you may know” feature to recommend friend \\nconnections.  \\nFrom this point of view, the acquisition of knowledge from business data in order to make \\ndecisions is one aspect of data science. This is similar to what BI scientists work on. For this \\nreason, many BI scientists are also called data scientists. However, compared to BI issues, data \\nscience focuses more on common issues in the analysis of various business data, i.e., the issues \\non BI methodology. \\nVP3: Data science is an integration of statistics, computing technology, and artificial \\nintelligence (AI). \\nThis viewpoint often comes up in discussions on what data scientists are. It is generally \\nbelieved that data scientists should have skills in statistics, computing technology, AI, and \\nrelated fields and that data scientists are not individual people specializing in one field so much \\nas teams consisting of statisticians, computer scientists, AI experts, and domain experts.  \\nFor example, the data scientist teams at Google and Facebook are composed of statisticians, \\ncomputer scientists, AI scientists, and experts in other relevant fields.  \\nThis viewpoint is simple: Because statistics, computing technology, and AI are all used to \\nprocess and analyze data, they are all a natural part of data science. \\nVP4: The purpose of data science is to solve scientific and business problems by extracting \\nknowledge from data. \\nIn 2013, Dhar discussed the implications of data science from a business and research \\nstandpoint3. He defined data science as “the study of the generalizable extraction of knowledge \\nfrom data”3. He also pointed out that a data scientist needs to have comprehensive skills covering \\nstatistics, machine learning, AI, and database management and have a deep understanding of \\nproblem design. This viewpoint can be seen as an integration of the first three viewpoints.  \\nWhat is data science? \\nThe basic ideas underlying the definitions given above are that data science is used to acquire \\nknowledge from data in some relevant fields and to provide support for existing scientific \\nresearch and management decision-making schema. However, all work described above is still \\nnot enough to establish data science as a new, unique branch of science. This is because the \\nobjects of their study are things in the natural world, and their research issues are also addressed \\nin existing scientific fields.  \\nWith the development of digital equipment, things in the natural world are increasingly being \\nstored in cyberspace in the form of data. Data are entered, generated, and created in cyberspace \\nin a variety of ways and have become more and more diverse, complex, and out of human \\ncontrol. More and more data are unknown to or poorly understood by humans. Data in the \\ncyberspace already show features of an independent world, like the natural world, so all data in \\ncyberspace are here referred to as datanature. \\nIt should be noted that there are two types of data in the cyberspace. The first is the data that \\nrepresent things in the natural world, here called real data. An example is personal information, \\nwhich is data representative of personal characteristics. The second is data that do not represent \\nthings in the natural world, here called virtual data. Virtual data means that the instances of such \\ndata have no references in the natural world. An example is computer viruses, which are neither \\nviruses in the natural world nor data representation of real viruses; instead, they only exist in \\ncyberspace (Figure 1). \\n \\n \\nThe formation of datanature has produced new objects of study and new scientific issues. \\nThese new objects of study are not things that exist in the natural world or in human society but \\nrather in datanature, i.e., data. There are new scientific issues about datanature. What size is \\ndatanature? What is the growth rate of global data? How do the data flow in cyberspace? How \\nshould the authenticity of datanature be determined? None of these issues are addressed by the \\nnatural or social sciences. These new scientific issues need to be studied by a new science. \\nData science is the science of studying datanature and the science of data itself. On a basic \\nlevel, it involves extracting knowledge from data. Because some of the data in datanature do \\nrepresent real things, the knowledge acquired from these data can be used for natural and social \\nscience. This type of work is considered as data science according to VP1-VP4. However, it is \\nonly one part of data science research.  \\nData science is here defined as follows:  \\nData science is the theory, method, and technology of studying datanature. It has two main \\ncomponents.  \\nThe first component is the study of the patterns and rules of data itself. Its goal is to \\nexplore datanature and scientific issues related to datanature. This does not take into account \\nthe meaning of the data in the natural world.  \\nThe second component is the study of the rules of the natural world as reflected by data, \\ni.e., the study the natural world performed through the study of data. For example, the \\npurpose of performing a study on data representing a person’s behavior is to study that person’s \\nbehavior. \\nAs mentioned above, studies on data have been under way for some time, and data \\ntechniques such as data mining have been developed. However, the data science research \\ncommunity needs to establish fundamental theory and basic methods for scientific observation \\nand measurement and to further develop data techniques. Figure 2 shows the main topics of data \\nscience.   \\n \\nFigure 2. Research Topics of Data Science \\nConclusions \\nData science is gaining more and more and widespread attention, but no consensus viewpoint on \\nwhat data science is has emerged. As a new science, its objects of study and scientific issues \\nshould not be covered by established sciences. In the present paper, data science is defined as the \\nscience of exploring datanature. We believe this is the most logical and accurate definition of \\ndata science, and it includes key parts of definitions VP1-VP4.  \\n \\nReference. \\n1. \\nAiden, E., and Michel, J. B. Uncharted: Big data as a lens on human culture. Penguin Group (2013). \\n2. \\nCleveland, W. S. Data Science: an action plan for expanding the technical areas of the field of statistics. \\nInternational Statistical Review 69, 1 (2001), 21-26. \\n3. \\nDhar, V. Data Science and Prediction. Commun. ACM 56, 12 (Dec. 2013), 64-73. \\n4. \\nHey, T. et al. The fourth paradigm: Data intensive scientific discovery. Microsoft Research. (Oct. 2009); \\nhttp://research.microsoft.com/en-us/collaboration/fourthparadigm/. \\n5. \\nLoukides, \\nM. \\n \\nWhat \\nis \\ndata \\nscience? \\n \\nAn \\nO'Reilly \\nRadar \\nReport \\n2010; \\nhttp://radar.oreilly.com/2010/06/what-is-data-science.html. \\n6. \\nMay, M. Life science technologies: Big biological impacts from big data. Science (Science Products, \\nProduct Articles), (13 June 2014); http://www.sciencemag.org/site/products/ lst_20140613.xhtml.  \\n7. \\nProvost, F., and Fawcett, T. Data science and its relationship to big data and data-driven decision making. \\nBig Data 1, 1 (2013), 51-59; http://online.liebertpub.com/doi/pdfplus/ 10.1089/big.2013.1508. \\n8. \\nSmith, F. J. Data Science as an academic discipline. Data Science Journal 5 (2006), 163-164. \\n9. \\nZhu, Y. Y. et al. Data Explosion, Data Nature and Dataology. In Proceedings of International Conference \\non Brain Informatics. 2009. \\n10. Zhu, Y. Y., and Xiong, Y. Dataology and Data science: Up to Now. http://www.paper.edu.cn/ \\nen_releasepaper/content/4432156, 2011. \\n \\nYangyong Zhu (yyzhu@fudan.edu.cn) is the director of the Shanghai Key Laboratory of Data \\nScience, Fudan University, China and a professor at the School of Computer Science, Fudan \\nUniversity, Shanghai, China. \\nYun Xiong (yunx@fudan.edu.cn) is an associate professor at the School of Computer Science, \\nFudan University, Shanghai, China and a data scientist at the Shanghai Key Laboratory of Data \\nScience, Fudan University, China. \\nTable 1.  Research projects in data fields in which our team participated. \\n \\n\",\n",
       " '1602.00203v1.pdf': '\\uf020 \\nAbstract—In this work we propose a new deep learning tool – \\ndeep dictionary learning. Multi-level dictionaries are learnt in a \\ngreedy fashion – one layer at a time. This requires solving a simple \\n(shallow) dictionary learning problem; the solution to this is well \\nknown. We apply the proposed technique on some benchmark \\ndeep learning datasets. We compare our results with other deep \\nlearning tools like stacked autoencoder and deep belief network; \\nand state-of-the-art supervised dictionary learning tools like \\ndiscriminative K-SVD and label consistent K-SVD. Our method \\nyields better results than all.   \\n \\nIndex Terms—Deep Learning, Dictionary Learning, Feature \\nExtraction \\n \\nI. INTRODUCTION \\nN recent years there has been a lot of interest in dictionary \\nlearning. However the concept of dictionary learning has \\nbeen around for much longer. Its application in vision [1] and \\ninformation retrieval [2] dates back to the late 90’s. In those \\ndays, the term ‘dictionary learning’ had not been coined; \\nresearchers were using the term ‘matrix factorization’. The goal \\nwas to learn an empirical basis from the data. It basically \\nrequired decomposing the data matrix to a basis / dictionary \\nmatrix and a feature matrix – hence the name ‘matrix \\nfactorization’.  \\nThe current popularity of dictionary learning owes to K-SVD \\n[3, 4]. K-SVD is an algorithm to decompose a matrix (training \\ndata) into a dense basis and sparse coefficients. However the \\nconcept of such a dense-sparse decomposition predates K-SVD \\n[5]. Since the advent of K-SVD in 2006, there have been a \\nplethora of work on this topic. Dictionary learning can be used \\nboth for unsupervised problems (mainly inverse problems in \\nimage processing) as well as for problems arising in supervised \\nfeature extraction.  \\nDictionary learning has been used in virtually all inverse \\nproblems arising in image processing starting from simple \\nimage [6, 7] and video [8] denoising, image inpainting [9], to \\nmore complex problems like color image restoration [10], \\ninverse half toning [11] and even medical image reconstruction \\n[12, 13]. Solving inverse problems is not the goal of this work; \\nwe are more interested in dictionary learning from the \\nperspective of machine learning. We briefly discussed [6-13] \\nfor the sake of completeness. \\nMathematical transforms like DCT, wavelet, curvelet, Gabor \\netc. have been widely used in image classification problems \\n \\n \\n[14-16]. These techniques used these transforms as a \\nsparsifying step followed by statistical feature extraction \\nmethods like PCA or LDA before feeding the features to a \\nclassifier. Just as dictionary learning is replacing such fixed \\ntransforms (wavelet, DCT, curvelet etc.) in signal processing \\nproblems, it is also replacing them in feature extraction \\nscenarios. Dictionary learning gives researchers the opportunity \\nto design dictionaries to yield not only sparse representation \\n(like curvelet, wavelet, DCT etc.) but also discriminative \\ninformation.  \\nInitial techniques proposed naïve approaches which learnt \\nspecific dictionaries for each class [17-19]. Later approaches \\nincorporated discriminative penalties into the dictionary \\nlearning framework. One such technique is to include softmax \\ndiscriminative cost function [20-22]; other discriminative \\npenalties include Fisher discrimination criterion [23], linear \\npredictive classification error [24, 25] and hinge loss function \\n[26, 27]. In [28, 29] discrimination is introduced by forcing the \\nlearned features to map to corresponding class labels. \\nAll prior studies on dictionary learning (DL) are ‘shallow’ \\nlearning models just like a restricted boltzman machine (RBM) \\n[30] and autoencoder (AE) [31]. DL, RBM and AE – all fall \\nunder the broader topic of representation learning. In DL, the \\ncost function is Euclidean distance between the data and the \\nrepresentation given the learned basis; for RBM it is Boltzman \\nenergy; in AE, the cost is the Euclidean reconstruction error \\nbetween the data and the decoded representation / features.  \\nAlmost at the same time, when dictionary learning started \\ngaining popularity, researchers in machine learning observed \\nthat better (more abstract and compact) representation can be \\nachieved by going deeper. Deep Belief Network (DBN) is \\nformed by stacking one RBM after the other [32, 33]. Similarly \\nstacked autoencoder (SAE) were created by one AE inside the \\nother [34, 35].  \\nFollowing the success of DBN and SAE, we propose to learn \\nmulti-level deep dictionaries. This is the first work on deep \\ndictionary learning. The rest of the paper will be organized into \\nseveral sections…. \\n \\nII. LITERATURE REVIEW \\nWe will briefly review prior studies on dictionary learning, \\nstacked autoencoders and deep Boltzmann machines.   \\nA. Dictionary Learning \\nEarly studies in dictionary learning wanted to learn a basis for \\nGreedy Deep Dictionary Learning \\nSnigdha Tariyal, Angshul Majumdar, Member IEEE, Richa Singh, Senior Member IEEE, and Mayank \\nVatsa, Senior Member, IEEE \\nI\\nrepresentation. There were no constraints on the dictionary \\natoms or on the loading coefficients. The method of optimal \\ndirections [36] was used to learn the basis: \\n2\\n,\\nmin\\nF\\nD Z\\nX\\nDZ\\n\\uf02d\\n                 (1) \\nHere X is the training data, D is the dictionary to be learnt and \\nZ consists of the loading coefficients  \\nFor problems in sparse representation, the objective is to learn \\na basis that can represent the samples in a sparse fashion, i.e. Z \\nneeds to be sparse. The KSVD [3, 4] is the most well known \\ntechnique for solving this problem. Fundamentally it solves a \\nproblem of the form: \\n2\\n0\\n,\\nmin\\nsuch that \\nF\\nD Z\\nX\\nDZ\\nZ\\n\\uf074\\n\\uf02d\\n\\uf0a3\\n         (2) \\nKSVD proceeds in two stages. In the first stage it learns the \\ndictionary and in the next stage it uses the learned dictionary to \\nsparsely represent the data. Solving the l0-norm minimization \\nproblem is NP hard [37]. KSVD employs the greedy (sub-\\noptimal) orthogonal matching pursuit (OMP) [38] to solve the \\nl0-norm minimization problem approximately. In the dictionary \\nlearning stage, KSVD proposes an efficient technique to \\nestimate the atoms one at a time using a rank one update. The \\nmajor disadvantage of KSVD is that it is a relatively slow \\ntechnique owing to its requirement of computing the SVD \\n(singular value decomposition) in every iteration. There are \\nother efficient optimization based approaches for dictionary \\nlearning [39, 40] – these learn the full dictionary instead of \\nupdating the atoms separately. \\nThe dictionary learning formulation in (2) is unsupervised. As \\nmentioned before there is a large volume of work on supervised \\ndictionary learning problems. We will briefly discuss the major \\nones here. The first work on Sparse Representation based \\nClassification (SRC) [41] was not much of a dictionary learning \\ntechnique, but was a simple dictionary design problem where \\nall the training samples are concatenated in a large dictionary. \\nThe assumption is that the training samples for a basis for any \\nnew test sample belonging to the correct class. Their proposed \\nmodel is: \\nx\\nXa\\n\\uf03d\\n                   (3) \\nwhere x is the test sample and X is dictionary consisting of all \\nthe training samples. \\nIt is assumed in [41] that since the correct class only \\nrepresents x, the vector a is going to be sparse. Based on this \\nassumption they solved a using some sparse recovery \\ntechnique. Once a is obtained, the problem is to classify x. This \\nis achieved by computing the error between the test image and \\nits representation from each class c obtained by Xcac. where c \\ndenotes the cth class. The test sample is simply assigned to the \\nclass having the lowest error. \\nSeveral improvements to the basic SRC formulation was \\nproposed in [42-44]. In [42, 43] it was proposed that since a has \\na known class structure, one can improve upon the basic sparse \\nclassification approach by incorporating group-sparsity. In [44] \\na non-linear extension to the SRC was proposed. Later works \\nhandled the non-linear extension in a smarter fashion using the \\nkernel trick [45-47]. \\nThe SRC does exactly fit into the dictionary learning \\nparadigm. However [48] proposed a simple extension of SRC – \\ninstead of using raw training samples as the basis, they learnt a \\nseparate basis for each class and used these dictionaries for \\nclassification. This approach is naïve; there is no guarantee that \\ndictionaries from different classes would not be similar. In [49] \\nthis issue is corrected. Here an additional incoherency penalty \\non the dictionaries. This penalty assures that the dictionaries \\nfrom different classes look different from each other. The \\nformulation is given as: \\n\\uf07b\\n\\uf07d\\n2\\n2\\n1\\n,\\n1\\nmin\\ni\\ni\\nC\\nT\\ni\\ni\\ni\\ni\\nj\\nF\\nF\\nD Z\\ni\\ni\\nj\\nX\\nDZ\\nZ\\nD D\\n\\uf06c\\n\\uf068\\n\\uf03d\\n\\uf0b9\\n\\uf02d\\n\\uf02b\\n\\uf02b\\n\\uf0e5\\n\\uf0e5\\n     (4) \\nUnfortunately this formulation does not improve the overall \\nresults too much. It learns dictionaries that look different from \\neach other but does not produce features that are distinctive; i.e. \\nthe feature generated for the test sample from dictionaries of all \\nclasses looked more or less the same. \\nThe aforesaid issue was rectified in [50]; it combined two \\nconcepts. The first one is the discrimination of the learned \\nfeatures and the second one is the discrimination of the class \\nspecific dictionaries. The second criteria demands that the \\nfeatures from a particular class will reconstruct the samples of \\nthe same class accurately; however it will not represent samples \\nof the other classes. This idea is formulated as follows: \\n2\\n2\\n2\\n(\\n,\\n)\\ni\\nj\\ni\\ni\\ni\\ni\\ni\\ni\\ni\\nj\\ni\\nF\\nF\\nF\\ni\\nj\\nC X D Z\\nX\\nDZ\\nX\\nD Z\\nD Z\\n\\uf0b9\\n\\uf03d\\n\\uf02d\\n\\uf02b\\n\\uf02d\\n\\uf02b\\uf0e5\\n (5) \\nHere \\n\\uf05b\\n\\uf05d\\n1 |...|\\n|...|\\nc\\nC\\nD\\nD\\nD\\nD\\n\\uf03d\\n is the augmented dictionary \\nand Dc are the class specific dictionaries, Xi are the training \\nsamples for the ith class, Zi is the representaion over all the \\ndictionaries. According to their assumption, only the portion of \\nZi pertaining to the correct class should represent the data well \\n- this leads to the second term in the expression; the other \\ndictionaries should not represent the data well hence the third \\nterm.  \\nSo far, we have discussed about the discriminative \\ndictionaries. As mentioned before, [50] has a second term that \\ndiscriminates among the learned features. This term arises from \\nthe Fisher Discriminant Analysis - it tries to increase the \\ncovariance between the classes and decrease covariance within \\nthe class. This is represented by: \\n2\\n( )\\n(\\n)\\n(\\n)\\nW\\nB\\nF\\nf Z\\ntr S\\ntr S\\nZ\\n\\uf068\\n\\uf03d\\n\\uf02d\\n\\uf02b\\n          (6) \\nwhere\\n\\uf028\\n\\uf029\\uf028\\n\\uf029\\n1\\nC\\nT\\nW\\ni\\nc\\ni\\nc\\nc\\ni\\nS\\nz\\nz\\nz\\nz\\n\\uf03d\\n\\uf03d\\n\\uf02d\\n\\uf02d\\n\\uf0e5\\uf0e5\\nand \\n, \\n\\uf028\\n\\uf029\\uf028\\n\\uf029\\n1\\nC\\nT\\nW\\nc\\nc\\nc\\nS\\nz\\nz\\nz\\nz\\n\\uf03d\\n\\uf03d\\n\\uf02d\\n\\uf02d\\n\\uf0e5\\n; \\nthe \\nregularization \\nhelps \\nin \\nstabilizing the solution. \\nThe complete formulation given in [50] is as follows: \\n1\\n2\\n1\\n,\\nmin\\n(\\n, )\\n( )\\nD Z C XD Z\\nZ\\nf Z\\n\\uf06c\\n\\uf06c\\n\\uf02b\\n\\uf02b\\n          (7) \\nThe label consistent KSVD is one of the more recent \\ntechniques for learning discriminative sparse representation. It \\nis simple to understand and implement; it showed good results \\nfor face recognition [28, 29]. The first technique called \\nDiscriminative K-SVD [28] or LC-KSVD1 [29]; it proposes an \\noptimization problem of the following form: \\n2\\n2\\n2\\n1\\n2\\n3\\n1\\n, ,\\nmin\\n+\\nF\\nF\\nF\\nD Z A X\\nDZ\\nD\\nZ\\nQ\\nAZ\\n\\uf06c\\n\\uf06c\\n\\uf06c\\n\\uf02d\\n\\uf02b\\n\\uf02b\\n\\uf02d\\n   (8) \\nHere Q is the label of the training samples, it is a canonical basis \\nwith a one for the correct class and zeroes elsewhere. A is a \\nparameter of the linear classifier.  \\nIn [29] a second formulation is proposed that adds another \\nterm to penalize classification error. The LC-KSVD2 \\nformulation is as follows: \\n2\\n2\\n1\\n2\\n1\\n, , ,W\\n2\\n2\\n3\\n4\\nmin\\n+\\nF\\nF\\nD Z A\\nF\\nF\\nX\\nDZ\\nD\\nZ\\nQ\\nAZ\\nH\\nWZ\\n\\uf06c\\n\\uf06c\\n\\uf06c\\n\\uf06c\\n\\uf02d\\n\\uf02b\\n\\uf02b\\n\\uf02d\\n\\uf02b\\n\\uf02d\\n         (9) \\nH is a ‘discriminative’ sparse code corresponding to an input \\nsignal sample, if the nonzero values of Hi occur at those indices \\nwhere the training sample Xi and the dictionary item dk share \\nthe same label. Basically this formulation imposes labels not \\nonly on the sparse coefficient vectors Zi’s but also on the \\ndictionary atoms.  \\nDuring training, the LC-KSVD learns a discriminative \\ndictionary D. The dictionary D and the classification weights A \\nneed to be normalized. When there is a new test sample, the \\nsparse coefficients for the same are learnt using normalized \\ndictionary using l1-minimization: \\n2\\n2\\n1\\nmin\\ntest\\ntest\\nz\\nz\\nx\\nDz\\nz\\n\\uf06c\\n\\uf03d\\n\\uf02d\\n\\uf02b\\n          (10) \\nOnce the sparse representation of the test sample is obtained, \\nthe classification task is straightforward – the label of the test \\nsample is assigned as: \\nargmax(\\n)\\ntest\\nj\\nj\\nAz\\n\\uf03d\\n              (11) \\nB. Deep Boltzman Machine \\n  \\n \\n \\nFig. 1. Restricted Boltzman Machine \\n \\nRestricted Boltzmann Machines are undirected models that \\nuses stochastic hidden units to model the distribution over the \\nstochastic visible units. The hidden layer is symmetrically \\nconnected with the visible unit and the architecture is \\n“restricted” as there are no connections between units of the \\nsame layer. Traditionally, RBMs are used to model the \\ndistribution of the input data p(x).  \\nThe schematic diagram of RBM is shown in Fig. 1. The \\nobjective is to learn the network weights (W) and the \\nrepresentation (H). This is achieved by optimizing the \\nBoltzman cost function given by: \\n(\\n,\\n)\\n(\\n,\\n)\\nE W H\\np W H\\ne\\uf02d\\n\\uf03d\\n                (12) \\nWhere, \\n(\\n,\\n)\\n-\\nT\\nE W H\\nH WX\\n\\uf03d\\nincluding the bias terms.   \\nThe conditional distributions are given by (assuming \\nindependence) –  \\n(\\n|\\n)\\n( | )\\np X H\\np x h\\n\\uf03d\\uf0d5\\n \\n(\\n|\\n)\\n( | )\\np H X\\np h x\\n\\uf03d\\uf0d5\\n  \\nAssuming binary input variable, the probability that a node \\nwill be active can be given as follows, \\n(\\n1| )\\n(\\n)\\nT\\np x\\nh\\nsigm W h\\n\\uf03d\\n\\uf03d\\n  \\n(\\n1| )\\n(\\n)\\np h\\nx\\nsigm Wx\\n\\uf03d\\n\\uf03d\\n \\nComputing the exact gradient of this loss function is almost \\nintractable. However, there is a stochastic approximation to \\napproximate the gradient termed as contrastive divergence \\ngradient. A sequence of Gibbs sampling based reconstruction, \\nproduces an approximation of the expectation of joint energy \\ndistribution, using which the gradient can be computed. \\nUsually RBM is unsupervised, but there are studies which \\ntrained discriminative RBMs by utilizing the class labels [51]. \\nThere are also RBMs which are sparse [52]; the sparsity is \\ncontrolled by the firing the hidden units only if they are over \\nsome threshold. Supervision can also be achieved using sparse \\nRBMs by extending it to have similar sparsity structure within \\nthe group / class [53]. \\nDeep Boltzmann Machines (DBM) [54] is an extension of \\nRBM by stacking multiple hidden layers on top of each other \\n(Fig. 2). DBM is an undirected learning model and thus it is \\ndifferent from the other stacked network architectures that each \\nlayer receives feedback from both the top-down and bottom-up \\nlayer signals. This feedback mechanism helps in managing \\nuncertainty in learning models. While the traditional RBM can \\nmodel logistic units, a Gaussian-Bernoulli RBM [55] can be \\nused as well with real valued visible units. \\n \\n \\n \\nFig. 2. Deep Boltzman Machine \\n \\nC. Stacked Autoencoder  \\n \\n \\nFig. 3. Single Layer Autoencoder \\n \\nAn autoencoder consists (as seen in Fig. 3) of two parts – the \\nencoder maps the input to a latent representation, and the \\nW\\nX\\nH\\nW2\\nW1\\nX\\nH1\\nH2\\nW \\nW’ \\nInput Layer \\nOutput Layer \\nHidden Layer \\ndecoder maps the latent representation back to the data. For a \\ngiven input vector (including the bias term) x, the latent space \\nis expressed as: \\nh\\nWx\\n\\uf03d\\n                    (13) \\nHere the rows of W are the link weights from all the input nodes \\nto the corresponding latent node. Usually a non-linear \\nactivation function is used at the output of the hidden nodes \\nleading to: \\n(\\n)\\nh\\nWx\\n\\uf066\\n\\uf03d\\n                   (14) \\nThe sigmoid function is popular; other non-linear activation \\nfunctions (like tanh) can be used as well. Rectifier units and \\nlarge neural networks employ linear activation functions \\n(identity) – this considerably speeds up training.  \\nThe decoder portion reverse maps the latent variables to the \\ndata space.  \\n\\' (\\n)\\nx\\nW\\nWx\\n\\uf066\\n\\uf03d\\n                  (15) \\nSince the data space is assumed to be the space of real numbers, \\nthere is no sigmoidal function here. \\nDuring training the problem is to learn the encoding and \\ndecoding weights – W and W’. These are learnt by minimizing \\nthe Euclidean cost: \\n2\\n,\\n\\'\\nargmin\\n\\' (\\n) F\\nW W\\nX\\nW\\nWX\\n\\uf066\\n\\uf02d\\n             (16) \\nHere \\n1\\n[\\n|...|\\n]\\nN\\nX\\nx\\nx\\n\\uf03d\\n consists all the training sampled stacked \\nas columns. The problem (16) is clearly non-convex, but is \\nsmooth and hence can be solved by gradient descent techniques; \\nthe activation function needs to be smooth and continuously \\ndifferentiable. \\n \\n \\nFig. 4. Stacked Autoencoder \\n \\nThere are several extensions to the basic autoencoder \\narchitecture. Stacked autoencoders have multiple hidden layers \\n– one inside the other (see Fig. 4). The corresponding cost \\nfunction is expressed as follows: \\n1\\n1\\n1\\n2\\n...\\n,\\n\\' ...\\n\\'\\nargmin\\n(\\n)\\nL\\nL\\nF\\nW\\nW\\nW\\nW\\nX\\ng\\nf X\\n\\uf02d\\n\\uf02d\\n           (17) \\nwhere\\n\\uf028\\n\\uf029\\n\\uf028\\n\\uf029\\n1\\n2\\n\\'\\n\\'...\\n\\'\\n(\\n)\\nL\\ng\\nW\\nW\\nW\\nf X\\n\\uf066\\n\\uf03d\\nand \\n\\uf028\\n\\uf029\\n\\uf028\\n\\uf029\\n1\\n2\\n1\\n... (\\n)\\nL\\nL\\nf\\nW\\nW\\nW X\\n\\uf066\\n\\uf066\\n\\uf066\\n\\uf02d\\n\\uf02d\\n\\uf03d\\n  \\nSolving the complete problem (17) is computationally \\nchallenging. Also learning so many parameters (network \\nweights) lead to over-fitting. To address both these issues, the \\nweights are usually learned in a greedy fashion layer by layer \\n[32, 34].  \\nStacked denoising autoencoder [35] is a variant of the basic \\nautoencoder where the input consists of noisy samples and the \\noutput consists of clean samples. Here the encoder and decoder \\nare learnt to denoise noisy input samples.   \\nAnother variation for the basic autoencoder is to regularize \\nit, i.e. \\n2\\n(\\n)\\nargmin\\n(\\n)\\n(\\n,\\n)\\nF\\nW s\\nX\\ng\\nf X\\nR W X\\n\\uf02d\\n\\uf02b\\n         (18) \\nThe regularization can be a simple Tikhonov regularization \\n– however that is not used in practice. It can be a sparsity \\npromoting term [56, 57] or a weight decay term (Frobenius \\nnorm of the Jacobian) as used in the contractive autoencoder \\n[58]. The regularization term is usually chosen so that they are \\ndifferentiable and hence minimizable using gradient descent \\ntechniques.  \\nIII. DEEP DICTIONARY LEARNING \\n \\n \\nFig. 5. Schematic Diagram for Dictionary Learning \\n \\nIn this section we describe the main contribution of this work. \\nA single / shallow level of dictionary learning yields a latent \\nrepresentation of data and the dictionary atoms. Here we \\npropose to learn deeper latent representation of data by learning \\nmulti-level dictionaries. The idea of learning deeper levels of \\ndictionaries stems from the recent success of deep learning in \\nvarious areas of machine learning. \\nThe schematic diagram for dictionary learning is shown in \\nFig. 5. X is the data, D is the dictionary and Z is the feature / \\nrepresentation of X in D. Dictionary learning follows a \\nsynthesis framework, i.e. the dictionary is learnt such that the \\nfeatures synthesize the data along with the dictionary.  \\nX\\nDZ\\n\\uf03d\\n                    (19) \\nThere is also analysis K-SVD, but it cannot be used for feature \\nextraction, it can only produce a ‘clean’ version of the data and \\nhence is only suitable for inverse problems.  \\nIn this work, we propose to extend the shallow (Fig. 3) \\ndictionary learning into multiple layers – leading to deep \\ndictionary learning (Fig. 6). \\n \\n \\n \\nFig. 6. Schematic Diagram for Deep Dictionary Learning \\n \\nMathematically, the representation at the second layer is \\nrepresented as: \\nInput Layer \\nHidden Layer 1 \\nOutput Layer \\nHidden Layer L \\n……… \\nD1\\nX\\nZ\\nD2\\nD1\\nX\\nZ2\\n1\\n2\\n2\\nX\\nD D Z\\n\\uf03d\\n                 (20) \\nLearning the two dictionaries along with the deepest level \\nfeatures is a hard problem for two reasons: \\n1) Dictionary learning (19) is a bi-linear (hence non-convex) \\nproblem. Learning multiple layers of dictionaries along \\nwith the features makes the problem even more difficult to \\nsolve. Only recently, studies have proven some \\nconvergence guarantees for single level dictionary learning \\n[59-63]. These proofs would be very hard to replicate for \\nmultiple layers.   \\n2) Moreover, the number of parameters required to be solved \\nincreases when multiple layers are dictionaries are learnt \\nsimultaneously. With limited training data, this could lead \\nto over-fitting.  \\nHere we propose to learn the dictionaries in a greedy fashion. \\nThis is in sync with other deep learning techniques [32-34]. \\nMoreover, layer-wise learning will guarantee the convergence \\nat each layer. The diagram illustrating layer-wise learning is \\nshown in Fig. 5.  \\n \\n \\n \\nFig. 7. Greedy Layer-wise Learning \\n \\nIn a greedy fashion, we start with the first layer, i.e. we solve \\nfor D1 and Z1 from –  \\n1 1\\nX\\nD Z\\n\\uf03d\\n                  (21) \\nThe features from the first layer (Z1) acts as input to the second \\nlayer. Therefore the second layer learns the weights from –  \\n1\\n2\\n2\\nZ\\nD Z\\n\\uf03d\\n                  (22) \\nThe learning can be either dense or sparse, i.e. the features / \\nrepresentation can be dense or sparse. For dense features, the \\nlearning is simple and is given by (23) \\n2\\n2\\n,\\nmin\\nD Z X\\nDZ\\n\\uf02d\\n                 (23) \\nOptimality of solving (23) by alternating minimization has been \\nproven in [56]. Therefore we follow the same. The dictionary D \\nand the basis Z is learnt by: \\n2\\n1\\n2\\nmin\\nk\\nk\\nZ\\nZ\\nX\\nD\\nZ\\n\\uf02d\\n\\uf0ac\\n\\uf02d\\n             (24a) \\n2\\n2\\nmin\\nk\\nk\\nD\\nD\\nX\\nDZ\\n\\uf0ac\\n\\uf02d\\n             (24b) \\nThis is simply the method of optimal directions [36]. Both (24a) \\nand (24b) are simple least square problems having closed form \\nsolutions.  \\nFor learning sparse features, one just needs to regularize (23) \\nby an l1-norm on the features. This is given by: \\n2\\n2\\n1\\n,\\nmin\\nD Z X\\nDZ\\nZ\\n\\uf06c\\n\\uf02d\\n\\uf02b\\n             (25) \\nThis too is solved using alternating minimization.  \\n2\\n1\\n1\\n2\\nmin\\nk\\nk\\nZ\\nZ\\nX\\nD\\nZ\\nZ\\n\\uf06c\\n\\uf02d\\n\\uf0ac\\n\\uf02d\\n\\uf02b\\n         (26a) \\n2\\n2\\nmin\\nk\\nk\\nD\\nD\\nX\\nDZ\\n\\uf0ac\\n\\uf02d\\n             (26b) \\nAs before, solving (26b) is simple. It is a least square problem \\nhaving a closed form solution. The solution to (26a) although \\nnot analytic, is well known in signal processing and machine \\nlearning literature. It can solved using the Iterative Soft \\nThresholding Algorithm (ISTA) [64]. In every iteration, the \\nsteps for ISTA are: \\n\\uf028\\n\\uf029\\n1\\n1\\n1\\n( )max 0,\\n2\\nT\\nk\\nk\\nB\\nZ\\nD\\nX\\nD\\nZ\\nZ\\nsignum B\\nB\\n\\uf061\\n\\uf06c\\n\\uf061\\n\\uf02d\\n\\uf02d\\n\\uf03d\\n\\uf02b\\n\\uf02d\\n\\uf0e6\\n\\uf0f6\\n\\uf0ac\\n\\uf02d\\n\\uf0e7\\n\\uf0f7\\n\\uf0e8\\n\\uf0f8\\n  \\nIn this work, we have used dense dictionary learning for all \\nlayers till the penultimate layer and sparse dictionary learning \\nonly in the final layer, i.e. for the two layer problem, the first \\nlayer (D1, Z1) would be dense and the second layer (D2, Z2) \\nwould be sparse.  \\nIt must be noted that the two dictionaries cannot be collapsed \\ninto a single one. This is because the learning process is non-\\nlinear. For example, if the dimensionality of the sample is m \\nand the first dictionary is of size m x n1 and the second one is \\nn1 x n2, it is not possible to learn a single dictionary of size m x \\nn2 and expect the same results as a two-stage dictionary.  \\nA. Connection with RBM \\nRBM is an undirectional graph, whereas dictionary learning \\nis unidirectional. This is evident from figures 1 and 5. In both \\ncases, the task is to learn the network weights / atoms and the \\nrepresentation given the data. They differ from each other in the \\ncost functions used. For RBM it is the Boltzmann function. \\nHere one tries to learn the network weight and the output \\nfeatures such that the similarity between the projected data (at \\nthe input) and the features is maximized.   \\nIn dictionary learning, the cost function is different – instead \\nof maximizing similarity, we minimize the Euclidean distance \\nbetween the data (X) and the synthesis (DZ). RBM has a \\nstochastic formulation; dictionary learning is deterministic.  \\nRBMs can be formulated for features having values between \\n0 and 1. If the values are outside this range, they need to be \\nnormalized. In many cases, the normalization does not affect \\nthe performance, but there can be scenarios where it suppresses \\nimportant information. Dictionary learning can work both on \\nreal and complex inputs.    \\nB. Connection with Autoencoder \\nWe \\nmentioned \\nbefore \\nthat \\ndictionary \\nlearning \\nis \\npredominantly modeled as a synthesis problem, i.e. the \\ndictionary and the features are learnt such that they can \\nsynthesize the data. It is expressed as: X=DSZ where X is the \\ndata, DS is the learnt synthesis dictionary and Z are the sparse \\ncoefficients.  \\nUsually one promotes sparsity in the features and the learning \\nrequires minimizing the following, \\n2\\n1\\nF\\nS\\nX\\nD Z\\nZ\\n\\uf06c\\n\\uf02d\\n\\uf02b\\n               (26) \\nThis is the so called synthesis prior formulation where the \\nD2\\nD1\\nX\\nZ2\\nZ1\\ntask is to find a dictionary that will synthesize / generate signals \\nfrom sparse features. There is an alternate co-sparse analysis \\nprior dictionary learning paradigm [65] where the goal is to \\nlearn a dictionary such that when it is applied on the data the \\nresulting coefficient is sparse. The model is \\nˆ\\nA\\nD X\\nZ\\n\\uf03d\\n. The \\ncorresponding learning problem is framed minimizing: \\n2\\n1\\nˆ\\nˆ\\nA\\nF\\nX\\nX\\nD X\\n\\uf06c\\n\\uf02d\\n\\uf02b\\n              (27) \\nIf \\nwe \\ncombine \\nanalysis \\nand \\nsynthesis, \\nusing \\nˆ\\nˆ\\n,\\nA\\nS\\nX\\nD Z D X\\nZ\\n\\uf03d\\n\\uf03d\\nand impute it in (27) we get –  \\n2\\n1\\nˆ\\nˆ\\nS\\nF\\nA\\nA\\nX\\nD D X\\nD X\\n\\uf06c\\n\\uf02d\\n\\uf02b\\n            (28) \\nThis is the expression of a sparse denoising autoencoder [54] \\nwith linear activation at the hidden layer. If we drop the sparsity \\nterm, it becomes –  \\n2\\nˆ\\nA\\nS\\nF\\nX\\nD D X\\n\\uf02d\\n                (29) \\nThis formulation is similar to a denoising autoencoder with \\nlinear activation.   \\nWe can express autoencoder in the lingo of dictionary \\nlearning – autoencoder is a model that learnt the analysis and \\nthe synthesis dictionaries. To the best of our knowledge, this is \\nthe first work which shows the architectural similarity between \\nautoencoders and dictionary learning.  \\nIV. EXPERIMENTAL EVALUATION \\nA. Datasets \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 8. Top to bottom. basic, basic-rot, bg-rand, bg-img, bg-img-rot \\n \\nWe carried our experiments on several benchmarks datasets. \\nThe first one is MNIST dataset which consists of 28x28 images \\nof handwritten digits ranging from 0 to 9. The dataset has \\n60,000 images for training and 10,000 images for testing. No \\npreprocessing has been done on this dataset. \\nThe other datasets are variations of MNIST, which are more \\nchallenging primarily because they have fewer training samples \\n(10,000) and larger number of test samples (50,000).  \\n1. basic (smaller subset of MNIST) \\n2. basic-rot (smaller subset with random rotations) \\n3. bg-rand (smaller subset with uniformly distributed \\nnoise in background) \\n4. bg-img \\n(smaller \\nsubset \\nwith \\nrandom \\nimage \\nbackground) \\n5. bg-img-rot (smaller subset with random image \\nbackground plus rotation)  \\nSamples for each of the datasets are shown in Fig. 8. \\nB. Deep vs Shallow Dictionary Learning \\n \\n \\nFig. 9. First level dictionary for MNIST \\n \\nIn the first set of results, we show that the multi-level \\ndictionaries cannot be collapsed into a single one and expected \\nto perform the same. We carried out experiments on the MNIST \\nand its variations. In the first case, the number of basis in the \\nmulti-level dictionaries are: 300-15-50. In the second case, we \\nlearn a shallow dictionary with 50 atoms. The results from these \\ntwo would be the same, if the multi-level dictionaries would be \\ncollapsible.  \\nWe want to show that the representation learnt from a single \\nlevel of dictionary and multi-level dictionary are different. To \\nshowcase this, we show classification results with a simple K \\nNearest Neighbour (K=1). The classification accuracies are \\nshown in Table 1.  \\nWe use a deterministic initialization for dictionary learning. \\nUsually the dictionary atoms are initialized by randomly \\nchoosing samples from the training set – but this leads to \\nvariability in results. In this work we propose a deterministic \\ninitialization based on QR decomposition. Orthogonal vectors \\nfrom Q (in order) are used to initialize the dictionary.  \\n \\n \\n \\nTABLE I \\nDEEP VS SHALLOW \\n Dataset \\nDeep (300-\\n15-50) \\nShallow \\n(50) \\nMNIST \\n97.75 \\n97.35 \\nbasic \\n95.80 \\n95.02 \\nbasic-rot \\n87.00 \\n84.19 \\nbg-rand \\n89.35 \\n87.19 \\nbg-img \\n81.00 \\n78.86 \\nbg-img-rot \\n57.77 \\n54.40 \\n \\nThe discrepancy between multi-level dictionary learning and \\nsingle level dictionary learning is evident in Table 1. If the \\nlearning was linear, it would be possible to collapse multiple \\ndictionaries into one; but dictionary learning is inherently non-\\nlinear. Hence it is not possible to learn a single layer of \\ndictionary in place of multiple levels and expect the same \\noutput. \\nC. Comparison with other Deep Learning Approaches \\nWe compared our results with a stacked autoencoder (SAE) \\nand deep belief network (DBN). The implementation for these \\nhave been obtained from [66] and [67] respectively. Both SAE \\nand DBN has a three layer architecture. The number of nodes is \\nhalved in every subsequent layer. This is a standard approach; \\nwe tried other configurations but could not improve upon this. \\nWe want to compare the representation capability of our \\nproposed technique vis-à-vis other deep learning methods. The \\nresults for K Nearest Neighbour (KNN) and Support Vector \\nMachine (SVM) are shown in Tables 2 and 3.  \\n \\nTABLE II \\nCOMPARISON WITH KNN (K=1) CLASSIFICATION \\n Dataset \\nDDL \\nDBN \\nSAE \\nMNIST \\n97.75 \\n97.05 \\n97.33 \\nbasic \\n95.8 \\n95.37 \\n95.25 \\nbasic-rot \\n87.00 \\n84.71 \\n84.83 \\nbg-rand \\n89.35 \\n77.16 \\n86.42 \\nbg-img \\n81.00 \\n86.36 \\n77.16 \\nbg-img-rot \\n57.77 \\n50.47 \\n52.21 \\n \\nTABLE III \\nCOMPARISON WITH SVM CLASSIFICATION \\n Dataset \\nDDL \\nDBN \\nSAE \\nMNIST \\n98.64 \\n98.53 \\n98.5 \\nbasic \\n97.284 \\n88.44 \\n97.4 \\nbasic-rot \\n90.344 \\n76.59 \\n79.83 \\nbg-rand \\n92.38 \\n78.59 \\n85.34 \\nbg-img \\n86.17 \\n75.22 \\n74.99 \\nbg-img-rot \\n63.85 \\n48.53 \\n49.14 \\n \\nWe find that apart from one case each in Tables 1 and 2, our \\nproposed method yields better results than DBN and SAE. For \\nKNN, our results are slightly better, but for SVM we are doing \\nconsiderably better, especially for the more difficult datasets.  \\nWe have compared our technique with state-of-the-art \\ndictionary learning techniques like D-KSVD [28] and LC-\\nKSVD [29]. These were tuned to yield the best possible results. \\nComparison is also done with stacked denoising autoencoder \\n(SDAE) and deep belief network (DBN) fine tuned with soft-\\nmax classifier. We did not run these experiments; these results \\nare copied from [35]. \\n \\nTABLE IV \\nCOMPARISON WITH OTHER TECHNIQUES \\n Dataset \\nDDL-\\nSVM \\nLC-\\nKSVD \\nD-\\nKSVD \\nDBN-\\nSM* \\nSDAE-\\nSM* \\nMNIST \\n98.64 \\n93.30 \\n93.6 \\n98.76 \\n98.72 \\nbasic \\n97.28 \\n92.70 \\n92.20 \\n96.89 \\n97.16 \\nbasic-rot \\n90.34 \\n48.66 \\n50.01 \\n89.7 \\n90.47 \\nbg-rand \\n92.38 \\n87.70 \\n87.70 \\n93.27 \\n89.7 \\nbg-img \\n86.17 \\n80.65 \\n81.20 \\n83.69 \\n83.32 \\nbg-img-rot \\n63.85 \\n75.40 \\n75.40 \\n52.61 \\n56.24 \\n*Results from [35] \\n \\nWe find that the proposed deep dictionary learning \\ntechniques always yields better results than shallow dictionary \\nlearning (LC-KSVD and D-KSVD). In most cases, we can even \\nachieve better accuracy than highly tuned models like DBN and \\nSDAE.  \\nWe compare our technique with other deep learning \\napproaches in terms of speed (training time). All the algorithms \\nare run until convergence. SAE, DBN and DDL (proposed) are \\nrun until convergence. The machine used is Intel (R) Core(TM) \\ni5 running at 3 GHz; 8 GB RAM, Windows 10 (64 bit) running \\nMatlab 2014a. The run times for all the smaller MNIST \\nvariations are approximately the same. So we only report results \\nfor the larger MNIST dataset (60K) and the basic (10K) dataset. \\n \\nTABLE II \\nTRAINING TIME IN SECONDS \\n Dataset \\nDDL \\nDBN \\nSAE \\nMNIST \\n107 \\n30071 \\n120408 \\nbasic \\n26 \\n \\n \\n \\nWe see that our proposed deep dictionary learning algorithm \\nis more than 2 orders of magnitude faster than deep belief \\nnetwork and more than 3 orders of magnitude faster than \\nstacked autoencoder. This is a huge saving in training time.  \\nV. CONCLUSION \\nIn this work we propose the idea of deep dictionary learning, \\nwhere instead of learning one shallow dictionary – as has been \\ndone so far, we learn multiple levels of dictionaries. Learning \\nall the dictionaries makes the problem highly non-convex. Also \\nlearning so many parameters (atoms of many dictionaries) is \\nalways fraught with the problem of over-fitting. To account for \\nboth these issues, we learn the dictionaries in a greedy fashion \\n– one layer at a time. The representation / feature from one level \\nis used as the input to learn the following level. Thus, the basic \\nunit of deep dictionary learning is a simple shallow dictionary \\nlearning algorithm; which is a well known and solved problem.  \\nWe compare the new deep learning tool with the existing \\nones like the stacked autoencoder and deep belief network. We \\nfind that our method yields better results on benchmark deep-\\nlearning datasets. The main advantage of our method is that it \\nis few orders of magnitude faster than existing deep learning \\ntools like stacked autoencoder and deep belief network.  \\nThis is a preliminary work, we will carry out more extensive \\nexperimentation in the future. We plan to test the robustness of \\ndictionary learning in the presence of missing data, noise and \\nlimited number of training sample. \\nIn the future, we would also like to apply this technique for \\nother practical problems arising, biometrics, vision, speech \\nprocessing etc. Also there has been a lot of work on supervised \\ndictionary \\nlearning; \\nour \\npreliminary \\nformulation \\nis \\nunsupervised. In future, we expect to improve the results even \\nfurther by incorporating techniques from supervised learning.  \\nREFERENCES \\n[1] B. Olshausen and D. Field, \"Sparse coding with an overcomplete basis \\nset: a strategy employed by V1?\", Vision Research, Vol. 37 (23), pp. \\n3311-3325, 1997. \\n[2] D. D. Lee and H. S. Seung, \"Learning the parts of objects by non-negative \\nmatrix factorization\", Nature 401 (6755), pp. 788–791, 1999. \\n[3] R. Rubinstein, A. M. Bruckstein and M. Elad, \"Dictionaries for Sparse \\nRepresentation Modeling\", Proceedings of the IEEE, Vol. 98 (6), pp. \\n1045-1057, 2010 \\n[4] M. Aharon, M. Elad and A. Bruckstein, \"K-SVD: An Algorithm for \\nDesigning Overcomplete Dictionaries for Sparse Representation\", IEEE \\nTransactions on Signal Processing, Vol. 54 (11), pp. 4311-4322, 2006. \\n[5] J. Eggert and E. Körner, \"Sparse coding and NMF\", IEEE International \\nJoint Conference on Neural Networks, pp. 2529-2533, 2004. \\n[6] M. Elad and M. Aharon, \"Image Denoising Via Sparse and Redundant \\nRepresentations Over Learned Dictionaries,\" IEEE Transactions on \\nImage Processing, Vol.15 (12), pp. 3736-3745, 2006. \\n[7] M. Elad and M. Aharon, \"Image Denoising Via Learned Dictionaries and \\nSparse representation,\" IEEE Conference on Computer Vision and Pattern \\nRecognition, Vol.1, pp. 895-900, 2006. \\n[8] M. Protter and M. Elad, \"Image Sequence Denoising via Sparse and \\nRedundant Representations,\" IEEE Transactions on Image Processing, \\nVol.18 (1), pp. 27-35, 2009. \\n[9] K. Min-Sung and E. Rodriguez-Marek, \"Turbo inpainting: Iterative K-\\nSVD with a new dictionary,\" IEEE International Workshop on \\nMultimedia Signal Processing, pp. 1-6, 2009. \\n[10] J. Mairal, M. Elad and G. Sapiro, \"Sparse Representation for Color Image \\nRestoration,\" IEEE Transactions on Image Processing, Vol.17 (1), pp. 53-\\n69, 2008. \\n[11] C.-H. Son and H. Choo, \"Local Learned Dictionaries Optimized to Edge \\nOrientation for Inverse Halftoning,\" IEEE Transactions on Image \\nProcessing, Vol. 23 (6), pp. 2542-2556, 2014. \\n[12] J. Caballero, A. N. Price, D. Rueckert and J. V. Hajnal, \"Dictionary \\nLearning and Time Sparsity for Dynamic MR Data Reconstruction,\" \\nIEEE Transactions on Medical Imaging, Vol. 33 (4), pp. 979-994, 2014. \\n[13] A. Majumdar and R. K. Ward, “Learning Space-Time Dictionaries for \\nBlind Compressed Sensing Dynamic MRI Reconstruction”, IEEE \\nInternational Conference on Image Processing, 2015. \\n[14] A. Majumdar and R. Ward, \"Multiresolution methods in face \\nrecognition\", in Recent Advances in Face Recognition, Eds. M. S. \\nBartlett, K. Delac and M. Grgic, I-Tech Education and Publishing, \\nVienna, Austria, pp. 79-96, 2009. \\n[15] V. Dattatray, R. Jadhav and S. Holambe, \"Feature extraction using Radon \\nand wavelet transforms with application to face recognition\", \\nNeurocomputing, Vol. 72 (7-9), pp. 1951-1959, 2009. \\n[16] S. Dabbaghchian, P. M. Ghaemmaghami, A. Aghagolzadeh, “Feature \\nextraction using discrete cosine transform and discrimination power \\nanalysis with a face recognition technology”, Pattern Recognition, Vol. \\n43 (4), pp. 1431-1440, 2010. \\n[17] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. \"Discriminative \\nlearned dictionaries for local image analysis\". IEEE Conference of \\nComputer Vision and Pattern Recognition, 2008. \\n[18] L. Yang, R. Jin, R. Sukthankar, and F. Jurie. \"Unifying discriminative \\nvisual codebook genearation with classifier training for object category \\nrecognition\". IEEE Conference of Computer Vision and Pattern \\nRecognition, 2008.  \\n[19] W. Jin, L. Wang, X. Zeng, Z. Liu and R. Fu, \"Classification of clouds in \\nsatellite \\nimagery \\nusing \\nover-complete \\ndictionary \\nvia \\nsparse \\nrepresentation\", Pattern Recognition Letters, Vol. 49 (1), pp. 193-200, \\n2014. \\n[20] Y. Boureau, F. Bach, Y. LeCun, and J. Ponce. \"Learning mid-level \\nfeatures for recognition\". IEEE Conference of Computer Vision and \\nPattern Recognition, 2010.  \\n[21] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. \"Supervised \\ndictionary learning\". Advances in Neural Information Processing \\nSystems, 2009.  \\n[22] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. \\n\"Discriminative sparse image models for class-specific edage detection \\nand image interpretation\". European Conference on Computer Vision, \\n2008.  \\n[23] K. Huang and S. Aviyente. \"Sparse representation for signal \\nclassification\". Advances in Neural Information Processing Systems, \\n2007.  \\n[24] D. Pham and S. Venkatesh. \"Joint learning and dictionary construction for \\npattern recognition\". IEEE Conference of Computer Vision and Pattern \\nRecognition, 2008.  \\n[25] Q. Zhang and B. Li. \"Discriminative k-svd for dictionary learning in face \\nrecognition\". IEEE Conference of Computer Vision and Pattern \\nRecognition, 2010.  \\n[26] J. Yang, K. Yu, and T. Huang. \"Supervised translation-invariant sparse \\ncoding\". IEEE Conference of Computer Vision and Pattern Recognition, \\n2010.  \\n[27] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. \\n\"Discriminative sparse image models for class-specific edage detection \\nand image interpretation\". European Conference on Computer Vision , \\n2008.  \\n[28] Q. Zhang and B. Li, \"Discriminative K-SVD for dictionary learning in \\nface recognition\". IEEE Conference of Computer Vision and Pattern \\nRecognition, 2010. \\n[29] Z. Jiang, Z. Lin and L. S. Davis, \"Learning A Discriminative Dictionary \\nfor Sparse Coding via Label Consistent K-SVD\", IEEE Transactions on \\nPattern Analysis and Machine Intelligence, Vol. 35, pp. 2651-2664, 2013 \\n[30] G. E. Hinton and R. R. Salakhutdinov, \"Reducing the Dimensionality of \\nData with Neural Networks\", Science, Vol. 313 (5786), pp. 504–507, \\n2006.  \\n[31] H. Bourlard and Y. Kamp, \"Auto-association by multilayer perceptrons \\nand singular value decomposition\". Biological Cybernetics, Vol. 59 (4–\\n5), pp. 291–294, 1989. \\n[32] Y. Bengio, P. Lamblin, P. Popovici and H. Larochelle, “Greedy Layer-\\nWise Training of Deep Networks”, Advances in Neural Information \\nProcessing Systems, 2007. \\n[33] G. E. Hinton, S. Osindero and Y. W. Teh, “A fast learning algorithm for \\ndeep belief nets”, Neural Computation, Vol. 18, pp. 1527-1554, 2006. \\n[34] Y. Bengio, “Learning deep architectures for AI”, Foundations and Trends \\nin Machine Learning, Vol. 1(2), pp. 1-127, 2009. \\n[35] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio and P. A. Manzagol, \\n“Stacked denoising autoencoders: Learning useful representations in a \\ndeep network with a local denoising criterion”, Journal of Machine \\nLearning Research, Vol. 11, pp. 3371-3408, 2010. \\n[36] K. Engan, S. Aase, and J. Hakon-Husoy, “Method of optimal directions \\nfor frame design,” IEEE International Conference on Acoustics, Speech, \\nand Signal Processing, 1999. \\n[37] B. K. Natarajan, \"Sparse approximate solutions to linear systems\", SIAM \\nJournal on computing, Vol. 24, pp. 227-234, 1995. \\n[38] Y. Pati, R. Rezaiifar, P. Krishnaprasad, \"Orthogonal Matching Pursuit : \\nrecursive function approximation with application to wavelet \\ndecomposition\", Asilomar Conference on Signals, Systems and \\nComputers, 1993. \\n[39] M. Yaghoobi, T. Blumensath and M. E. Davies, \"Dictionary Learning for \\nSparse Approximations With the Majorization Method,\" IEEE \\nTransactions on Signal Processing, Vol.57 (6), pp. 2178-2191, 2009. \\n[40] A. Rakotomamonjy, \"Applying alternating direction method of \\nmultipliers for constrained dictionary learning\", Neurocomputing, Vol. \\n106, pp. 126-136, 2013. \\n[41] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. “Robust face \\nrecognition via sparse representation”. IEEE Transactions on Pattern \\nAnalysis and Machine Intelligence, Vol. 31, 2, pp. 210–227, 2009. \\n[42] A. Majumdar and R. K. Ward, \"Robust Classifiers for Data Reduced via \\nRandom Projections\", IEEE Transactions on Systems, Man, and \\nCybernetics, Part B, Vol. 40 (5), pp. 1359 - 1371. \\n[43] A. Majumdar and R. K. Ward, \"Fast Group Sparse Classification\", IEEE \\nCanadian Journal of Electrical and Computer Engineering, Vol. 34 (4), \\npp. 136-144, 2009 \\n[44] A. Majumdar and R. K. Ward, \"Improved Group Sparse Classifier\", \\nPattern Recognition Letters, Vol. 31 (13), pp. 1959-1964, 2010 \\n[45] J. Yin, Z. Liu, Z. Jin and W. Yang, \"Kernel sparse representation based \\nclassification\", Neurocomputing, Vol. 77 (1), pp. 120-128, 2012. \\n[46] Y. Chen, N. M. Nasrabadi, T. D. Tran, \"Hyperspectral Image \\nClassification via Kernel Sparse Representation,\" IEEE Transactions on \\nGeoscience and Remote Sensing, Vol. 51 (1), pp. 217-231, 2013. \\n[47] L. Zhang, W. D. Zhou, P. C. Chang, J. Liu, Z. Yan, T. Wang and F. Z. Li, \\n\"Kernel Sparse Representation-Based Classifier,\" IEEE Transactions on \\nSignal Processing, Vol. 60 (4), pp. 1684-1695, 2012 \\n[48] M. Yang, L. Zhang, J. Yang, and D. Zhang. metaface learning for sparse \\nrepresentation based face recognition. IEEE International Conference on \\nImage Processing, 2010. \\n[49] I. Ramirez, P. Sprechmann, and G. Sapiro. Classification and clustering \\nvia dictionary learning with structured incoherence and shared features. \\nIEEE Conference of Computer Vision and Pattern Recognition, 2010. \\n[50] M. Yang, L. Zhang, X. Feng, and D. Zhang. Fisher discrimination \\ndictionary learning for sparse representation. IEEE International \\nConference on Computer Vision, 2011. \\n[51] H. Larochelle and Y. Bengio, “Classification using Discriminative \\nRestricted Boltzmann Machines”, International Conference on Machine \\nLearning, 2008. \\n[52] Z. Cui, S. S. Ge, Z. Cao, J. Yang and H. Ren, “Analysis of Different \\nSparsity Methods in Constrained RBM for Sparse Representation in \\nCognitive Robotic Perception”, Journal of Intelligent Robot and Systems, \\npp. 1-12, 2015. \\n[53] H. Luo, R. Shen and C. Niu, “Sparse Group Restricted Boltzmann \\nMachines”, arXiv:1008.4988v1 \\n[54] R. Salakhutdinov and G. Hinton, “Deep Boltzmann Machines”, \\nInternational Conference on Artificial Intelligence and Statistics, 2009. \\n[55] K. H. Cho, T. Raiko and A. Ilin, \"Gaussian-Bernoulli deep Boltzmann \\nmachine,\" IEEE International Joint Conference on Neural Networks, \\n2013, pp.1-7, 2013. \\n[56] A. Makhzani and B. Frey, \"k-Sparse Autoencoders\", arXiv:1312.5663, \\n2013. \\n[57] K. Cho, \"Simple sparsification improves sparse denoising autoencoders \\nin denoising highly noisy images\", International Conference on Machine \\nLearning, 2013. \\n[58] S. Rifai, P. Vincent, X. Muller, X. Glorot, Y. Bengio: Contractive Auto-\\nEncoders: Explicit Invariance During Feature Extraction, International \\nConference on Machine Learning, 2011 \\n[59] P. Jain, P. Netrapalli and S. Sanghavi, “Low-rank Matrix Completion \\nusing Alternating Minimization”, Symposium on Theory Of Computing, \\n2013. \\n[60] A. Agarwal, A. Anandkumar, P. Jain and P. Netrapalli, “Learning \\nSparsely Used Overcomplete Dictionaries via Alternating Minimization”, \\nInternational Conference On Learning Theory, 2014. \\n[61] D. A. Spielman, H. Wang and J. Wright, “Exact Recovery of Sparsely-\\nUsed Dictionaries”, International Conference On Learning Theory, 2012 \\n[62] S. Arora, A. Bhaskara, R. Ge and T. Ma, “More Algorithms for Provable \\nDictionary Learning”, arXiv:1401.0579v1 \\n[63] C. Hillar and F. T. Sommer, “When can dictionary learning uniquely \\nrecover sparse data from subsamples?”, arXiv:1106.3616v3 \\n[64] I. Daubechies, M. Defrise, C. De Mol, \"An iterative thresholding \\nalgorithm for linear inverse problems with a sparsity constraint\", \\nCommunications on Pure and Applied Mathematics, Vol. 57: 1413-1457, \\n2004. \\n[65] R. Rubinstein, T. Peleg and M. Elad, Analysis K-SVD: A Dictionary-\\nLearning Algorithm for the Analysis Sparse Model, IEEE Transactions \\non Signal Processing, Vol. 61 (3), pp. 661-677, 2013. \\n[66] http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html \\n[67] http://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html \\n \\n',\n",
       " '1607.00858v1.pdf': 'Embracing Data Science\\nAdam Loy\\nDepartment of Mathematics, Lawrence University\\nAugust 26, 2015\\nAbstract\\nStatistics is running the risk of appearing irrelevant to today’s undergraduate\\nstudents.\\nToday’s undergraduate students are familiar with data science projects\\nand they judge statistics against what they have seen. Statistics, especially at the\\nintroductory level, should take inspiration from data science so that the discipline is\\nnot seen as somehow lesser than data science. This article provides a brief overview\\nof data science, outlines ideas for how introductory courses could take inspiration\\nfrom data science, and provides a reference to materials for developing stand alone\\ndata science courses.\\n1\\nIntroduction\\nStatistics is running the risk of appearing irrelevant to many of today’s undergraduate\\nstudents. At ﬁrst, this may sound absurd since the number of statistics degrees awarded\\nat the undergraduate level approximately tripled from 2003 to 2013 [Pierson 2014]. I am\\nnot, however, referring to the popularity of the major, but rather the opportunities we\\nas statistics educators have largely been missing. Rather than emphasizing that statistics\\nis about “thinking with and about data” [Cobb 2015, p. 3], we emphasize only part of\\nthe statistical thought process in our curricula.\\nThis issue is most pronounced at the\\nintroductory level. Consequently, we are failing to communicate what the ﬁeld of statistics\\nis about, making statistics seem irrelevant to many students, and perhaps even to many\\nof our colleagues. In my experience many students who believe statistics is irrelevant are\\n1\\narXiv:1607.00858v1  [stat.OT]  4 Jul 2016\\ninterested in thinking with and about data, so where do they turn? Many are turning to\\ndata science.1\\n2\\nWhat is Data Science?\\nIn 2001, W. S. Cleveland outlined a plan for the discipline of statistics. This plan encour-\\naged statistics departments to focus on the practice of data analysis, resulting in an altered\\nﬁeld called data science. If one takes Cleveland’s view of data science, then it is a subset\\nof statistics, essentially equivalent to applied statistics. Perhaps if every statistician, or at\\nleast a vast majority, had bought into Cleveland’s plan for the discipline this would be the\\ncase, but not all statistics departments/curricula resemble Cleveland’s ideal, so another\\ndeﬁnition has emerged.\\nFigure 1 shows an adaptation of Drew Conway’s venn diagram summarizing his view\\nof data science [Conway 2013]. According to Conway, data science is the intersection of\\nstatistics, computer science2, and domain knowledge. What is clear from ﬁgure 1 is that\\nall three areas are necessary to deﬁne data science—e.g., without domain knowledge, we\\nwould just be talking about machine learning. Consequently, we cannot view data science\\nas simply a subset of statistics, but rather it utilizes a subset of statistics. This intersection\\ndeﬁning data science is what is often visible to undergraduate students in the media—for\\nexample, The Upshot section of The New York Times often presents data science products\\non a variety of topics. So it is important to understand data science in order to understand\\nthe background and expectations of our students.\\nHaving deﬁned what data science is, we must now turn to what a data scientist does.\\nTo better understand this, I have found the three steps of a data science project outlined\\nby Wickham [2014a] to be a useful guide:\\n1. Collect data and questions.\\n1I base this assessment on personal experience and observing the increasing popularity of data science\\ntraining programs, such as the Coursera data science certiﬁcate run by Caﬀo, Leek, and Peng at Johns\\nHopkins and the Data Camp tutorials.\\n2Conway takes a narrower view, using the phrase “hacking skills,” but this view seems needlessly\\nlimiting.\\n2\\nComputer\\nScience\\nDomain Knowledge\\nStatistics\\nData\\nScience\\nMachine\\nLearning\\nFigure 1:\\nAn adaptation of Drew Conway’s diagram explaining what knowledge and skills\\na data scientist needs. The original venn diagram was published on Drew Conway’s blog\\nin 2010.\\n2. Analyze the data, using both exploratory and conﬁrmatory methods.\\n3. Communicate the results in a way that convinces someone to “take action.”\\nAt ﬁrst, the above steps may sound identical to the steps in a applied statistics project, but\\nI ﬁnd statisticians and data scientists apply diﬀerent weights to the tasks outlined above.\\nFor example, many statisticians downplay the importance of graphics in formal analyses\\nbelieving that they are not rigorous enough, while data scientists fully embrace the power\\nof graphical exploration. Further, many statisticians focus on formally reporting the results\\nof their analyses, often in ways that are not accessible to a general audience. While the\\nbest applied statisticians are able to communicate their ﬁndings to a general audience, this\\nseems to be a core requirement of a data science. Data scientists take communication to\\nanother level through the development of web apps and other data products, with the goal\\nof appealing to a general audience.\\nWhile I have focused on the diﬀerences between the statistics and data science, there\\nare far more similarities than diﬀerences as both disciplines are deeply rooted in the data\\nanalytic cycle. So why not take inspiration from data science when we design our statistics\\n3\\ncourses?\\n3\\nHow it can inform our teaching\\nBeing mindful that today’s undergraduate students are familiar with data science projects\\nand that they judge statistics against what they have seen will help us better teach statistics.\\nMany students believe that statistics is irrelevant because we are not “rising to the level” of\\nthe data science projects they have seen. This is especially pronounced at the introductory\\nlevel3 because we still mainly focus on the traditional pillars of statistics—data collection via\\nsurveys and experiments, and inference—rather than focusing on the entire data analytic\\ncycle. Since both statisticians and data scientists think with and about data, we should\\ntake advantage of our students’ prior exposure to data science in our statistics curricula.\\nIn this section I outline a way to incorporate the elements of a data science project into an\\nintroductory statistics course. Such an introductory course should provide a trickle down\\nbeneﬁt to the upper-level courses, assuming that students take an introductory course prior.\\nI also provide a reference concerning the development of a stand alone data science course.\\n3.1\\nIntroductory courses\\nThe goals of an introductory course in any discipline are to (1) provide an overview of that\\ndiscipline and (2) begin to develop a core knowledge base and skill set that is necessary\\nfor more advanced topics. I do not believe that many of the “traditional” introductory\\nstatistics courses achieve both goals, including those that I have taught. In fact, I think we\\noften fail to achieve either goal because we try to cover too many modes of inference and\\nsome of us still emphasize by-hand calculations.\\nRather than trying to cover so many inferential procedures we should take inspiration\\nfrom data science and strive to provide an overview of the entire data analytic cycle.\\nFocusing on the entire cycle will reveal the core knowledge base and skill set that should\\n3I consider the “traditional” introductory statistics course to be one that closely follows the AP cur-\\nriculum College Board [2010]. This includes the new randomization-based approaches to the course, e.g.\\nLock et al. [2013] and Tintle et al. [2015].\\n4\\nbe emphasized. More speciﬁcally, by breaking down the steps of a data science project the\\nnecessary topics that are largely missing from the “traditional” introductory course emerge.\\nThese topics are core aspects of thinking with and about data, so we must incorporate them\\ninto our courses to stay relevant.\\n1. Collection\\n(a) Collect and reﬁne questions\\nToo little time is spent questioning the data sets we use in class. While time con-\\nstraints may be cited as a reason to discuss only “focused” analyses, a guided\\ndiscussion of what questions could be answered using a speciﬁc data set will\\nhone our students’ thought processes. This will allow them to question data\\nsets outside of our classes, rather than only being able to rely on a data analytic\\n“script.” Based on the recommendations from the American Statistical Asso-\\nciation [ASA 2005; 2014], this change is already taking place, but we need to\\ncontinue in this direction. One way to facilitate such discussions is to focus on\\ncase studies rather than textbook examples.\\n(b) Collect data relevant to the questions\\nToo often we provide students with the data directly, rather than having them\\ncollect or access data. In some courses, ﬁnal projects provide students with their\\nﬁrst opportunity to collect data. Rather than waiting until the end of the course,\\nintegrating data collection into the cycle will provide students with important\\ntools for future analyses. In addition to traditional modes of data collection, it\\nis essential to include some introduction to accessing web-based data.\\n2. Analysis\\n(a) Data manipulation\\nWe must stop ignoring data manipulation. Data manipulation is often the most\\ntime consuming task in the data analysis cycle, so how can we justify it receiving\\nlittle or no coverage in our introductory course? Discussing tidy data [Wickham\\n2014b] and the tools available to reshape data into the forms necessary for\\nanalyses are necessary additions to introductory courses.\\n5\\n(b) Visualization\\nRather than going through a laundry list of exploratory graphics without mo-\\ntivation, we should incorporate graphics throughout the entire course.\\nThis\\nwill allow new graphics to be introduced on a just-in-time basis, and will avoid\\nstudents thinking that graphics are unimportant or overly simplistic, as they\\nseem to when we force the entire section into the ﬁrst third of the traditional\\ncourse. Further, we should reﬁne the vocabulary we use in class by appealing to\\nthe grammar of graphics [Wilkinson 2006] and incorporate multivariate topics\\nthrough the use of aesthetics and facetting. Kaplan [2015] provides an example\\nof how this might be done at an appropriate level.\\n(c) Modeling\\nWe already discuss statistical models in introductory courses, but why must\\nwe treat t-tests and simple linear regression models separately? A uniﬁed dis-\\ncussion of modeling from a linear model perspective, such as the one taken by\\nKaplan [2012], would help students focus on the thought process rather than the\\ndiﬀerences between formulas. Further, we need to discuss the inherent link be-\\ntween visualization and modeling in the data analytic cycle. This can easily be\\ndone if we model the thought process for our students during class and through\\nassignments.\\n3. Communication\\nMany introductory statistics courses address communication in some way. Courses\\nthat require ﬁnal projects seem to best develop a student’s ability to communicate\\nstatistical results. However, we need to do more. Students are used to absorbing\\nmaterial online, so enabling them to communicate their ﬁndings online will have\\nimmediate impact. This can be done, for example, by teaching students how to build\\nwebpages using rmarkdown [Allaire et al. 2015] and knitr [Xie 2013], or basic web\\napps using Shiny [Chang et al. 2015]. These skills can be built incrementally at the\\nend of each case study, and will provide very marketable skills, as well as the basic\\nskills to perform reproducible research.\\nWhile adapting an introductory statistics course to include these topics requires a lot\\n6\\nof work, and numerous iterations to hone, the eﬀort will be rewarded. After leaving a\\ncourse where the entire data analytic cycle is examined I believe that students will be\\nable to access new data sets and perform their own analyses in new situations. While\\nless inferential material may be covered, students will understand how to manipulate and\\nvisualize data far better. (Many of the questions I ﬁeld from former students relate to these\\ntopics, so they are certainly topics students encounter outside of our classes.) Additionally,\\nless time will need to be devoted to developing basic data skills in later courses, adding\\nroom in the curriculum for more statistical topics. Finally, this approach provides a realistic\\noverview of the ﬁeld of applied statistics, and will help broadcast what statistics is about,\\neven to those students we only see in an introductory course.\\n3.2\\nTeaching data science\\nIn addition to changes in the introductory curriculum, the increasing visibility of data sci-\\nence opens the door for new courses. One possibility is to create a stand alone data science\\ncourse. If there is not room in the curriculum for such a course, a hybrid course discussing\\nmore advanced statistical models and modern methods in a data science framework may\\nbe more feasible. I refer the interested reader to Hardin et al. [2014] for details from seven\\nvarieties of data science courses.\\n3.3\\nConclusion\\nWhile statistics still seems to be increasing in popularity based on the number of degress\\ngranted, it is running the risk of appearing irrelevant to a large population of students.\\nIf we take inspiration from data science and focus attention on the aspects of a data\\nscience project in our applied statistics courses, then we will show students that statistics\\nis relevant while providing them with the tools to think with and about data, starting in\\nthe introductory course. This can only strengthen our applied statistics curricula and draw\\nstudents into the discipline.\\n7\\nReferences\\nAllaire, J., J. Cheng, Y. Xie, J. McPherson, W. Chang, J. Allen, H. Wickham, and R. Hyn-\\ndman (2015). rmarkdown: Dynamic Documents for R. R package version 0.7.\\nASA (2005).\\nGuidelines for assessment and instruction in statistics education report.\\nAlexandria, VA: American Statistical Association.\\nASA (2014). Curriculum Guidelines for Undergraduate Programs in Statistical Science.\\nAlexandria, VA: American Statistical Association.\\nChang, W., J. Cheng, J. Allaire, Y. Xie, and J. McPherson (2015). shiny: Web Application\\nFramework for R. R package version 0.12.1.\\nCleveland, W. S. (2001). Data science: an action plan for expanding the technical areas of\\nthe ﬁeld of statistics. International statistical review 69(1), 21–26.\\nCobb, G. W. (2015).\\nMere renovation is too little too late: We need to rethink our\\nundergraduate curriculum from the ground up. arXiv preprint arXiv:1507.05346.\\nCollege Board (2010). Statistics: Course description. Technical report.\\nConway, D. (2013, mar). The data science venn diagram.\\nHardin, J., R. Hoerl, N. J. Horton, and D. Nolan (2014, October). Data Science in Statistics\\nCurricula: Preparing Students to ”Think with Data”. ArXiv e-prints.\\nKaplan, D. (2012). Statistical modeling: A fresh approach (2nd ed.).\\nKaplan, D. (2015). Data Computing: An Introduction to Wrangling and Visualization with\\nR (Preview ed.). Project Mosaic Books.\\nLock, R. H., P. F. Lock, K. L. Morgan, E. F. Lock, and D. F. Lock (2013). Statistics:\\nUnlocking the power of data. Wiley.\\nPierson, S. (2014).\\nBachelor’s degrees in statistics surge another 20%.\\nAMSTAT\\nNews (447), 27.\\n8\\nTintle, N., B. Chance, G. Cobb, A. Rossman, S. Roy, T. Swanson, and J. VanderStoep\\n(2015). Introduction to statistical investigations (Preliminary ed.). Wiley.\\nWickham, H. (2014a). Data science: how is it diﬀerent to statistics? IMS Bulletin 43.\\nWickham, H. (2014b). Tidy data. Journal of Statistical Software 59(10).\\nWilkinson, L. (2006). The grammar of graphics. Springer.\\nXie, Y. (2013). Dynamic Documents with R and knitr. Boca Raton, Florida: Chapman\\nand Hall/CRC. ISBN 978-1482203530.\\n4\\nAcknowledgements\\nI would like to thank the participants of the Harnessing Big Data Workshop sponsored\\nby the Associated Colleges of the Midwest.\\nThe conversations we had there made me\\nthink more speciﬁcally about my views on data science and its role in the undergraduate\\ncurriculum.\\n9\\n',\n",
       " '1705.03921v1.pdf': 'WHY & WHEN DEEP LEARNING WORKS: LOOKING INSIDE DEEP LEARNING \\n \\n1 \\nWhy & When Deep Learning Works: \\nLooking Inside Deep Learning \\nRonny Ronen \\nronny.ronen@intel.com \\nThe Intel Collaborative Research Institute for \\nComputational Intelligence (ICRI-CI)1 \\nIn recent years, Deep Learning has emerged as the leading technology for accomplishing broad \\nrange of artificial intelligence tasks (LeCun et al. (2015); Goodfellow et al. (2016)). Deep learning is \\nthe state-of-the-art approach across many domains, including object recognition and identification, \\ntext understating and translation, question answering, and more. In addition, it is expected to play a \\nkey role in many new usages deemed almost impossible before, such as fully autonomous driving. \\nWhile the ability of Deep Learning to solve complex problems has been demonstrated again and \\nagain, there is still a lot of mystery as to why it works, what is it really capable of accomplishing, and \\nwhen it works (and when it does not). Such an understanding is important for both theoreticians and \\npractitioners, in order to know how such methods can be utilized safely and in the best possible \\nmanner. An emerging body of work has sought to develop some insights in this direction, but much \\nremains unknown. The general feeling is that Deep learning is still by and large “black magic” we \\nknow it works, but we do not truly understand why. This lack of knowledge disturbs the scientists \\nand are a cause for concern for developers would you let an autonomous car be driven by a system \\nwhose mechanisms and weak spots are not fully understood? \\nThe Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been \\nheavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We \\nhave asked six leading ICRI-CI Deep Learning researchers to address the challenge of “Why & When \\nDeep Learning works”, with the goal of looking inside Deep Learning, providing insights on how \\ndeep networks function, and uncovering key observations on their expressiveness, limitations, and \\npotential. \\nThe output of this challenge call was quite impressive, resulting in five papers that address \\ndifferent facets of deep learning. These papers summarize the researchers’ ongoing recent work \\npublished in leading conferences and journals as well as new research results made especially for this \\ncompilations. These different facets include a high-level understating of why and when deep networks \\nwork (and do not work), the impact of geometry on the expressiveness of deep networks, and making \\ndeep networks interpretable. \\nUnderstating of why and when deep networks work (and do not work) \\n1. Naftali Tishby and Ravid Schwartz-Ziv in Opening the Black Box of Deep Neural \\nNetworks via Information study Deep Networks by analyzing their information-theoretic \\nproperties, looking at what information on the input and output each layer preserves, and \\nsuggests that the network implicitly attempts to optimize the Information-Bottleneck (IB) \\ntradeoff between compression and prediction, successively, for each layer. Moreover, they \\nshow that the stochastic gradient descent (SGD) epochs used to train such networks have two \\n                                                             \\n1  This work was done with the support of the Intel Collaborative Research institute for Computational \\nIntelligence (ICRI-CI). This paper is the preface part of the ’Why & When Deep Learning works looking \\ninside Deep Learning’ ICRI-CI paper bundle. \\nWHY & WHEN DEEP LEARNING WORKS: LOOKING INSIDE DEEP LEARNING \\n \\n2 \\ndistinct phases for each layer: fast empirical error minimization, followed by slow \\nrepresentation compression. They then present a new theoretical argument for the \\ncomputational benefit of the hidden layers \\n2. Shai Shalev-Shwartz, Ohad Shamir and Shaked Shamma in Failures of Gradient-Based \\nDeep Learning attempt to gain a deeper understanding of the difficulties and limitations \\nassociated with common approaches and algorithms. They describe four families of problems \\nfor which some of the commonly used existing algorithms fail or suffer significant difficulty, \\nillustrate the failures through practical experiments, and provide theoretical insights explaining \\ntheir source and suggest remedies to overcome the failures that lead to performance \\nimprovements. \\nThe impact of geometry on the expressiveness of deep networks \\n3. Amnon Shashua, Nadav Cohen, Or Sharir, Ronen Tamari, David Yakira and Yoav \\nLevine in Analysis and Design of Convolutional Networks via Hierarchical Tensor \\nDecompositions analyze the expressive properties of deep convolutional networks. Through \\nan equivalence to hierarchical tensor decompositions, they study the expressive efficiency and \\ninductive bias of various architectural features in convolutional networks (depth, width, pooling \\ngeometry, inter-connectivity, overlapping operations etc.). Their results shed light on the \\ndemonstrated effectiveness of convolutional networks, and in addition, provide new tools for \\nnetwork design. \\n4. Nathan Srebro, Behnam Neyshabur, Ryota Tomioka and Ruslan Salakhutdinov in \\nGeometry of Optimization and Implicit Regularization in Deep Learning argue that the \\noptimization methods used for training neural networks play a crucial role in generalization \\nability of deep learning models, through implicit regularization. They demonstrate that \\ngeneralization ability is not controlled simply by network size, but rather by some other implicit \\ncontrol. Then, by studying the geometry of the parameter space of deep networks and devising \\nan optimization algorithm attuned to this geometry, they demonstrate how changing the \\nempirical optimization procedure can improve generalization performance. \\nInterpretability of deep networks \\n5. Shie Mannor, Tom Zahavy and Nir Baram in Graying the black box: Understanding \\nDQNs present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind \\nmatter. They propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), \\nand an algorithm that learns it automatically. Using these tools they reveal that the features \\nlearned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. \\nMoreover, they are able to look into the network to understand and describe the policies learned \\nby DQNs for three different Atari2600 games and suggest ways to interpret, debug and \\noptimize deep neural networks in reinforcement learning. \\nReferences \\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT Press, 2016. \\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553): 436–444, \\n2015. \\n',\n",
       " '1711.03577v1.pdf': 'What Really is Deep Learning Doing? ∗\\nChuyu Xiong\\nIndependent Researcher, New York, USA\\nEmail: chuyux99@gmail.com\\nAbstract\\nDeep learning has achieved a great success in many areas, from computer vision to natural\\nlanguage processing, to game playing, and much more. Yet, what deep learning is really doing is\\nstill an open question. There are a lot of works in this direction. For example, [6] tried to explain\\ndeep learning by group renormalization, and [5] tried to explain deep learning from the view of\\nfunctional approximation. In order to address this very crucial question, here we see deep learning\\nfrom perspective of mechanical learning and learning machine (see [1], [2]). From this particular\\nangle, we can see deep learning much better and answer with conﬁdence: What deep learning is\\nreally doing? why it works well, how it works, and how much data is necessary for learning. We\\nalso will discuss advantages and disadvantages of deep learning at the end of this work.\\nKeywords: Mechanical learning, learning machine, deep learning, X-form, universal\\nlearning machine, internal representation space, data sufficiency\\n1\\nIntroduction\\nIn recent years, deep learning (a branch of machine learning) has achieved many successes in lot of\\nﬁelds. However, a clear theoretical framework of deep learning is still missing. Consequently, there are\\nmany fundamental questions about deep learning are still open. For example: What is deep learning\\nreally doing? Is it really learning, or just a kind of fancy approximation to a function? Why it indeed\\nhas so many success? Why deep learning needs big data? For a particular problem, how much data\\nis suﬃcient to drive deep learning to learn? Up to now, there is no satisfactory answer for these\\nfundatamental questions. We here are trying to address these questions from a new angle.\\nWe introduced term ”Mechanical learning” in [1]. Mechanical learning is a computing system that is\\nbased on a simple set of ﬁxed rules (this is so called mechanical), and can modify itself according to\\nincoming data (this is so called learning). A learning machine is a system that realizes mechanical\\nlearning.\\nIn [2], we described learning machine in a lot of details. By doing so, we gained some useful knowledges\\nand insights of mechanical learning.\\nIn this short article, by using those knowledges and insights, we are trying to view deep learning from\\na new angle. First, we will brieﬂy discuss learning machine, pattern, internal representation space,\\nX-form, data suﬃciency, and learning strategies and methods. Then, we will use the view of learning\\nmachine to see deep learning. We start from the simplest deep learning, i.e., 2-1 RBM, then go to\\n3-1 RBM, N-1 RBM, N-M RBM, stacks of RBMs. Then we discuss the learning dynamics of deep\\nlearning. By this approach, we see clearly what deep learning is doing, why deep learning is working,\\nunder what condition deep learning can learn well, how much data are needed, and what disadvantages\\ndeep learning has.\\n2\\nMechanical Learning and Learning Machine\\nWe here very brieﬂy sum up the discussions that we did in [1] and [2]. A learning machine M has 2\\nmajor aspects: it is an IPU, i.e. it is able to process information; and it is learning, i.e. its information\\n∗Great thanks for whole heart support of my wife.\\n1\\narXiv:1711.03577v1  [cs.LG]  6 Nov 2017\\n2\\nWhat really is deep learning?\\nprocessing ability is changing according to data. Without learning (since it is a machine we design,\\nwe can stop learning), M is very similar to a CPU. However, one major diﬀerence between learning\\nmachine and CPU is: learning machine treat incoming data according to its pattern, not bit-wise.\\nThus, in order to understand a learning machine, it’s absolutely necessary to understand pattern well.\\nThere are 2 kinds of patterns: objective pattern and subjective pattern. Subjective pattern is crucial\\nfor learning machine. In [2], we proved one theorem: For any objective pattern Po, we can ﬁnd a\\nproper subjective pattern p that can express Po well and is build upon a least set of base patterns.\\nTo describe subjective patterns, it is best to use X-form, which is one algebraic expression upon some\\nbase patterns. X-form is one very important mathematical object. X-form could have sub-forms.\\nX-form and its sub-forms actually forms the fundamental fabric of a learning machine.\\nWe also deﬁned learning by teaching and learning without teaching. Then, further specify typical\\nmechanical learning. Learning by teaching requires we know learning machine and the pattern to\\nlearn well. By these knowledges, we can design a teaching sequence to make learning machine learn\\nwell. We proved that if a learning machine has certain capabilities for learning by teaching, it is\\nuniversal, i.e. able to learn anything.\\nHowever, most learning is not learning by teaching. In order to understand typical mechanical learning,\\nwe introduced internal representation space. Structurally, a learning machine M has components:\\ninput space, output space, internal representation space, and learning methods and learning strategies.\\nThe most important part is internal representation space. We studies internal representation space\\nE in details, and revealed, in fact, it is equivalent to a collection of X-forms.\\nThis fact tells us\\nthat learning is nothing but a dynamics on E, moving from one X-form to another.\\nWith clear\\nand reachable internal representation space, learning can be understood much better, and can be\\ndone much more eﬃciently.\\nFor example, we can unify all 5 kinds of learning – logic reasoning,\\nconnectionism, probabilistic approach, analogy, and evolution (see [3]) – together on E naturally.\\nFor mechanical learning, we need to understand data suﬃciency. This is very crucial concept. We\\nuse X-form and its sub-forms to deﬁne data suﬃcient to support one X-form and suﬃcient to bound\\none X-form. With suﬃcient data, we can see how learning strategy and learning method work. There\\ncould be many learning strategies and learning methods. We show 3 learning strategies: 1. Embed\\nX-form into parameter space. 2. Squeeze X-form from inside to higher abstraction. 3. Squeeze X-\\nform from inside and outside to higher abstraction. We prove that with certain capabilities, the last\\n2 strategies and methods will make universal learning machine. Of course, this is theoretical results,\\nsince we have not designed a speciﬁc learning machine yet.\\nHere, we will show that deep learning is actually doing mechanical learning by the ﬁrst strategy, i.e.\\nembed X-forms into parameter space. Such a fact will help us to understand deep learning much\\nbetter.\\n3\\nSee Deep Learning from View of Learning Machine\\nAccording to our deﬁnition, if without human intervention, deep learning is mechanical learning. Of\\ncourse, this ”if” is a big if. Often, deep learning program is running with a lot of human intervention,\\nspecially at the time of model set up. We will restrict our discussion to Hinton’s original model [4],\\ni.e., a stack of RBMs. Each level of RBM is clearly a N-M learning machine (N andM are dimensions\\nof input and output). Hinton’s deep learning model is by stacking RBM together. If without further\\nhuman intervention, it is a learning machine. This is the original model of deep learning. Other deep\\nlearning program can be thought as variations based on this model. Though in the past few years deep\\nlearning has leaped forward greatly, stacking RBM together still reﬂects the most typical properties\\nof deep learning.\\nThus, we would expect many things in mechanical learning could be applied to deep learning. The\\npoint is, we are viewing deep learning from a quite diﬀerent angle – angle of mechanical learning. For\\nexample, we can view Hinton’s original deep learning program [4] as one 258-4 learning machine, and\\nwe ask what is the internal representation space of it? We expect such angle and questions would\\nreveal useful things for us.\\nThe central questions indeed are: what is the internal representation\\nspace of deep learning? what is the learning dynamics? At ﬁrst, seems it is quite hard since learning\\nis conducted on a huge parameter space (dimension could be hundreds of millions), and learning\\nmethods are overwhelmingly a big body of math. However, when we apply the basic thoughts of\\nlearning machine to deep learning, starting from simplest RBM, i.e. 2-1 RBM, we start to see much\\nChuyu Xiong\\n3\\nmore clearly.\\n2-1 RBM\\n2-1 RBM is the simplest. However, it is also very useful since we can examine all details, and such\\ndetails will give us a good guide on more general RBM.\\n2-1 RBM is one IPU. We know 2-1 IPU totally has 16 processing (222). But, we only consider those\\nprocessing: p(0, 0) = 0, so totally 8 processing, which we denote as Pj, j = 0, . . . , 7 (see [1]). For 2-1\\nRBM, any processing p can be written as: for input (i1, i2), the output o is:\\no = p(i1, i2) = Sign(ai1 + bi2), where (a, b) ∈R2, Sign(x) =\\n\\x1a 1\\nif x ≥0\\n0\\nif x < 0\\nThe parameters (a, b) determine what the processing really is. Parameter space R2 has inﬁnite many\\nchoices of parameters. But, there are only 6 processing, thus, for many diﬀerent parameters, the\\nprocessing is actually same. We can see all processing in below table:\\nP0\\nP1\\nP2\\nP3\\nP4\\nP5\\nP6\\nP7\\n(0,0)\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n(1,0)\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n(0,1)\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n(1,1)\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n1\\nRegion\\nR4\\nR3\\nR5\\nR2\\nR6\\nNone\\nNone\\nR1\\nX-form\\n0\\nb1\\nb2\\nb1 + b3\\nb2 + b3\\nb1 + b2\\nb3\\nb1 + b2 + b3\\nTab. 1. Table for all processing of 2-1 RBM\\nFig. 1. Parameter space of 2-1 RBM that is cut into 6 regions\\nIn ﬁrst row of table, Pi, i = 0, . . . , 7 are all processing of 2-1 IPU. Under ﬁrst row, there is value table\\nfor each processing. We point out some quite familiar processing: P7 is actually logical OR gate, P6\\nis logical AND gate, P5 is logical XOR gate. Note, P5, P6 are processing for 2-1 IPU, but not in 2-1\\nRBM. It is well known, 2-1 RBM has no XOR and AND (i.e. no P5, P6).\\nRj, j = 1, . . . , 6 indicate regions in parameter space R2, each region for one processing. There are\\nonly 6 regions, since 2-1 RBM only has 6 processing. We brieﬂy discuss how we get these regions. See\\nillustration in ﬁgure.\\nSuppose p is processing. Line a = 0 cuts R2 into 2 regions: a ≥0 and a < 0. If (a, b) is in ﬁrst region,\\nthen p(1, 0) = 1, in second, p(1, 0) = 0. Line b = 0 is perpendicular to a = 0, so, it cuts the previous\\n4\\nWhat really is deep learning?\\n2 regions into 4 regions: a ≥0, b ≥0 and a ≥0, b < 0 and a < 0, b ≥0 and a < 0, b < 0. Clearly, if\\nb ≥0, p(0, 1) = 1, if b < 0, p(0, 1) = 0. Line a + b = 0 could no longer cuts the previous 4 regions into\\n8 regions, it could only cut 2 regions (2nd, 4th quadrant) into 4 regions (R2, R3, R5, R6). So, totally,\\nwe have 6 regions, and each region is for one processing. This argument about regions is very simple,\\nyet very eﬀective. We can extend this argument to N-1 RBM.\\nEach region is for one processing. So, region can be used to represent processing. That is 6th row\\nin the table shown. Yet, a much better expression is by X-form ([2]). We explain them here. Here\\nb0 = (0, 0), b1 = (1, 0), b2 = (0, 1), b3 = (1, 1) are base patterns. For 2-dim pattern space, there are\\nonly these 4 base patterns. But, bi can also be used to represent one processing of 2-1 IPU, i.e. bi\\nis such a processing: when input is bi, output is 1, otherwise output is zero. X-forms are expressions\\nbased on all N-1 base patterns, operations +, ·, ¬, composition, Cartesian product, and apply them\\nconsecutively. Example, b1 + b3, b1 · b2, b1 + ¬b2 are X-forms. Any processing of 2-1 IPU can be\\nexpressed by at least one X-form [2]. For example, if region is R3, processing is P1, X-form is b1.\\nAnother example, region is R1, processing is P7 (this is OR gate), X-form is b1 + b2 + b3. P5 is a\\nprocessing of 2-1 IPU (XOR gate), but not in 2-1 RBM, its X-form is b1 +b2. The 7th row in the table\\nshows X-forms representing processing. We can say that each processing is represented by a region,\\nand by a X-form as well.\\nWhen 2-1 RBM is learning, clearly, parameter (a, b) is adapting. But, only when (a, b) cross region,\\nprocessing changes. Before crossing, change of parameters is just for preparation for crossing (perhaps\\nmany parameter changes are just wasted).\\nLearning is moving from one region to another.\\nOr,\\nequivalently, learning is from one X-form to another. Such view is crucial. Now, we are clear, on\\nsurface, learning on 2-1 RBM is a dynamics on parameter space R2, but real learning dynamics is on\\n6 regions (or X-forms). Such indirectness causes a lot of problem.\\n3-1 RBM\\nJust increase input dimension 1, we have 3-1 RBM. To discuss it, we can gain some insights for general\\nRBM. For 3-1 RBM, still we can write: for any input (i1, i3, i3) ∈B3, output o ∈B is:\\no = p(i1, i2, i3) = Sign(ai1 + bi2 + ci3), where (a, b, c) ∈R3, Sign(x) =\\n\\x1a 1\\nif x ≥0\\n0\\nif x < 0\\nHowever, while we can easily write down all possible processing of 2-1 RBM, it would be hard to do\\nso for 3-1 RBM. For 3-1 IPU, we know that the number of all possible processing is 223 = 28 = 256.\\nSince only considering such processing p: p(0, 0, 0) = 0, the number becomes 256/2 = 128. We expect\\n3-1 RBM has less processing. But, how many possible processing of 3-1 RBM could have?\\nFollowing the guidance that 2-1 RBM gives to us, i.e.\\nto consider the hyperplane generated by\\nnonlinearity that cuts parameter spaces, we examine parameter space (a, b, c) ∈R3, and following\\nplanes: a = 0, b = 0, c = 0, a + b = 0, a + c = 0, b + c = 0, a + b + c = 0.\\nThese planes are\\nnaturally obtained. For example, if we consider the input (1, 0, 0), it is easy to see plane a = 0 is\\nwhere cut the value of output: 1 or 0. So, the above planes are associated with following inputs:\\n(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\\nWe can clearly see that in one region that is cut out by above 7 planes, the output values are same.\\nTherefore, one region actually is representing one processing: in the region, processing is same. So,\\nquestion of how many possible processing becomes how many possible regions cut out by the 7 planes.\\nWe do counting for the regions below.\\nFirst, a = 0 cuts parameter space into 2 pieces: R1\\n1, R1\\n2. Second, b = 0 perpendicular to a = 0,\\nso, it cuts each region R1\\n1, R1\\n2 into 2 pieces, we then have 4 regions: R2\\n1, R2\\n2.R2\\n3, R2\\n4. Then, c = 0\\nperpendicular to a = 0 and b = 0, so, we have 8 regions: R3\\nj, j = 1, . . . , 8. Then, consider a + b = 0.\\nThis plane no longer could be perpendicular to all a = 0, b = 0, c = 0. We will not have 2 ∗8 = 16\\nregions. We only have 1.5 ∗8 = 12 regions. Following the same argument, we have: For a + c = 0,\\n1.5 ∗12 = 18 regions. For b + c = 0, 1.5 ∗18 = 27 regions. For a + b + c = 0, 1.5 ∗27 < 41 regions.\\nSo, for 3-1 RBM, there are at most 41 possible processing, comparing to 128 possible processing of\\nfull 3-1 IPU. However, there are possibility that the number of processing is even less than 41, since\\namong those regions, it is possible that 2 diﬀerent regions give same processing. We do not consider\\nthese details here.\\nSince regions can be represented by X-form, each processing 3-1 RBM can be represented by at least\\none X-form. b1 = (1, 0, 0), b2 = (0, 1, 1), b3 = (0, 0, 1), . . . , b7 = (1, 1, 1) are X-form for all base patterns.\\nChuyu Xiong\\n5\\nExamples, X-form b1 +b2 is in 3-1 RBM. But, b1 ·b2 is not in 3-1 RBM. There are a lot of such X-form\\nthat is not in 3-1 RBM.\\nLearning dynamics on 3-1 RBM is also in such way: on surface, it is dynamics on R3, but real learning\\ndynamics is on 41 regions (or X-forms).\\nN-1 RBM\\nThe argument for 3-1 RBM can be extended to N-1 RBM (See details in [2]). We consider hyperplanes\\nand regions cut oﬀby these hyperplanes. The number of these regions is less than: 2N1.52N−N−1.\\nCompare to the number of all processing of N-1 IPU, which is 22N−1, easy to see, N-1 RBM has much\\nless processing. This means that N-1 RBM could not express many processing.\\nFor N-1 RBM, still we can write: for any input (i1, . . . , iN) ∈BN, output o ∈B is:\\no = p(i1, . . . , i3) = Sign(a1i1 +a2i2 +. . .+aNi3), where (a1, . . . , aN) ∈RN, Sign(x) =\\n\\x1a 1\\nif x ≥0\\n0\\nif x < 0\\na1 = 0, . . . , a1 + a2 = 0, . . . , a1 + a2 + a3 = 0 . . . , . . . , a1 + a2 + a3 + . . . = 0, . . .\\n(1)\\nThere are\\n\\x00N\\n1\\n\\x01\\nhyperplanes such as a1 = 0;\\n\\x00N\\n2\\n\\x01\\nhyperplanes such as a1 + a2 = 0; . . . . We also have\\nthis: First N hyperplanes will cut parameter space into 2N regions. Then, later each hyperplanes will\\ncut more regions by the rate of multiplying factor 1.5. Thus, we can see the number of regions are:\\n2N ∗1.5K2 ∗1.5K3 ∗. . . ∗1.5KN\\nwhere K2 =\\n\\x00N\\n2\\n\\x01\\nis the number of hyperplanes such as a + b = 0, etc.\\nAnd, we have the equation:\\n2N =\\n\\x12N\\n0\\n\\x13\\n+\\n\\x12N\\n1\\n\\x13\\n+\\n\\x12N\\n2\\n\\x13\\n+ . . . +\\n\\x12N\\nN\\n\\x13\\n(2)\\nSo,\\nK2 + K3 + . . . + KN =\\n\\x12N\\n2\\n\\x13\\n+\\n\\x12N\\n3\\n\\x13\\n+ . . . +\\n\\x12N\\nN\\n\\x13\\n= 2N −\\n\\x12N\\n1\\n\\x13\\n−\\n\\x12N\\n0\\n\\x13\\n= 2N −N −1\\n(3)\\nThus, the number of regions are\\n2N ∗1.52N−N−1 = 2N ∗(3\\n2)2N−N−1\\nThis is a very big number. Yet, compare to the total possible processing of full IPU, it is quite small.\\nSee their quotient:\\n22N\\n2N ∗( 3\\n2)2N−N−1 = 2 ∗(4/3)2N−N−1\\nIt tells that full IPU has f = 2 ∗(4/3)2N−N−1 times more processing comparing to RBMs. This is\\nhuge diﬀerence. Say, just for N = 10, f is more than 120 digits, i.e. the number of processing of full\\nIPU would has more 120 digits at the end than the number of RBMs.\\nAlso, each region can be expressed by at least one X-form. For examples, b1 + bN, b1 + b3 + b5, etc.\\nLearning dynamics on N-1 RBM is in such way: on surface, it is dynamics on RN, but real learning\\ndynamics is on those 2N1.52N−N−1 regions (or X-forms).\\nN-M RBM\\nSuppose Ri, i = 1, . . . , M are M N-1 RBMs, we can form a N-M RBM, denote as R = (R1, . . . .RM),\\nwhose processing are p = (p1, p2, . . . , pM), where pi, i = 1, . . . , M are processing of Ri. So, R is\\nCartesian product of Ri, i = 1, . . . , M.\\nSince all Ri are cut into regions, and in each region, processing is same, we can see R is also cut into\\nregions, and each region is a Cartesian product of regions of Ri: R = R1 × R2 × . . . × RM, where Ri\\n6\\nWhat really is deep learning?\\nis one region from i-th RBM Ri. Thus, the number of all possible regions of R is (2N1.52N−N−1)M =\\n2NM1.5M(2N−N−1). This is a much smaller number than 2M2N , which is the number of total possible\\nprocessing for N-M IPU.\\nX-form for each region of R, are actually Cartesian product of X-form for those regions of Ri. Suppose\\nR = R1 × R2 × . . . × RM, and fi are X-forms for region Ri in Ri, i = 1, . . . , M, then X-form for R is\\nf = (f1, . . . , fM). For example, (b1, b1 + b3, . . . , b2 · b4) is one X-form of R.\\nLearning dynamics on N-M RBM is in such way: on surface, it is dynamics on parameter space RNM,\\nbut real learning dynamics is on those 2NM1.5M(2N−N−1) regions (or X-forms).\\nStacking RBMs\\nConsider a N-M RBM R1, and a M-L RBM R2, stacking them together, we get one N-L IPU R: A\\nprocessing p of R are composition of processing p1, p2 of R1, R2: p(i1, i2, . . . , iN) = p2(p1(i1, i2, . . . , iN)).\\nAnd we denote as: R = R1 ⊗R.\\nThe parameter space of R clearly is RNM × RML. We know RNM is cut into some regions, in each\\nregion processing is same. So does RML. Thus, RNM+ML is cut into some regions, in each region\\nprocessing is same, and these regions are Cartesian product of regions in RNM and RML. So, we\\nknow number of total possible processing R equals total possible processing of R1 times R2, i.e.\\n2NM1.5M(2N−N−1) × 2ML1.5L(2M−M−1) = 2NM+ML1.5M(2N−N−1)+L(2M−M−1).\\nWe can easily see if M is large enough, the above number will become greater than 2L2N , which is\\ntotal possible processing of R. We can see, at least potentially, R has enough ability to become a N-L\\nfull IPU. But, we will not consider here. In fact, it is very closely related to the so called Universal\\nApproximation Theorem. Indeed, stacking RBM together is powerful.\\nX-form can be expressed as composition as well. For example, consider 3 2-1 RBM R1, R2, and\\nR3.\\nUsing R1 and R2 to form a 2-2 RBM, and using R3 to stack on it, we get a 2-1 IPU R:\\nR = R3 ⊗(R1, R2). If for this case, R1 has X-form b1, and R2 has X-form b2, and R3 has X-form\\nb1 + b2 + b3, them, R has X-form (b1 + b2 + b3)(b1, b2). Easy to see this X-form is processing P5 (XOR\\ngate), which is not expressible by one 2-1 RBM. So, putting 3 2-1 RBMs together, more X-form can\\nbe expressed.\\n4\\nLearning Dynamics of Deep Learning\\nWith these understandings RBMs, which is the most essential building block of deep learning, we can\\nsee how the model of deep learning is build up, and how learning dynamics is doing. Clearly, today’s\\ndeep learning is much more than original Hinton’s model of stacking RBMs (see [4]). But, we would\\nﬁrst talking about this model.\\nThe deep learning model is by stacking more RBMs (this is so called deep). Once several RBMs are\\nputting together, a deep learning model is formed. Suppose Rj, are Nj-Nj+1 RBM, j = 1, . . . , J, where\\nN1, . . . , NJ, NJ+1 are sequence of integers. So, we can stack these RBM together to form one N1-NJ+1\\nIPU, whose processing could be written in such way: p(i1, i2, . . . , iN1) = pJ(. . . p2(p1(i1, i2, . . . , iN1))\\nwhere each pj is processing of Rj. We denote this IPU as R = R1 ⊗. . . ⊗RJ. All parameters of R\\nform a huge Euclidean space RN1N2+...+NJNJ+1. We can denote this huge parameter space as R∗.\\nThen, clearly, deep learning is conducted on R to reach a good processing by modifying the parameters\\nin R∗. Of course, it requires skills to modify such a huge number of parameters. There are methods,\\nsuch as CD (convergent divergence), SGD (stochastic gradient descent), etc. are invented for such\\npurpose.\\nHowever, no matter what methods are used to modify the parameters, it is modifying parameters\\nto form the dynamics of learning. So, seems the phase space of learning dynamics is on the space\\nR∗. But, this is just on surface. As we discussed in last section, the true dynamics is not conducted\\non parameters, but on those regions. The learning dynamics is conducted on these regions cut by\\nhyperplanes and Cartesian products. The number of regions are huge: 2N1N21.5N2(2N1−N1−1) × . . ..\\nMore precisely, the situation is: as learning, a huge number of parameters are adapting, but only\\nwhen parameters cross region, the processing of R changes. Before crossing, processing remains same,\\nthe changes of parameter at most can be thought as the preparation for crossing (perhaps many\\nsuch changes of parameters are just wasted). Thus, learning dynamics is to move from one region\\nChuyu Xiong\\n7\\nto another. We also know each region is associated with one X-form. Thus, learning dynamics is to\\nmove from one X-form to another X-form.\\nFig. 2. Illustration on parameter space is cut into regions\\nFig.\\n2 gives one illustration on parameter space is cut into regions.\\nOf course, R∗is very high\\ndimension Euclidean space, so regions could be shown on paper precisely, and Fig.\\n2 is just one\\nillustration. However, this illustration gives us one clear picture about deep learning structure.\\nThe deep learning structure is formed by these factors: how many RBMs, dimension of each RBM, how\\nto stack, how to do Cartesian product. Once the structure is formed, if no further human intervention\\n(such as manually adjust numbers or subroutines in model), the structure will not change.\\nSuch\\nstructure is formed by people at set up time. So, for ﬁxed structure, we will have ﬁxed region cut (as\\nillustrated in Fig. 2). Further, we will have a ﬁxed set of X-forms, and learning is conducted on this\\nset of X-forms.\\nWe can see one example. R1, R2, R3 are 3 2-1 RBMs. We put them like this: R = R3 ⊗(R1, R2).\\nWe have 3 parameter space (a, b), (c, d), (e, f). We have 6 regions for each parameter space. Put them\\ntogether, we have 6x6x6 = 216 regions. R is one 2-1 IPU. So, R at most has 8 processing. Thus,\\namong those 216 regions some diﬀerent regions will have same processing. But, each region will have\\none X-form. That is to say, for one processing, there could have several X-form associated with.\\nFor example, consider this region: R3\\n1 × (R1\\n3, R2\\n5). This gives processing P5 (XOR gate). Normally,\\nfor this processing, we can use X-form b1 + b2 for it. But, for the region, naturally, the X-form is:\\n(b1+b2+b3)⊗(b1, b2). That is to say, this X-form will generate the same processing as b1+b2. Another\\nregion: R3\\n2 × (R1\\n2, R2\\n6) will give the same processing. And, the X-form is: (b1 + b3) ⊗(b1 + b3, b2 + b3).\\nFor deep learning model build on stacking RBMs (original Hinton’s model, [4]), as we discussed in\\nlast section, the situation is same: the parameter space R∗is cut into regions (by hyperplanes, etc),\\nand each region is associated with one X-form, when parameter cross the boundary of the regions,\\none X-form moves to another X-form, learning dynamics is conducted on this set of X-form.\\nThis is the learning dynamics of deep learning, this is what really deep learning is doing.\\nFor more complicated deep learning, such as convolution is used, connection pruning is done, non-\\nlinearity is other than sign function (like ReLU), the situation will be more complicated. However,\\nif there is no human intervention, it will surely be mechanical learning. We still can prove that the\\nlearning dynamics is the same: the parameter space R∗is cut into regions, and each region is as-\\nsociated with one X-form, when parameter cross the boundary of the regions, one X-form moves to\\nanother X-form, learning dynamics is conducted on this set of X-form.\\nTo prove this for general deep learning mathematically, additional works are needed. We will do this\\nwork in other place. But, we have no doubt this can be done.\\nSuch a learning strategy is exactly what we described in [2]: ”Embedded X-forms into parameter\\nspace”.\\n8\\nWhat really is deep learning?\\n5\\nRemark\\nNow, we know the fact: deep learning is using strategy of ”Embedded X-forms into parameter space”.\\nThis fact is very essential and many consequences can be derived from it.\\nHere we make some\\ncomments.\\nTrue nature of deep learning:\\nOn surface, deep learning seems build a model from data feed into eventually (by using neural network,\\nstacking RBMs, and more other tools). However, as we reveal in previous sections, it is not such a\\ncase. Essentially, a deep learning model is doing this: at the time of model setting up, to cut the\\nhuge parameter space R∗into many regions, and each region is associated with one X-form that is\\none logical statement, then driven by big amount of data, following certain learning dynamics, i.e. to\\nmove from region to another region, which is equivalent to move from one X-form to another X-form,\\nand eventually to reach a satisfactory X-form, which is the learning result.\\nSo, we say that deep learning is not to building up a model from data input, but it is to choose a\\ngood X-form from a set of X-forms established at the time deep learning model is set up. This is the\\ntrue nature of deep learning.\\nSuch a view is diﬀerent than popular view about deep learning. However, it is true and help us to\\nunderstand deep learning better. For example, [6] might be right, there are some group renormalization\\ngoing on, but, it missed this issue: group renormalization happens at the setting stage not at the\\nlearning stage. Another example, [5] gives a very good explanation about the power of multi-stage-\\ncomposition. However, it failed to realize that learning is not only to get a good processing, but to\\nﬁnd a best possible X-form for the good processing (since one particular processing could have many\\nassociated X-form, and some of such X-form is bad, some of such X-form is good).\\nFundamental limitation of deep learning:\\nThe fundamental limitation of a deep learning model is from its nature: it acts on a pre-selected set\\nof X-forms X, which is formed at the time the model is set up.\\nSo, mostly likely, a deep learning model could not be an universal learning machine [2].\\nIf it is\\nuniversal, X must contains at least one X-forms for any possible processing. This is nearly impossible.\\nActually, a deep learning model is set up by human, and is for one particular task. So, most likely, X\\nonly contains X-forms specially for this task. And, this deep learning model is limited by X.\\nIf the learning target is for a particular processing, and if in X there is at least one X-form associated\\nto this processing, a deep learning model could possible to get the target. Otherwise, a deep learning\\nmodel could not reach the processing, no matter how hard to try and how much data. In another\\nword, the model is a bad model. But, deep learning has no any method to tell if a model is good or\\nbad before trying it out. This is one huge limitation.\\nYet, even X contains a X-form associated with the desired processing, we still do not know if the\\nX-form is a good one. As we know in [2], there are many X-forms associated with one processing,\\nsome is bad, and some is good. For example, one X-form is more robust for certain conditions. If X\\ndoes not contain the robust X-form, no matter how hard to try and how much data, learning could\\nnot get a robust solution. Again, deep learning has no any method to tell if X contains such X-form.\\n.\\nThese limitations are fundamental and are derived from the fact: deep learning is chosen X-form from\\na pre-selected set X, not dynamically building a X-form from input data.\\nLogical or Probabilistic:\\nQuite often, people think deep learning is doing probability learning. They think that a probability\\nmodel is essential for deep learning since a lot data feed in, specially, stochastic gradient descent is one\\nvery essential learning method. However, we would like to point out: deep learning fundamentally is\\nviewing its learning target logically. Why? Each X-form in X is a logical statement, often a very long\\nlogical statement (so called deep). So, the very essential thing is: when a deep learning model does its\\ninformation processing, it is doing according to a solid logical statement. Not doing the information\\nprocessing probabilistically.\\nOf course, the way to get the X-form might not be pure logical, it could involve a lot of probabilistic\\nviews and actions, such as stochastic gradient descent. However, we would point out, even the way\\nto get the X-form, could be pure deterministic not probabilistic.\\nIt is possible to design a pure\\ndeterministic learning dynamics, at least in theory.\\nChuyu Xiong\\n9\\nWhy works well:\\nPractice shows, deep learning works very well for many problems. Now, we can see the reason for\\nsuch success clearly: when a deep learning model for one problem is set up by experienced people,\\nthe desired X-form is already build into the model. More precisely, if the desired X-form is X, when\\ndeep learning model is set up, we have X ∈X, where X is the pre-selected set of X-forms. If so, it is\\npossible to learn the X-form X successfully, so the processing associated with X. Thus, the success\\nof a deep learning model depends on its set up. Having a good set up, the model will work well.\\nOtherwise, no matter what data and what eﬀorts, the model will not work well.\\nOf course, besides the set up of model, learning methods are crucial. It is not easy to choose the right\\nX from X at all. We would like to point out the methods currently used indeed have some advantages:\\n1. Methods act on Euclidean space, which is most easy to calculate, with many sophisticated\\nalgorithms, library, packages, and hardwares available.\\n2. Methods are mostly doing linear algebraic calculus, which are easy to be parallelized. And, high\\nparallelization is the key of its success. However, this advantage is build on this fact: no dynamic\\nadaption. If dynamic adaption is used (such as recently introduced Capsule), this advantage\\nmight be lost.\\nData for deep learning:\\nAs discussed above, the logical statement (X-form) is the core of deep learning. Without supporting\\ndata, a deep learning model could not reach a sophisticated logical statement (X-form). We deﬁned\\ndata suﬃciency in [2], which tells what kind of data can support one X-form (logical statement).\\nOf course, the data suﬃciency we deﬁned is only the ﬁrst step to understand data. Since deep learning\\napproaching the desired X-form by some learning methods, it is easy to see that we need more data\\nthan just suﬃcient to support one X-form. The relationships here could be quite complicated, which\\nwould be the topic for further research.\\nHowever, we can tell that data suﬃcient to support and suﬃcient to bound the desired X-form is\\nthe necessary condition for deep learning. In this sense, for so called big data for deep learning, we\\nunderstand the necessity and lower bound.\\nDisadvantages of deep learning:\\nDeep learning has some fundamental disadvantages from its root. We list some of them below:\\n1. It is acting on huge parameter space, but, actual learning dynamics is on a ﬁxed set of regions\\n(which is equivalent to a set of X-forms). This indirectness makes every aspects of learning\\nharder, especially, it is near impossible to know what is exactly happening in learning dynamics.\\n2. Successful learning needs data suﬃcient to support and to bound. This is very costly.\\n3. The structure of learning is setup by human. Once setup, structure (how many layers, how big\\na layer is, how layers ﬁtting together, how to do convolution, etc) is not be able to change. This\\nmeans that learning is restricted on a ﬁxed group of regions, equivalent a ﬁxed group of X-forms.\\nIf best X-form is not in this set, deep learning has no way to reach best X-form, no matter how\\nbig data are and how hard to try. Consequently, it is not universal learning machine.\\n4. It is very costly to embed X-forms into a huge parameter space. Perhaps, among all computing\\nspend on learning, only a very small fraction is used on critical part, i.e. moving X-form to\\nanother, and most are just wasted.\\n5. Since there is no clear and reachable internal representation (due to the embedding), it will be\\nvery hard to do advanced learning, such as to unite all 5 learning methods together (see [3], [2]).\\n6. Since there is no clear internal representation space, it is hard to deﬁne initial X-form, which is\\nvery essential to eﬃciency improving and several stages of learning.\\nLooking forward to universal learning machine:\\nSince deep learning model is not universal learning machine, naturally, we looking forward to universal\\nlearning machine. We discussed this in [1] and [2]. There, we proved that with certain capabilities,\\nwe can make universal learning machine. Also, we actually invented a concrete universal learning\\nmachine which is in the patent application process. We think universal learning machine has many\\nadvantages over deep learning model. There are many research works needed to be done for universal\\nlearning machine.\\n10\\nWhat really is deep learning?\\nReferences\\n[1] Chuy Xiong. Discussion on Mechanical Learning and Learning Machine, arxiv.org, 2016.\\nhttp://arxiv.org/pdf/1602.00198.pdf\\n[2] Chuy Xiong. Descriptions of Objectives and Processes of Mechanical Learning, arxiv.org, 2017.\\nhttp://arxiv.org/abs/1706.00066.pdf\\n[3] Pedro Domingos. The Master Algorithm, Talks at Google.\\nhttps://plus.google.com/117039636053462680924/posts/RxnFUqbbFRc\\n[4] E. G. Hinton. Learning multiple layers of representation, Trends in Cognitive Sciences, Vol. 11,\\npp 428-434.. http://www.cs.toronto.edu/ hinton/absps/tics.pdf\\n[5] Henry W. Lin, Max Tegmark, David Rolnick. Why does deep and cheap learning work so well?,\\narxiv.org, 2016.\\nhttp://arxiv.org/pdf/1608.08225.pdf\\n[6] Pankaj Mehta, David J. Schwab, David Rolnick. An exact mapping between the Variational\\nRenormalization Group and Deep Learning, arxiv.org, 2014.\\nhttp://arxiv.org/pdf/1410.3831.pdf\\n',\n",
       " '1805.03551v2.pdf': 'A Uniﬁed Framework of Deep Neural Networks by\\nCapsules\\nYujian Li\\nCollege of Computer Science\\nFaculty of Information Technology\\nBeijing University of Technology\\nBeijing, China 100124\\nliyujian@bjut.edu.cn\\nChuanhui Shan\\nCollege of Computer Science\\nFaculty of Information Technology\\nBeijing University of Technology\\nBeijing, China 100124\\nchuanhuishan@emails.bjut.edu.cn\\nAbstract\\nWith the growth of deep learning, how to describe deep neural networks uniﬁedly\\nis becoming an important issue. We ﬁrst formalize neural networks mathematically\\nwith their directed graph representations, and prove a generation theorem about the\\ninduced networks of connected directed acyclic graphs. Then, we set up a uniﬁed\\nframework for deep learning with capsule networks. This capsule framework could\\nsimplify the description of existing deep neural networks, and provide a theoretical\\nbasis of graphic designing and programming techniques for deep learning models,\\nthus would be of great signiﬁcance to the advancement of deep learning.\\n1\\nIntroduction\\nDeep learning has made a great deal of success in processing images, audios, and natural languages\\n[1-3], inﬂuencing academia and industry dramatically. It is essentially a collection of various methods\\nfor effectively training neural networks with deep structures. A neural network is usually regarded as\\na hierarchical system composed of many nonlinear computing units (or neurons, nodes). The most\\npopular neural network was once multilayer perceptron (MLP) [4]. A MLP consists of an input layer,\\na number of hidden layers and an output layer, as shown in Figure 1. The depth of it is the number of\\nlayers excluding the input layer. If the depth is greater than 2, a neural network is now called “deep”.\\nFor training MLPs, backpropagation (BP) is certainly the most well-known algorithm in common\\nuse [4], but it seemed to work only for shallow networks. In 1991, Hochreiter indicated that typical\\ndeep neural networks (DNNs) suffer from the problem of vanishing or exploding gradients [5]. To\\novercome training difﬁculties in DNNs, Hinton et al. started the new ﬁeld of deep learning in 2006\\n[6, 7].\\nBesides deep MLPs, DNNs also include convolutional neural networks (CNNs) and recurrent neural\\nnetworks (RNNs). Here, we omit RNNs for saving space. Theoretically, a CNN can be regarded\\nas a special MLP or feedforward neural network. It generally consists of an input layer, alternating\\nconvolutional and pooling layers, a fully connected layer, and an output layer, as shown in Figure\\n2. Note that “convolutional layers” are also called \"detection layers\", and “pooling layers\" are also\\ncalled “downsampling layers”. There have been a large number of CNN variants, for example, LeNet\\n[8], AlexNet [1], VGGNet [9], GoogLeNet [10], ResNet [11], Faster R-CNN [12], DenseNet [13],\\nMask R-CNN [14], YOLO [15], SSD [16], and so on. They not only take the lead in competitions of\\nimage classiﬁcation and recognition as well as object localization and detection [9-12], but also in\\nother applications such as deep Q-networks [17], AlphaGo [18], speech recognition [2], and machine\\ntranslation [3]. To cope with the disadvantages of CNNs, in 2017 Hinton et al. further proposed a\\ncapsule network [19], which is more convincing from the neurobiological point of view. So many\\ndeep models are dazzling with different structures. Some of them have added shortcut connections,\\nPreprint. Work in progress.\\narXiv:1805.03551v2  [cs.LG]  10 May 2018\\nFigure 1: The structure of a MLP.\\nFigure 2: The structure of a CNN.\\nparallel connections, and even nested structures to traditional layered structures. How to establish a\\nuniﬁed framework for DNNs is becoming a progressively important issue in theory. We are motivated\\nto address it.\\nThis paper is organized as follows. In Section 2, we propose a mathematical deﬁnition to formalize\\nneural networks, give their directed graph representations, and prove a generation theorem about the\\ninduced networks of connected directed acyclic graphs. In Section 3, we use the concept of capsule\\nto extend neural networks, deﬁne an induced model for capsule networks, and establish a uniﬁed\\nframework for deep learning with a universal backpropagation algorithm. Finally, in Section 4 we\\nmake a few conclusions to summarize the signiﬁcance of the capsule framework to advance deep\\nlearning in theory and application.\\n2\\nFormalization of Neural networks\\n2.1\\nMathematical deﬁnition\\nA neural network is a computational model composed of nodes and connections. Nodes are di-\\nvided into input nodes and neuron nodes. Input nodes can be represented by real variables, e.g.\\nx1, x2, · · · , xn . The set of input nodes is denoted as X = {x1, x2, · · · , xn} . A neuron node can\\nreceive signals through connections both from input nodes and the outputs of other neuron nodes,\\nand perform a weighted sum of these signals for a nonlinear transformation. Note that the weight\\nmeasures the strength of a connection, and the nonlinear transformation is the effect of an activation\\nfunction. Let F be a set of activation functions, such as sigmoid, tanh, ReLU, and so on.\\nOn X and F, a neural network can be formally deﬁned as a 4-tuple net = (S, H, W, Y ), where S is\\na set of input nodes, H is a set of neuron nodes, W is a set of weighting connections, and Y is a set\\nof outputs. The neural network is recursively generated by four basic rules as follows:\\n1) Rule of variable. For any z ∈X, let yz = z. If S = {z}, H = ∅, W = ∅, Y = {yz}, then the\\n4-tuple net = (S, H, W, Y ) is a neural network.\\n2) Rule of neuron. For any nonempty subset S ⊆X, ∀f ∈F, ∀b ∈R, construct a node\\nh ̸∈X that depends on (f, b) and select a set of weighting connections wxi→h(xi ∈S). Let\\nyh = f(P\\nxi∈S wxi→hxi + b) be the output of node h. If H = {h}, W = {wxi→h|xi ∈S}, and\\nY = {yh}, then net = (S, H, W, Y ) is a neural network.\\n3) Rule of growth. Suppose net = (S, H, W, Y ) is a neural network. For any nonempty subset\\nN ⊆S ∪H, ∀f ∈F, ∀b ∈R, construct a node h ̸∈S ∪H that depends on (f, b) and select a\\n2\\nFigure 3: (a)A trivial network; (b)A 1-input-1-neuron network.\\nFigure 4: Three 1-input-2-neuron networks.\\nset of weighting connections wzj→h(zj ∈N). Let yh = f(P\\nzj∈N wzj→hyzj + b) be the output\\nof node h. If S′ = S, H′ = H ∪{h}, W ′ = W ∪{wzj→h|zj ∈N}, and Y ′ = Y ∪{yh}, then\\nnet′ = (S′, H′, W ′, Y ′) is also a neural network.\\n4) Rule of convergence. Suppose netk = (Sk, Hk, Wk, Yk)(1 ≤k ≤K) are K neural networks,\\nsatisfying that ∀1 ≤i ̸= j ≤K, (Si ∪Hi) ∩(Sj ∪Hj) = ∅. For any nonempty subsets Ak ⊆\\nSk ∪Hk(1 ≤k ≤K), N = SK\\nk=1 Ak, ∀f ∈F, ∀b ∈R, construct a node h ̸∈SK\\nk=1(Sk ∪Hk) that\\ndepends on (f, b), select a set of weighting connections wz→h(z ∈N). Let yh = f(P\\nz∈N wz→hyz+\\nb) be the output of the node h. If S = SK\\nk=1 Sk, H = (SK\\nk=1 Hk) ∪{h}, W = (SK\\nk=1 Wk) ∪\\n{wz→h|z ∈N}, and Y = (SK\\nk=1 Yk) ∪{yh}, then net = (S, H, W, Y ) is also a neural network.\\nAmong the four generation rules, it should be noted that the rule of neuron is not independent. This\\nrule can be derived from the rule of variable and the rule of convergence. Moreover, the weighting\\nconnection wz→h should be taken as a combination of the weight and the connection, rather than just\\nthe weight. Additionally, if a node h depends on (f, b), f is called the activation function of h, and b\\nis called the bias of h.\\n2.2\\nDirected graph representation\\nLet X be a set of real variables and F be a set of activation functions. For any neural network\\nnet = (S, H, W, Y ) on X and F, a directed acyclic graph Gnet = (V, E) can be constructed with\\nthe vertex set V = S ∪H and the directed edge set E = {z →h|wz→h ∈W}. Gnet = (V, E)\\nis called the directed graph representation of net = (S, H, W, Y ). Two cases of the representation\\ngeneration are discussed in the following.\\n1)The case of X = {x1}\\nUsing the rule of variable, for x1 ∈X, let yx1 = x1. If S = {x1}, H = ∅, W = ∅, and Y = {yx1},\\nthen net = (S, H, W, Y ) is a neural network. Since this network has only one input node without any\\nfunction for nonlinear transformation, it is also called a trivial network, as shown in Figure 3(a). Using\\nthe rule of neuron, for a nonempty subset S = {x1} ⊆X, ∀f ∈F, ∀b ∈R, construct a node h1 ̸∈S\\nthat depends on (f, b), select a weighting connection wx1→h1, and let yh1 = f(wx1→h1x1 + b). If\\nH = {h1}, W = {wx1→h1}, and Y = {yh1}, then net = (S, H, W, Y ) is a neural network, which\\nhas one input and one neuron. It is also called a 1-input-1-neuron network, as shown in Figure 3(b).\\nUsing the rule of growth on the network, three new neural networks with different structures can be\\ngenerated, as shown in Figures 4(a-c). Likewise, they are called 1-input-2-neuron networks. Using\\nthe rule of growth on the three networks, twenty-one new neural networks with different structures\\ncan be totally generated further. Seven out of them for Figure 4(a) are displayed in Figures 5(a-g).\\nThey are called 1-input-3-neuron networks.\\n2)The case of X = {x1, x2}\\nUsing the rule of variable, for x1, x2 ∈X, let yx1 = x1 and yx2 = x2. If S1 = {x1}, S2 = {x2},\\nH1 = H2 = ∅, W1 = W2 = ∅, Y1 = {yx1}, and Y2 = {yx2}, then net1 = ({x1}, ∅, ∅, {yx1}) and\\nnet2 = ({x2}, ∅, ∅, {yx2}) are neural networks. Obviously, both of them are trivial networks. Using\\n3\\nFigure 5: Seven 1-input-3-neuron networks.\\nFigure 6: A 2-input-1-neuron network.\\nthe rule of neuron, for a nonempty subset S ⊆X, if S = {x1} or S = {x2}, the neural network can\\nbe similarly constructed with the case of X = {x1}.\\nIf X = {x1, x2}, ∀f ∈F, ∀b ∈R, construct a node h1 ̸∈S that depends on (f, b), select a set\\nof weighting connections wxi→h1(xi ∈S) and let yh1 = f(P\\nxi∈S wxi→h1xi + b). If H = {h1},\\nW = {wx1→h1, wx2→h1}, and Y = {yh1}, then net = (S, H, W, Y ) is a neural network. This is a\\n2-input-1-neuron network, as depicted in Figure 6. Using the rule of growth on this network, seven\\n2-input-2-neuron networks with different structures can be generated, as shown in Figures 7(a-g).\\nFinally, the rule of convergence is necessary. In fact, it cannot generate all neural networks only using\\nthe three rules of variable, neuron and growth. For example, the network in Figure 8(c) cannot be\\ngenerated without using the rule of convergence on the two in Figures 8(a-b).\\n2.3\\nInduced network and its generation theorem\\nSuppose G = (V, E) is a connected directed acyclic graph, where V denotes the vertex set and E\\ndenotes the directed edge set. For any vertex h ∈V , let INh = {z|z ∈V, z →h ∈E} be the set of\\nvertices each with a directed edge to h, and OUTh = {z|z ∈V, h →z ∈E} be the set of vertices\\nfor h to have directed edges each to. If INh = ∅, then h is called an input node of G. If OUTh = ∅,\\nthen h is called an output node of G. Otherwise, h is called a hidden node of G. Let X stand for\\nFigure 7: Seven 2-input-2-neuron networks.\\n4\\nFigure 8: A necessary explanation for the rule of convergence.\\nthe set of all input nodes, O for the set of all output nodes, and M for the set of all hidden nodes.\\nObviously, V = X ∪M ∪O, and M = V −X ∪O.\\nFurthermore, let yh be the output of node h, and wz→h be the weighting connection from z to h.\\nThen, a computational model of graph G can be deﬁned as follows:\\n1) ∀z ∈X, yz = z.\\n2) ∀h ∈M ∪O, select f ∈F and b ∈R to compute yh = f(P\\nz∈INh wz→hyz + b).\\nIf S = X, H = M ∪O, W = {wz→h|z →h ∈E}, and Y = {yh|h ∈V }, then netG =\\n(S, H, W, Y ) is called an induced network of graph G. The following generation theorem holds on\\nthe induced network.\\nGeneration Theorem: For any connected directed acyclic graph G = (V, E), its induced network\\nnetG is a neural network that can be recursively generated by the rules of variable, neuron, growth,\\nand convergence.\\nProof: By induction on |V | (i.e. number of vertices), we prove the theorem as follows.\\n1) When |V | = 1, we have |X| = 1 and |O| = 0, so the induced network netG is a neural network\\nthat can be generated directly by the rule of variable.\\n2) When |V | = 2, we have |X| = 1 and |O| = 1, so the induced network netG is a neural network\\nthat can be generated directly by the rule of growth.\\n3) Assume that the theorem holds for |V | ≤n. When |V | = n + 1 ≥3, the induced network netG\\nhas at least one output node h ∈O. Let Eh = {z →h ∈E} denote the set of edges heading\\nto the node h. Moreover, let V ′ = V −{h} and E′ = E −Eh. Based on the connectedness of\\nG′ = (V ′, E′), we have two cases to discuss in the following:\\ni) If G′ = (V ′, E′) is connected, then applying the induction assumption for |V ′| ≤n, the\\ninduced network netG′ = (S′, H′, W ′, Y ′) can be recursively generated by the rules of\\nvariable, neuron, growth, and convergence. Let N = INh. In netG = (S, H, W, Y ),\\nwe use f ∈F and b ∈R to stand for the activation function and bias of node h, and\\nwz→h(z ∈N) for the weighting connection from node z to the node h. Then, netG can\\nbe obtained by using the rule of growth on netG′, to generate the node h and its output\\nyh = f(P\\nz∈N wz→hyz + b).\\nii) Otherwise, G′ comprises a number of disjoint connected components Gk = (Vk, Ek)(1 ≤\\nk ≤K). Using the induction assumption for |Vk| ≤n(1 ≤k ≤K), the induced\\nnetwork netGk = (Sk, Hk, Wk, Yk) can be recursively generated by the rules of variable,\\nneuron, growth, and convergence. Let Ak = (Sk ∪Hk) ∩INh, and N = SK\\nk=1 Ak. In\\nnetG = (S, H, W, Y ), we use f ∈F and b ∈R to stand for the activation function and\\nbias of the node h, and wz→h(z ∈N) for the weighting connection from node z to node\\nh. Then, netG can be obtained by using the rule of convergence on netGk(1 ≤k ≤K), to\\ngenerate the node h and its output yh = f(P\\nz∈N wz→hyz + b).\\nAs a result, the theorem always holds.\\n3\\nCapsule framework of Deep learning\\n3.1\\nMathematical deﬁnition of capsules\\nIn 2017, Hinton et al. pioneered the idea of capsules and considered a nonlinear “squashing” capsule\\n[19]. From the viewpoint of mathematical models, a capsule is essentially an extension of the\\n5\\nFigure 9: Mathematical model of a general capsule.\\nFigure 10: The capsule structure of a MLP.\\ntraditional activation function. It is primarily deﬁned as an activation function with a vector input and\\na vector output. More generally, a capsule can be an activation function with a tensor input and a\\ntensor output.\\nAs shown in Figure 9, a general capsule may have n input tensors X1, X2, · · · , Xn, n weight tensors\\nW1, W2, · · · , Wn, and a capsule bias B, and n weighting operations ⊗1, ⊗2, · · · , ⊗n. Note that a\\nweighting operation may be taken as an identity transfer, a scalar multiplication, a vector dot product,\\na matrix multiplication, a convolution operation, and so on. Meantime, Wi ⊗i Xi(1 ≤i ≤n) and B\\nmust be tensors with the same dimension. The total input of the capsule is U = P\\ni Wi ⊗i Xi + B,\\nand the output Y is a tensor computed by a nonlinear capsule function cap, namely,\\nY = cap(U) = cap(\\nX\\ni\\nWi ⊗i Xi + B).\\n(1)\\nFor convenience, we use F to stand for a nonempty set of capsule functions, and T for the set of all\\ntensors.\\n3.2\\nCapsule Networks\\nSuppose G = (V, E) is a connected directed acyclic graph, where V denotes the vertex set and E\\ndenotes the directed edge set. For any vertex H ∈V, let INH be the set of vertices each with a\\ndirected edge to H, and OUTH be the set of vertices for H to have a directed edge each to. If\\nINH = ∅, then H is called an input node of G. If OUTH = ∅, then H is called an output node of\\nG. Otherwise, H is called a hidden node of G. Let X stand for the set of all input nodes, O for the\\nset of all output nodes, and M for the set of all hidden nodes. Obviously, V = X ∪M ∪O, and\\nM = V −X ∪O.\\nFurthermore, let YH be the output of node H, and (WZ→H, ⊗Z→H) be the tensor-weighting con-\\nnection from Z to H. If ∀H ∈M ∪O, ∀Z ∈INH, WZ→H ⊗Z→H YZ and B are tensors with the\\nsame dimension, then a tensor-computational model of graph G can be deﬁned as follows:\\n1) ∀Z ∈X, YZ = Z.\\n2)\\n∀H\\n∈\\nM ∪O,\\nselect\\ncap\\n∈\\nF\\nand\\nB\\n∈\\nT\\nto\\ncompute\\nYH\\n=\\ncap(P\\nZ∈INH WZ→H ⊗Z→H YZ + B).\\nIf S = X, H = M ∪O, W = {(WZ→H, ⊗Z→H)|Z →H ∈E}, and Y = {YH|H ∈V}, then\\nnetG = (S, H, W, Y) is called a tensor-induced network of graph G. This network is also called a\\ncapsule network.\\nUsing a capsule network, a MLP can be simpliﬁed as a directed acyclic path of capsules. For\\nexample, the MLP in Figure 1 has ﬁve layers: an input layer, three hidden layers, and an output\\nlayer. On the whole, each layer could be thought of as a capsule. Let X = (x1, x2, · · · , x5)T\\nstand for the input capsule node, Hi = (capi, Bi)(i = 1, 2, 3) for the hidden capsule nodes, and\\nO = (cap4, B4) for the output capsule node. Note that capsule function capi and capsule bias Bi are\\ndeﬁned by the elementwise activation function and the bias vector respectively of the correspond-\\ning layer in the MLP. If the weighting operations ⊗X→H1, ⊗H1→H2, ⊗H2→H3, and ⊗H3→O are\\n6\\nFigure 11: The Capsule structure of a CNN, with “∗” standing for convolution, “→” for identity\\ntransfer, “◁” for tensor reshaping, and “×” for matrix multiplication.\\nall taken as matrix multiplication “×”, then we have (WX→H1, ⊗X→H1) = ((wX→H1\\nm,n\\n)7×5, ×),\\n(WH1→H2, ⊗H1→H2) = ((wH1→H2\\nm,n\\n)7×7, ×), (WH2→H3, ⊗H2→H3) = ((wH2→H3\\nm,n\\n)7×7, ×) and\\n(WH3→O, ⊗H3→O) = ((wH3→O\\nm,n\\n)4×7, ×), which are the tensor-weighting connections from X\\nto H1, H1 to H2, H2 to H3 and H3 to O. Finally, let YHi(i = 1, 2, 3) stand for the output vec-\\ntor of Hi, and YO for the output vector of O. Setting YHO = X and YH4 = YO, we obtain\\nYHi = capi(WHi−1→Hi × YHi−1 + Bi). Therefore, the capsule structure of the MLP is a directed\\nacyclic path, as displayed in Figure 10.\\nBesides MLPs, capsule networks can also be used to simplify the structures of other DNNs. Let\\nus consider the CNN in Figure 2. This CNN has 7 layers: one input layer, two convolutional lay-\\ners, two downsampling (pooling) layers, one fully connected layer, and one output layer. On the\\nwhole, each of the layers could be thought of as a capsule. Let X stand for the input capsule node,\\nHi = (capi, Bi)(i = 1, · · · , 5) for the hidden capsule nodes, and O = (cap6, B6) for the output cap-\\nsule node. Note that cap1 and cap3 are capsule functions deﬁned by elementwise ReLUs. cap2 and\\ncap4 are capsule functions deﬁned by downsampling “↓”. cap5 is an identity function. cap6 is a cap-\\nsule function deﬁned by softmax. In addition, Bi(i = 1, · · · , 6) are capsule biases each deﬁned by the\\nbias tensor of the corresponding layer in the CNN. Let both ⊗X→H1 and ⊗H2→H3 be the convolution\\noperation “∗”, both ⊗H1→H2 and ⊗H3→H4 be the identity transfer “→”, ⊗H4→H5 be the tensor-\\nreshaping operation “◁”, and ⊗H5→O be the matrix multiplication “×”. Then, (WX→H1, ⊗X→H1) =\\n(WX→H1, ∗), (WH1→H2, ⊗H1→H2)\\n=\\n(””, →), (WH2→H3, ⊗H2→H3)\\n=\\n(WH2→H3, ∗),\\n(WH3→H4, ⊗H3→H4) = (””, →), (WH4→H5, ⊗H4→H5) = (””, ◁), and (WH5→O, ⊗H5→O) =\\n(WH5→O, ×), which are the tensor-weighting connections from X to H1, H1 to H2, H2 to H3, H3\\nto H4, H4 to H5, and H5 to O. Finally, let YHi(i = 1, 2, 3, 4, 5) stand for the output tensor of Hi,\\nand YO for the output tensor of O. This leads to the following computations:\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nYH1 = cap1(WX→H1 ∗X + B1) = ReLU(WX→H1 ∗X + B1),\\nYH2 = cap2(WH1→H2 ⊗H1→H2 YH1 + B2) = cap2(→YH1 + B2) =↓YH1 + B2,\\nYH3 = cap3(WH2→H3 ∗X + B3) = ReLU(WH2→H3 ∗YH2 + B3),\\nYH4 = cap4(WH3→H4 ⊗H3→H4 YH3 + B4) = cap4(→YH3 + B4) =↓YH3 + B4,\\nYH5 = cap5(◁YH4 + B5) = ◁YH4,\\nYO = cap6(WH5→O × YH5 + B6) = softmax(WH5→O × YH5 + B6).\\n(2)\\nTherefore, the capsule structure of the CNN is also a directed acyclic path, as depicted in Figure 11.\\nBesides simplifying the description of existing DNNs, the capsule networks can also be used to\\ngraphically design a variety of new structures for complex DNNs, such as displayed in Figure 12.\\n3.3\\nUniversal backpropagation of capsule networks\\nSuppose G = (V, E) is a connected directed acyclic graph. Let X = {X1, X2, · · · , Xn} stand\\nfor the set of all input nodes, O = {O1, O2, · · · , Om} for the set of all output nodes, and M =\\nV −X ∪O = {H1, H2, · · · , Hl} for the set of all hidden nodes. netG = (S, H, W, Y) is an tensor-\\ninduced network of graph G. This is also a capsule network. If the number of nodes |S ∪H| ≥2,\\nthen for ∀H ∈H,\\n\\x1aUH = P\\nZ∈INH WZ→H ⊗Z→H YZ + BH,\\nYH = capH(UH) = capH(P\\nZ∈INH WZ→H ⊗Z→H YZ + BH).\\n(3)\\nFor any output node H ∈O, let YH and TH be its actual output and expected output for input X,\\nrespectively. The loss function between them is deﬁned as LH = Loss(YH, TH). Accordingly, we\\n7\\nFigure 12: Structure of a general capsule network.\\nAlgorithm 1: One iteration of the universal backpropagation algorithm.\\n1) Select a learning rate η > 0,\\n2) ∀H ∈M ∪O, ∀Z ∈INH, initialize WZ→H and BH,\\n3) ∀H ∈O, compute δH = ∂Loss(YH,TH)\\n∂YH\\n· ∂capH\\n∂UH ,\\n4) ∀H ∈M, compute δH = P\\nP ∈OUTH δP · ∂UP\\n∂YH · ∂capH\\n∂UH ,\\n5) Compute ∆WZ→H = δH ·\\n∂UH\\n∂WZ→H and ∆BH = δH,\\n6) Update WZ→H ←WZ→H −η · ∆WZ→H,BH ←BH −η · ∆BH.\\nhave the total loss function L = P\\nH∈O LH. Let δH =\\n∂L\\n∂UH denote the backpropagated error signal\\n(or sensitivity) for capsule node H. By the chain rule, we further obtain:\\n∀H ∈O,\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nδH\\n=\\n∂L\\n∂UH = ∂Loss(YH,TH)\\n∂YH\\n· ∂capH\\n∂UH ,\\n∂L\\n∂BH\\n=\\n∂L\\n∂UH · ∂UH\\n∂BH = δH,\\n∂L\\n∂WZ→H\\n=\\n∂L\\n∂UH ·\\n∂UH\\n∂WZ→H = δH ·\\n∂UH\\n∂WZ→H .\\n(4)\\n∀H ∈M,\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδH\\n=\\n∂L\\n∂UH = P\\nP ∈OUTH\\n∂L\\n∂UP · ∂UP\\n∂YH · ∂YH\\n∂UH\\n= P\\nP ∈OUTH δP · ∂UP\\n∂YH · ∂capH\\n∂UH ,\\n∂L\\n∂BH\\n=\\n∂L\\n∂UH · ∂UH\\n∂BH = δH,\\n∂L\\n∂WZ→H\\n=\\n∂L\\n∂UH ·\\n∂UH\\n∂WZ→H = δH ·\\n∂UH\\n∂WZ→H .\\n(5)\\nNote that in formulae (4)-(5), ∂capH\\n∂UH\\ndepends on the speciﬁc form of capsule function capH. For\\nexample, when capH is an elementwise sigmoid function, the result is ∂capH\\n∂UH\\n= sigmoid(UH)(1 −\\nsigmoid(UH)). Meanwhile,\\n∂UH\\n∂WZ→H and ∂UP\\n∂YH also depend on the speciﬁc choice of the weighting\\noperation ⊗Z→H.\\nBased on formulae (4)-(5), a universal backpropagation algorithm can be designed theoretically for\\ncapsule networks, with one iteration detailed in Algorithm 1. In practice, this algorithm should be\\nchanged to one of many variants with training data [20].\\n4\\nConclusions\\nBased on the formalization of neural networks, we have developed capsule networks to establish a\\nuniﬁed framework for deep learning. This capsule framework could not only simplify the description\\nof existing DNNs, but also provide a theoretical basis of graphical designing and programming for\\nnew deep learning models. As future work, we will try to deﬁne an industrial standard and implement\\na graphic platform for the advancement of deep learning with capsule networks, and even with a\\nsimilar extension to recurrent neural networks.\\n8\\nReferences\\n[1] Krizhevsky, A., Sutskever, I. & Hinton, G.E. (2012) Imagenet classiﬁcation with deep convolutional neural\\nnetworks. In F. Pereira, C.J.C. Burges, L. Bottou and K.Q. Weinberger (eds.), Advances in neural information\\nprocessing systems 25, pp. 1097–1105. Cambridge, MA: MIT Press.\\n[2] Amodei, D., Ananthanarayanan, S. & Anubhai, R. et al. (2016) Deep speech 2: End-to-end speech recognition\\nin English and Mandarin. International Conference on Machine Learning, pp. 173–182.\\n[3] Wu, Y., Schuster, M. & Chen, Z. et al. (2016) Google’s Neural Machine Translation System: Bridging the\\nGap between Human and Machine Translation. arXiv preprint arXiv:1609.08144.\\n[4] Rumellhart, D.E. (1986) Learning internal representations by error propagation. Parallel distributed\\nprocessing: Explorations in the microstructure of cognition 1:319-362.\\n[5] Schmidhuber, J. (2014) Deep learning in neural networks: An overview. Neural Network 61:85-117.\\n[6] Hinton, G.E. & Salakhutdinov, R.R. (2006) Reducing the dimensionality of data with neural networks.\\nScience 313(5786):504-507.\\n[7] Hinton, G.E., Osindero, S. & Teh, Y.W. (2006) A fast learning algorithm for deep belief nets. Neural\\ncomputation 18(7):1527-1554.\\n[8] LeCun, Y., Bottou, L. & Bengio Y, et al. (1998) Gradient-based learning applied to document recognition.\\nProceedings of the IEEE 86(11):2278-2324.\\n[9] Simonyan, K. & Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition.\\nComputer Science.\\n[10] Szegedy, C. Liu, W. & Jia, Y. et al. (2015) Going deeper with convolutions. IEEE Conference on Computer\\nVision and Pattern Recognition.\\n[11] He, K. Zhang, X. & Ren, S. et al. (2016) Deep residual learning for image recognition. Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pp. 770–778\\n[12] Ren, S., He, K. & Girshick, R. et al. (2015) Faster r-cnn: Towards real-time object detection with region\\nproposal networks. Advances in neural information processing systems, pp. 91–99.\\n[13] Huang, G., Liu, Z. & Weinberger, K.Q. et al. (2017) Densely connected convolutional networks. Proceedings\\nof the IEEE conference on computer vision and pattern recognition, pp. 1(2):3.\\n[14] He, K., Gkioxari, G. & Dollár, P. et al. (2017) Mask r-cnn. Computer Vision (ICCV), 2017 IEEE International\\nConference on. IEEE, pp. 2980–2988.\\n[15] Redmon, J., Divvala, S. & Girshick, R. et al. (2016) You only look once: Uniﬁed, real-time object detection.\\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788.\\n[16] Liu, W., Anguelov, D. & Erhan, D. et al. (2016) Ssd: Single shot multibox detector. European conference\\non computer vision. Springer, Cham, pp. 21–37.\\n[17] Mnih, V., Kavukcuoglu, K. & Silver, D. et al. (2015) Human-level control through deep reinforcement\\nlearning. Nature 518(7540):529.\\n[18] Silver, D., Schrittwieser, J. & Simonyan, K. et al. (2017) Mastering the game of Go without human\\nknowledge. Nature 550(7676):354-359.\\n[19] Sabour, S., Frosst, N. & Hinton, G.E. (2017) Dynamic routing between capsules. In I. Guyon, U.V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R. Garnett (eds.), Advances in Neural Information\\nProcessing Systems 30, pp. 3859-3869. Cambridge, MA: MIT Press.\\n[20] Ruder, S. (2016) An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.\\n9\\n',\n",
       " '1805.04825v1.pdf': '1 \\n \\nDeep Learning in Software Engineering \\nXiaochen Li1, He Jiang1,2, Zhilei Ren1, Ge Li3, Jingxuan Zhang4 \\n1School of Software, Dalian University of Technology \\n2School of Computer Science & Technology, Beijing Institute of Technology \\n3Software Institute, Peking University \\n4College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \\nli1989@mail.dlut.edu.cn, jianghe@dlut.edu.cn, zren@dlut.edu.cn,  \\nlige@pku.edu.cn, jingxuanzhang@mail.dlut.edu.cn \\nAbstract \\nRecent years, deep learning is increasingly prevalent in the field of Software \\nEngineering (SE). However, many open issues still remain to be investigated. How do \\nresearchers integrate deep learning into SE problems? Which SE phases are \\nfacilitated by deep learning? Do practitioners benefit from deep learning? The \\nanswers help practitioners and researchers develop practical deep learning models \\nfor SE tasks. To answer these questions, we conduct a bibliography analysis on 98 \\nresearch papers in SE that use deep learning techniques. We find that 41 SE tasks in \\nall SE phases have been facilitated by deep learning integrated solutions. In which, \\n84.7% papers only use standard deep learning models and their variants to solve SE \\nproblems. The practicability becomes a concern in utilizing deep learning techniques. \\nHow to improve the effectiveness, efficiency, understandability, and testability of \\ndeep learning based solutions may attract more SE researchers in the future. \\nIntroduction \\nDriven by the success of deep learning in data mining and pattern recognition, recent \\nyears have witnessed an increasing trend for industrial practitioners and academic \\nresearchers to integrate deep learning into SE tasks [1]-[3]. For typical SE tasks, deep \\nlearning helps SE participators extract requirements from natural language text [1], \\ngenerate source code [2], predict defects in software [3], etc. As an initial statistics of \\nresearch papers in SE in this study, deep learning has achieved competitive \\nperformance against previous algorithms on about 40 SE tasks. There are at least 98 \\nresearch papers published or accepted in 66 venues, integrating deep learning into \\nSE tasks.  \\nDespite the encouraging amount of papers and venues, there exists little overview \\nanalysis on deep learning in SE, e.g., the common way to integrate deep learning into \\nSE, the SE phases facilitated by deep learning, the interests of SE practitioners on deep \\nlearning, etc. Understanding these questions is important. On the one hand, it helps \\npractitioners and researchers get an overview understanding of deep learning in SE. \\nOn the other hand, practitioners and researchers can develop more practical deep \\nlearning models according to the analysis. \\nFor this purpose, this study conducts a bibliography analysis on research papers in \\nthe field of SE that use deep learning techniques. In contrast to literature reviews, \\n2 \\n \\nbibliography analysis can reflect the overview trends, techniques, topics on deep \\nlearning in SE by statistical data. First, we collect 4,443 research papers that contain \\nboth SE and deep learning related keywords. Next, we filter the research papers by \\nreading their contents, citations and references. Finally, 98 research papers related \\nto both SE and deep learning are identified. With these papers, we analyze the \\npublication trend, research topics, used deep learning models, and industrial research \\ninterests of these papers. \\nWe find that research papers related to deep learning has increased significantly in \\nSE in recent years, which have facilitated 41 SE tasks. Both communities of SE and \\nArtificial Intelligent (AI) show great interests in utilizing deep learning in SE. \\nSurprisingly, more than one fifth research papers have industrial practitioners to \\nparticipate in, which means that industrial practitioners are also interested in \\nintegrating deep learning into their SE solutions. Despite the encouraging success of \\ndeep learning, we find several concerns about using deep learning in SE. Practitioners \\nand researchers worry about the practicability of utilizing a complex method with \\nalmost opaque internal representations like deep learning [6]. Hence, the \\neffectiveness and efficiency [7], understandability [8], and testability [9] become the \\nburden to use deep learning in practice. Fortunately, recent studies have conducted \\nsome initial investigation on these problems [6]-[9]. These findings may guide the \\nfuture studies of using deep learning in SE. \\nExample of using deep learning in SE \\nDeep learning is a technique that allows computational models composed of multiple \\nprocessing layers to learn representations of data with multiple levels of abstraction \\n[14]. In this section, we present an example of using deep learning in SE. In this \\nexample, we apply the deep learning model AutoEncoder on a typical SE task, i.e., bug \\nreports summarization [10].  \\nSE Task\\nSE Data\\nCollection\\nSE Data \\nPreprocessing\\nModel \\nSelection & \\nConfiguration\\nInput\\nConstruction\\nModel \\nTraining\\nApplying\\nModels\\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\nRaw SE \\nData \\nProcessed \\nSE Data \\nDesigned  \\nModel  \\nInput Vectors \\n& Designed \\nModel  \\nTrained \\nModel \\nFig. 1. A framework to summarize bug reports with AutoEncoder \\n \\n3 \\n \\nBug reports are texts to describe the bugs in software. Facing numerous bug \\nreports, bug report summarization aims to generate a summary by extracting and \\nhighlighting informative sentences of a bug report to shorten the reading time. To \\nidentify informative sentences, researchers utilize AutoEncoder to encode the words \\nin bug report sentences in an unsupervised way. Since the hidden state of \\nAutoEncoder provides a compressed representation of the input data, the weights of \\nwords in a bug report can be measured by calculating how much information of a \\nword is reserved in the hidden states. Based on the word weights, informative \\nsentences are identified [10]. As shown in Fig. 1, the example consists of six steps.  \\n1. SE data collection decides the available data for an SE task. For bug report \\nsummarization, the commonly available data are bug reports. Each bug report \\nmainly contains a title, a description of the bug, and several comments.  \\n2. SE data preprocessing removes the noises in SE data. For a bug report, the \\nEnglish stop words and some programming-specific ones are the noises. \\nBesides, extremely short sentences are also noises, since they may be \\nuninformative. \\n3. Model selection and configuration select suitable deep learning models for \\nSE data and decide model configurations, e.g., the number of layers and neural \\nunits of each layer. The widely used deep learning models include \\nAutoEncoder, CNN, RNN, etc. (explained in Fig. 3). These standard models \\nusually have several variants, e.g., LSTM, Bi-LSTM, and attention-based RNN \\nare classical variants of RNN. In this example, AutoEncoder is selected. \\nAutoEncoder usually has a symmetric architecture, i.e., the number of neural \\nunits of input and output layers are the same. The output layer is defined as a \\npattern to reconstruct the input layer. The number of neural units of hidden \\nlayers decreases as towards the middle of the network. After training, the \\nhidden states reserve the key information for reconstructing the input layer. \\n4. Input construction transforms SE data into vectors for deep learning models. \\nFor bug report summarization, researchers calculate the word frequency in \\nbug reports and transform the word frequency values into vectors. These \\nvectors are regarded as a training set for AutoEncoder.   \\n5. Model training trains the designed model with the training set. A deep \\nlearning model usually has thousands of parameters representing the weights \\nof connections among neural units. Hence, training the model is to tune these \\nparameters according to the training set. For AutoEncoder, the parameters are \\ntrained by minimizing the difference between the input and output layers in \\nan unsupervised way.  \\n6. Applying models is to utilize the trained model to solve SE problems. In this \\nexample, the trained model can encode the word frequency vector of a new \\nbug report into the hidden states. We can trace and calculate the changes of \\nthe value in each vector dimension along with the encoding process, and then \\ndeduce the weights of words in each dimension. These word weights help \\nresearchers assign weights of the sentences and select informative ones.  \\n4 \\n \\nData collection \\nTo collect deep learning related papers in SE, we design three criteria to search \\nresearch papers from four well-known digital libraries, including Web of Science, \\nACM Digital Library, IEEE Xplore, and Scopus. \\nC1. Research papers should contain at least one of the following SE phrases, \\nincluding \"software engineer*\", \"software develop*\", \"software test*\", \\n\"software design\", \"requir* analysis\", \"software requir*\", \"software \\nmaintain*\", and \"software manag*\". The sign \"*\" is a wildcard character to \\nmatch zero or more characters in a word. \\nC2. Research papers should contain at least one phrase about deep learning \\nconcept, i.e., \"deep learn*\" and \"neural network*\". \\nC3. Research papers are conference or journal papers written in English on the \\ntopic of computer science. \\nUnder these criteria, we achieve 4,443 candidate research papers published before \\nMarch 2018, including 414 from Web of Science, 207 from ACM Digital Library, 2,271 \\nfrom IEEE Xplore, and 1,551 from Scopus. We remove the duplicate papers and short \\npapers with less than 4 pages. At last, 3,351 research papers are reserved. We \\ndownload and manually examine the contents of the papers:  \\n1. We remove 35 papers that the full-contents cannot be downloaded.  \\n2. We remove 2,441 papers that the searching phrases in C1 and C2 merely \\nmatch some supplementary information in the paper. For example, \"software \\nengineer*\" may match the phrase of \"school of software engineering\" in author \\nbiography or the publication venue \"Transaction on Software Engineering\". \\n3. After step 1 and 2, another 812 papers are removed as they do not focus on SE \\nor deep learning. For example, \"deep learning\" is also a concept in computer \\neducation and \"neural network\" may refer to a shallow network structure with \\na single hidden layer. \\nAt last, 63 research papers are remained. We take these papers as seeds to search \\ntheir references and citations. If a new SE research paper about deep learning is found, \\nwe recursively examine the new paper. Finally, another 35 research papers are found. \\nHence, we collect in total 98 published or accepted research papers for analysis. \\nBibliography Analysis \\nWe analyze the collected papers to investigate the status of deep learning in SE. \\nA. The prevalence of deep learning in SE \\nWe count the number of research papers each year and the venues of the publications \\nin Fig. 2(a) and Fig. 2(b) respectively. In Fig. 2(a), we find that deep learning attracts \\nlittle attention in SE for a long time, i.e., only less than 3 papers are published each \\nyear before 2015. The reason may be that although deep learning performs well on \\nimage processing, speech recognition, etc., it takes time for the practitioners and \\n5 \\n \\nresearchers in SE to validate deep learning on domain-specific SE tasks. However, the \\nresearches boom in SE after 2015, e.g., 28 publications in 2016 and 39 publications in \\n2017. Furthermore, only in the first three months in 2018, there are already 12 \\npublications using deep learning in SE. \\nFor these research papers, we count the publication venues. Surprisingly, out of the \\n98 SE papers, 66 venues have published at least one paper on the topic of deep \\nlearning. Fig. 2(b) presents the publication venues that publish more than one paper \\nWe explain these venue names in Fig. 2(c). We find that using deep learning in SE \\nattracts the attention from both communities of SE and AI, including some premier \\nSE venues like ICSE, ESEC/FSE, ASE, ICSME, ICPC and some renowned AI venues like \\nAAAI, ICLR, ICML, NIPS, ACL, IJCAI. These venues may be a good guidance to study the \\nprogress of deep learning in SE.  \\nTo conclude, deep learning is prevalent in SE. It attracts the attention from both SE \\nand AI communities. \\nB. The way to integrate deep learning into SE \\nAs the prevalence of deep learning in SE, we analyze the way to integrate deep \\nlearning into SE. Fig. 3 shows the name of deep learning models and the number of \\npapers using these models. We find that most studies (55 papers) directly transfer \\nstandard deep learning models into SE, including AutoEncoder, CNN, DBN, RNN and \\na simple fully-connected DNN. Meanwhile, the classical variants of these models in AI \\nare also widely used (28 papers) such as SDAEs, LSTM, etc. The above models are \\nused in 84.7% research papers. Besides using a single model, combined deep learning \\n(c) Full names of publication venues \\nVenue \\nExplanation \\nICSE \\nInt’l Conf.on Softw. Eng. \\nESEC/FSE Joint European Softw. Eng. Conf. and Symposium on \\nthe Foundations of Softw. Eng. \\nASE \\nInt’l Conf. on Automated Softw. Eng. \\nICSME \\nInt’l Conf. on Softw. Maintenance and Evolution \\nAAAI \\nAAAI Conf. on Artificial Intelligence \\nICLR \\nInt’l Conf. on Representation Learning \\nICML \\nInt’l Conf. on Machine Learning \\nICPC \\nInt’l Conf. on Program Comprehension \\nNIPS \\nConf. on Neural Information Processing Systems \\nACL \\nAnnual Meeting of the Association for Computational \\nLinguistics \\nESWA \\n \\nExpert Systems With Applications \\nIJCAI \\nInt’l Joint Conf. on Artificial Intelligence \\nIST \\nInformation and Softw. Tech. \\nKSEM \\nInt’l Conf. on Knowledge Science, Eng. and \\nManagement \\nQRS \\nInt’l Conf. on Quality Softw \\nSEKE \\nInt’l Conf. on Softw. Eng. and Knowledge Eng. \\nSNAPL \\nSummit on Advances in Programming Languages \\n \\n6\\n4\\n4\\n4\\n3\\n3\\n3\\n3\\n3\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n2\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7ASEESEC/FSEICSEICSMEAAAIICLRICMLICPCNIPSACLESWAIJCAIISTKSEMQRSSEKESNAPL\\nNumber of papers\\nJournal/Conference\\n(b) Top venues of the publications\\n1\\n1\\n1\\n3\\n1\\n2\\n1\\n1\\n1\\n7\\n28\\n39\\n12\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n2000 2003 2007 2008 2009 2010 2011 2013 2014 2015 2016 2017 2018\\nNumber of papers\\nYear\\n(a) The number of publications per year\\nFig. 2. Basic information of deep learning in SE \\n6 \\n \\nmodels (8 papers) also show competitiveness in SE, e.g., a combination of RNN and \\nCNN. For the remaining papers, researchers design specific deep learning \\narchitectures for SE data like Stepped AutoEncoder and TBCNN. The above \\nphenomenon suggests that when integrating deep learning into SE tasks, a new \\npractitioner may be willing to first try some standard models and their variants to \\ninvestigate whether deep learning works or not. \\nFurthermore, we investigate what types of SE data are usually fed into these \\nmodels. We analyze the inputs of the 98 papers. The inputs can be categorized into \\nfive categories. \\n1. Predefined software metrics (25 papers). Researchers first manually define and \\ncalculate some software metrics, e.g., lines of code, the number of bugs in source \\ncode. Then, they construct vectors based on the values of these metrics to feed \\ninto deep learning models. \\n2. Dynamic software status (14 papers). This category takes the dynamic \\ninformation when running the software as input, e.g., the CPU utilization, the \\ninvoked APIs. The values of these dynamic information can be transformed into \\nvectors for deep learning models. \\n3. Raw text or source code without sequence (32 papers). These papers treat the \\nbag-of-words of text and source code as deep learning input without \\nExplanation of abbreviation \\nAutoEncoder\\nAE\\n• SAE: Stacked AE\\n• SDAE: Stacked Denoising AE\\nConvolutional Neural Network\\nCNN\\n• NPCNN: Natural language and Programming \\nlanguage CNN\\n• TBCNN: Tree-Based CNN\\nDeep Belief Network\\nDBN\\nRecurrent Neural Network\\nRNN\\n• LSTM: Long Short-term Memory\\n• Bi-LSTM: Bi-Directional LSTM\\n• R3NN: Reverse-Recursive-Reverse NN\\n• RNNBPTT: RNN with Backpropagation \\nThrough Time\\nMIX\\n• DNN: Deep Neural Network\\n• ANFIS: Artificial Neural network with Fuzzy \\nInference System\\n• FG-HPNN: Fuzzy Granule-based Hierarchical \\nPolynomial NN\\n• FNN: Fuzzy Neural Network,\\n• LPN: Latent Predictor Network\\n• PNN: Probabilistic NN\\n• RFNN: Rule-based Fuzzy NN\\n2\\n1\\n4\\n1\\n12\\n1\\n2\\n3\\n13\\n3\\n9\\n1\\n1\\n1\\n2\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n25\\n4\\n1\\n1\\n1\\n1\\n1\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nstandard\\nSAEs\\nSDAEs\\nstepped AE\\nstandard\\nNPCNN\\nTBCNN\\nstandard\\nstandard\\nattention-based RNN\\nLSTM\\nR3NN\\nRNNBPTT\\nStackAugment RNN\\nBi-LSTM\\nDBN+SAEs\\nLSTM+RNN\\nAE+DNN\\nsequential & averaged RNNs\\nRNN+CNN\\nBi-LSTM+CNN\\nBi-LSTM+DNN\\nBi-LSTM+RNN\\n(fully-connected) DNN\\nANFIS\\nFG-HPNN\\nFNN\\nLPNs\\nPNN\\nRFNN\\nAE\\nCNN\\nD\\nRNN\\nMIXED\\nOther\\nNumber of papers\\nDeep learning models\\nFig. 3. Deep learning models in the research papers \\nDBN \\n7 \\n \\nconsidering word sequences [10]. Based on the bag-of-words, word embedding, \\none-hot representation and word frequency are widely used to transform \\nwords into vector space for deep learning. \\n4. Raw text or source code in sequence (22 papers). In contrast to category 3, this \\ncategory considers the sequence of words [2].  Such inputs are usually \\nassociated with RNN-based models, which utilizes the order of words to predict \\nthe next word or class label of software documents and source code, e.g., \\nprogram learning and program synthesis. \\n5. Others (5 papers). Most of the other inputs are multimedia data such as images. \\nFor example, researchers utilize images to test deep learning models [9]. The \\npixels of the images are fed into deep learning models. \\nTo conclude, practitioners and researchers can achieve competitive results on \\nmore than 80% SE problems when only using standard deep learning models and \\ntheir variants. Deep learning can well handle many types of SE data, including \\npredefined software metrics, dynamic software status, and raw text or source code. \\nC. The SE phases facilitated by deep learning \\nDue to the diversity of SE tasks, it is important to identify the existing SE tasks \\nfacilitated by deep learning, since it helps practitioners find the potential to leverage \\ndeep learning in their own problems. \\nAs suggested by classical SE models, i.e., Waterfall Model and Incremental Model \\n[11], SE can be divided into five phases, including requirement analysis, software \\ndesign, development, testing and maintenance. In addition, since SE is an activity \\ninvolving many stakeholders (developers, testers, project managers, etc.), we also \\nadd project management as an SE phase. Fig. 4 shows the SE tasks facilitated by deep \\nlearning in the six phases.  \\nAs shown in Fig. 4, researchers have tried deep learning on at least 41 SE tasks. In \\nrequirement analysis, deep learning helps requirement analysts automatically \\nextract requirements from natural language texts [1]. In software design, design \\npatterns of software can be recognized [12]. In software development, deep learning \\nhelps developers on 14 SE tasks from 30 research papers, including program learning \\nand program synthesis [2], code suggestion [6], etc. Besides, software testing and \\nmaintenance are also major phases to attempt deep learning. There are 54 research \\npapers in these two phases which cover 21 SE tasks like defect prediction [3] and \\nreliability or changeability estimation [4]. For the 41 SE tasks, program learning and \\nprogram synthesis [2], malware detection [5], defect prediction [3], reliability or \\nchangeability estimation [4], and development cost or effort estimation [13] are the \\ntop 5 tasks studied by the researchers. Hence, practitioners may have a board \\nselection of methodologies and deep learning models when using deep learning on \\nthese tasks. \\nTo conclude, deep learning has facilitated at least 41 SE tasks in all SE phases, \\nincluding requirement analysis, software design, development, testing, maintenance, \\nand project management. \\n8 \\n \\nD. Research interests of industrial practitioners  \\nWe analyze the SE tasks participated by industrial practitioners to understand \\nresearch interests in practice. \\nThe industrial practitioners are identified when at least one author affiliation in the \\nauthor list of a research paper is a company. In Fig. 4, we label the industry-\\nparticipated SE tasks in bold and list the company names. To our surprise, there are \\n21 research papers (more than one fifth) on 13 SE tasks with at least one industrial \\npractitioner, which implies the interest of industrial practitioners in integrating deep \\nlearning into SE problems. Among the 13 tasks, program learning and program \\nsynthesis attract the most attention [2]. Eight research papers from four companies \\nhave tried deep learning on this task, including DeepMind, Facebook, Google, and \\nMicrosoft. Besides, practitioners also apply deep learning on SE tasks like malware \\ndetection [5], development cost or effort estimation [13], etc., and achieve \\ncompetitive results. Hence, deep learning may be useful on these tasks from the \\nperspective of practitioners. This finding provides a guidance for academic \\nresearchers to apply deep learning in practice. \\nHowever, we find a mismatch from the top researched SE tasks and the industrial \\ninterests. For the top 5 tasks studied by deep learning in Fig. 4, only program learning \\n# \\nTasks in development (30 papers) \\nC1 Program learning and program \\nsynthesis (14) \\nC2 Automatic software repair (2) \\nC3 Code suggestion (2) \\nC4 Knowledge unit linking in Stack \\nOverflow (2) \\nC5 Autonomous driving software (1) \\nC6 API description selection (1) \\nC7 API sequence recommendation (1) \\nC8 Cross-lingual question retrieval (1) \\nC9 Code comment generation (1) \\nC10 Commit message generation (1) \\nC11 Hot path prediction (1) \\nC12 Just-in-time defection prediction (1) \\nC13 Model visualization (1) \\nC14 Source code summarization (1) \\n# \\nTasks in design (1 papers) \\nB1 Design pattern recognition (1) \\n# \\nTasks in Testing (27 papers) \\nD1 Defect prediction (9) \\nD2 Reliability or changeability \\nestimation (8) \\nD3 Deep learning testing (3) \\nD4 Energy consumption estimation (1) \\nD5 Grammar-based fuzzing testing (1) \\nD6 Retesting necessity estimation (1) \\nD7 Reliability model selection (1) \\nD8 Robot testing (1) \\nD9 Test input generation for mobile (1)\\nD10 Testing effort estimation (1) \\n# Tasks in requirement (1 paper) \\nA1 Requirement extraction from natural \\nlanguages (1)  \\n \\n# Tasks in management (12 papers) \\nF1 Development cost or effort \\nestimation (6) \\nF2 Source code classification (4) \\nF3 Software size estimation (1) \\nF4 Traceable link generation (1) \\n \\n# Tasks in maintenance (27papers)\\nE1 Malware detection (10) \\nE2 Bug localization (4) \\nE3 Clone detection (3) \\nE4 System anomaly prediction (2) \\nE5 Workload prediction in the cloud (2) \\nE6 Bug report summarization (1) \\nE7 Bug triager (1) \\nE8 Duplicate bug report detection (1) \\nE9 Feature location (1) \\nE10 Real-time task scheduling (1) \\nE11 Test report classification (1) \\n \\nIndustrial practitioners participate \\nin 13 SE tasks (21 papers) \\n• C1: DeepMind, Facebook, Google, \\nMicrosoft (8 papers) \\n• C5: Fiat Chrysler Automobiles (1) \\n• C7: Microsoft (1) \\n• C11: Clinc Inc. (1) \\n• C13: Facebook (1) \\n• D2: URU Video, Inc. (1) \\n• D5: Microsoft (1) \\n• D9: IBM (1) \\n• E1: Baidu, Microsoft (2) \\n• E4: Tencent Corporation (1) \\n• E8: Accenture Tech. (1) \\n• E9: ABB Corporate (1) \\n• F1: Motorola Canada Ltd. (1) \\nFig. 4. The SE tasks solved by deep learning and participated by industrial practitioners \\n9 \\n \\nand program synthesis, and malware detection attract more than one industrial \\npractitioner to participate in. The reason may be that, on the one hand, practitioners \\nhave not found a suitable way to apply deep learning on other SE tasks in practice. On \\nthe other hand, practitioners already select some lightweight methods to solve these \\ntasks. Hence, there is still a long way to apply a complex method like deep learning in \\nindustry. \\nTo conclude, practitioners participate in more than one fifth research papers. They \\nbenefit from deep learning on 13 SE tasks, including program learning and program \\nsynthesis, malware detection, etc. \\nE. Concerns to use deep learning in SE \\nDespite the prevalence of improving SE tasks with deep learning, many concerns \\nemerge on the practicability of using deep learning in SE [6]. As a complex and almost \\nopaque model, several factors limit the practicability of deep learning, including the \\neffectiveness, efficiency, understandability, and testability. These issues may \\ninfluence the development of deep learning in SE.  \\nEffectiveness and Efficiency. Recent studies show that by applying a simple \\noptimizer Differential Evolution to fine tune SVM, it achieves similar results with deep \\nlearning on linking the knowledge unit in Stack Overflow [7]. Most importantly, this \\nmethod is 84 times faster than training deep learning models. The same phenomenon \\nis also observed on code suggestion, in which an adapting n-gram language model \\nspecifically designed for software surpasses RNN and LSTM [6]. Although techniques \\nlike off-line training and cloud computing may partially resolve the efficiency \\nproblem [10], there is still a tradeoff between deep learning and other lightweight, \\ndomain-specific models. Such tradeoff drives a deep investigation on deep learning, \\ne.g., what types of SE data and tasks are suitable for deep learning and how to \\nintegrate the domain knowledge into deep learning.  \\nUnderstandability. The understandability is a burden to \"control\" deep learning. \\nRecently, several methods are proposed to improve the understandability of deep \\nlearning. For example, practitioners in Facebook explore to visualize industry-scale \\ndeep neural networks [8]. The proposed tool help software engineers understand the \\nneuron activations, individual instances, classification results, and differences \\nbetween activation patterns of deep learning. Such tool is a good start to increase the \\nunderstandability of deep learning in SE. \\nTestability. As a complex model, the testability limits the security of applying deep \\nlearning in SE. Hence, researchers attempt to use software testing techniques to \\nimprove the testability of deep learning, i.e., deep learning testing [9]. To test deep \\nlearning models, coverage testing and metamorphic testing are recently applied [9]. \\nCoverage testing validates whether all the neural units in deep learning are correctly \\nactivated. Metamorphic testing generates the test oracle for coverage testing. These \\nstudies demonstrate the importance of SE techniques on validating artificial \\nintelligence techniques like deep learning. \\nTo conclude, the practicability of deep learning is still a rising and hot topic for SE \\npractitioners and researchers. \\n10 \\n \\nConclusion \\nDeep learning recently plays an important role for solving SE tasks. In this study, we \\nconduct a bibliography analysis on the status of deep learning in SE. We find that deep \\nlearning has been integrated into more than 40 SE tasks by both industrial \\npractitioners and academic researchers. Most studies use standard deep learning \\nmodels and their variants to solve SE problems. The practicability of deep learning \\nmay hider SE practitioners from using deep learning in practice, which is a rising and \\nhot topic for further investigation.  \\nReference \\n[1] Madala, K., Gaither, D., Nielsen, R., & Do, H. Automated identification of component state \\ntransition model elements from requirements. International Requirements Engineering Conference \\nWorkshops. 2017. (pp.386-392). \\n[2] Joulin, A., & Mikolov, T. Inferring algorithmic patterns with stack-augmented recurrent nets. \\nNIPS’15. (pp. 190-198).  \\n[3] Tong, H., Liu, B., & Wang, S. Software defect prediction using stacked denoising autoencoders \\nand two-stage ensemble learning. IST’17. \\n[4] Pang, Y., Xue, X., & Wang, H. Predicting vulnerable software components through deep neural \\nnetwork. International Conference on Deep Learning Technologies. 2017. (pp.6-10).  \\n[5] Dahl, G. E., Stokes, J. W., Deng, L., & Yu, D. Large-scale malware classification using random \\nprojections and neural networks. International Conference on Acoustics, Speech and Signal \\nProcessing. 2013. (pp. 3422-3426). \\n[6] Hellendoorn, V. J., & Devanbu, P. Are deep neural networks the best choice for modeling source \\ncode? FSE’17. (pp.763-773). \\n[7] Fu, W., & Menzies, T. Easy over hard: a case study on deep learning. FSE’17. (pp.49-60). \\n[8] Kahng, M., Andrews, P.Y., Kalro, A., & Chau, D.H.P. ActiVis: visual exploration of industry-\\nscale deep neural network models. IEEE Transactions on Visualization and Computer Graphics, \\n24(1), 2018. (pp.88-97). \\n[9] Tian, Y., Pei, K., Jana, S., Ray, B. DeepTest: automated testing of deep-neural-network-driven \\nautonomous cars. ICSE’18. \\n[10] Li, X., Jiang, H., Liu, D., Ren, Z., & Li, G. Unsupervised deep bug report summarization. ICPC’18. \\n[11] Munassar, N.M.A., & Govardhan, A. A comparison between five models of software engineering. \\nInternational Journal of Computer Science Issues, 5, 2010. (pp.95-101). \\n[12] Dwivedi, A. K., Tirkey, A., Ray, R. B., & Rath, S. K. Software design pattern recognition using \\nmachine learning techniques. Region 10 Conference. 2016. (pp.222-227). \\n[13] Huang, X., Ho, D., Ren, J., & Capretz, L.F. Improving the COCOMO model using a neuro-fuzzy \\napproach. Applied Soft Computing, 7(1), 2007. (pp.29-40). \\n[14] Bengio, Y., LeCun, Y., Hinton, G. Deep Learning. Nature. 2015, 521. (pp.436-444). \\n \\n',\n",
       " '1805.08355v1.pdf': 'Opening the black box of deep learning\\nDian Lei , Xiaoxiao Chen , Jianfei Zhao\\nSchool of Mechatronics Engineering and Automation,\\nShanghai University, Shanghai 200072, China\\nAbstract\\nThe great success of deep learning shows that its technology contains profound\\ntruth, and understanding its internal mechanism not only has important impli-\\ncations for the development of its technology and effective application in various\\nﬁelds, but also provides meaningful insights into the understanding of human brain\\nmechanism. At present, most of the theoretical research on deep learning is based\\non mathematics. This dissertation proposes that the neural network of deep learn-\\ning is a physical system, examines deep learning from three different perspectives:\\nmicroscopic, macroscopic, and physical world views, answers multiple theoretical\\npuzzles in deep learning by using physics principles. For example, from the per-\\nspective of quantum mechanics and statistical physics, this dissertation presents\\nthe calculation methods for convolution calculation, pooling, normalization, and\\nRestricted Boltzmann Machine, as well as the selection of cost functions, explains\\nwhy deep learning must be deep, what characteristics are learned in deep learn-\\ning, why Convolutional Neural Networks do not have to be trained layer by layer,\\nand the limitations of deep learning, etc., and proposes the theoretical direction\\nand basis for the further development of deep learning now and in the future. The\\nbrilliance of physics ﬂashes in deep learning, we try to establish the deep learning\\ntechnology based on the scientiﬁc theory of physics.\\n1\\nIntroduction\\nDeep learning is the main representative of the breakthrough in artiﬁcial intelligence today, it has\\nreached nearly human level in image classiﬁcation [1], speech recognition [2], natural language\\nprocessing [3] and so on. The method of deep learning is developing rapidly, which almost subverts\\nall branches of computer vision ﬁeld. However, the fundamental problem of deep learning at present\\nis the lack of theoretical research on its internal principles, and there is no accepted theoretical\\nexplanation, namely, the so-called black box problem: Why use such a deep model in deep learning?\\nWhy is deep learning successful? What’s the key inside? The lack of theoretical basis has led to\\nthe academic community being unable to explain the fundamental reason for the success of deep\\nlearning. The theoretical basis is not clear, and we simply do not know from what angle to look at\\nit. The black box model is purely based on data without considering the physical laws of the model,\\nit lacks the ability to adhere to mechanistic understandings of the underlying physical processes.\\nHence, even if the model achieves high accuracy but it lacks of theoretical support, it cannot be\\nused as a basis for subsequent scientiﬁc developments [4]. We must not rely solely on intuition\\ndesigned algorithmic structures and several empirically tried examples to prove the general validity\\nof an algorithm. This research method has the potential to learn false modes from non-generic\\nrepresentations of data, the explanatory nature of the model is very low, and the resulting research\\nresults are difﬁcult to pass on in the long term. As people’s new ideas have been replaced by more\\nand more complex model architectures, which are almost invisible under the weight of layers of\\nmodels, calls for attention to the explanatory nature of machine learning are also getting higher.\\nTherefore, we need to thoroughly understand the entire system operation of deep learning. We\\nneed to explain what the most fundamental problems are in the ﬁeld of deep learning and whether\\n1\\narXiv:1805.08355v1  [cs.LG]  22 May 2018\\nthese fundamental issues are mature enough to be accurately described in mathematical and physical\\nlanguages.\\nThe great success of deep learning shows that its technology contains profound truth, but the most\\nwidely understood way is mathematical analysis, so far, very little attention has been paid to its sci-\\nentiﬁc issues. However, purely mathematical explanations may lead to misdirection. For example,\\nthe neural network is mathematically trying to approximate any function. In mathematics, it has\\nbeen proved that a single-layer neural network can approximate any function if it is long enough,\\nthis viewpoint has greatly hindered the development of neural networks, this is why most people in\\nthe past neglected multi-layer networks for a long time and without studying in depth. Only a small\\nnumber of people such as Yann LeCun, Geoffrey Hinton, and Yoshua Bengio still insist on research\\nin multi-layer neural networks [5]. Therefore, from the great successes achieved in deep learning,\\nit is far from enough to explain deep learning in mathematics, and the technology of deep learning\\nneeds to be based on scientiﬁc theory.\\nAs deep learning has made breakthroughs in many aspects such as images, phonetics, and text,\\nmethods based on deep learning are increasingly being applied in various other ﬁelds, for example,\\nrecently effective in solving many-body quantum physics problems has also been proved. Therefore,\\nthe theory of deep learning methods must reﬂect some objective laws of the real world. obviously the\\nmost basic and universal theory is quantum physics and statistical physics. What is science? Physics\\nis the most perfect science that has been developed so far. Just as most engineering disciplines\\nare based on physics, the engineering foundation for deep learning now and in the future will be\\nphysics. We need to describe the deep learning concept model in the language of physics, so that\\nwe can scientiﬁcally guide the development and design of deep learning. From this we say that the\\nkey to the current and future success of artiﬁcial intelligence depends not only on the mathematical\\ncalculation, but also on the laws of physics. The theory of deep learning requires physics.\\nThe data in the information world is divided into two different types of data: one is symbolic data,\\nwhich is designated by our humans; the other is physical data, which objectively reﬂects the data\\nof the real world, any actual data set we care about (whether it is a natural image or a voice signal,\\netc.) is physical data. Reference [6] shows that the reason why neural networks can perform clas-\\nsiﬁcation tasks well is that these physical data x follow some simple physical laws and models can\\nbe generated with relatively few free parameters: for example, they may exhibit symmetry, locality,\\nor a simple form as an exponent of a low-order polynomial; and symbolic data, such as ”variable\\ny=cat” is speciﬁed by humans, in this case the symmetry or polynomial is meaningless, and they are\\nnot related to physics. However, the probability distribution of non-physical data y can be obtained\\nby Bayes’ theorem using the physical characteristics of x. In the reference [4], a Physics-guided\\nNeural Networks (PGNN) is proposed, which combines the scientiﬁc knowledge of physics-based\\nmodels with the deep learning. The PGNN leverages the output of physics-based model simulations\\nalong with observational features to generate predictions using a neural network architecture. Ref-\\nerence [7] shows that deep learning is intimately related to one of the most important and successful\\ntechniques in theoretical physics, the renormalization group (RG). Reference [8] using DBM and\\nRBM to represent quantum many-body states illustrates why the depth of neural networks in the\\nquantum world is so important, revealing the close relationship between deep neural networks and\\nquantum many-body problems. Reference [9] establishes a mapping of tensor network (TN) based\\non quantum mechanics and neural network in deep learning. Reference [10] mentions that people\\nhave found more and more connections between basic physics and artiﬁcial intelligence, such as\\nRestricted Boltzmann Machine and spin systems, deep neural networks and renormalization groups;\\nthe effectiveness of machine learning allows people to think about the deeper connection between\\nphysics and machine learning, and perhaps it can help us gain insights into intelligence and the\\nnature of the universe.\\nThe research of the above reference mainly takes the neural network as a computational tool, or as a\\nmethod to solve the quantum many-body problem. This dissertation studies the artiﬁcial deep neural\\nnetwork as a real physical system, considers that the neural network model is a real physical model.\\nThe goal of deep learning training is to obtain the neural network system model which accords with\\nthe physical laws by the interaction or response between the neural network system and the input\\nphysical information. Because the deep neural network is a physical system, its trained model and\\nits evolution in training must meet the laws of physics.\\n2\\nThis dissertation analyzes the principles of physics embodied in deep learning from three different\\nperspectives: microscopic, macroscopic, and world view, and describes deep learning with physics\\nlanguage, aiming to provide theoretical guidance and basis for further study and future development\\ndirection, and tries to establish the technology of deep learning based on the scientiﬁc theory of\\nphysics.\\n2\\nA microscopic view of deep learning\\nThe biggest rule of the universe is that the world is made up of microscopic particles such as atoms,\\nelectrons and photons, which obey quantum mechanics. Quantum mechanics is the science of study-\\ning the motion law of the microscopic particles in the material world, so the neural network model of\\ndeep learning as a physical system requires that the model must be governed by quantum mechanics.\\nThe following explains deep learning from the basic principles of quantum mechanics.\\nThe human brain neural network is composed of atoms, the number of billions of neurons is the\\nsame, and the computational methods of the human brain should be similar. The neural network,\\nas an interactive quantum many-body system, determines the deep learning system to be described\\nby the wave function. The coordinate operator, momentum operator (corresponding translation\\noperator), angular momentum operator (corresponding rotation operator), energy operator, and spin\\noperator in the neural network are the most basic and important physical quantity or mechanical\\nquantity operator.\\n2.1\\nThe physical meaning of neurons\\nInformation has both physical and symbolic meanings, so neurons also have two meanings: 1)\\nphysical, 2) symbolic mappings. Now discuss the meaning of its physics. In this dissertation, the ﬁrst\\nhypothesis is that the neuron is the scattering source of the quasi-particle wave and the superposition\\nof receiving the quasi-particle wave. First look at a classic physics experiment—Young’s double slit\\nexperiment.\\nFigure 1: Young’s double slit experiment.\\nAs shown in Figure 1, the electrons are diffracted through the aperture a, and then diffracted and\\ninterfered by b and c. The bright diffraction fringes and patches at F indicate that there is a greater\\nprobability of electrons appearing there, and the dark part is there is little or no chance of the appear-\\nance of electrons. This dissertation holds that neurons act as electron diffraction interference. When\\nwe look at a neuron as a physical unit, the neuron is a scattering potential well that causes scattering\\nof quasiparticles (perhaps this scattering originates from the quantum effect in the microtubules of\\nthe neurons, perhaps the electron-phonon coherence coupling in the biological system, perhaps some\\nother kind of elementary excitation). Therefore, the output after the input of the neuron calculation\\nis like the scattering output of the electrons through the circular hole, and the law is determined by\\nthe quantum physics theorem. The physical meaning of neurons indicates that, as white light can be\\nscattered as red, orange, yellow, green, cyan, blue, violet, it is a natural classiﬁer, calculator.\\n3\\nIn the Young’s double slit experiment, whatever the input is light, or electrons, polarized electrons,\\nneutrons, and any particle including atomic and subatomic levels will cause diffraction and interfer-\\nence effects. In the same way, the physical meaning of neurons indicates that neurons are inherently\\ncapable of discriminating characterizations and are inherently capable of excellent generalization.\\nThe structure of Young’s double slit experiment provides the foundation for a universal quantum\\nneural network. The physical model of neurons assumed in this dissertation is:\\nThe input of neurons is a multichannel wave function, for example, the input image is a wave\\nfunction of multiple pixel points, and the photon (quasi-particle) wave function is superimposed to\\nbecome the new scattering source output. The state value of the neuron is the number of quasi-\\nparticles (probability) of excitation after superposition, if you visualize multiple or large numbers\\nof neurons, you see images of the same nature as the interference diffraction experimentłstreaks and\\npatches.\\nFigure 2: Neuron physical model diagram.\\nThe fundamental of this model is that the neuron is a physical model, which reﬂects the probabil-\\nity of its state value, and the related theories and deep learning techniques discussed later in this\\ndissertation can conﬁrm the correctness of its hypothesis.\\n2.2\\nQuantum physical model of CNN\\nIn this section, we ﬁrst establish the quantum physics model of the Convolutional Neural Network,\\nand then make a scientiﬁc analysis of the CNN based on this model, give a physical explanation of\\nthe success of the convolutional neural network, and explore the prospects for further development.\\nThe structure of Young’s double slit experiment provides the foundation for a universal quantum\\nneural network, as shown in Figure 3:\\nFigure 3: Universal quantum neural network.Figure courtesy: [11]\\n4\\nSimilar to the quantum neural network in reference [11], this dissertation considers that the neural\\nnetwork model of deep learning is the physical model of the interference diffraction of photon or\\nquasiparticle. The difference is that the input of the network is changed from the photon gun to\\nthe image of the input layer. The detection screen is made up of many neurons. Some neurons\\nsuperimpose the input photon or quasi-particle states on each path to obtain intensity (probability of\\nphotons or quasiparticles) and re-scattering output, the intensity values of all neurons constitute an\\ninterference diffraction pattern. In addition to the input, the interference diffraction pattern is also\\nclosely related to the structure of the neural network; the connecting line between neurons is related\\nto the excitation mode or interaction potential of the neuron and the structure of the neural network,\\nand has nothing to do with the input, which is consistent with the usual concept of artiﬁcial neural\\nnetwork.\\nIs this model correct? Let’s look at two ﬁgures:\\nFigure 4: A real interference pattern\\nFigure 5: The ﬁrst convolution layer of a typical convolution neural network after training.Figure\\ncourtesy:http://cs231n.github.io/understanding-cnn/\\nFigure 4 is a diffraction interference fringe obtained from a real double-slit experiment, which con-\\ntains the total intensity distribution information of double-slit interference and single-slit diffrac-\\ntion. Figure 5 shows the ﬁrst convolution layer after a typical CNN training, it also sees stripes and\\npatches. Comparing the two graphs, we can see that the two are very similar, and all of them are\\nstripes and patches, which fully shows that our neural network model is the quasi-particle diffrac-\\ntion interference model in physics. The neuron receives multiple signals from the previous layer,\\nperforms quantum superposition, if the interference superposition elimination, then the neuron re-\\nmains silent; if the interference superposition intensity is extremely high (the stripe or patches in the\\nﬁgure), then the neuron activates the impulse, emits the superposition intensity output signal. This\\nalso explains why physiological neurons may remain silent even if they have strong signal input.\\nFrom the ﬁrst layer of the convolutional layer in Figure 5, we can see that most people think that\\nthe ﬁrst layer of convolution is to learn the edge of the image, and what the later convolution layer\\n5\\nlearned is no one can understand. In this theoretical model, the ﬁrst and the back layers are in-\\nterference diffraction fringes! Interferometric diffraction or scattering experiments in physics can\\nanalyze and discover elements, and deep learning neural networks can be used for a wide range of\\napplications such as image recognition because its mechanism is also the mechanism of interference\\ndiffraction or scattering experiments. Each fringe or patch reﬂects the momentum and angular mo-\\nmentum characteristics of the system, and the momentum or wave vector mechanics respectively\\nrepresent the translation and rotation invariance of the space.\\nLet us ﬁrst look at the relationship between translation operation and convolutional neural networks.\\n2.2.1\\nCNN and translation operation\\nAccording to quantum mechanics, the space translation operator is:\\nˆD(a) = eia.ˆp/hor ˆD(a) = eiˆk.a\\n(1)\\nWhere a is the translational magnitude, the wave vector operator ˆk = −i∇, the momentum op-\\nerator ˆp = −ih∇, ˆp ≡hˆk, h is the Planck constant, the wave vector k and the momentum p are\\nindependent of a, and are mechanical quantities only related to the system.\\nThe physical quantities that describe the spatial invariance or spatial symmetry of the arbitrary func-\\ntion f(x + a) = f(x) are the momentum or wave vector, i.e. the physical conservation quantities\\ncorresponding to the spatial symmetry are the wave vector k or the momentum p. For example,\\nthe free particle or plane wave function ψ(x) = Ceikx−iωt , where k is the wave vector, equal to\\nthe reciprocal of the wavelength, it describes its spatial properties. The edge boundary, texture, and\\ncolor change of the image must destroy the translation symmetry. The mechanical quantity wave\\nvectors can be used to express whether the edge, texture, color of the image is equal or unequal in\\nspace. The wave vector is the basic physical quantity quantifying these attributes.\\nThe translation in a one-dimensional case is easy to prove:\\nea d\\ndx f(x) = f(x) +\\n∞\\nX\\nn=1\\nan\\nn!\\ndn\\ndxn f(x) = f(x + a)\\n(2)\\nwhich is:\\nˆD(a)f(x) = f(x + a)\\n(3)\\nThe convolution calculation in the CNN actually reﬂect the translation operation, and the one-\\ndimensional convolution in book [12] for image I(x) and kernel function K(a) is as follows:\\nS(x) =\\nX\\na K(a)I(x −a)\\n=\\nX\\na K(a)e−a d\\ndx +a d\\ndy I(x)\\n=\\nX\\na K(a)e−ˆk.aI(x)\\n=\\nX\\na K(a) ˆD(a)I(x)\\n(4)\\nThe two-dimensional form is as follows:\\nS(i, j) =\\nX\\nm\\nX\\nn K(m, n)I(i −m, j −n)\\n(5)\\nThe entire feature map layer image is obtained by translation. To understand the image, consider the\\none-dimensional case, the minimum unit of translation is a pixel, if K(a) =\\n\\x1a\\n1, (a < r)\\n0, (a ≥r)\\nis set.\\nFor the input layer is the image, only two points are taken within the |r| range, the range is small,\\nand the output of the convolutional layer is:\\nS(x) =\\nX\\na K(a) ˆD(a)I(x) = ( ˆD(0)+ ˆD(1))I(x) ≈(2 + d\\ndx)I(x)\\n(6)\\n6\\nSo the ﬁrst convolution layer sees the edge of the image element in the background, but it is not\\nequal to the edge of the image element because of the convolution kernel.\\nIf the image I(x) has spatial symmetry with wavelength λ, or if the wave vector has spatial symmetry\\nof k = 2π/λ , i.e. I(x) = sin(kx) , then the output of the convolutional layer is:\\nS(x) =\\nZ r\\n0\\nˆD(a)I(x)da =\\nZ r\\n0\\nsin k(x + a)da = 2\\nk sin kr\\n2 sin(kx + kr\\n2 )\\n(7)\\nAfter convolution, the value still has the same wave vector k volatility, but the intensity is\\n4\\nk2 sin2 kr\\n2 ,\\nthe interference diffraction envelope appears, the maximum and the minimum intensity values ap-\\npear in:\\nkr = nπ\\n\\x1a\\nn = 1, 3, 5, · · · the intensity is extremely high\\nn = 2, 4, 6, · · · the intensity is zero\\nThe above results show that an interferometric diffraction image is obtained by spatial convolution.\\nWhen the input wavelength is longer, the intensity at the maximum by the convolution is ampliﬁed\\nto 4/k2 times the original input wave intensity. When the convolution kernel structure is certain,\\nonly the wave vectors satisfying certain conditions will appear extremely large. Assuming that only\\nthe brightest point is taken, then only the input wave vector satisﬁes k = π/λ sine wave through the\\nconvolution after the ”bright spot.” That is to say, in the case of a convolutional kernel structure, the\\n”brightest” reﬂects the wave vector k of the sine wave. The actual wave is the superposition of sine\\nwave, and the ”brightest” point of the interferometric diffraction image will not have only one point.\\nThe interference diffraction image reﬂects the spatial symmetry of the input wave.\\nThe next section explains what the physical meaning of the convolution formula (Equation 4) is,\\nexplains the nature of the convolution kernel and explains why the convolution calculation is not\\nsummed over the entire image range, but instead use a ﬁnite window convolution calculation such\\nas 3*3? Why do these moving convolution windows use the same convolution kernel, the so-called\\nshared weight? Why does a convolutional layer use multiple feature mapping layers? Why does\\neach feature mapping layer convolutional kernels differently? Why pooling? Why use Relu acti-\\nvate functions, etc. In our model, the neuron is a unit that receives multiple inputs and quantum\\nsuperimposes and then scatters, so a perfect answer can be obtained by using the scattering theory\\nof quantum mechanics.\\n2.2.2\\nPhysical meaning of convolution calculations and convolution kernels\\nA general CNN interprets a convolution kernel as a ﬁlter, but what does it ﬁlter? What does it\\nkeep? It is not clear, so this argument is difﬁcult to convince, and the ﬁlter’s opinion is not clear.\\nConvolutional neural networks have achieved great success in the ﬁeld of computer vision, one of the\\nmost important components is the convolution kernel, which is primarily responsible for capturing\\nmost of the abstraction of the network. In contrast, this component is the least understood processing\\nblock because it requires the maximum computational learning [13].\\nThis section builds the convolutional calculations on the quantum physics model described in Sec-\\ntion 2.1. The physical basis of the interference diffraction experiment is quantum mechanical scat-\\ntering theory. According to the scattering theory, the system Hamiltonian does not contain time, and\\nthe incident wave is directed at the target. The wave function ψ at the position r is: [14].\\nψk(r) = eik·r −\\nZ\\nG(r −r′)U(r′)ψ(r′)dr′\\n(8)\\nEquation 8 is the integral equation, the integral is performed in the entire space, the ﬁrst is the input\\nwave, and the second is the scattering wave. The physical meaning of the second item is very clear,\\nU is the interaction potential function. The input wave scatters in the dr′ range near r′ point to\\nform a scattering point source with intensity U(r′)ψ(r′)dr′ . This point source propagates to the\\nobservation screen r point according to the outgoing Green function, and the scattering of the points\\nis superimposed as the scattering probability amplitude ψsc .\\nThe Green function is:\\nG(r, r′) = −\\neikr\\n4π |r −r′|\\n(9)\\n7\\nThe incident wave requirement in Equation 8 is monochromatic and planar wave, and does not do\\nany other approximation. The equation is applicable both to elastic scattering and inelastic scattering\\nor collision scattering. So the total intensity of a neuron scattering from a quasi particle of a certain\\nwavelength is\\nS =\\nZ\\n|ψ(r)|2dr\\n(10)\\nThe integral r is in the range of neuron size. According to this model, the total intensity of a neuron\\nis deﬁned as the probability density ψ2 of the excited particle, so\\nS ≈|ψ(0)|2\\n(11)\\nLet S be the square of the convolution s , that is S = s2 , then the convolution s is:\\ns =\\n\\x0c\\x0c\\x0c\\x0c\\nZ\\nG(r′)U(r′)ψ(r′)dr + b\\n\\x0c\\x0c\\x0c\\x0c\\n(12)\\nNote that the left side of Equation 8 is a wave function, and there are also wave functions in the\\nintegral term, so it is difﬁcult to solve. As an approximate calculation, let the ψ in integral as input\\nimage wave, converted to discrete expression. The volume element dxdydz = 1 , so the convolution\\nfor a wave vector k is sk :\\nsk =\\n\\x0c\\x0c\\x0c\\nX\\nr′ G(r′; k)U(r′)ψ(r′) + b\\n\\x0c\\x0c\\x0c\\n(13)\\nWhere the sum range of r′ is within the neuron volume range. The convolution kernel is:\\nK = G(r′; k)U(r′)\\n(14)\\nψ(r′) is the input image wave, it is only related to x′, y′ , and has nothing to do with other coordi-\\nnates:\\nsk =\\n\\x0c\\x0c\\x0c\\nX\\nx′y′z′ G(x′, y′, z′; k)U(x′, y′, z′)ψ(x′, y′) + b\\n\\x0c\\x0c\\x0c\\n=\\n\\x0c\\x0c\\x0c\\nX\\nx′y′\\nX\\nz′ G(x′, y′, z′; k)U(x′, y′, z′)ψ(x′, y′) + b\\n\\x0c\\x0c\\x0c\\n(15)\\nThe square s2\\nk of sk represents the quasi-particle probability density that is excited when the incident\\nwave (or quasi-particle) has a wave vector k (size, direction, and polarization direction). So the\\nconvolution kernel:\\nKx′y′(k) =\\nX\\nz′ G(x′, y′, z′; k)U(x′, y′, z′)\\n(16)\\nx′, y′ is a two-dimensional image coordinate, sk represents the convolution of the feature mapping\\nlayer, and different wave vectors have different sk (if the quasi-particles are electrons also have a\\ndirection: spin)\\nsk =\\nX\\nx′y′ Kx′y′(k)ψx′y′ + b\\n(17)\\nThe convolution of all k is a set S = {sk}.\\nThe following is a discussion of its physical meaning and its implications for the calculation of\\nconvolutional neural networks.\\n2.2.3\\nQuantum mechanics interpretation of CNN\\nThere are various interpretations of convolutional neural networks, but according to the above quan-\\ntum physics model, CNN can be perfectly interpreted. The interpretation is fundamental and deter-\\nministic, it will inspire the architectural design of deep convolutional networks.\\n(1) When the inner product of the convolution is represented by the norm and the angle:\\n∥ω∥∥x∥cos(θ(ω,x)) , the trained inner product of the convolutional neural network can\\nbe decoupled to ﬁnd the relationship between norm and angle in the feature map [15].\\n8\\nFigure 6: CNN learned features are naturally decoupled.Figure courtesy: [15]\\nIf the convolution of spherical coordinates is used, the relationship between the norm and\\nangle in the feature map is more obvious:\\nFigure 7: 2D feature visualization on MNIST dataset with natural training.Figure courtesy: [15]\\nThe ﬁgure shows that the angles represent semantic/label differences, and the feature norm\\nrepresents within-class differences. This result can be explained in our model. It can be\\nseen from (Equation 14) that the general action potential U(r) is only related to r , i.e.\\nU(r) = U(r) . So the convolution kernel is related to the norm and has nothing to do with\\nthe angle, the CNN feature image is related to the angle. The above experiments verify the\\ncorrectness of the physical model of the convolutional neural network.\\n(2) The number of convolution kernels is related to the color of the image. The input re-\\nquirement in the scattering equation(Equation 8) is a monochromatic wave, that is, the\\nconvolution kernel(Equation 16) is related to the wavelength of the input wave, and differ-\\nent wavelength should have different convolution kernel Kmn. Applied to the color image,\\nconvolution neural network should have different convolution kernel for different colors\\nimage. For example, they can be composed of three kinds of convolution kernels: red,\\ngreen, and blue. Of course, they can also be composed of multiple kernels such as red,\\norange, yellow, green, blue, and purple.\\n(3) The number of convolution kernels is also related to the polarization direction of the inci-\\ndent wave. It can be seen from(Equation 16, 17) that even if the image is monochromatic,\\nthe convolution kernels of different z are different. So even if the input is a black-and-\\nwhite image, multiple feature mapping layers (multiple convolution kernels) are needed,\\njust as white light contains light of all colors, and random monochromatic light also con-\\ntains polarized light of all polarization directions. If the input is an electron, the number\\nof convolution cores is related to the spin direction. In a comparable size, the number of\\nfeature map layers is much larger than the convolutional window size.\\n9\\n(4) The reason of partial receptive ﬁelds is not because the pixels are usually highly corre-\\nlated in neighboring regions. The convolution kernel sharing of feature layer is not due to\\ntranslation invariance, they are due to the limited scope of the action potential U(r′) of the\\nneuron, that is, the coordinate x′, y′ in the convolution kernel (Equation 16) is 0 outside the\\nrange of action potential. Therefore, the summation of the x′, y′ in (Equation 17) does not\\nrequire full space, i.e.the CNN should not be fully connected but partial connection. For\\nthe same feature mapping layer, all the neuron potentials U(r′) are the same, so the convo-\\nlution kernels of all the sliding window convolutions of the same feature mapping layer are\\nthe same or share one, which explains from the scientiﬁc theory that the fundamental rea-\\nson for the success of CNN is the important characteristic of CNN: partial receptive ﬁelds\\n+ weight sharing. In the actual application of CNN, the convolution kernel size (that is, the\\nconvolution window size) is often 3x3, 5x5, 9x9 and so on, scattering is mainly concen-\\ntrated in the incident direction, so the window size is too large to be meaningful. However,\\nif the 1*1 size of the convolution kernel is used, the interference effect will also be poor. In\\naddition, considering the symmetry of K, convolution kernel size is generally used odd.\\n(5) The purpose of pooling operations is not only to reduce the size of the output eigenvectors.\\n(Equation 15) is the intensity of a neuron, and the image formed by all the neurons of a\\nconvolutional feature layer is an interference diffraction pattern. Because the coherence of\\nthe wave is strong in some places and weak in some places and not even in some places,\\nand those ”bright” neurons will be the scattering sources of the subsequent convolutions,\\nso each convolutional layer must be pooling. That is, in the convolution window area,\\nthe ”bright” neurons must be selected according to the intensity values of the neurons in\\neach convolution layer feature layer, which is also the physical reason why the biological\\nneuron can either output or not output even if it has a signal input. Obviously, pooling\\nis an important computing component. In the early years, many of the studies based on\\nconvolution architecture used average pooling, now they are replaced by Max Pooling.\\nFrom our model we can see that in practical application, because the window size is not\\nlarge, it is reasonable to select one of the most bright neurons, that is, to adopt the max\\npooling method. It is also seen from here that the pooling is of decisive importance to the\\ninitial convolution layer, but as the number of layers increases, the interferogram sharpens\\nand bright points become less, so the effect of pooling is weakened, which is the same as\\nthe result of the reference [16]. Pooling can also be explained by the renormalization group\\nin Section3.4.\\n(6) The convolution kernel of different layer is not same, because each of the ”brightest” neu-\\nron positions represents the coherence of the corresponding convolution window image,\\nwhich is related to the symmetry of the image and the structure of neural network. After\\npooling, the spacing as a new scattering source is different, so subsequent convolution ker-\\nnels will also be different. That is, except for the same feature mapping layer, convolution\\nkernels of each feature map layer of the convolution layer are different.\\n(7) Coherence is the most important condition for multilayer CNN. In our model, the purpose\\nof convolution is to create the entanglement of each pixel in the image under the action\\nof neurons, thus forming the interference diffraction fringes or patches according to the\\nspatial translation structure. According to the quantum mechanics, this is a coherence\\nphenomenon caused by the superposition principle of waves. The total intensity after inter-\\nference superposition is not necessarily equal to the sum of the intensity of the sub-beams,\\nmay be more strong or may be equal to 0 under the interference. The important condition\\nof coherence is the coherent wave. There are two ways to produce coherent wave, the ﬁrst\\none is to ensure that the monochrome of the wave and the phase of each wave of the ﬁxed,\\nso need multi-layer convolution; the second is the interference of oneself and oneself, so\\nneed multi-layer convolution, generally need at least 3 or more.\\n10\\nFigure 8: Interfering with oneself causes CNN to require multiple layers of convolutional layers.\\nIf we understand from the biological neural network, the wavelength of the natural color\\nof the physicist’s world changes monotonously from red to purple, but the system of hu-\\nman perception of color is closed-loop, such as the combination of red light and purple\\nlight is understood as the monochromatic magenta color. However, does not physically\\nexist with a single physical wavelength of light corresponding to the color, but the human\\nperception system fusion understands as a single color. Therefore, the convolution neural\\nnetwork must be fused or transformed after the input scattering (decomposition) must be\\nmultilayered.\\n(8) Convolution is not the extraction of human knowledge such as image edges or colors, the\\nconvolution is the physical law and does not require human prior knowledge. From (Equa-\\ntion 2) to know the image gradient is obtained after the translation operation, so for the ﬁrst\\nfeature layer of the image we can see that it reﬂects the edge of the image. But as the subse-\\nquent continuous pooling and convolution, feature layer image will become more and more\\ndifﬁcult to understand and abstract compared to the original layer. However, in our model,\\nthe convolution image is a diffraction stripe or patch image after destructive interference\\nor constructive interference, which is the superposition of the same or coherent wave. That\\nis, the images are decomposed and classiﬁed by the same coherent attribute. The wave\\nof the same phase has a destructive interference and the opposite phase has a constructive\\ninterference, and the interference diffraction patterns will effectively deconstruct the entire\\ninformation of the incident image. And this coherent property is the symmetry of the im-\\nage, according to quantum mechanics, the physical quantity described by the symmetry is\\nthe wave vector. It can also be said that CNN is to measure the wave vector of the image,\\nbecause it is the XY plane convolution of the image, i.e. the measurement of the momen-\\ntum px, py in the x, y direction. According to quantum mechanics, the z directional angular\\nmomentum operator is deﬁned as: ˆlz = xˆpy −yˆpx , so it also detects angular momentum\\nor rotational symmetry around the z direction. That’s to say, a trained CNN measures the\\ntranslational symmetry of the x, y direction of the image or the rotational symmetry of the\\nz direction through the translational operation and interaction of the image, is classiﬁed\\nas the interference diffraction fringes or patch image that we see. The feature map layer\\nis effectively classiﬁed according to the symmetry, and then the whole connection layer is\\nmapped, which can effectively recognize the image.\\nIn summary, the above research shows that LeCun’s most famous contribution, the convolutional\\nneural network, is entirely based on prior knowledge and that the idea of not requiring human struc-\\ntured knowledge is completely correct. CNN is based on the scientiﬁc theory, so it is the best method\\nin image recognition. CNN is a physical model, so it is very successful in the processing of physical\\ndata such as images, videos, sounds, condensed matter physics, etc., but the performance is worse\\nthan other models when processing the strong subjectivity of symbolic modeling. For example, no\\nmatter how much data is input, it is impossible to train a model that can read product descriptions\\nand generate an appropriate code base [17].\\n11\\n2.2.4\\nSigniﬁcance of classiﬁcation layers in CNN\\nThe convolutional layer is followed by a classiﬁcation layer whose meaning maps the mechanical\\ncharacteristics learned in the convolution layer to human symbols (marks). The signiﬁcance of the\\nclassiﬁcation layer is expressed in:\\n• On the signiﬁcance of the classiﬁcation layer, the ﬁrst is mathematics. In order to effec-\\ntively apply the ability of neural network that can approximate arbitrary functions mathe-\\nmatically, the classiﬁcation layer must be fully connected.\\n• The classifying layer in neural network is also physical. In our model, the neuron is a\\nprobability wave. The output of the last layer’s activation function must be guaranteed\\nto meet the normalized requirement, generally, the softmax function with a clear physical\\nmeaning is used. The activation function is shown in Section 2.5.\\n• The classiﬁcation must have generalization and must obey the laws of statistical physics\\nand must cooperate with entropy. The content of entropy is shown in Section 3.2.\\n2.2.5\\nDevelopment prospects of CNN\\n(1) According to the neuron scattering theory, strictly speaking, the convolutional neural net-\\nwork should be extended to the complex number ﬁeld so that it can use the wave function\\nwith phase information. Thus, the input image is the intensity, and it should be squared\\nroot when used as a wave function, which is equivalent to the fact part:\\nψ(x, y) =\\np\\nI(x, y)\\nHowever, in the actual calculation of CNN, the calculation on the complex number ﬁeld\\nmay not be signiﬁcant. In Equation 10, when we compute S, we need to square ψ , and\\n|ψ|2 is proportional to the intensity of the image. The calculation of S in the real ﬁeld\\nwill appear negative, but subsequent Relu rectiﬁcation guarantees that the output is posi-\\ntive. It still conforms to the model where the value of the neuron is the intensity value, the\\nsigniﬁcance of max pooling is to ﬁnd the brightest neurons. In short, if the convolution\\nis calculated in the complex number ﬁeld, the square root is computed ﬁrst, and then the\\nsquare is computed, which is equivalent to repeating computation. And if the convolu-\\ntion is calculated in the real ﬁeld, input do not square root, output also do not square, the\\nRelu rectiﬁcation function guarantees that no negative number will appear. By reducing\\nthe repetition calculation and saving the calculation amount, the calculation error can be\\nreduced. There is no big difference between the calculated effect on the real ﬁeld and the\\nstrict complex number ﬁeld.\\n(2) Choosing the appropriate convolution kernel is crucial for obtaining the most signiﬁcant\\nand important information contained in the input signal. This allows the model to make\\nbetter inferences about the content of the signal. The goal of this transformation is to\\nchange the data in a way that is more easily separated by the classiﬁer. According to\\nEquation 16, a better model is designed using the convolutional features, such as better\\ncoordinates: cylinder coordinates or spherical coordinates of CNN [15].\\n(3) The training of CNN based on quantum scattering theory obtains a large amount of input\\ninformation, but the ﬁnal classiﬁcation only occupies a small part of the information. These\\nare correct and objective reﬂections of the input information, it is easy to drown in the\\ntraining process. This is undoubtedly a waste of valuable prior probabilities that can be used\\nto migrate large-scale network knowledge into small-scale networks. Therefore, making\\nfull use of this information or data migration learning in other ﬁelds is also a topic worthy\\nof study.\\n(4) Multilayer CNN can not only perceive the translational symmetry and rotational symme-\\ntry of input, but also transform and perceive the fusion of various combinations of col-\\nors. In addition, it should also can transform and perceive the polarization direction of\\nlight. That is, CNN can perceive very rich three-dimensional scenes and behavioral in-\\nformation from static two-dimensional images. The geometry school will laugh : how\\ncan an image calculate three-dimensional, which is mathematically impossible. It doesn’t\\nmake any sense mathematically, but it makes sense physically! Human beings can perceive\\n12\\nthree-dimensions from two-dimensional images, and deep convolution neural networks can\\nextract 2.5-dimensional information from two-dimensional images. This is the quantum\\neffect: White light can be decomposed into red, orange, yellow, green, cyan, blue and vio-\\nlet through neuronal interaction or coupling or entanglement, and random monochromatic\\nlight can be decomposed into various polarized light. According to the physical model\\nabove, a lot of new information can be found from the polarization information, such as\\nstereo information, like the principle of stereoscopic ﬁlm. This is an important prediction\\nof the deep convolution neural network in this dissertation.\\n(5) The CNN discussed above is to classify the spatial symmetry of the image by spatially\\ntranslating and interacting with neurons. After the above measurement, the state after the\\ncollapse is measured as the new environment of Hamiltonian to state a new round of evo-\\nlution. If the incident wave changes, such as the time dependent dispersion of the packet,\\nthe visual retention, or the Hamiltonian contains time, then it will be a Schr¨odinger equa-\\ntion that contained time. At this time Equation 17 is time-containing, the green function\\nis divided into the delayed Green function and the advanced Green function, which can\\nbe computed to predict the short future by or to recall a brief past. For time translation\\noperations, according to quantum mechanics, the conserved quantity of time translation\\ninvariance is energy. Through the convolution of time translation, it is also possible to clas-\\nsify the energy properties of the input information. And the scattering of neurons contains\\ntime, so it will be an important research direction and may explain the important functions\\nof visual retention, visual prediction and memory.\\n(6) The convolution neural network based on quantum scattering theory is expected to compute\\nthe internal properties of the biological neurons and to gain more understanding of the\\nfunctions of the biological neurons and the human brain.\\n(7) CNN is supervised training, according to the above theory, it can be used in unsupervised\\npre-training and training.\\n(8) In our neural network model, all the neuron cells of CNN form an image of a grid cell and\\na location cell, which is very similar to the image of the biological grid cell and position\\ncell. It will be an important research direction to combine CNN with LSTM to achieve\\nnavigation like the brain grid cells.\\n2.3\\nDeep learning based on energy model\\nEnergy is one of the most important concepts in physics and even in the entire natural sciences.\\nAll forms of physical movement have the concept of energy, such as mechanical, electromagnetic,\\nthermal, light, chemical movement, and biology, etc. The analysis of energy can greatly simplify the\\nanalysis of material motion. Energy is the measure of movement transformation, and it is an additive\\namount, following the law of conservation of energy. Energy in quantum mechanics is represented\\nby the Hamiltonian operator.\\nIn our model, the strength of neurons is described by the square of the wave function. The typical\\nmodel of the neural network composed of interacting neurons in deep learning is the Boltzmann\\nmachine.\\nAccording to quantum mechanics, the wave function of the system is Ψ, and the statistical average\\nof the mechanical quantity O is:\\n⟨Ψ | O |Ψ⟩\\n⟨Ψ | Ψ⟩\\n=\\nP\\nx |Ψ(x)|2O(x)\\nP\\nx |Ψ(x)|2\\n(18)\\nThere is an important conclusion between the quantum mechanics measurement and the statistical\\nsampling of wave functions: if Ψ is the eigenstate of the system energy, then:\\n\\nH2\\x0b\\n=\\nD\\nH⟩2 = E2\\n0\\n(19)\\nThat is, the variance of the Hamiltonian energy H is 0, which means that if the system is in the\\nground state E0 , the statistical ﬂuctuation completely disappears. This feature is very important\\nbecause it means that the closer we are to the ground state, the less ﬂuctuations we have in the amount\\n13\\nwe want to minimize, and the less energy we have. According to quantum statistical physics, if the\\nsystem is a closed system, there is energy ﬂuctuations, but the total number of particles is constant,\\nthe multi-body probability density ρ(x) satisﬁes the Boltzmann distribution:\\nρ(XN) =\\n1\\ncNzN(T, V )e−βH(XN)\\n(20)\\nPartition function is: Z =\\n1\\ncN\\nR\\ndXNe−βH(XN) , probability density is the normalized |Ψ|2 . For a\\nneural network composed of interacting multi-body quantum systems, the general form of Hamilto-\\nnian is:\\nH = −ℏ2\\n2m\\nN\\nX\\ni\\n∇2\\n⃗ri +\\nX\\ni\\nV1(⃗ri) +\\nX\\ni<j\\nV2(⃗ri,⃗rj)\\n(21)\\nThe potential energy between two particles in physics is chosen as a function of the distance between\\nparticles in the form of:\\nV12 = V (⃗r1,⃗r2) = V (|⃗r1 −⃗r2|) = V (r12)\\n(22)\\nWithout regard to kinetic energy, only the interaction is considered, the interactions always interact\\nin pairs, that is, the interaction energy (potential energy) takes only quadratic term, so the energy\\nmodel is obtained:\\nE(v, h) = −bT v −cT h −vT Wh\\n(23)\\nWhere b,c and W are all unconstrained, real-valued learning parameters. The model is divided into\\nvisible layer v and hidden layer h, and the interaction between them is described by matrix W [18].\\nThis is the most basic component of deep learningłthe Restricted Boltzmann Machine energy model.\\nIt is consistent with the view put forward in this dissertation that neural network in deep learning is\\na physical system and conforms to the laws of physics. For Boltzmann distribution sampling, the\\nmethod of random gradient descent that minimize the energy can determine network parameters b,\\nc and W.\\n2.4\\nRelationship between energy model and convolution model\\nFrom the above analysis, CNN is based on accurate quantum physics, and RBM is based on statisti-\\ncal physics which will be described later, the difference is pure ensemble and hybrid ensemble.\\nThe traditional RBM can only express the probability distribution function with a positive value.\\nIn order to make it suitable for describing the wave function with phase information, Carleo and\\nothers extend the parameters of RBM to the complex ﬁeld. In addition, in the actual calculation, the\\nfunction form used by Carleo is the product of multiple RBM that share weights. This structure is\\nequivalent to a single hidden-layer convolutional neural network, thus ensuring the spatial translation\\ninvariance of the physical system in the structure [19].\\nIn addition, the RBM ignores the effect of the kinetic energy of the neuron, that is, the centroid\\nmotion, or ignores the quantum effect of the neuron itself, so the effect of the model is obviously\\nworse than that of the CNN.\\n2.5\\nNormalization of neural networks\\nIt has been explained that the wave function of the neural network system is ψ . According to\\nquantum mechanics, the meaning of the neuron is that each neuron represents the probability |cn|2\\nthat eigenvalue Fn can be measured by a group of mechanical quantities F , and the wave function\\nsatisﬁes the normalized condition, so P\\nn\\nc2\\nn = 1.\\nOnce the wave function of the system is determined, the average value of the mechanical quantity\\nand the probability distribution of the measurement result can be obtained through the mechanical\\nquantity measurement. The results obtained by the physical system are obtained by measurement,\\n14\\nwhereas in quantum mechanics, measurements differ from those in classical mechanics. The quan-\\ntum measurements affect the measured subsystems, such as changing the state of the measured\\nsubsystem, which called the wave function collapse, and the measured result accords with a certain\\nprobability distribution. In other words, when the mechanical quantity is measured, the state is col-\\nlapsed to the eigenstate of this mechanical quantity, then the mechanical quantity is its eigenvalue.\\nBut it may also be other eigenvalues, that is, it may also be collapsed to other momentum eigenstate,\\nwhich is a probability, so it must have:\\nX\\nihl\\ni = 1\\nThat is to meet the normalization conditions. Quantum measurement is the core issue of the quantum\\nmechanics interpretation system.\\nIn deep learning, considering the concept of mechanical quantity measurement, the result output of\\neach layer neural network characteristic (eigenvalue) learning should be a quantum measurement,\\nso the activation function is needed to determine its probability distribution. If there is no activation\\nfunction, then the network can only express the linear mapping. Even if there are more hidden layers,\\nthe entire network and the single-layer neural network are equivalent. The meaning of the activation\\nfunction is the output of the quantum measurement, which determines its probability distribution\\nand is normalized. The most important activation functions are:\\n• softmax(hi) =\\nexp(hi)\\nP\\nj exp(hj).\\n• sigmoid(hi) =\\n1\\n1 + exp(−hi)\\n• Relu(hi) =\\n\\x1a\\nhi, (hi > 0)\\n0, (hi ≤0)\\nThe ﬁrst activation function guarantees that the sum of all output neurons is 1.0, which guarantees\\nthe normalization of its probability distribution. So the last layer basically apply this activation\\nfunction. The softmax function has a clear physical meaning, which is the Boltzmann probability\\ndistribution of the ideal gas or quasi particle.\\nThe sigmoid activation function is more commonly used in traditional neural networks, but it is not\\nsuitable for deep neural networks. A major breakthrough in deep learning technology is the use of\\na third activation function. The Relu function directly outputs a probability or probability density\\ndistribution which is a quasi-particle number in our physical image, and it must be greater than or\\nequal to 0.\\n3\\nA macroscopic view of deep learning\\nThe deep neural network is an interacting quantum multibody system, according to quantum me-\\nchanics, by determining the wave function p of the quantum multi-body system, the whole in-\\nformation of the quantum system is grasped. The core goal is still to ﬁnd the solution ψ of the\\nSchr¨odinger equation, this is generally very difﬁcult. However, because we are mainly concerned\\nwith the macroscopic properties such as statistical mean of system observations, macroscopic per-\\nformance is subject to statistical physics. Statistical physics is a bridge between microscopic and\\nmacroscopic, which determines the condition of macroscopic equilibrium state and stable state, and\\nthe change direction of macroscopic state. Quantum statistical physics is described in two different\\nways, pure ensemble and hybrid ensemble, so there also are two different kinds of neural networks.\\nEnsemble is an abstract concept, which is divided into pure ensemble and mixed ensemble. If\\nthe N subsystems in the ensemble are all in the same state |ψ⟩, then the ensemble is called pure\\nensemble and they are described in pure state |ψ⟩. If N systems have N1 systems in state |ψ1⟩, N2\\nsystems are in state |ψ2⟩, ... Ni are in state |ψi⟩,..., the probability of each measurement system\\nbeing in |ψ1⟩, |ψ2⟩, · · · , |ψi⟩, · · · state is P1 =\\nN1\\nN , P2 =\\nN2\\nN , · · · , Pi =\\nNi\\nN , · · · , and the set\\nof N systems(N →∞)is called a hybrid ensemble, which is described by a set of all |ψi⟩and\\nPi(i = 1, 2, · · · ) .\\n15\\n3.1\\nPurely ensemble and hybrid ensemble neural networks\\nThe theory of quantum statistical physics is used to design the neural network architecture, which\\nhas two kinds of neural networks, pure ensemble and mixed ensemble. The practical applications of\\ndeep learning generally have both pure ensemble and mixed ensemble neural network structures.\\nPure ensemble is described by the pure state wave function, so the pure ensemble neural network\\narchitecture goal is to ﬁnd the connection weights of the neural network through training so that the\\npure state can be constructed together with the input, and each layer can obtain the probability of\\neach value (characteristic quantity) of the observed amount. What it pursues is a numerical solution\\nof the wave function.\\nThe neural network structure of hybrid ensemble is suitable for using unsupervised training to ﬁnd\\nthe set—mixed state, which does not correspond to an observation. and is designed to reconstruct\\ninput. It is not from the Schr?dinger equation of quantum mechanics to ﬁnd the wave function of\\nthe system. Instead, it believes that under certain macroscopic conditions, at certain moments the\\nsystem will be in a quantum state with a certain probability. The hybrid ensemble requires secondary\\nstatistics, which cant be solved in quantum mechanics, so wo need to introduce additional basic\\nassumptions of statistical physics—when the isolated system reaches equilibrium, the probability\\nof appearance of various microscopic states is equal. The macroscopic quantity of the system is\\nthe statistical average of the various quantum states that the system may be in according to the\\ncorresponding microscopic quantity.\\nOne important technical difference between these two neural network structures is:\\nPure Ensemble only needs to ﬁnd out the probability distribution |cn|2 of each layer’s observable\\namount through supervised training, it can satisfy the requirement of human application, do not have\\nto perform layer-by-layer pre-training, and each individual layer is entirely determined by quantum\\nmechanics. From Equation 13 we know that the process is a physical calculation and the result is\\na possible microscopic state, whether this microscopic state is a macroscopic state that satisﬁes the\\nrequirements will be determined by supervised training at all levels. The deep learning of hybrid\\nensemble, because there is no computational formula of quantum mechanics like Equation 13, it\\nmust be pre-trained layer by layer to ﬁnd the probability distribution |ψi(x)|2 of the various states of\\nthe ensemble, then all layers should have supervised training to ﬁnd the probability distribution P of\\nthe systems state ψi(x) , so there must be a second statistical calculation. The size of P cant be solved\\nby quantum mechanics, it requires statistical mechanics principles or assumptions. This answers\\nYoshua Bengio’s perplexity in the paper [20]: training depth-supervised neural networks is usually\\nvery difﬁcult without pre-training with unsupervised learning, but the exception is CNNs. So we are\\nvery curious about what special point in the structure of CNN make it has very good generalization\\nperformance in tasks such as image processing? The answer is that CNN is the purely ensemble\\nof neural network structures, as explained earlier, its individual layers are completely described by\\nquantum mechanics. The hybrid ensemble of neural networks that must be used for unsupervised\\nlearning layer-by-layer pre-training, such as the RBM described previously.\\n3.1.1\\nPure ensemble neural network\\nThe pure ensemble neural network are similar to instrumentation, belongs to the category of physical\\nmeasurement, so its learning performance can be checked because of its clear physical meaning.\\nThe pure ensemble neural network has a prominent advantage. The model it trains is likely to be\\na general model, sometimes it is not only useful in this ﬁeld, but also can be used across ﬁelds.\\nFor example, the CNN trained on ImageNet can also be used for image recognition in furniture\\nprojects, as well as for a wide range of computer vision applications, and can effectively serve as a\\ngeneral model for our visual world. However, as the network deepens, the versatility of the neural\\nnetwork away from the input layer will be signiﬁcantly reduced, and it is generally not reusable. The\\nreusability depends on the depth of the model, because measurement is essentially the interaction of\\nthe input signal and the neural network. As the layer increases, the more damage to the system, the\\nstronger the speciﬁcity of the measurement results and the less versatility. For example, in a speciﬁc\\nCNN, layers appearing earlier in the model will extract locally highly generalized feature maps,\\nwhile higher levels will abstract more abstract concepts (such as cat ears or dog eyes). Therefore, if\\nyour new dataset differs signiﬁcantly from the dataset trained by the original model, it is better to use\\n16\\nonly the ﬁrst few layers of the model for feature extraction instead of using the entire convolution\\nmodel.\\nThe pure ensemble neural networks are completely determined by quantum mechanics, so it is bound\\nto have a good effect once applied. For example, using CNN to recognize the image is the best, and\\ncan be widely applied to the front end of various types of neural networks, and can be widely used\\nin the front-end of various types of neural networks.\\n3.1.2\\nHybrid ensemble neural network\\nThe hybrid ensemble neural networks are neural networks such as classiﬁer, generator, automatic\\nencoder decoder, and the like. The hybrid ensemble neural networks and pure ensemble neural\\nnetworks are often opposite in their characteristics, and the well-trained hybrid ensemble neural net-\\nworks are generally not reusable; the performance of learning is not checkable, and it is essentially\\na black box, because the view of the wave function does not have any macro meaning. For example,\\nthe information that the classiﬁer in image recognition learns about the probability of existence of a\\nclass in the entire graph, completely frees from the concept of space. For issues where the location\\nof the object is important, densely connected characteristic will be essentially useless.\\nIn hybrid ensemble, the observations of mechanical quantities are the results of two statistics. The\\nﬁrst is quantum mechanics, which is caused by the probability property of the state vector; and the\\nsecond is due to the mixture of states, which is caused by the statistical nature of the state at which the\\nsystem may be. So the hybrid ensemble of neural networks must be pre-trained with unsupervised\\nlearning to meet the probability distribution of the state vector of quantum mechanics, and then use\\nsupervised learning to ﬁne tune the entire network (second statistic) to determine macroeconomic\\nequilibrium or stable state.\\n3.2\\nUnderstanding deep learning from the concept of entropy\\n3.2.1\\nThe role of entropy in deep learning\\nInformation is not a simple mathematical concept, but a basic physical concept like matter and\\nenergy. Therefore, all the processing of information (such as computing) is subject to the basic laws\\nof physics, and a key concept in information theory is entropy. Thermodynamic entropy, which is\\nwell known to physicists, is homologous to the information entropy that Shannon uses to measure\\ninformation. The physical metric that describes the number of system states is the entropy. For\\nexample, Brownian motion is the irregular motion exhibited by tiny particles, and the higher the\\ntemperature, the more violent the irregular motion of the molecules. From the microscopic point of\\nview, Brownian motion is disorganized, but from a macroscopic point of view, it is a macroscopic\\nlaw with irreversible (increased entropy). Therefore, the entropy points out the direction of the\\nevolution of the system and describes the conditions under which the system is in equilibrium. In\\ndeep learning, a large amount of data is needed to train the discovery system model. Different\\nmodels have different entropy, the number of all microscopic states corresponding to the correct\\nmodel is extremely large (namely, the entropy is maximum). The cost function of correlated entropy\\nin deep learning applications is deﬁned based on the entropy of the physical principle.\\nThe deﬁnition of entropy in physics is: H = k ln Ω, where Ωis the number of microscopic state.\\nEntropy is additive, the entropy of system is the sum of the entropies of each subsystem, and entropy\\nis a homogeneous function. Let X be a discrete random variable whose probability distribution is\\nP(X = xk) = pk, k = 1, 2, 3, · · · , n , the probability of occurrence of xk is pk , then the number\\nof microscopic state is 1/pk , so the cumulative sum of all the entropy of the random variable X is:\\nH =\\nX\\npi ln 1/pi =\\nX\\npi ln pi\\n(24)\\nThe probability distribution is P(x) , and the random variable is the number of microscopic states\\nof X, i.e. the entropy is:\\nH(X) = −\\nN\\nX\\nk=1\\nP(X = k) log P(X = k)\\n(25)\\n17\\nCross entropy is an extended concept of entropy, it introduces a second probability distribution.\\nThen the number of microstates (entropy) for a random variable X is:\\nH(p(x), q(x)) = −\\nX\\nx∈X\\np(x) logq(x)\\n(26)\\nCross entropy measures the number of microstates of a random variable X under these two proba-\\nbility distributions using physical quantity entropy. Cross entropy contains the difference between\\ntwo probability distributions, and when the two distributions are the same, the entropy is Equation\\n25. It is used in deep learning applications to measure the degree to which the model distribution\\napproximates the unknown distribution.\\nTherefore, entropy plays an important role in deep learning, it has become the objective function of\\nthe selection and adjustment of the parameters of the deep learning model:\\n• How the neural network adjusts parameters, in which direction the parameters are adjusted,\\nthe basis is the entropy of the random variables.\\n• How to determine the conditions for the physical balance and stability of the neural net-\\nwork, the basis is the entropy of the system. For example, the entropy of an isolated equi-\\nlibrium system must be maximum.\\n• Entropy allows us to precisely deﬁne ” correlation ” and extract features from massive data.\\n3.2.2\\nApplication of entropy in cost function\\nAccording to statistical physics, a macroscopic state with a steady state and balance is a state where\\nthe number of microscopic states is maximum (i.e. the entropy is maximum). This is the physical\\nbasis for the selection of the cost function.\\nThe deep learning neural network discussed above are divided into two parts. The former is the neu-\\nral networks that conforms to the laws of physics, such as each convolution layers of CNN, not only\\nthe characteristics of the trained convolution layer have physical signiﬁcance, each of the conditions\\nin the training also has physical signiﬁcance. Because it is calculated according to the physical\\nequations, except that this microscopic state does not correspond to the ﬁnal trained macroscopic\\nstate, or the number of microscopic states that the ﬁnal trained macroscopic state has is very small\\n(entropy is small). The latter part is the classiﬁcation of neural networks, such as a convolution\\nlayer that then maps human symbols (labels) to their physical characteristics. This is a distribution\\nof two completely different concepts, so the cross-entropy of the model’s output (physical) and the\\ntraining target (labeled) should be used as a cost function to approach the real physical model. The\\nmagic of the wonderful combination of classiﬁcation problems and cross-entropy is that even if the\\ncross-entropy on the test set is over-ﬁtted, the classiﬁcation error will not be overﬁt [21].\\nFor the regression problem, it is the same kind of problem, so the mean square error is often used\\nas the objective function or the cost function, which is equivalent to the general entropy maximum.\\nKL divergence is closely related to cross entropy, but it does not include the entropy of the model,\\nit reﬂects the difference between the two distributions. Unlike the use of cross entropy, the KL\\ndivergence is used when there is no more physical distribution. These analyses show the powerful\\npower of statistical physics in understanding deep learning.\\n3.3\\nUnderstanding deep learning from the concept of master equations\\nThe evolution equation of the Markov process probability distribution is the master equation, which\\nis one of the most important equations in statistical physics because it is almost universally applica-\\nble. The Markov process is a process in which most of the memory effects can be omitted during\\nthe evolution process. When the system evolution of a random variable occurs, the transfer process\\nwill occur between different values of the random variables, by transferring the probability of the\\nsystem changes in a given state until the system reaches a ﬁnal equilibrium state.\\nIn deep learning, one of the simplest examples of the Markov process is the Markov chain. The\\nMarkov chain is deﬁned by a random state Xt and a transition distribution T(Xt+1|Xt) . The\\ntransition distribution T is a probability distribution showing the probability of a random transfer to\\n18\\nXt+1 in the case of a given state Xt . Running a Markov chain is the process of constantly updating\\nthe state Xt based on the value Xt+1 of the transition distribution T. Among them, the probability\\ndistribution of the system state at time t + 1 is only related to the state at time t, and has nothing to\\ndo with the state before t.\\n3.4\\nUnderstanding deep learning from the concept of renormalization group\\nDavid Schwab and Pankaj Mehta found that the Deep Belief Network (DBN) invented by Hinton\\nresembled the renormalization in physics in a speciﬁc case, which means that the details of the phys-\\nical system are obtained in a coarse-graining manner to calculate its overall state. When Schwab and\\nMehta applied DBN to a magnetic model at the critical point (when the system is fractal and self-\\nsimilar at any scale), they found that the network automatically uses a reorganization-like process\\nto discover the state of the model. This discovery is shocking, as the biophysicist Ilya Nemenman\\ncommented, it shows that extracting related features in the context of statistical physics and extract-\\ning related features in the context of deep learning are not just similar, but they are completely the\\nsame.” [22].\\nThe renormalization group is a mathematical tool for examining changes in the physical system\\nat different length scales. The details of the physical system are physically obtained in a coarse-\\ngrained manner to calculate its overall state. The important feature of this method is that it is\\nindependent of the system type. Each key step in the renormalization group method is based on the\\nmain characteristics of the system, rather than putting the system into the framework we are familiar\\nwith, and then adjusting the parameters [23]. The standard renormalization process in statistical\\nphysics is equivalent to the feature extraction process of supervised learning in depth learning. The\\ninformation is transmitted layer by layer and eventually converges to the theoretical boundary (ﬁxed\\npoints). The purpose of the renormalization group method is to obtain new features, and to ensure\\nthat the Hamiltonian function forms are unchanged under the new scale of renormalization.\\nFigure 9: The pooling operation in convolution neural network.\\nFor example, the pooling operation (pooling layer) in CNN is based on the main characteristics of\\nthe system and uses the max pooling method to integrate the feature points in the small neighborhood\\naccording to self-similarity to obtain new features. As shown in Figure 9, the pooling layer uses the\\nmax pooling (where the size of the ﬁltering core is 2*2 and the step size is 2) to fuse a feature with\\nan input size of 4*4 and retains only the largest feature point in the area, then the characteristic size\\nafter the pooling operation is 2*2, and the pooling layer plays a role of dimensionality reduction.\\nIt has been proved that there is a one-to-one mapping relationship between RBM-based deep neural\\nnetworks and variational renormalization groups. The paper [7] illustrates the mapping relation-\\nship by analyzing the DNN and numerical two-dimensional Ising model of a one-dimensional Ising\\nmodel, and it ﬁnds that these DNNs self-realize a coarse graining process, i.e. Kadanoff block renor-\\nmalization. The results show that deep learning may adopt a generalized renormalization group class\\n19\\nscheme to learn relevant features from the data. The paper proved that deep learning is closely related\\nto the renormalization group, one of the most important and successful technologies in theoretical\\nphysics.\\n4\\nA physical world view of deep learning\\n4.1\\nThe interpretability of deep learning\\nThe interpretability of deep learning models can be divided into the following categories:\\n(1) Feature attribution VS Internal logic:\\nThe former maps the behavior of the model back to the original set of input features (or\\nartiﬁcially creates optional input features). In the complex decision-making process of the\\nmodel, the larger the inﬂuence of the characteristics will be assigned to the larger weight,\\nthe structure of human knowledge plays a decisive role in this model; The latter argues\\nthat: In the process of obtaining the ﬁnal answer of the model, it is the abstract role of\\nthe physical meaning of the model itself and the internal working logic, rather than human\\nstructural knowledge. Obviously, the interpretation of deep learning in this dissertation\\nbelongs to the latter. This paper analyzes deep learning from the internal logic according\\nto the principle of physics, while most of the papers use the former method to explain deep\\nlearning.\\n(2) Simulation acquires knowledge VS Introspection acquires knowledge:\\nKnowledge based on simulation means that we obtain an understanding of our own model\\nby generating some form of simulation data, capture how the model represents these data\\npoints for understanding; Introspection acquires knowledge comes from the ﬁxed orienta-\\ntion of the model and use them to gain knowledge without having to simulate the former.\\nObviously, the interpretation of deep learning in this dissertation belongs to the latter, while\\nmost papers belong to the former, the focus of their interpretation is to visualize the char-\\nacteristics of deep learning. However, this dissertation holds that the deep neural network\\ndata has high dimensional data characteristics, and human beings cannot understand the\\nvisual characteristics of high-dimensional data.\\nAccording to quantum statistical physics, a system with s classical degrees of freedom, the dynamic\\nstate of the system is determined by the wave function:\\nψσ1σ1,···(q1, q2, · · · qs, t)\\nWhere q is the classical coordinate and σ is the non-classical degree of freedom. People think that\\ndeep learning is incomprehensible. It is precisely because it uses the ψ to characterize the macro-\\nscopic nature, which cannot be understood through visualization, nor can it be understood through\\nmathematics. Deep learning is the interpretation of observed data using the dynamic characteristics\\nof microscopic layers that are not observed by humans or that are not intuitively understood.\\n4.2\\nLocality\\nOne of the deepest principles of physics is locality, that is, things directly affect their surroundings.\\nLocality is a relative concept, usually referring to the scope and degree of inﬂuence of a physical\\nquantity. Locality has two effects:\\n(1) Short-range effect:\\nThe interaction is only a nearby function, ignoring the effect of the remote, that is, the\\neffect of the remote is averaged or canceled. For example, it has been explained before\\nthat during the operation of the Markov chain, the system state at the current moment is\\nonly related to the state at the previous moment, has nothing to do with the state before the\\nprevious moment, that is, the short-range effect.\\n(2) Local coupling:\\nThe local roles of each regions are coupled and then coupled as localization. Scale changes,\\nrenormalization, long procedures, strong correlation, and coarse-graining. For example,\\n20\\npooling in CNN has already been described in the previous section. By using max pool-\\ning, similar features of neighboring regions are merged (that is, processes that are locally\\ncoupled), thereby achieving the effect of dimensionality reduction.\\n4.3\\nSymmetry\\nSymmetry and conservation law are the basic laws of nature. Symmetry can not only reduce the\\nnumber of parameters, but also reduce the computational complexity. See the convolutional neu-\\nral network under the symmetry (translation invariance) we introduced in Section 2.2. Whenever\\nHamiltonian obeys a certain symmetry, that is, invariable under some transformation, the number\\nof independent parameters required to describe it decreases further. The most important mechanical\\nquantities of atoms are momentum and angular momentum, so the corresponding translation and ro-\\ntation transformations are the most important and basic operations. For example, many probability\\ndistributions are constant in the case of translation and rotation.\\nIf the system has translational symmetry, then the state after the translation operation differs from\\nthe original state by a maximum of one phase factor, that is, the difference between the input state\\nand the output state mainly appears as a phase shift. The input wave function can be labeled by the\\nwave vector, and the physical meaning of the wave vector is momentum, which reﬂects the spatial\\nsymmetry of the system. The state with translational symmetry is the eigenstate of the momentum\\noperator, and the momentum represented by the wave vector is the corresponding eigenvalue. There-\\nfore, the process of learning the eigenvalue and the eigenvalue is the process of obtaining the wave\\nvector.\\n4.4\\nConjugacy and Duality\\nConjugacy is the amount of pairing. For example, in physics: pressure and volume, temperature\\nand entropy, intensity and extension are conjugate quantities, in mathematics: real and imaginary\\nnumbers, transposed conjugates of matrix, and so on. Duality is the correspondence between the\\ndifferent physical theories that lead to the same physical results, such as, there is a reaction force\\nwith acting force, there are holes with electrons. These are the important organizations of physics,\\nand they are related to each other and to each other in physical relations. This worldview is also\\nused in deep learning. Here are two examples:\\n(1) Stochastic gradient descent algorithm using momentum\\nAlthough random gradient descent is a very popular optimization method in the training process, the\\nlearning process is sometimes very slow, and even cannot found the best point. The basic physical\\nidea to solve this problem is to introduce conjugates and use duality to solve this problem.\\nThe cost function has a gradient g, there is ﬂow (or velocity or momentum), so the introduction of\\nthe momentum variable v. The gradient of the cost function is seen as a force that pushes the cost\\nfunction to accelerate downhill and the momentum increases; if there is only this kind of force, the\\noptimization process can never stop, so we need to add another force or gradient (called the viscous\\nresistance) to make the cost function converge to a minimum; but the minimum may not be the\\nsmallest, so wo need to increase a momentum, let him out of the local minimum, so as to achieve\\nthe overall optimal. Update the algorithm as follows:\\nv ←αν −εg\\nθ ←θ + ν\\nAs shown in Figure 10, the contour depicts a quadratic loss function (Hessian matrix with ill-\\nconditioned conditions). The red path across the proﬁle represents the path followed by the mo-\\nmentum learning rule, which minimizes the function. We draw an arrow at each step of the path,\\nindicating the step the gradient will take at that point. We can see that the quadratic objective func-\\ntion of a pathological condition looks like a long and narrow valley or a canyon with a steep edge.\\nThe momentum passes right through the gorge, while ordinary gradient steps waste time moving\\naround the narrow axis of the canyon [12].\\n21\\nFigure 10: Stochastic gradient descent algorithm using momentum. Figure courtesy: [12]\\n(2) Annealing and tempering\\nTemperature and entropy are conjugate quantities, when the temperature equals 0, the entropy equals\\n0(minimum); the temperature inﬁnity is the most chaotic, the model becomes uniform distribution.\\nUsing the temperature in the Boltzmann distribution, tempering: The Markov chain temporarily\\nsamples from a high temperature distribution and returns to sampling at unit temperature. Annealing:\\nThe Markov chain temporarily samples from a low temperature distribution and returns to sampling\\nat unit temperature to ﬁnd the best among the different peaks. However, it is necessary to pay\\nattention to the existence of a critical temperature.\\nMost of the models used for prediction in deep learning generally use the softmax activation function\\nto assign probability distributions to tags. The reference [24] mentions that, in the image classiﬁ-\\ncation problem, we divide the pictures into cats, dogs, and tigers three kinds. In a training, the\\nprobability of the three types is [0.0010, 0.0001, 0.9989], and the one-hot code of [0, 0, 1] is ob-\\ntained as the classiﬁcation result (hard-target). However, the intrinsic link between cats and tigers\\ncan easily be overwhelmed during training. This is undoubtedly a waste of valuable a priori prob-\\nabilities that can be used to transfer large-scale network knowledge into small-scale networks. In\\norder to make full use of the correlation between such class categories, we need to change the prob-\\nability distribution in some way to make it more smooth. In the reference, Hinton further modiﬁed\\nthe softmax function:\\nqi =\\nexp(zi/T)\\nP\\nj exp(zj/T)\\nWhen the temperature T = 1 in the equation, it degenerates into the traditional softmax function;\\nwhen T is inﬁnite, the result approaches 1/C, that is, the probabilities on all classes approach to\\nequal; when T > 1, we can obtain the soft target label. By increasing the temperature T, the\\nmapping curve of the softmax layer will be smoother, so the probability mapping of the instances\\nwill be more concentrate and the goal will be ”soft”. Therefore, in order to make full use of the\\ncorrelation between such class categories, the method of changing the probability distribution is to\\nincrease T.\\n4.5\\nHierarchy\\nOne of the most striking features of the physical world is its hierarchy. Spatially, it is an object\\nhierarchy: elementary particles form atoms, then form molecules, cells, organisms, planets, the solar\\nsystem, galaxies, and so on. Complex structures are usually layered and created through a series of\\nsimple steps. The observables of world are inherently hierarchical and cannot be commutative.\\nThe main reason for the success of deep learning is the hierarchical nature of neural networks (deep\\nneural networks). In order to extract uncomplicated features, a multi-layer neural network is required\\nto stack simple networks and then effectively implement the generation process through layering and\\ncombination. Multilayer networks provide layer-by-layer abstract channels from low to high levels.\\n22\\n4.6\\nModel Reusability\\nAs a physical model, especially the convolution neural network, the deep neural network can be used\\nas a universal method because of its physics, so it is reusable in any ﬁeld. The important basis for\\njudging whether a model can be reused or reused is its physical properties. The following analyzes\\nthe problem of model reusability of convolutional neural networks.\\n(1) The convolutional network used for image classiﬁcation consists of two parts: they start\\nwith a series of pooling and convolution layers and end up with densely connected clas-\\nsiﬁers. The ﬁrst part is called the ”convolution base” of the model, it learns completely\\nfrom the physical characteristics, so its model can be used in various ﬁelds, but the dense\\nconnection layer behind is the mapping between annotation and physics, and its model is\\nnot reusable. So in practice, ”feature extraction” simply use the convolved basis of the\\npreviously trained network, run new data through it, and train a new classiﬁer over the\\noutput.\\n(2) There are two conditions for the reusable base to be reused. First, the input and output are\\nthe probability waves in the physical sense. Second, it internally measures the wave vector\\nof the input physical quantity. According to the uncertainty principle of quantum mechan-\\nics, the coordinate and wave vector cannot be measured at the same time. Therefore, its\\noutput does not contain the input position information, it is impossible to obtain accurate\\nposition information of the original input image. And the position is very important for the\\nobject position, the dense connection is more completely useless.\\n(3) Feature characterization extracted using multi-layer architecture will evolve from simple\\nand local to abstract and global in structure, but because of the process, the further away\\nfrom the input layer the convolutional base is, the lower the reusability is. On the one hand,\\nbecause in the continuous pooling, the speciﬁcity of features is getting stronger, and more\\nand more physical information is lost. On the other hand, equation 8 is calculated using the\\ninput as the wave function, the error is getting bigger and bigger, it deviates more and more\\nfrom the real physical model, so the reusability is getting lower and lower. So ﬁne-tuning is\\nanother widely used model reuse technique. Fine-tuning freezes convolutional groups near\\nthe input layer without training, thaws convolutions away from the input layer, and train\\nwith the new classiﬁer to make them more relevant to the problem at hand.\\n4.7\\nModel vulnerability\\nAlthough deep learning has achieved great success in many applications, there are still many limi-\\ntations: for example, it needs a lot of data, the vulnerability of the algorithm etc. Why is the neural\\nnetwork easily disturbed by the input of small disturbances? From the physical model of deep neural\\nnetwork in this paper, we can see that on the one hand this is a physical problem, behind which we\\nhave not found new physical phenomena or new physical modes; On the other hand, labeling does\\nnot match the actual physical characteristics, such as cross-entropy, the two distributions cannot be\\napproximated, or they are approached in a group of training data, it does not mean that the test data\\ncan match nonexistent labels, especially the deception problem. Therefore, the key to the vulnera-\\nbility of the model is the limitation of human understanding. According to the physical world view\\nof solving problems, providing anti-classiﬁcation is a direction worthy of study.\\n4.8\\nCausality and Correlation\\nAt present, deep learning pays attention to relevance instead of causality, and uses joint probability\\ndistribution to replace traditional theorems and laws. The theoretical foundation of deep learning\\nmethods lies in the representation and transformation of statistical probability distributions. It is\\nconsistent with the physical model of the deep neural network in this paper. That is, the state of the\\nmicroscopic particle is completely described by the wave function ψ . After the wave function is\\ndetermined, all the characteristics of the system can be obtained, the average value of any mechanical\\nquantity and its measurement possible value and the corresponding probability distribution are also\\ncompletely determined.\\nHowever, in practice, a mathematical model of causality derived from statistical data will be com-\\nprehensively used. For example, these models can be used to establish a causal relationship between\\n23\\nsmoking and cancer, or to analyze the risks of a construction project, and so on. Can these mathe-\\nmatical models be extended to the microscopic world dominated by quantum mechanics? Can it be\\nincorporated into the deep learning quantum physics model? Since quantum mechanics itself has\\nmany strange features, for example, if two or more quantum systems are entangled with each other,\\nit is difﬁcult to deduce whether the statistical correlation between them is causal.\\nThe concept of causal information actually exceeds statistical relevance. For example, we can com-\\npare these two sentences: ”The number of cars is related to the amount of air pollution” and ”The\\ncar causes air pollution.” The ﬁrst sentence is a statistical statement, and the latter sentence is a\\ncausal statement. Causality differs from relevance because it tells us how the system will change\\nunder intervention. In statistics, causal models can be used to extract causality from the empirical\\ndata of complex systems. However, there is only one component in a system of quantum physicsłthe\\nwave function ψ , so mathematical models that use causality derived from statistical data cannot be\\napplied, including Bayesian inference. John-Mark Allen of Oxford University in the United King-\\ndom proposed a generalized quantum causal model based on Reichenbach’s principle of common\\nreason [25], successfully combining causal intervention and Bayesian inference into a model.\\n5\\nConclusions\\nAt present, the research on the internal theory of deep learning is very scarce, and the successful\\napplication of deep learning and the limitations of its existing technology further illustrate the im-\\nportance of studying its internal technical mechanism from a scientiﬁc perspective. Only knowing\\nwhy to do it can transform existing methods or means from a deeper level. This is the scientiﬁc way\\nof thinking. Based on the principles of physics, this paper interprets the deep learning techniques\\nfrom three different perspectives: microscopic, macroscopic, and physical world perspectives. In-\\nspired by the biological neural network, a new neuron physics model was proposed. Based on this,\\nit explains the success of deep learning well, and fully reveals the internal mechanism of deep learn-\\ning by scientiﬁc methods. A good theory can not only explain existing experiments, but also predict\\nnew phenomena and technologies. Therefore, this dissertation also proposes the direction of further\\nresearch in deep learning. Some of the main conclusions of this paper are as follows:\\n(1) The deep neural network is a physical system, and its architecture and algorithm should\\nconform to the principles of physics. The technical foundation for deep learning is physics,\\nespecially quantum physics and quantum statistical physics.\\n(2) In this dissertation, the physical meaning of neurons in deep neural network is proposed:\\nits output value is the distribution of quasi-particles.\\n(3) Two physical models of deep neural networks are proposed in this paper, one is pure ensem-\\nble deep neural network and the other is hybrid ensemble deep neural network. The former\\nlearning model corresponds to a quantum measurement of a microscopic state, such as\\nCNN; the latter corresponds to a microscopically statistically averaged macroscopic state,\\nsuch as RBM.\\n(4) The physical model of neurons in CNN is a quantum superposition of a quasi-particle\\nincident wave (Figure 1) and is excited by the output. This excitation may be the elastic\\nscattering caused by the incident wave (exit only includes the incident wave), or inelastic\\nscattering (exit also includes internal new excited states), or various possible actions such\\nas chemical reactions (exit only includes new quasiparticles). It obeys quantum mechanics.\\nAccording to the superposition principle of quantum mechanics, the excitation output of a\\nneuron is related not only to the intensity of incident quasi-particles in other neurons, but\\nalso to their coherence, and to their polarization direction or spin.\\n(5) The input of the deep learning network is treated as a wave function, and the image is also a\\nwave. The state of the neuron is also expressed by a wave function. If the measured neuron\\nis the number or probability of excited quasi-particles, the value of a layer of neurons\\nin the neural network is a probability distribution. The deep learning operator should be\\nperformed on the complex number ﬁeld, but because the activation function is ReLU, the\\ncomputational difference in the real number domain may not be large.\\n(6) Under such a physical model, the convolutional neural network algorithm is exactly the\\nsame as the quantum calculation method for measuring the number of quasi-particles ex-\\n24\\ncited by neurons, so that it can perfectly explain the technology of each important com-\\nponents of a convolutional neural network algorithm (convolution, rectiﬁcation, activation,\\npooling, etc.) The purpose of convolution is to measure the number or probability of quasi-\\nparticles excited by each neuron. The convolution kernel is related to the Hamiltonian\\ninteraction potential of neurons. All neurons present an interference diffraction pattern -\\nstripes and patches. That is, the deep convolutional neural network can measure the in-\\nput wave vector or momentum, and its computational model can decompose white light\\ninto monochromatic light and decompose random-direction vibration into single-direction\\npolarization. Therefore, the physical model of this paper can explain deep learning tech-\\nnology and success. This physical model shows that the deep convolutional neural network\\nhas natural learning ability and cognitive ability, and the model learns the ability to char-\\nacterize the input micromechanical quantities, so it is reusable and can be applied across\\nﬁelds.\\n(7) The basis for parameter adjustment and optimization in the deep learning classiﬁcation\\ntraining process is the entropy in statistical physics, which is the number of microscopic\\nstates corresponding to the corresponding macroscopic state. Different types of training\\nmodels should choose different cost functions according to the meaning of entropy, for\\nexample, the convolutional neural network model should use cross entropy as the objective\\nfunction (cost function).\\n(8) A large number of operators, techniques, and methods in deep learning are related to the\\nprinciples of physics such as energy, entropy, renormalization techniques, and translation\\noperations; they are also related to physical world views such as symmetry, conjugacy,\\nlocality, hierarchy, etc.\\nThe research in this paper shows that there are physics glimmers everywhere in deep learning. Deep\\nlearning techniques can be based on scientiﬁc theories. From the principles of physics, this dis-\\nsertation presents the calculation methods for convolution calculation, pooling, normalization, and\\nRBM, as well as the selection of cost functions, explains why deep learning must be deep, what\\ncharacteristics are learned in deep learning, why convolutional neural networks do not have to be\\ntrained layer by layer, and the limitations of deep learning, etc. The physical model proposed in\\nthis paper can not only explain the successful technology of existing deep learning, but also predict\\nmany researchable directions and topics, such as positional neurons (these are in the research stage,\\nand still need to be experimentally veriﬁed).\\nThere is a striking homogeneity in the appearance and structure of the human cerebral cortex. In\\nother words, the cerebral cortex uses the same calculations to accomplish all its functions. All the\\nintelligence that humans exhibit (vision, auditory, physical movement...) are based on a uniﬁed set\\nof algorithms. The deep learning technology is also based on a uniﬁed algorithm and is supported\\nby physical theories. It will have a broad prospect for development.\\nReferences\\n[1] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predict-\\ning 10,000 classes. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 1891–1898, 2014.\\n[2] David Imseng, Petr Motlicek, Philip N. Garner, and Herve Bourlard. Impact of deep mlp\\narchitecture on different acoustic modeling techniques for under-resourced speech recognition.\\nIn Automatic Speech Recognition and Understanding, pages 332–337, 2013.\\n[3] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. Recent trends in deep\\nlearning based natural language processing. arXiv preprint arXiv:1708.02709, 2017.\\n[4] Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar. Physics-guided neural net-\\nworks (pgnn): An application in lake temperature modeling. arXiv preprint arXiv:1710.11431,\\n2017.\\n[5] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436,\\n2015.\\n[6] Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so\\nwell? Journal of Statistical Physics, 168(6):1223–1247, 2017.\\n25\\n[7] Pankaj Mehta and David J Schwab. An exact mapping between the variational renormalization\\ngroup and deep learning. arXiv preprint arXiv:1410.3831, 2014.\\n[8] Xun Gao and Lu-Ming Duan. Efﬁcient representation of quantum many-body states with deep\\nneural networks. Nature communications, 8(1):662, 2017.\\n[9] Yoav Levine, Or Sharir, Nadav Cohen, and Amnon Shashua. Bridging many-body quantum\\nphysics and deep learning via tensor networks. arXiv preprint arXiv:1803.09780, 2018.\\n[10] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artiﬁcial\\nneural networks. Science, 355(6325):602–606, 2017.\\n[11] Ajit Narayanan and Tammy Menneer. Quantum artiﬁcial neural network architectures and\\ncomponents. Information Sciences, 128(3-4):231–255, 2000.\\n[12] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, vol-\\nume 1. MIT press Cambridge, 2016.\\n[13] Isma Hadji and Richard P Wildes. What do we understand about convolutional networks?\\narXiv preprint arXiv:1803.08834, 2018.\\n[14] Ramamurti Shankar. Principles of quantum mechanics. Springer Science & Business Media,\\n2012.\\n[15] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and\\nLe Song. Decoupled networks. arXiv preprint arXiv:1804.08071, 2018.\\n[16] Avraham Ruderman, Neil Rabinowitz, Ari S Morcos, and Daniel Zoran. Learned deformation\\nstability in convolutional neural networks. arXiv preprint arXiv:1804.04438, 2018.\\n[17] Francois Chollet. Deep learning with Python. Manning Publications Co., 2017.\\n[18] Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xiang. Equivalence of restricted\\nboltzmann machines and tensor network states. Physical Review B, 97(8):085104, 2018.\\n[19] Song Cheng, Jing Chen, and Lei Wang. Quantum entanglement: from quantum states of matter\\nto deep learning. Physics, 2017.\\n[20] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R⃝in Machine\\nLearning, 2(1):1–127, 2009.\\n[21] T Poggio, K Kawaguchi, Q Liao, B Miranda, L Rosasco, X Boix, J Hidary, and HN Mhaskar.\\nTheory of deep learning iii: the non-overﬁtting puzzle. Technical report, CBMM memo 073,\\n2018.\\n[22] Shuo-Hui Li and Lei Wang.\\nNeural network renormalization group.\\narXiv preprint\\narXiv:1802.02840, 2018.\\n[23] Maciej Koch-Janusz and Zohar Ringel. Mutual information, neural networks and the renor-\\nmalization group. Nature Physics, page 1, 2018.\\n[24] Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Ge-\\noffrey E Hinton. Large scale distributed neural network training through online distillation.\\narXiv preprint arXiv:1804.03235, 2018.\\n[25] John-Mark A Allen, Jonathan Barrett, Dominic C Horsman, Ciar´an M Lee, and Robert W\\nSpekkens.\\nQuantum common causes and quantum causal models.\\nPhysical Review X,\\n7(3):031021, 2017.\\n[26] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum\\nphysics: A fundamental bridge. arXiv preprint arXiv:1704.01552, 2017.\\n[27] W Kinzel. Physics of neural networks. Europhysics News, 21(6):108–110, 1990.\\n[28] Gary Marcus. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631, 2018.\\n[29] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quan-\\ntum entanglement: Fundamental connections with implications to network design. CoRR,\\nabs/1704.01552, 2017.\\n[30] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R⃝in Machine\\nLearning, 2(1):1–127, 2009.\\n[31] IE Lagaris, A Likas, and DI Fotiadis. Artiﬁcial neural network methods in quantum mechanics.\\nComputer Physics Communications, 104(1-3):1–14, 1997.\\n26\\n[32] Alaa Sagheer and Mohammed Zidan. Autonomous quantum perceptron neural network. arXiv\\npreprint arXiv:1312.4149, 2013.\\n[33] Max Tegmark. Why the brain is probably not a quantum computer. Information Sciences,\\n128(3-4):155–179, 2000.\\n[34] G Perry, ET Rolls, and SM Stringer. Continuous transformation learning of translation invari-\\nant representations. Experimental brain research, 204(2):255–270, 2010.\\n[35] Giuseppe Carleo, Matthias Troyer, Giacomo Torlai, Roger Melko, Juan Carrasquilla, and\\nGuglielmo Mazzola. Neural-network quantum states. Bulletin of the American Physical Soci-\\nety, 2018.\\n[36] Rongxin Xia and Sabre Kais. Quantum machine learning for electronic structure calculations.\\narXiv preprint arXiv:1803.10296, 2018.\\n[37] Rene Vidal, Joan Bruna, Raja Giryes, and Stefano Soatto. Mathematics of deep learning. arXiv\\npreprint arXiv:1712.04741, 2017.\\n[38] IE Lagaris, A Likas, and DI Fotiadis. Artiﬁcial neural network methods in quantum mechanics.\\nComputer Physics Communications, 104(1-3):1–14, 1997.\\n[39] Giacomo Torlai, Guglielmo Mazzola, Juan Carrasquilla, Matthias Troyer, Roger Melko, and\\nGiuseppe Carleo. Neural-network quantum state tomography. Nature Physics, page 1, 2018.\\n[40] MV Altaisky. Quantum neural network. arXiv preprint quant-ph/0107012, 2001.\\n[41] Judea Pearl. Theoretical impediments to machine learning with seven sparks from the causal\\nrevolution. arXiv preprint arXiv:1801.04016, 2018.\\n[42] Daniel J Buehrer. A mathematical framework for superintelligent machines. arXiv preprint\\narXiv:1804.03301, 2018.\\n27\\n',\n",
       " '1806.01756v1.pdf': 'Concept-Oriented Deep Learning \\n \\nDaniel T. Chang (张遵) \\n \\nIBM (Retired) dtchang43@gmail.com \\nAbstract:  Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We \\npropose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and \\nconceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, \\ntransferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of \\nCODL including concept graph, concept representations, concept exemplars, and concept representation learning systems \\nsupporting incremental and continual learning.   \\n1 Introduction \\n1.1 Human Deep Learning \\nIn human learning, deep learning [1] is an approach that involves the critical analysis of new topics and facts, linking \\nthem to already known concepts or forming new concepts, and leads to long term retention of concepts so that they can be \\nused for problem solving in new situations. Deep learning promotes understanding and application for life. This is in contrast \\nto surface learning which is the rote acceptance of facts and memorization as isolated and unlinked facts. It leads to \\nsuperficial retention of facts and does not promote understanding or long term retention of knowledge. \\nThe major characteristics of deep learning are: aiming for understanding, focusing on concepts, and relating new and \\nprevious knowledge. According to the Bloom’s taxonomy [2], there are four types of knowledge: factual, conceptual, \\nprocedural and metacognitive, and six levels of cognitive processes: remember, understand, apply, analyze, evaluate and \\ncreate. Factual knowledge, such as topics and facts, is locked in time, place, and/or situation. Factual knowledge does not \\npromote understanding. Concepts [3] (e.g., dog) are general ideas derived or inferred from facts. They are abstract and broad, \\nrepresented by different instances that share common attributes, universal in application, and timeless. Conceptual knowledge \\nis required for  understanding and provides the framework for relating new and previous knowledge. \\nThe ultimate goal of learning is knowledge transfer [2]. Factual knowledge doesn\\'t transfer, but conceptual \\nunderstanding does. Conceptual understanding is built by abstracting “up” from factual knowledge or examples to understand \\nconcepts and the relationships among concepts. Whenever we try to apply our insights from one situation to another we are \\nalways abstracting to the conceptual level before our knowledge helps us unlock the new situation. When tasks remain \\nsimilar to one another, this is known as low-road transfer. To transfer knowledge to dissimilar tasks requires high-road \\n2 \\n \\ntransfer which involves highly generalized concepts. A deep foundation of facts or surface learning is key for deep learning \\nand knowledge transfer. Synergistic thinking, which involves the interaction between the factual and conceptual levels of \\nthinking, is essential for deep learning. \\n1.2 Machine Deep Learning \\nIn machine learning, deep learning has more than one definition. A useful, though narrow, definition [4] is: deep learning \\nis neural networks with a large number of layers and parameters in one of four fundamental network architectures: \\nunsupervised pretrained networks, convolutional neural networks, recurrent neural networks, and recursive neural networks \\nAutomatic feature extraction is one of the major facets, and great advantages, that deep learning has.  \\nDeep learning is more broadly defined as feature representation learning in [5, 6]. It uses machine learning to discover \\nnot only the mapping from feature representations to output but also the feature representations themselves. To learn features \\nthat best represent data, the goal is to separate the factors of variation that explain the data. Deep learning does this by \\nlearning successive layers of increasingly meaningful feature representations. The ‘deep’ in deep learning stands for this idea \\nof deep layers of feature representations. Deep learning is therefore layered feature representation learning. These layered \\nfeature representations are generally learned via neural networks. \\nDeep learning has achieved near-human accuracy levels in various types of classification and prediction tasks including \\nimages, text, speech, and video data. However, the current technology of deep learning is largely at the level of surface \\nlearning, not deep learning, in human learning, focusing on rote memorization of factual knowledge in the form of feature \\nrepresentations. A deep-learning model [6, 7] is just a chain of simple, continuous geometric transformations mapping one \\ndata manifold into another. Anything that needs reasoning is out of reach for deep-learning models. There are other major \\nlimitations as well. Firstly, the knowledge learned using deep-learning models cannot be transferred because, as discussed \\nearlier, factual knowledge (feature representations) doesn\\'t transfer, but conceptual understanding does. Secondly, deep-\\nlearning models are difficult to understand or interpret [8]. This is to be expected since, as discussed earlier, factual \\nknowledge (feature representations) does not promote understanding. Conceptual knowledge is required for understanding. \\nAs a result of the first two limitations, deep-learning models cannot leverage contextual knowledge. They are developed in \\nisolation, within the narrow confine of the specific training data used, and do not support contextual adaptation [7]. Lastly, \\nbut not the least, deep-learning models require lots of labeled data for training, which can be hard to come by. This is not \\n3 \\n \\nsurprising since deep learning relies on rote memorization of feature representations to perform classification and prediction \\ntasks. It has no conceptual understanding of the data. \\n1.3 Goal and Outline \\nFrom the above discussions it should be apparent that conceptual knowledge learning and conceptual understanding are \\nneeded to elevate machine deep learning toward the level of human deep learning. We propose Concept-Oriented Deep \\nLearning (CODL) as a general approach to achieve that goal. CODL is an extension of (machine) deep learning. It extends \\nfeature representation learning with concept representation learning and it adds the conceptual understanding capability to \\ndeep learning. The major aspects of CODL include: concept graph, concept representations, concept exemplars, and concept \\nrepresentation learning systems. These are discussed in the following sections. The last section provides the summary and \\nconclusion.  \\n2 Concept Graph \\nThe Big Book of Concepts [9] states that “Concepts are the glue that holds our mental world together.” Without \\nconcepts, there would be no mental world. As discussed earlier, concepts [3] (e.g., dog) are general ideas derived or inferred \\nfrom facts. They are abstract and broad, represented by different instances that share common attributes. A concept may \\ncontain a set of attributes that describe the concept and a set of sub-concepts that are components of the concept. Concepts \\nmay also be related by relationships. The most common relationships include isA relationships.  \\nConcepts and categories go together [9]. That is, whatever the concept is, there is a category of things that would be \\ndescribed by it. For material things (objects), ‘category’ is usually referred to as ‘class’; for abstract things (entities), \\n‘category’ is commonly referred to as ‘type’. Thus, concepts denote categories, classes or types, and instances denote things, \\nobjects or entities. \\nMicrosoft Concept Graph [10, 11] aims to give machines \"common-sense computing capabilities\" and an awareness of a \\nhuman\\'s mental world, which is underpinned by concepts. Microsoft Concept Graph is built upon Probase, which uses the \\nworld as its model. The concept graph in Probase is automatically learned from billions of web pages and years\\' worth of \\nsearch logs. The core taxonomy of Microsoft Concept Graph contains over 5.4 million concepts. Microsoft Concept Graph \\nalso has a large data space (each concept contains a set of instances or sub-concepts), a large attribute space (each concept is \\ndescribed by a set of attributes), and a large relationship space (e.g., \"isA\", \"locatedIn\"). \\n4 \\n \\nThe Microsoft Concept Tagging Model [11 – 16], a part of Microsoft Concept Graph, maps text entities into concepts \\nwith some probabilities, which may depend on the context texts of the entities. Given an input text entity, it returns a ranked \\nlist of concepts. Each concept on the list has a probability denoting the possibility of the text entity belonging to this concept. \\nBesides, some common measures for conceptualization (e.g., Typicality) are provided simultaneously. \\nCODL uses Microsoft Concept Graph as the common / background conceptual knowledge base and the framework for \\nconceptual understanding, due to its probabilistic nature and extensive scope. Microsoft Concept Graph plays an important \\nrole in CODL. Its usage in CODL is discussed in the following sections. However, CODL is not limited to using Microsoft \\nConcept Graph as the common / background conceptual knowledge base. Other comparable system can be used as such. \\n3 Concept Representations \\nIn deep learning, feature representations are generally learned as a blob of ungrouped features. However, an increasing \\nnumber of visual applications nourish from inferring knowledge from imagery which requires scene understanding. Semantic \\nsegmentation is a task that paves the way towards scene understanding. Deep semantic segmentation [17] uses deep learning \\nfor semantic segmentation.  \\nDeep semantic segmentation makes dense predictions inferring labels for every pixel. It can be carried out at three \\ndifferent levels: \\n\\uf0b7 \\nClass segmentation: each pixel is labeled with the class of its enclosing object or region \\n\\uf0b7 \\nInstance segmentation: separate labels for different instances of the same class \\n\\uf0b7 \\nPart segmentation: decomposition of already segmented classes into their component sub-classes \\nCODL extends and generalizes deep semantic segmentation. In CODL, feature representations are always learned \\nsemantically segmented in a concept-oriented manner. Concept orientation means that each feature representation is \\nassociated with a concept, an instance or an attribute. These concepts, instances and attributes form a concept graph. In \\naddition, the concept graph are generally linked to Microsoft Concept Graph, thus leveraging and integrating with the \\ncommon conceptual knowledge and conceptual understanding capability provided by Microsoft Concept Graph. \\nA concept representation consists of a concept, its instances and attributes, and all the feature representations associated \\nwith the concept and its instances and attributes. If a concept has sub-concepts, its concept representation also consists of the \\n5 \\n \\nconcept representations of its sub-concepts. Concept representations, therefore, are the same as concept-oriented feature \\nrepresentations, but provide a different view. The latter is data driven and provides a bottom-up view starting from feature \\nrepresentations; the former is concept driven and provides a top-down view starting from concepts. Due to the focus on \\nconcepts instead of low-level feature representations, concept representations provide the proper view to work with in CODL.  \\n3.1 Supervised Concept Representation Learning \\nConcept representations can be learned using supervised learning. Similar to deep semantic segmentation [17], discussed \\nabove, it can be carried out at different levels: \\n\\uf0b7 \\nConcept level: each feature representation is labeled with the concept that owns the feature \\n\\uf0b7 \\nInstance level: separate labels for different instances of the same concept \\n\\uf0b7 \\nAttribute level: separate labels for different attributes of the same concept \\n\\uf0b7 \\nComponent level: decomposition of already learned concept representations into their sub-concept \\nrepresentations \\nThe concept, instance and attribute names used for labeling should be taken from Microsoft Concept Graph, if available. \\nThis provides direct link to Microsoft Concept Graph to leverage its common conceptual knowledge and conceptual \\nunderstanding capability. \\n4 Concept Exemplars \\nAs is the case for deep semantic segmentation, it can be difficult to gather and create labeled concept representation \\ndatasets to use for training in supervised learning. Due to the semantically-segmented nature of concepts, a good alternative is \\nto use concept exemplars.  \\nA concept exemplar set is a set of one or more typical instances of a concept, possibly augmented with instances \\ngenerated from the typical instances using identity-preserving transformations. As an example, for image objects the identity-\\npreserving transformations [18] typically include: scaling, translation, rotation, contrast and color. Each concept is associated \\nwith at most one concept exemplar set. \\n6 \\n \\nWith concept exemplars one can use supervised concept representation learning, as discussed earlier, or unsupervised \\nconcept representation learning, as discussed below. Concept exemplars can facilitate incremental and continual learning, to \\nbe discussed later. \\n4.1 Unsupervised Concept Representation Learning \\nExemplar-CNN [18] is an approach for training a convolutional network using only unlabeled data. It trains the network \\nto discriminate between a set of surrogate classes. Each surrogate class is formed by applying a set of transformations to a \\nrandomly sampled ’seed’ image patch. The resulting feature representations are not task specific. They are generic and \\nprovide robustness to the transformations that have been applied during training. The applied transformations thus define the \\ninvariance properties that are to be learned by the network. \\nUnsupervised concept representation learning uses the same approach as Exemplar-CNN. In CODL, concept exemplars \\nplay the role of surrogate classes, and identity-preserving transformations that of applied transformations, in Exemplar-CNN. \\nWhereas surrogate classes are based on randomly sampled ’seed’ image patches, concept exemplars are based on \\nsemantically distinct, typical instances of concepts. Therefore, we expect unsupervised concept representation learning based \\non concept exemplars to result in generic, transferable concept representations. \\n5 Concept Representation Learning Systems \\nConcept representation learning systems provide the platforms and tools for use in CODL. They support supervised \\nconcept representation learning as well as unsupervised concept representation learning based on concept exemplars. They \\nalso provide access to the common / background conceptual knowledge base, such as Microsoft Concept Graph. The \\nfollowing are major aspects of concept representation learning systems which are important for the success of CODL and its \\napplication. \\n5.1 Incremental and Continual Learning \\nIn real-world scenarios, concepts and their associated data are almost always collected in an incremental manner. As \\nsuch, incremental and continual learning [19] is a critical aspect of CODL. A good concept representation learning system \\nmust accommodate new concepts and their associated data that it is exposed to and gradually expands its capacity to predict \\nincreasing number of new concepts. \\n7 \\n \\nDoing incremental learning using deep neural network faces inherent technical challenges. Neural networks embed \\nfeature extraction and classification within the same model. This gives rise to the so-called “catastrophic forgetting / \\ninterference\" problem [20] which refers to the destruction / modification of existing feature representations learned from \\nearlier data, when the model is exclusively trained with data of new concepts.  \\nTherefore, the challenge is to be able to incrementally and continually learn over time by accommodating new concepts \\nand their data while retaining previously learned concept representations. There are various approaches [19] for incremental \\nand continual learning that mitigate, to different extents, catastrophic interference.  The regularization approaches alleviate \\ncatastrophic interference by imposing constraints on the update of the neural weights. The dynamic architecture approaches \\nchange architectural properties in response to new concepts and their data, either by dynamically accommodating novel \\nneural resources or by re-training with an increased number of neurons or network layers. \\nA good approach to use in CODL is that of iCaRL [21] (incremental classifier and representation learning), which is a \\ndynamic architecture approach. The approach allows learning in a concept-incremental way: only the training data for a small \\nnumber of concepts has to be present at the same time and new concepts can be added progressively. Concept-incremental \\nlearning has the following properties:  \\n\\uf0b7 \\nit is trainable from a data stream in which examples of different concepts appear at intermittent times, \\n\\uf0b7 \\nat any time it provides a competitive multi-concept classifier for all the concepts learned so far, and \\n\\uf0b7 \\nit does not require storing all training data or retraining already-learned concepts whenever new data becomes \\navailable. \\nThe main components [21] that enable simultaneous learning of concept representations and concept classifiers in the \\nconcept-incremental manner are: \\n\\uf0b7 \\nconcept representation learning using knowledge distillation and prototype rehearsal, \\n\\uf0b7 \\nconcept exemplar selection / learning based on herding, and \\n\\uf0b7 \\nconcept classification by the nearest mean of concept exemplars. \\nThese are discussed below. \\n8 \\n \\nPrior to that we note that the deep learning network is used only for concept representation learning and concept \\nexemplar selection, not for classifying new data, which is done using concept classifiers based on concept exemplars. The \\nconcept representation learning scheme, using knowledge distillation, addresses the “catastrophic forgetting / interference” \\nproblem of incremental and continual learning. The use of concept exemplars, for concept classifiers, avoids the other \\nproblem: storing of all training data. \\nConcept Representation Learning \\nWhenever data for new concepts arrive, the concept representations and concept exemplar sets are updated. Firstly, the \\nnetwork outputs for the existing concepts are stored. Secondly, an augmented training set is constructed, consisting of the \\n(newly available) training examples for the new concepts together with the concepts exemplar sets for the existing concepts. \\nFinally, the deep learning network is trained / updated by minimizing a loss function to output the correct concept indicators \\nfor new concepts (classification loss), and for old concepts, to reproduce the scores stored in the first step (distillation loss). \\nThe classification loss enables improvements of the concept representations that allow classifying the new concepts well; the \\ndistillation loss ensures that the discriminative information learned for existing concepts is not lost while training for the new \\nconcepts. \\nConcept Exemplars Selection / Learning \\nConcept exemplars selection is required for each concept only once, when it is first learned and its training data is \\navailable. For each concept, concept exemplars are selected and stored iteratively until the target number is met. In each step \\nof the iteration, one more example of the training set is added to the concept exemplar set, namely the one that causes the \\naverage feature vector over all concept exemplars to best approximate the average feature vector over all training examples. \\nThus, the concept exemplar set is a prioritized list, with concept exemplars earlier in the list being more important. The \\nconcept exemplar set may be augmented using identity-preserving transformations, as discussed earlier. \\nConcept Classification Based on Concept Exemplars \\nFor concept classification, a nearest-mean-of-concept-exemplars classification strategy is used. To predict a concept \\nlabel for a new sample, it first computes a prototype vector for each concept by computing the average feature vector of all \\nconcept exemplars for the concept. It then computes the feature vector of the sample that should be classified and assigns the \\nconcept label with the most similar prototype. The concept prototypes automatically change whenever the concept \\n9 \\n \\nrepresentations change, making the classifier robust against changes of the concept representations (as new concepts are \\nlearned) \\n5.2 Concept Taxonomy and Basic-Level Concepts \\nConcept taxonomy [9] is one particular kind of concept organization: the hierarchical structure of concepts with each \\nbranch being a sequence of progressively larger concepts in which each concept includes all the previous ones. Different \\nlevels of concepts reflect different levels of abstraction, which associate with different attributes. These taxonomic concepts \\nare important for thought and communication. \\nIn a concept taxonomy, the concepts that are higher in the hierarchy are superordinate to the lower-level concepts; the \\nlower-level concepts are subordinate to the higher-level ones. The only relationship allowed between concepts in the \\nhierarchy is the set inclusion relationship: the set of instances of  a superordinate concept (e.g., dog) includes the set of \\ninstances of its subordinate concept (e.g., bull dog). The set inclusion relationship is called the ‘‘isA’’ relationship, which is \\nasymmetric and transitive. The transitivity of concept relationship in the hierarchy leads to a similar transitivity of attribute \\nascription, called attribute inheritance. Every attribute of a concept is also an attribute of the concept’s subordinates. \\nBy being able to locate a concept in its proper place in the concept taxonomy, one can learn a considerable amount about \\nthe concept, e.g., its superordinates and inherited attributes. In CODL, this is achieved by accessing Microsoft Concept \\nGraph, as discussed near the beginning. Clearly, this is an important ability, since it allows one to immediately access \\nknowledge (concepts) about new objects or entities without the need to directly learn. \\nBasic-Level Concepts \\nThe objects and entities that we encounter every day do not each fit into a single concept, but can be classified with a \\nlarge number of different concepts. It is important to know the preferred concept by which people think about any one object \\nor entity.  \\nAny object or entity can be thought of as being in a set of hierarchically organized concepts, i.e., a concept taxonomy, \\nranging from extremely general (e.g., animal) to extremely specific (e.g., bull dog). Classification at the most general level \\nmaximizes accuracy of classification. Most specific concepts, on the other hand, allow for greater accuracy in prediction. Of \\nall the possible concepts in a concept taxonomy to which a concept belongs, a middle level of specificity, the basic level [9], \\n10 \\n \\nis the most natural, preferred level at which to conceptually carve up the world. The basic level (e.g., dog) can be seen as a \\ncompromise between the accuracy of classification at a most general level and the predictive power of a most specific level.  \\nSuperordinate concepts (e.g., animal) are distinctive but not informative. Subordinate concepts (e.g., bull dog) are \\ninformative but not distinctive. It is only basic-level concepts (e.g., dog) that are both informative and distinctive. Basic-level \\nconcept is important because it provides rich information with little cognitive efforts. When a person obtains the basic-level \\nconcept of an unfamiliar object or entity, she will associate the object or entity with the known attributes of the basic-level \\nconcept. \\nTherefore, to be effective, in CODL one should focus on learning and using concept representations for basic-level \\nconcepts [11]. Superordinate concepts are automatically “learned” by accessing Microsoft Concept Graph, as discussed \\nearlier. When needed, this can be supplemented by learning and using concept representations for selective subordinate \\nconcepts. \\n6 Summary and Conclusion \\n \\nConcepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. The current \\ntechnology of machine deep learning is largely at the level of surface learning in human learning, focusing on rote \\nmemorization of factual knowledge in the form of feature representations. To elevate machine deep learning toward the level \\nof human deep learning, we proposed concept-oriented deep learning (CODL) which extends (machine) deep learning with \\nconcept representations and conceptual understanding capability. \\nCODL leverages Microsoft Concept Graph, or something comparable, as the common / background conceptual \\nknowledge base and the framework for conceptual understanding. In particular, concept names and concept taxonomies (isA \\nrelationships) originate from Microsoft Concept Graph. In CODL, feature representations are always learned semantically \\nsegmented in a concept-oriented manner. Concept representations are the same as concept-oriented feature representations, \\nbut from a top-down, concept-driven perspective which is the focus of CODL. It can be difficult to gather and create labeled \\nconcept representation datasets to use for training. Due to the semantically-segmented nature of concepts, a good alternative \\nis to use concept exemplars. \\nConcept representation learning systems provide the platforms and tools for use in CODL. They support supervised \\nconcept representation learning as well as unsupervised concept representation learning based on concept exemplars. Since, \\n11 \\n \\nin real-world scenarios, concepts and their associated data are almost always collected in an incremental manner, a good \\nconcept representation learning system must support incremental and continual learning (using concept exemplars). Also, to \\nbe effective, in CODL one should focus on learning and using concept representations for basic-level concepts. \\nBy focusing on learning and using concept representations and concept exemplars, CODL is able to address some of the \\nmajor limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled \\ntraining data.  \\nReferences \\n \\n[1] W. Houghton, Engineering Subject Centre Guide: Learning and Teaching Theory for Engineering Academics \\n(Loughborough: HEA Engineering Subject Centre, 2004). \\n[2] Julie Stern, Krista Ferraro and Juliet Mohnkern, Tools for Teaching Conceptual Understanding, Secondary: Designing \\nLessons and Assessments for Deep Learning (Corwin, 2017). \\n[3] Jean Donham, “Deep Learning through Concept-based Inquiry,” School Library Monthly, Volume XXVII, Number 1, 8 \\n(2010). \\n[4] Josh Patterson and Adam Gibson, Deep Learning A Practitioner’s Approach (O’Reilley, 2017). \\n[5] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning (MIT Press, 2016) \\n[6] Francois Chollet, Deep Learning with Python (Manning, 2018) \\n[7] John Launchbury, A DARPA Perspective on Artificial Intelligence (DARPA, 2017). \\n[8] Z. C. Lipton, “The mythos of model interpretability,” arXiv preprint arXiv:1606.03490 (2017). \\n[9] G. L. Murphy, The Big Book of Concepts (MIT Press, 2002). \\n[10] Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Zhu, “Probase: A Probabilistic Taxonomy for Text \\nUnderstanding,”  in ACM International Conference on Management of Data (SIGMOD), May 2012. \\n[11] Zhongyuan Wang, Haixun Wang, Ji-Rong Wen, and Yanghua Xiao, “An Inference Approach to Basic Level of \\nCategorization,”  in ACM International Conference on Information and Knowledge Management (CIKM), ACM – \\nAssociation for Computing Machinery, October 2015. \\n[12] Yangqiu Song, Haixun Wang, Zhongyuan Wang, Hongsong Li, and Weizhu Chen, “Short Text Conceptualization using \\na Probabilistic Knowledgebase,” in IJCAI, 2011. \\n[13] Zhongyuan Wang, Haixun Wang, and Zhirui Hu, “Head, Modifier, and Constraint Detection in Short Texts,” in \\nInternational Conference on Data Engineering (ICDE), 2014. \\n[14] Zhongyuan Wang, Kejun Zhao, Haixun Wang, Xiaofeng Meng, and Ji-Rong Wen, “Query Understanding through \\nKnowledge-Based Conceptualization,”  in IJCAI, July 2015. \\n[15] Wen Hua, Zhongyuan Wang, Haixun Wang, Kai Zheng, and Xiaofang Zhou, “Short Text Understanding Through \\nLexical-Semantic Analysis,” in International Conference on Data Engineering (ICDE), April 2015.  \\n[16] Zhongyuan Wang and Haixun Wang, “Understanding Short Texts,” in the Association for Computational Linguistics \\n(ACL) (Tutorial), August 2016. \\n[17]  A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, and J. Garcia-Rodriguez, “A Review on Deep \\nLearning Techniques Applied to Semantic Segmentation,” arXiv preprint arXiv:1704.06857 (2017). \\n[18] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox, “Discriminative \\nUnsupervised Feature Learning with Exemplar Convolutional Neural Networks,” arXiv preprint arXiv: 1406.6909 (2015). \\n[19] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter, “Continual Lifelong Learning \\nwith Neural Networks: A Review,” arXiv preprint arXiv:1802.07569 (2018) \\n[20] I. J. Goodfellow, M. Mirza, X. Da, A. Courville, and Y. Bengio, “An Empirical Investigation of Catastrophic Forgeting \\nin Gradient-Based Neural Networks,” arXiv preprint arXiv:1312.6211 (2015). \\n[21] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “iCaRL: Incremental Classifier and Representation \\nLearning,” CVPR’14, pages 2001–2010 (2017). \\n \\n',\n",
       " '1812.05448v4.pdf': 'A First Look at Deep Learning Apps on Smartphones\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu\\nKey Lab of High-Confidence Software Technologies\\n(Peking University), MoE, Beijing, China\\n{mwx,1500012828,yuanqiangliu}@pku.edu.cn\\nFelix Xiaozhu Lin\\nPurdue ECE\\nWest Lafayette, Indiana, USA\\nxzl@purdue.edu\\nYunxin Liu\\nMicrosoft Research\\nBeijing, China\\nyunxin.liu@microsoft.com\\nXuanzhe Liu\\nKey Lab of High-Confidence Software Technologies\\n(Peking University), MoE, Beijing, China\\nxzl@pku.edu.cn\\nABSTRACT\\nWe are in the dawn of deep learning explosion for smartphones.\\nTo bridge the gap between research and practice, we present the\\nfirst empirical study on 16,500 the most popular Android apps,\\ndemystifying how smartphone apps exploit deep learning in the\\nwild. To this end, we build a new static tool that dissects apps and\\nanalyzes their deep learning functions. Our study answers threefold\\nquestions: what are the early adopter apps of deep learning, what do\\nthey use deep learning for, and how do their deep learning models\\nlook like. Our study has strong implications for app developers,\\nsmartphone vendors, and deep learning R&D. On one hand, our\\nfindings paint a promising picture of deep learning for smartphones,\\nshowing the prosperity of mobile deep learning frameworks as well\\nas the prosperity of apps building their cores atop deep learning. On\\nthe other hand, our findings urge optimizations on deep learning\\nmodels deployed on smartphones, protection of these models, and\\nvalidation of research ideas on these models.\\nKEYWORDS\\nMobile Computing, Deep Learning, Empirical Study\\n1\\nINTRODUCTION\\nBeing ubiquitous, smartphones are among the most promising\\nplatforms for deep learning (DL), the key impetus towards mobile\\nintelligence in recent years [3, 6, 8, 28]. Such a huge market is driven\\nby continuous advances in DL, including the introduction of latest\\nneural network (NN) hardware [48, 50, 62, 118], improvement in\\nDL algorithms [58, 86, 91, 99], and increased penetration in huge\\ninformation analytics [54, 59, 88, 90]. The research community has\\nbuilt numerous DL-based novel apps [46, 74, 75, 85, 92, 117]. The\\nindustry has also tried to utilize DL in their mobile products. For\\nexample, in the newly released Android 9 Pie OS, Google introduces\\na small feed-forward NN model to enable Smart Linkify, a useful\\nAPI that adds clickable links when certain types of entities are\\ndetected in text [39].\\nYear 2017 marked the dawn of DL for smartphones. Almost si-\\nmultaneously, most major vendors roll out their DL frameworks for\\nsmartphones, or mobile DL framework for short. These frameworks\\ninclude TensorFlow Lite (TFLite) from Google [38] (Nov. 2017),\\nXuanzhe Liu is the paper’s corresponding author.\\nWWW’19, May 2019, San Francisco, USA\\n2019. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\nCaffe2 from Facebook [9] (Apr. 2017), Core ML from Apple [12]\\n(Jun. 2017), ncnn from Tencent [36] (Jul. 2017), and MDL from\\nBaidu [25] (Sep. 2017). These frameworks share the same goal: exe-\\ncuting DL inference solely on smartphones. Compared to offloading\\nDL inference from smartphones to the cloud [2, 20, 22], on-device\\nDL inference better protects user privacy, continues to operate in\\nthe face of poor Internet connectivity, and relieves app authors\\nfrom paying the expense of running DL in the cloud [48, 69, 72, 73,\\n73, 75, 80, 114, 118].\\nFollowing the DL framework explosion, there emerges the first\\nwave of smartphone apps that embrace DL techniques. We deem\\nit crucial to understand these apps and in particular how they use\\nDL, because history has proven that such early adopters heavily\\ninfluence or even decide the evolution of new technologies [95] –\\nsmartphone DL in our case.\\nTo this end, we present the first empirical study on how real-\\nworld Android apps exploit DL techniques. Our study seeks to\\nanswer threefold questions: what are the characteristics of apps\\nthat have adopted DL, what do they use DL for, and what are their\\nDL models. Essentially, our study aims findings on how DL is being\\nused by smartphone apps in the wild and the entailed implications,\\nfilling an important gap between mobile DL research and practice.\\nFor the study, we have examined a large set of Android apps\\nfrom the official Google Play market. We take two snapshots of the\\napp market in early Jun. 2018 and early Sep. 2018 (3 months apart),\\nrespectively. Each snapshot consists of 16,500 the most popular apps\\ncovering 33 different categories listed on Google Play. We generate\\ninsights by inspecting individual apps as well as by comparing the\\ntwo snapshots. To automate the analysis of numerous Android apps,\\nwe build a new analyzer that inspects app installation packages,\\nidentifies the apps that use DL (dubbed “DL apps”), and extracts DL\\nmodels from these apps for inspection. To realize such a tool, we\\neschew from looking for specific code pattern and instead identify\\nthe usage of known DL frameworks, based on a rationale that most\\nDL apps are developed atop DL frameworks.\\nOur key findings are summarized as follows.\\nEarly adopters are top apps (§4.2) We have found 211 DL apps in\\nthe set of apps collected in Sep. 2018. Only 1.3% of all the apps, these\\nDL apps collectively contribute 11.9% of total downloads of all the\\napps and 10.5% of total reviews. In the month of Sep. 2018, the 221\\nDL apps are downloaded for around 13,000,000 times and receive\\n9,600,000 reviews. DL apps grow fast, showing a 27% increase in\\ntheir numbers over the 3 months in our study.\\narXiv:1812.05448v4  [cs.LG]  13 Jan 2021\\nWWW’19, May 2019, San Francisco, USA\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu\\nDL is used as core building blocks (§4.3) We find that 81% DL\\napps use DL to support their core functionalities. That is, these apps\\nwould fail to operate without their use of DL. The number of such\\nDL apps grow by 23% over the period of 3 months.\\nPhoto beauty is the top use (§4.3) DL is known for its diverse\\napplications, as confirmed by the usage discovered by us, e.g. emoji\\nprediction and speech recognition. Among them, photo beauty is\\nthe most popular use case: 94 (44.5%) DL apps use DL for photo\\nbeauty; 61 (29%) DL apps come from the photography category.\\nMobile DL frameworks are gaining traction (§4.4) While full-\\nfledged DL frameworks such as TensorFlow are still popular among\\nDL apps due to their momentum, DL frameworks designed and\\noptimized for constrained resources are increasingly popular. For\\ninstance, the number of DL apps using TFLite has grown by 258%\\nover the period of 3 months.\\nMost DL models miss obvious optimizations. (§5.2) Despite\\nwell-known optimizations, e.g. quantization which can reduce DL\\ncost by up to two orders of magnitude with little accuracy loss [63],\\nwe find only 6% of DL models coming with such optimizations.\\nOn-device DL is lighter than one may expect. (§5.3) Despite\\nthe common belief that the power of DL models comes from rich\\nparameters and deep layers, we find that DL models used in apps are\\nvery small, with median memory usage of 2.47 MB and inference\\ncomputation of 10M FLOPs, which typically incurs inference delay\\nof tens of milliseconds. These models are not only much lighter\\nthan full models for servers (e.g. ResNet-50 with 200 MB memory\\nand 4G FLOPs inference computations) but also lighter than well-\\nknown models specifically crafted for smartphones, e.g. MobileNet\\nwith 54 MB memory and 500M FLOPs inference computations.\\nDL models are poorly protected. (§5.4) We find only 39.2% dis-\\ncovered models are obfuscated and 19.2% models are encrypted.\\nThe remaining models are trivial to extract and therefore subject\\nto unauthorized reuse.\\nSummary of implications: Overall, our findings paint a promis-\\ning picture of DL on smartphones, motivating future research and\\ndevelopment. Specifically, the findings show strong implications\\nfor multiple stakeholders of the mobile DL ecosystem. To app de-\\nvelopers: our findings show that DL can be very affordable on\\nsmartphones; developers, especially individuals or small compa-\\nnies, should have more confidence in deploying DL in their apps;\\ninterested developers should consider building DL capability atop\\nmobile DL frameworks; a few app categories, notably photography,\\nare most likely to benefit from DL techniques. To DL framework\\ndevelopers: our findings encourage continuous development of\\nframeworks optimized for smartphones; our findings also show\\nthe urgent need for model protection as the first-class concern of\\nframeworks. To hardware designers: our findings motivate DL\\naccelerator designs to give priority to the layers popular among\\nmobile DL models. To DL researchers: our findings suggest that\\nnew proposal for optimizing DL inference should be validated on\\nlightweight models that see extensive deployment on smartphones\\nin the wild.\\nIn summary, our contributions are as follows.\\n• We design and implement a tool for analyzing the DL adop-\\ntion in Android apps. Capable of identifying the DL usage\\nin Android apps and extracting the corresponding DL mod-\\nels for inspection, our tool enables automatic analysis of\\nnumerous apps for DL.\\n• We carry out the first large-scale study of 16,500 Android\\napps for their DL adoption. Through the empirical analysis,\\nwe contribute new findings on the first wave of apps that\\nadopt DL techniques. In the dawn of DL explosion for smart-\\nphones, our findings generate valuable implications to key\\nstakeholders of the mobile ecosystem and shed light on the\\nevolution of DL for smartphones. We plan to publicize our\\ntools and datasets.\\nThe remainder of the paper is organized as follows. We describe\\nthe background knowledge and our motivations in Section 2. We\\npresent our research goal and the analyzing tool which helps us\\nidentify DL usage and extract DL models in Section 3. We present\\nthe analysis results of DL apps and DL models in Section 4 and\\nSection 5, respectively. We discuss the limitations and possible\\nfuture work in Section 6. We survey the related work in Section 7,\\nand conclude in Section 8.\\n2\\nBACKGROUND\\nDL models and frameworks DL has revolutionized many AI\\ntasks, notably computer vision and natural language processing,\\nthrough substantial boosts in algorithm accuracy. In practice, DL\\nalgorithms are deployed as two primary parts. The first one is\\nDL models, which often comprise neuron layers of various types,\\ne.g. convolution layers, pooling layers, and fully-connected lay-\\ners. Based on the constituting layers and their organizations, DL\\nmodels fall into different categories, e.g., Convolutional Neural Net-\\nwork (CNN) containing convolution layers, and Recurrent Neural\\nNetwork (RNN) processing sequential inputs with their recurrent\\nsub-architectures. The second part is DL frameworks that produce\\nDL models (i.e. training) and execute the models over input data\\n(i.e. inference). Since a production DL framework often entails\\ntremendous engineering efforts, most app developers tend to ex-\\nploit existing frameworks by major vendors, such as TensorFlow\\nfrom Google.\\nDeploying mobile DL As training models is intensive in both\\ndata and computing [79], smartphone developers often count on\\ncloud servers for modeling training offline prior to app deployment.\\nAt app installation time, the trained models are deployed as part of\\nthe app installation package. At runtime, apps perform inference\\nwith the trained models by invoking DL frameworks, and therefore\\nexecute AI tasks such as face recognition and language translation.\\nInference: on-cloud vs. on-device Towards enabling DL on\\nsmartphones, model inference can be either offloaded to the cloud\\nor executed solely on smartphones. Offloading to the cloud is a\\nclassical use case of Software-as-a-Service (SaaS), and has been well\\nstudied in prior work [44, 67, 94, 116]. The mobile devices upload\\ndata and retrieve the inference results, transparently leveraging\\nrich data center resources as server-class GPU. Yet, we have ob-\\nserved on-device DL inference is quickly gaining popularity due\\nto its unique advantages of stronger privacy protection, resilience\\nagainst poor Internet connectivity, and lower cloud cost to app de-\\nvelopers. We will present more evidence in the paper. In this work,\\nA First Look at Deep Learning Apps on Smartphones\\nWWW’19, May 2019, San Francisco, USA\\nwe focus our empirical study on such on-device deep learning for\\nsmartphones.\\n3\\nGOAL AND METHODOLOGY\\n3.1\\nResearch Goal\\nThe goal of our study is to demystify how smartphone apps\\nexploit DL techniques in the wild. Our study focuses on two types\\nof subjects: i) smartphone apps that embrace DL and ii) the DL\\nframeworks and models used in practice. Accordingly, we charac-\\nterize the apps, the frameworks, and the models. We will present\\nthe results in Section 4 and Section 5 respectively.\\nScope We focus our analysis on Android apps, as Android rep-\\nresents a dominant portion of smartphone shipment (88% in the\\nsecond quarter of 2018) and hence serves a good proxy for the entire\\nsmartphone app population [19].\\nDatasets We retrieve from the Google Play store the full dataset\\nused in this work. We select 16,500 apps in total, which consist of\\nthe top 500 free apps with most downloads from each of the 33\\ncategories defined by Google Play1. We have crawled two datasets at\\ndifferent moments, June 2018 and September 2018, which are three\\nmonths apart. The two app datasets have more than 2/3 overlapped\\napps. For each app, we download its apk file and crawl its meta\\ninformation (e.g. app description and user rating) from the Google\\nPlay web page for analysis. Our analysis primarily focuses on the\\nnewer dataset, i.e., Sep. 2018, and the difference between the two\\ndata sets, unless specified otherwise.\\n3.2\\nWorkflow Overview\\nWe design and implement an analyzing tool to enable our re-\\nsearch goal on large-scale Android apps. The tool runs in a semiau-\\ntomatic way, as illustrated in Figure 1.\\nThe very first step of the analyzing tool is identifying DL apps\\namong a given set of Android apps as input. This is achieved via the\\nmodule named DL Sniffer. The core idea of DL Sniffer, is detecting\\nthe usage of popular DL frameworks, instead of directly finding\\nthe usage of DL. After identifying DL apps, it performs analysis\\non those apps. During analysis, we use the manifest files extracted\\nfrom DL apps via tool aapt [4] and the meta information crawled\\nfrom the corresponding Google Play web page. The manifest files\\ninclude information such as package name, app version, required\\npermissions, etc. The web pages include information such as app\\ndescription, user rating, app developer, etc.\\nThe analyzing tool further extracts DL models from those DL\\napps. This extraction is achieved via a module called Model Ex-\\ntractor. After extracting DL models, it performs analysis on them.\\nHowever, we here face the challenge that the models are mostly\\nin different formats. Though developers are investing substantial\\nefforts in integrating different model formats, such as designing\\na standardized one [26], the ecosystem of DL frameworks is still\\nbroken and fragmented nowadays. Thus, when looking into the\\ninternal structures of DL models, we substantially leverage the\\navailable tools and source of different frameworks. Fortunately,\\n1Different app categories on Google Play can be visited via url https://play.google.\\ncom/store/apps/category/XXX, where XXX can be GAME or other category names.\\nmost of the frameworks we investigated (details in Table 2) are\\nopen-source and well-documented.\\nWe discuss more details of DL Sniffer and Model Extractor in\\nSection 4.1 and Section 5.1, respectively.\\n4\\nAPPLICATION ANALYSIS\\nThis section presents our analysis of smartphone DL apps. We\\nfirst describe our methodology in Section 4.1 and then the following\\nthree major aspects of the DL apps:\\n• The characteristics of DL apps (§4.2): their popularity, their dif-\\nference from non-DL apps, and their developers.\\n• The role of DL (§4.3): the popular usage of DL in apps, the cat-\\negories of DL apps, and evidence that DL is already used as core\\nbuilding blocks of apps.\\n• An analysis of DL frameworks (§4.4): which frameworks are used,\\nthe cost, and their adoption trend over time.\\n4.1\\nMethodology: finding DL apps\\nAs a part of our analyzing tool (Section 3.2), DL Sniffer takes\\napk file(s) as input, and outputs which of them use DL technique.\\nDetecting DL usage is difficult, instead DL Sniffer mainly detects the\\nusage of popular DL frameworks with Android support. Currently,\\nDL Sniffer supports the detection of 16 popular DL frameworks,\\nincluding TensorFlow, Caffe, TFLite, etc, and the details of those\\nframeworks will be presented later in Section 4.4 & Table 2. DL\\nSniffer uses two ways to mine the usage of DL frameworks: (1) For\\nthose who provide native C++ APIs, DL Sniffer first decomposes\\nthe apk files via Apktool [5], and extracts the native shared libraries\\n(with suffix “.so”). DL Sniffer then searches for specific strings on the\\nrodata section of those libraries. Those strings can be regarded as\\nidentifications of corresponding frameworks, and are pre-defined\\nby us. For example, we notice that the shared libraries that use\\nTensorFlow always have “TF_AllocateTensor” in its rodata section.\\n(2) For those who only support Java APIs, DL Sniffer first converts\\nthe apk file into smali code via dex2jar [14]. The smali code, which\\nis a disassembled version of the DEX binary used by Android’s\\nDavik VM, enables us to carry out static program analysis. DL\\nSniffer statically goes through the class/method names of smali\\ncode and checks whether certain APIs exist. For example, the class\\nMultiLayerConfiguration is used in almost every app that embeds\\nDeepLearning4J framework.\\nBesides detecting the usage of DL frameworks, we also try to\\nidentify DL apps that don’t use the frameworks listed in Table 2\\n(called “no lib” in this work). Similarly, this is achieved by searching\\nspecific strings on the rodata section of native libraries as men-\\ntioned above, but the strings we use here are pre-defined such as\\n“neural network”, “convolution”, “lstm”, etc, rather than extracted\\nfrom existing frameworks. We then manually check the detection\\ncorrectness through reverse engineering, filtering those don’t really\\nhave DL usage (false positive). This manual check is also performed\\non other DL apps detected using the aforementioned approach, to\\nensure good accuracy in DL identification.\\nWWW’19, May 2019, San Francisco, USA\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu\\nmanif-\\nest file\\nDL Sniffer\\nsmali\\ncode\\nnative\\nlib\\napktool\\ndex2jar\\naapt\\nlib\\nscanner\\nsmali\\nanalyzer\\nhas\\nDL?\\nApplication\\nAnalysis\\nModel\\nAnalysis\\nmeta\\ninfo\\ncrawler\\nModel Extractor\\nresearchers\\nreverse\\nengineer\\nmodel\\nscanner\\nmodel\\nvalidator\\nDL Models\\nfull app\\ndataset\\nDL app\\ndataset\\nFigure 1: The overall workflow of our analyzing tool.\\n4.2\\nCharacteristics of DL apps\\n• Is DL gaining popularity on smartphones? Our study shows:\\nover our investigation period (June 2018 – Sept. 2018), the total\\nnumber of DL apps has increased by 27.1%, from 166 to 211. We\\nfurther investigate the new downloads and reviews of DL apps\\nwithin one month of Sept. 2018: in that period, the 221 DL apps are\\ndownloaded for around 13,000,000 times and receive 9,600,000 new\\nreviews. The results indicate a substantial amount of smartphones\\nare running DL apps nowadays.\\n• How are DL apps different from non-DL apps? We investi-\\ngate the following three aspects with results illustrated in Figure 2.\\nDownloads and Reviews are representative of apps’ popularity.\\nAs observed, the median number of downloads and reviews of\\nDL apps are 5,000,000 and 41,074 respectively, much larger than\\nnon-DL apps, i.e., 100,000 and 1,036 respectively. We also count\\nthe download rankings of each DL apps within the corresponding\\ncategory. The median number of such ranking is 89 among total\\n500 apps for each category. We deem the above statistics as strong\\nevidences that top apps are early adopters in deploying DL in mobile\\napps. Such phenomenon can be explained that making DL work\\nin the wild, though appealing, takes a lot of engineering efforts.\\nThe cycle of developing DL functionality on smartphones includes\\nmodel construction, data collection, model training, offline/online\\ntesting, etc. Thus, many small companies or individual developers\\nlack the resources to exploit DL on their apps.\\nRatings show how much appreciation users give to apps. The\\nFigure shows that DL apps and non-DL apps have similar ratings\\nfrom users, with the same median number 4.3.\\nApp size: as shown in Figure 2, DL apps have much larger apk\\nfiles than non-DL apps (median number: 35.5MB vs 12.1MB). This\\nis reasonable since having DL not only adds DL frameworks and\\nmodels to the apps, it also confidently indicates that the apps have\\nmuch richer features.\\n• Who are the developers of DL apps? We also study the de-\\nvelopers of DL apps. The results show that the identified 211 DL\\napps belong to 172 developers (companies), among which 27 de-\\nvelopers have more than one DL apps. The developers with most\\nDL apps are “Google LLC” (10) and “Fotoable,Inc” (6). We observe\\nmany big companies own more than one DL apps, including Google,\\nDL apps\\nnon-DL apps\\n(a) download\\n0\\n2\\n4\\n6\\n8\\n10\\nMillion Downloads\\nDL apps\\nnon-DL apps\\n(b) review\\n0\\n10000\\n20000\\n30000\\n40000\\n50000\\n# of Reviews\\nDL apps\\nnon-DL apps\\n(c) rating\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\nRatings\\nDL apps\\nnon-DL apps\\n(d) app size\\n0\\n20\\n40\\n60\\n80\\nApp Size (MB)\\nFigure 2: Comparisons between DL apps and non-DL apps on\\nvarious aspects (a)–(d). Each box shows the 75th percentile,\\nmedian, and 25th percentile from top to bottom. We manu-\\nally set the y-axis limits for better presentation in (b): the\\nmissed out 75th percentile of DL apps is 324,044.\\nAdobe, Facebook, Kakao, Meitu, etc. This suggests that those big\\ncompanies are pioneers in adopting DL into their products. We also\\nnotice that the DL apps from same developer often have identical\\nDL frameworks. For example, four products from Fotoable Inc use\\nthe exactly same native library called libncnn_style.0.2.so to support\\nDL technique. This is because that DL frameworks and even the\\nDL models are easily reusable: a good nature of DL technique that\\ncan help reduce the engineering efforts of developers.\\nA First Look at Deep Learning Apps on Smartphones\\nWWW’19, May 2019, San Francisco, USA\\nusage\\ndetailed usage\\nas core feature\\nphoto beauty: 97\\n94 (96.9%)\\nface detection: 52\\n44 (84.6%)\\naugmented reality: 19\\n5 (26.3%)\\nface identification: 8\\n7 (87.5%)\\nimage classification: 11\\n6 (54.5%)\\nobject recognition: 10\\n9 (90%)\\nimage: 149\\ntext recognition:11\\n4 (36.3%)\\nword&emoji prediction: 15\\n15 (100%)\\nauto-correct: 10\\n10 (100%)\\ntranslation: 7\\n3 (42.8%)\\ntext classification: 4\\n2 (50%)\\ntext:26\\nsmart reply: 2\\n0 (0%)\\nspeech recognition: 18\\n7 (38.9%)\\naudio: 24\\nsound recognition: 8\\n8 (100%)\\nrecommendation: 11\\n2 (18.1%)\\nmovement tracking: 9\\n4 (44.4%)\\nsimulation: 4\\n4 (100%)\\nabnormal detection: 4\\n4 (100%)\\nvideo segment: 2\\n1 (50%)\\nother: 19\\naction detection: 2\\n0 (0%)\\ntotal: 211\\n171 (81.0%)\\nTable 1: The DL usage in different apps. Note: as one app may\\nhave multiple DL uses, the sum of detailed usage (column 2)\\nmight exceed the corresponding coarse usage (column 1).\\nImplications: The popularity of DL among top smartphone apps,\\nespecially ones developed by big companies, should endow smaller\\ncompanies or independent developers with strong confidence in de-\\nploying DL in their apps.\\n4.3\\nThe roles of DL in apps\\n• What are the popular uses of DL? To understand the roles\\nplayed by DL, we manually classify the usage of DL on different\\napps. This is achieved by looking into the app description and app\\ncontents. The results are shown in Table 1. Each app has one or\\nmore usages, and the usage is represented in two different levels\\n(coarse and detailed). 10 apps are left out since we cannot confirm\\ntheir DL usage.\\nOverall, image processing is the most popular usage of DL on\\nsmartphones, far more than text and audio processing (149 vs. 26 &\\n24). This is not surprising since computer vision is the field where\\nDL starts the revolution [70], and the progress on this field has\\nbeen lasting since then [78]. In more details, photo beauty (97)\\nand face detection (52) are mostly widely used in DL apps, usually\\nfound in photo editing and camera apps to beautify pictures. In text\\nfield, word & emoji prediction (15) and auto-correct (10) are also\\npopular, usually found in input method apps like GBoard. For audio\\nprocessing, DL is mainly used for speech recognition (14). Besides,\\nthere are other types of usage such as recommendation (11) which\\nis often found in shopping apps.\\n• Which categories do DL apps come from? Figure 3 summa-\\nrizes the number of DL apps in different categories. As observed,\\nalmost 29% DL apps (61 out of 211) are in category photograph, all\\nof which use DL for image processing. Social category is another\\n0% \\n20% \\n40% \\n60% \\n80% \\n100% \\nPhotograph\\nSocial\\nVideo_Players\\nProductivity\\nLibraries\\nCommunications\\nFinance\\nEntertainment\\nPersonalization\\nShopping\\nSports\\nimage\\ntext\\naudio\\nother\\n61\\n23\\n14\\n13\\n10\\n10\\n8\\n7\\n6\\n6\\n6\\nFigure 3: Distributions of DL apps over categories defined by\\nGoogle Play. Numbers on top: the counts of DL apps in the\\ncorresponding categories. Apps in each category are further\\nbroken down by DL usage (see Table 1 for description). Cat-\\negories with fewer than 5 DL apps are not shown.\\nhotspot with 23 DL apps in total, 78% of which use DL for image\\nprocessing while others use it for text, audio, etc. The category\\nof productivity also contains 13 DL apps, but most of them (62%)\\nuse DL for text processing. Overall, we can see that the DL us-\\nage is somehow diverse, with 11 categories has more than 5 DL\\napps among the top 500. Such diversity gives credits to the good\\ngenerality of DL technique.\\nImplications: Our findings encourage developers of certain types\\nof apps, notably the ones with photo beauty, to embrace DL. Our\\nfindings also motivate encapsulating DL algorithms within higher\\nlevel abstractions that cater to popular uses in apps. For instance,\\ncompanies such as SenseTime already starts to ship DL-based face\\ndetection libraries to app developers. Masking the details of DL models\\nand frameworks, such abstractions would make DL more friendly to\\ndevelopers.\\n• Is DL a core building block? We also manually tag each DL\\nusage as core feature or not. We define the DL functionality as apps’\\ncore feature if and only if two conditions are satisfied: (1) hot: the\\nDL functionality is very likely to be invoked every time the apps are\\nopened and used by users, (2) essential: without the DL functionality,\\nthe apps’ main functionality will be severely compromised or even\\nbecome infeasible. For example, DL on text recognition is treated as\\ncore feature in a scanner app (Adobe Scan) that helps users translate\\nan image into text, but not in a payment app (Alipay) that uses it\\nto scan ID card for identification. Similarly, DL on photo beauty is\\ntreated as core feature in a camera app (Meitu), but not in a social\\napp (Facebook Messenger Kids).\\nOverall, 171 out of 211 (81%) apps use DL to support core features.\\nSpecifically, since photo beauty (96.9%) and face detection (84.6%)\\nare primarily used in photo & camera apps, their usage is essen-\\ntial. Similarly, word & emoji prediction (100%) and auto-correct\\n(100%) are treated as core features in keyboard apps, helping users\\ninput more efficiently and accurately. However, recommendation\\n(18.1%) is often provided as complementary feature to others such\\nas shopping apps, thus not treated as core feature.\\nWWW’19, May 2019, San Francisco, USA\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu\\n51\\n26\\n22\\n28\\n31\\n12\\n25\\n34\\n47\\n25\\n22\\n21\\n12\\n10\\n15\\n25\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nTOTAL\\nTensorFlow\\nCaffe\\nParrots\\nncnn\\nTFLite\\nCaffe2\\nother lib\\nno lib\\nNumber of DL apps\\nJun. 2018\\nSep. 2018\\n166\\n211\\nFigure 4: Numbers of DL apps using various mobile DL\\nframeworks. “other lib”: the DL apps developed on the\\nframeworks in Table 2 but not itemized here, e.g. mace, SNPE,\\nand xnn. “no lib”: apps with DL functions but using no DL\\nframeworks from Table 2. Note that the number in “TOTAL”\\nis lower than the sum of others since some DL apps have in-\\ntegrated multiple frameworks.\\nImplications: Our findings support future investment on R&D of\\nmobile DL, as core user experience on smartphones will likely depend\\non DL performance [73] and security [89, 102].\\n4.4\\nDL frameworks\\nAs mentioned in Section 2, DL frameworks are critical to DL\\nadoption, as most developers use those frameworks to build their DL\\napps. In this subsection, we investigate into how those frameworks\\nare used in DL apps: the numbers, the sizes, the practice, etc.\\n• A glance over popular DL frameworks We first make an in-\\nvestigation into popular DL frameworks, and the results are summa-\\nrized in Table 2. We select those 21 frameworks for their popularity,\\ne.g., forks and stars on GitHub, gained attention on StackOver-\\nflow and other Internet channels. Among those 21 frameworks, 16\\nframeworks support Android platform via Java (official language on\\nAndroid) and/or C++ (native support via cross-compilation). Most\\nof them are open-source, while others are either provided to public\\nas a binary SDK (SNPE, CoreML), or only accessible by the providers’\\n(collaborators’) own products (xNN, Parrots). Most of them use cus-\\ntomized format to store and represent the model files, but some\\nleverage existing approaches, such as ProtoBuf [29]. We also no-\\ntice a trend on lightweight DL inference frameworks, which are\\ndesigned specifically for mobile apps but have no training-support\\nback-end (ncnn, FeatherCNN, MACE, etc). Those frameworks can-\\nnot train DL models, but can predict with pre-trained models via\\nother frameworks such as TensorFlow or Caffe. Note that our later\\nanalysis substantially relies on the openness of DL frameworks:\\nit enables us to use the existing tools to analyze or even visualize\\n147\\n142\\n48\\nJun. 2018\\nSep. 2018\\nCheck-out (5)\\nCheck-in\\nFigure 5: The number of check-in and check-out DL apps.\\nthe DL models such as TensorFlow, or build our own interpreting\\nscripts to analyze them based on the open code such as ncnn.\\n• What is the adoption of DL frameworks? As summarized in\\nFigure 4, the most popular DL frameworks used in Sep. 2018 are\\nTensorFlow (51), TFLite (31), and ncnn (28), as they contribute to\\nalmost 50% of the total number of DL apps. Other popular frame-\\nworks include Caffe, Parrots, and Caffe2. We have made several\\nobservations from those 6 dominant frameworks as following.\\n(1) All these frameworks are developed by big companies (e.g.\\nGoogle), AI unicorns (e.g. SenseTime), or renowned universities\\n(e.g. Berkeley).\\n(2) 5 out of these 6 frameworks are open-source, except Parrots\\nwhich is provided as SDK to consumers. In fact, it is believed that\\nopenness is already an important feature in machine learning, es-\\npecially DL society, as it supposes to [101]. It helps developers\\nreproduce the state-of-the-art scientific algorithms, customize for\\npersonal usage, etc. As a result, for example, TensorFlow has more\\nthan 1,670 contributors up to Oct. 2018, going far beyond the com-\\nmunity of Google.\\n(3) Most (4 out of 6) frameworks are optimized for smartphones,\\nexcept Caffe and TensorFlow. Those mobile DL frameworks are\\ndesigned and developed specifically for mobile devices, usually\\nwithout training back-end, so that the resulted libraries can be\\nfaster and more lightweight. As an example, TFLite stems from Ten-\\nsorFlow, but is designed for edge devices and reported to have lower\\ninference time and smaller library size than TensorFlow [121]. Be-\\nsides those popular frameworks, we identify 34 (16.1%) DL apps that\\ndon’t use any framework in Table 2. These apps use self-developed\\nengines to support DL functionality.\\n• Are mobile DL frameworks gaining traction? As shown in\\nFigure 4, DL frameworks optimized for smartphones, such as TFLite\\nand ncnn, quickly gain popularity: the number of TFLite-based DL\\napps has increased from 12 to 31; that of ncnn increases from 21\\nto 28. We deem it as the trend of mobile DL ecosystem: To train a\\nmodel offline, use large, mature, and generic frameworks that focus\\non developer friendliness and feature completeness. To deploy a model\\non edge devices, switch to mobile-oriented frameworks that focus on\\nperformance (inference latency, memory footprint, and library size).\\nWe also investigate into the DL check-in and check-out behavior\\nin mobile apps. We define the check-in DL apps as those that have\\nno DL usage in earlier version (Jun. 2018) but add the DL usage\\nin newer version (Sep. 2018), and the check-out vice versa. Note\\nthat the app list of our two datasets are not identical since we\\ncrawl the most popular ones, but the popularity is changing. So\\nwe only consider the apps that exist in both lists (11,710 in total),\\nand conclude the results in Figure 5. As observed, 48 out of the 190\\n(25.3%) DL apps in newer version are checked in between Jun. 2018\\nA First Look at Deep Learning Apps on Smartphones\\nWWW’19, May 2019, San Francisco, USA\\nFramework\\nOwner\\nSupported Mobile Platform\\nMobile API\\nIs Open-\\nsource\\nSupported Model\\nFormat\\nSupport\\nTraining\\nTensorFlow [37]\\nGoogle\\nAndroid CPU, iOS CPU\\nJava, C++\\n✓\\nProtoBuf (.pb, .pbtxt)\\n✓\\nTF Lite [38]\\nGoogle\\nAndroid CPU, iOS CPU\\nJava, C++\\n✓\\nFlatBuffers (.tflite)\\n✗\\nCaffe [71]\\nBerkeley\\nAndroid CPU, iOS CPU\\nC++\\n✓\\ncustomized, json\\n(.caffemodel, .prototxt)\\n✓\\nCaffe2 [9]\\nFacebook\\nAndroid CPU, iOS CPU\\nC++\\n✓\\nProtoBuf (.pb)\\n✓\\nMxNet [49]\\nApache Incubator\\nAndroid CPU, iOS CPU\\nC++\\n✓\\ncustomized, json (.json,\\n.params)\\n✓\\nDeepLearning4J [13]\\nSkymind\\nAndroid CPU\\nJava\\n✓\\ncustomized (.zip)\\n✓\\nncnn [36]\\nTencent\\nAndroid CPU, iOS CPU\\nC++\\n✓\\ncustomized (.params, .bin)\\n✗\\nOpenCV [27]\\nOpenCV Team\\nAndroid CPU, iOS CPU\\nC++\\n✓\\nTesnorFlow, Caffe, etc\\n✗\\nFeatherCNN [17]\\nTencent\\nAndroid CPU, iOS CPU\\nC++\\n✓\\ncustomized (.feathermodel)\\n✗\\nPaddlePaddle [25]\\nBaidu\\nAndroid CPU, iOS CPU & GPU\\nC++\\n✓\\ncustomized (.tar)\\n✓\\nxNN [41]\\nAlibaba\\nAndroid CPU, iOS CPU\\nunknown\\n✗\\nunknown\\nunknown\\nsuperid [35]\\nSuperID\\nAndroid CPU, iOS CPU\\nunknown\\n✗\\nunknown\\nunknown\\nParrots [31]\\nSenseTime\\nAndroid CPU, iOS CPU\\nunknown\\n✗\\nunknown\\nunknown\\nMACE [24]\\nXiaoMi\\nAndroid CPU, GPU, DSP\\nC++\\n✓\\ncustomized (.pb, .yml, .a)\\n✗\\nSNPE [32]\\nQualcomm\\nQualcomm CPU, GPU, DSP\\nJava, C++\\n✗\\ncustomized (.dlc)\\n✗\\nCNNDroid [76]\\nOskouei et al.\\nAndroid CPU & GPU\\nJava\\n✓\\nMessagePack (.model)\\n✗\\nCoreML [12]\\nApple\\niOS CPU, GPU\\nSwift, OC\\n✗\\ncustomized, ProtoBuf\\n(.proto, .mlmodel)\\n✓\\nChainer [10]\\nPreferred Networks\\n/\\n/\\n✓\\ncustomized\\n(.chainermodel)\\n✓\\nCNTK [23]\\nMicrosoft\\n/\\n/\\n✓\\nProtoBuf (.proto)\\n✓\\nTorch [40]\\nFacebook\\n/\\n/\\n✓\\ncustomized (.dat)\\n✓\\nPyTorch [30]\\nFacebook\\n/\\n/\\n✓\\ncustomized, pickle (.pkl)\\n✓\\nTable 2: An overview of popular deep learning frameworks and their smartphone support at the time of writing (Nov. 2018).\\nand Sep. 2018. We also notice that some DL apps in old version\\nchecked out, but the number is much smaller (5). The reasons of\\ncheck-out can be that the developers remove the corresponding\\nfunctionality or just replace the DL with other approaches. Overall,\\nthe statistics support the fact that DL technique is increasingly\\nadopted in mobile apps.\\n• What is the storage overhead of frameworks? Figure 6 shows\\nthe sizes of DL libs, i.e. the physical incarnation of DL frameworks.\\nAs shown, the average size of DL libs is 7.5MB, almost 6 times\\ncompared to the non-DL libs. Here, we only use the non-DL libs\\nfound within DL apps. The results show that DL libs are commonly\\nheavier than non-DL libs, because implementing DL functionality,\\neven without training backend, is quite complex. Looking into\\ndifferent frameworks, using TensorFlow and Caffe results in larger\\nDL libs, i.e., 15.3MB and 10.1MB respectively, while others are all\\nlower than 5MB. The reason is that mobile supports of TensorFlow\\nand Caffe are ported from the original frameworks and substantially\\nreuse the code base from them. However, these two frameworks\\nare designed for distributed on-cloud DL. As comparison, other\\nframeworks in Figure 6 are specifically designed for mobile devices\\nto the purpose of good performance.\\nOne app may incorporate multiple DL frameworks. Surpris-\\ningly, we find that 24 DL apps embed more than one DL frameworks.\\nFor example, AliPay, the most popular payment app in China, has\\nboth xnn and ncnn inside. We deem such multi-usage as (poten-\\ntially) bad practice, since it unnecessarily increases the apk size\\nand memory footprint when these frameworks need to be loaded\\n15.3\\n10.1\\n4.7\\n2.5\\n2.1\\n4.1\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\nTF\\nCaffe\\nParrots\\nncnn\\nTFLite\\nCaffe2\\nDL-lib Size (MB)\\nDL-libs Average: 7.5\\nnon-DL-libs Average: 1.3\\nFigure 6: The binary library sizes of DL frameworks.\\nsimultaneously. According to our statistics, the overhead is around\\n5.4MB, contributing to 13.6% to the total apk size on average. Such\\noverhead can be avoided by running different tasks based on one\\nframework, since most DL frameworks are quite generic and can\\nsupport various types of DL models. Even if not, they can be eas-\\nily extended to support the missing features [1]. The reason of\\nsuch multi-usage behavior can be twofold. First, one app might be\\ndeveloped by different engineers (groups), who introduce differ-\\nent frameworks for their own DL purpose. Second, the developers\\nmay just reuse existing code and models for specific tasks, without\\nmerging them together in one DL implementation.\\nImplications: Our findings highlight the advantages and popularity\\nof mobile DL frameworks, encouraging further optimizations on them.\\nOur findings also motivate app developers to give these frameworks\\npriority considerations in choosing the incarnation of DL algorithms.\\nWWW’19, May 2019, San Francisco, USA\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu\\nLayer\\ntype\\n% of\\nmodels\\n# in each\\nmodel\\nLayer\\ntype\\n% of\\nmodels\\n# in each\\nmodels\\nconv\\n87.7\\n5 / 14.8\\nrelu\\n51.0\\n6 / 16.3\\npooling\\n76.5\\n2 / 2.8\\nsplit\\n46.9\\n1 / 7.5\\nsoftmax\\n69.1\\n1 / 1.1\\nprelu\\n32.1\\n4 / 4.6\\nfc\\n60.5\\n3 / 5.6\\nreshape\\n28.4\\n2 / 24.1\\nadd\\n56.8\\n9.5 / 23.8\\ndropout\\n21.0\\n1 / 1.0\\nTable 3: Layers used in DL models. “% of models” shows how\\nmany models contain such layer, while “# in each model”\\nshows the median/mean numbers of occurrences in each\\nmodel that contains such layer. “conv” and “fc” are short for\\nconvolution and fully-connect.\\n5\\nMODEL ANALYSIS\\nThis section focuses on the model-level analysis of DL technique.\\nWe first describe the methodology details, e.g., the design of Model\\nExtractor in Section 5.1. Then, we show the analysis results on\\nthose DL models from three main aspects.\\n• The structures of DL models: the model types, layer types,\\nand optimizations used (Section 5.2).\\n• The resource footprint of DL models: storage, memory, exe-\\ncution complexity, etc (Section 5.3).\\n• The security of DL models: using obfuscation and encryption\\nto protect models from being stolen (Section 5.4).\\n5.1\\nModel Extractor: finding DL models\\nAs a part of our analyzing tool (Section 3.2), Model Extractor\\ntakes DL apps which we have already identified as input, and out-\\nputs the DL model(s) used in each app. Model Extractor scans the\\nassets folder of each decomposed DL apps, tries to validate each\\nmodel file inside. Since DL frameworks use different formats to\\nstore their model files, Model Extractor has a validator for each of\\nsupported framework. However, we observe that many models are\\nnot stored as plaintext inside apk files. For example, some of them\\nare encrypted on storage, and decrypted when apps running on\\ndevices. For such cases, Model Extractor tries to reverse engineer\\nthe apps, and extract the analyzable models.\\nOverall results We extract DL models based on the most pop-\\nular frameworks, i.e., TFLite, TensorFlow, ncnn, Caffe, or Caffe2. In\\nsummary, we successfully extract 176 DL models, which come from\\n71 DL apps. The reasons why we cannot extract models from the\\nother DL apps could be (i) the models are well protected and hidden\\nin the apk files; (ii) the models are retrieved from Internet during\\nruntime. Among the extracted models, we can analyze 98 of them,\\nwhich come from 42 DL apps. The other extracted models cannot\\nbe parsed via our framework currently because (i) the models are\\nin format which we have no insights into, such as Parrots-based\\nmodels, since the corresponding frameworks are not open-source;\\n(ii) the models are encrypted.\\n5.2\\nModel Structures\\n• DL model types Among the DL models extracted, 87.7% mod-\\nels are CNN models, 7.8% models are RNN models, while others\\nare not confirmed yet. The CNN models are mostly used in im-\\nage/video processing and text classification. The RNN models are\\n1-bit Quan.\\n8-bit Quan.\\n16-bit Quan.\\nSparsity\\nTF\\nunsupported\\n4.78%\\n0.00%\\n0.00%\\nTFLite\\nunsupported\\n66.67%\\nunsupported\\nunsupported\\nCaffe\\nunsupported\\n0.00%\\nunsupported\\nunsupported\\nCaffe2\\nunsupported\\n0.00%\\nunsupported\\nunsupported\\nncnn\\nunsupported\\n0.00%\\nunsupported\\nunsupported\\nTotal\\n0.00%\\n6.32%\\n0.00%\\n0.00%\\nTable 4: Optimizations applied on DL models.\\nmostly used in text/voice processing, such as word prediction, trans-\\nlation, speech recognition, etc. The results are consistent with the\\nconventional wisdom: CNN models are good at capturing visual\\ncharacteristics from images, while RNN models are powerful at\\nprocessing sequential data with temporal relationships such as text\\nand audio.\\n• DL layer types We then characterize different types of layers\\nused in DL models. As shown in Table 3, convolution (conv) is the\\nmost commonly used type. 87.7% models have at least one convolu-\\ntional layer, and the median (mean) number of convolutional layers\\nused in those models is 5 (14.8). This is not surprising since convo-\\nlution is the core of CNN models, and CNN is the dominant model\\narchitecture used in vision tasks. Our previous analysis already\\ndemonstrates that image processing is the most popular use case\\nof mobile DL. Similarly, pooling is also an important layer type in\\nCNN models, included in 76.5% DL models. Besides, softmax is also\\nfrequently used (69.1%), but don’t repeatedly show up in one single\\nmodel. This is because softmax layer usually resides at the end\\nof DL models to get the probabilities as output. As a comparison,\\nfully-connected (fc) layers are less common, only used in 60.5% DL\\nmodels. A possible reason is that fully-connected layer is known to\\nbe very parameter- and computation-intensive, and can be replaced\\nby other layers such as convolution [18]. Other frequently used\\nlayer types include add, split, relu, prelu, dropout, and reshape.\\nImplications: Our findings motivate framework and hardware ven-\\ndors who are interested in optimizing mobile DL to focus on the\\npopular DL layers we discovered in deployed models, e.g. convolution.\\nWe also notice that a small number (5) of DL models contain\\ncustomized layer types. Such customization is made as an extension\\nto existing DL frameworks [1]. The result indicates that the func-\\ntionalities of current DL frameworks are mostly complete enough.\\n• Model optimizations Various techniques have been proposed\\nto optimize the DL models in consideration of their sizes and com-\\nputation complexity. Here, we study the usage of two most popular\\ntechniques in the wild: quantization and sparsity. Quantization\\ncompresses DL models by reducing the number of bits required\\nto represent each weight. The quantization has different levels, in-\\ncluding 16-bit [61], 8-bit [105], and even 1-bit [53, 93], while the\\noriginal models are usually presented in 32-bit floating points. Spar-\\nsity [77, 107] has also been extensively studied in prior literatures\\nas an effective approach to make compact DL models. It’s mainly\\nused in matrix multiplication and convolutional layers to reduce\\nparameters. Such optimizations are long known to reduce DL cost\\nby up to two orders of magnitude without compromising model\\naccuracy [63].\\nA First Look at Deep Learning Apps on Smartphones\\nWWW’19, May 2019, San Francisco, USA\\nTensorFlow\\nTFLite\\nCaffe\\nncnn\\nParrots\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\nModel Size (MB)\\nFigure 7: The size of DL models in different frameworks.\\nWe leave out Caffe2 since we only successfully extract one\\nmodel in Caffe2 format.\\nLeNet-5\\nsqueezenet\\nmobilenet\\nvgg-\\n16\\nAlexnet\\nResNet-50\\nFigure 8: The cost of memory and computation of DL models\\nextracted from apps. Red dots: classical CNN architectures\\nas references. Black crosses: extracted DL models, which are\\nvisually summarized by a covariance error ellipse [16].\\nTable 4 summarizes the optimizations applied on DL models.\\nHere we focus on the DL models for 5 popular frameworks, i.e., Ten-\\nsorFlow (TF), TFLite, Caffe, Caffe2, and ncnn. Overall, most of these\\nframeworks only support 8-bit quantization, except TensorFlow who\\nhas 16-bit quantization and sparsity support. However, only a frac-\\ntion of DL models apply the optimization techniques: 6.32% models\\nare quantized into 8-bit, while others are non-optimized.\\nImplications: The findings that well-known DL optimizations are\\nmissing in real-world deployment suggest the efficiency potential of\\nmobile DL is still largely untapped. The findings also urge immediate\\nactions to fix the missing optimizations.\\n5.3\\nModel Resource Footprint\\n• Model size. Figure 7 illustrates the size of DL models (in storage).\\nOverall, we find that the extracted DL models are quite small (me-\\ndian: 1.6MB, mean: 2.5MB), compared to classical models such as\\nVGG-16 [100] (around 500MB) and MobileNet [69] (around 16MB).\\nThe models in TensorFLow format (median: 3.2MB) are relatively\\nlarger than the models in other formats such as TFLite (median:\\n0.75MB) and ncnn (median: 0.86MB).\\n• Runtime overhead. We then study the runtime performance of\\nDL models. Here, we focus on two aspects: memory and computa-\\ntions. The memory usage includes both the model parameters and\\nthe generated intermediate results (feature maps). For computation\\ncomplexity, we use floating point operations (FLOPs) during one\\ninference as the metric. Here we use only part of models in Tensor-\\nFlow and ncnn formats since some others don’t have fixed input\\nsizes, e.g., image size, so that the computation complexity can only\\nbe determined at runtime [15]. We also include the performance of\\nsome other classical CNN models such as AlexNet, MobileNet, etc.\\nAs illustrated in Figure 8, the black crosses represent the DL\\nmodels we have extracted, while the red dots represent the classical\\nCNN architectures. Overall, the results show that in-the-wild DL\\nmodels are very lightweight in consideration of memory usage and\\ncomputation complexity, with median value of 2.47 MB and 10M\\nFLOPs respectively. Running such models on mobile processors is\\ninexpensive. For example, as estimated on the CPU of Snapdragon\\n8452, the execution time of 80% models are less than 15ms which\\nis translated to 67 FPS [33]. To be compared, ResNet-50, one of\\nthe state-of-the-art models in image classification task, has around\\n200MB memory usage and 4GFLOPs computations. Even MobileNet\\nand SqueezeNet, which are designed and optimized for mobile\\nscenarios, require more memory usage and computations than 90%\\nthose mobile DL models that we have discovered.\\nImplications: Our findings of dominant lightweight DL models on\\nsmartphones give app developers a valuable assurance: DL inference\\ncan be as cheap as a few MBs of memory overhead and tens of ms\\nexecution delay. Our findings challenge existing research on DL in-\\nference, which are typically centered on full-blown models (e.g. VGG)\\nand validated on these models [55, 64, 72, 73, 80, 82]. Given the signifi-\\ncance of smartphones as DL platforms, future DL algorithm proposals\\nshould consider applicability on lightweight models and treat resource\\nconstraints as the first-class concern.\\n5.4\\nModel Security\\nFinally, we investigate into how DL models are protected. We\\ndeem model protection as an important step to AI system/app\\nsecurity, because if attackers can acquire the model, they can (i)\\nsteal the intellectual property by reusing the model file or re-train\\na new model based on the stolen one; (ii) easily attack the DL-\\nbased apps via adversarial attack [89]. We focus on two practical\\nprotection mechanisms.\\n• Obfuscation is a rather shallow approach to prevent at-\\ntackers from gaining insights into the model structures by\\nremoving any meaningful text, e.g., developers-defined layer\\nnames.\\n• Encryption is better in security by avoiding attackers from\\ngetting the model structures/parameters, but also causes in-\\nevitable overhead for apps to decrypt the models in memory.\\nHere, we deem encrypted models as always obfuscated too.\\nWe investigate into how obfuscation and encryption are employed\\non DL models that we have extracted. We analyze the DL models\\nextracted from apps using TensorFLow, TFLite, ncnn, caffe, and Caffe2.\\nIn total, we confirm the security level of 120 DL models. Note that\\n2A typical mobile chip used by many popular smartphones such as Galaxy S8.\\nWWW’19, May 2019, San Francisco, USA\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu\\nhere encryption doesn’t necessarily mean encryption algorithm, but\\nalso includes cases where developers customize the model format\\nso that the model cannot be parsed via the DL framework.\\nThe results show that among the 120 DL models, we find 47\\n(39.2%) models are obfuscated and 23 (19.2%) models are encrypted.\\nNote that these two sets of apps are overlapped: encrypted apps\\nare also obfuscated. The results indicate that most DL models are\\nexposed without protection, thus can be easily extracted and utilized\\nby attackers. In fact, only few frameworks in Table 2 support ob-\\nfuscation, e.g., ncnn can convert models into binaries where text\\nis all striped [34], and Mace can convert a model to C++ code [11].\\nWhat’s worse, no framework provides help in model encryption\\nas far as we know. Thus, developers have to implement their own\\nencryption/decryption mechanism, which can impose non-trivial\\nprogramming overhead.\\nImplications: The grim situation of model security urges strong\\nprotection over proprietary models in a way similar to protecting\\ncopyright digital contents on smartphones [42, 87, 96, 119]. This ne-\\ncessitates a synergy among new tools, OS mechanisms and policies,\\nand hardware mechanisms such as Intel SGX [21] and ARM Trust-\\nZone [7].\\n6\\nLIMITATIONS AND FUTURE WORK\\nLimitations of our analyzing tool Though we carefully de-\\nsign our analyzer to capture as many DL apps as possible, and\\ninvolve a lot of manual efforts to validate the results, we can still\\nhave false identifications. For example, those DL apps that neither\\ndepend on any popular DL frameworks nor have any string patterns\\nin the native libraries, will be missed out. In addition, the apps that\\nhave integrated DL frameworks but don’t really use them will be\\nfalsely classified as DL apps, which shouldn’t happen though. For\\nthe first case, we plan to mine the code pattern of DL implementa-\\ntion and use the pattern to predict more DL apps that might involve\\nDL. For the second one, we plan to further enhance our analyzer\\nwith advanced static analysis technique [43] so that it can detect\\nwhether the API calls (sinks) of DL libraries will be invoked or not.\\nLonger-term analysis Currently, we carry out our empirical\\nstudy based on the app datasets obtained in Jun. and Sep. 2018.\\nIn the future, we plan to actively maintain and update our study\\nby extending to more time steps, e.g., every 3 months. We believe\\nthat more solid and interesting results can be made through such\\nlong-term analysis. We will also keep watch on newly emerging\\nDL frameworks and add them to our analyzing tool.\\nMore platforms In this work, we only analyze the adoption of\\nDL on Android apps. Though Android is quite representative of\\nthe mobile ecosystem, more interesting findings might be made by\\nexpanding our study on other platforms such as iOS and Android\\nWear. We believe that comparing the DL adoption on different plat-\\nforms can feed in more implications to researchers and developers.\\nInvolving dynamic analysis For now, our analysis remains\\nstatic. Though static analysis technology is quite powerful, we\\nbelieve that dynamic analysis can provide more useful findings.\\nFor example, by running the DL models on off-the-shelf mobile\\ndevices, we can characterize the accurate runtime performance, e.g.,\\nend-to-end latency and energy consumption.\\n7\\nRELATED WORK\\nIn this section, we discuss existing literature studies that relate\\nto our work in this paper.\\nMobile DL Due to their ubiquitous nature, mobile devices can\\ngenerate a wide range of unique sensor data, and thus create count-\\nless opportunities for DL tasks. The prior efforts on mobile DL can\\nbe mainly summarized into two categories. First, researchers have\\nbuilt numerous novel applications based on DL [46, 74, 75, 85, 92].\\nFor example, MobileDeepPill [117] is a small-footprint mobile DL\\nsystem that can accurately recognize unconstrained pill images.\\nSecond, various optimization techniques have been proposed to\\nreduce the overhead of DL on resource-constrained mobile devices.\\nThe optimizations include model compression [55, 64, 73, 80, 108],\\nhardware customizations [48, 50, 62, 118], lightweight models [47,\\n69, 75, 106], knowledge distilling [45, 68, 117], and cloud offload-\\ning [66, 72, 120]. These studies are usually carried out under lab\\nenvironments, based on classical models such as VGG and ResNet,\\nin lack of real-world workloads and insights. Thus, our work is\\nmotivated by those enormous efforts that try to bring DL to mo-\\nbile devices, and fill the gap between the academic literature and\\nindustry products.\\nML/DL as cloud services Besides on-device fashion, the DL\\nfunctionality, or in a broader scope of Machine Learning (ML), can\\nalso be accessed as cloud services. The service providers include\\nAmazon [2], Google [20], Microsoft Azure [22], etc. There are some\\nprior analyzing studies focusing on such MLaaS (machine learning\\nas a service) platforms. Yao et al. [116] comprehensively investi-\\ngate into effectiveness of popular MLaas systems, and find that\\nwith more user control comes greater risk. Some other literature\\nstudies [57, 98, 104] focus on the security issues of those platforms\\ntowards different types of attacks. MLaaS platforms have some\\nadvantages in protecting intellectual property and performance.\\nHowever, compared to on-device fashion, they also have some\\nshortcomings such as privacy concerns and unstable accessibility.\\nThus, some DL tasks are more fit to be run on local devices, e.g.,\\nword-prediction in keyboard. Our work also proves that on-device\\nDL has been already adopted in many real-world apps to some\\nextent, and is going to be popular on smartphones.\\nEmpirical study of DL Prior empirical analysis mainly focuses\\non assisting developers to build better DL apps/systems/models.\\nZhang et al. [122] characterize the defects (bugs) in DL programs\\nvia mining the StackOverflow QA pages and GitHub projects. Con-\\nsequently, the results are limited in only open-source, small-scale\\nprojects. Fawzi et al. [56] analyze the topology and geometry of the\\nstate-of-the-art deep networks, as well as their associated decision\\nboundary. Senior et al. [97] investigate into how the learning rate\\nused in stochastic gradient descent impacts the training perfor-\\nmance, and propose schemes to select proper learning rate. These\\nstudies mostly focus on classical and small-scale DL models pro-\\nposed in previous literature, while our study mine the knowledge\\nfrom large-scale in-the-wild mobile apps.\\nDL Model protection Some recent efforts have been investi-\\ngated in protecting DL models. For example, various watermarking\\nmechanisms [42, 87, 96, 119] have been proposed to protect intel-\\nlectual property of DL models. This approach, however, cannot\\nprotect models from being extracted and attacked. As a closer step,\\nA First Look at Deep Learning Apps on Smartphones\\nWWW’19, May 2019, San Francisco, USA\\nsome researchers [60, 65, 103] secure the DL systems/models based\\non secure execution environments (SEE) such as Intel SGX [21].\\nHowever, those techniques are still not practical for in-the-wild\\ndeployment. Some DL frameworks also provide mechanisms for\\nmodel protection. For example, Mace [24] supports developers to\\nconvert models to C++ code [11]. However, our results show that a\\nlarge number of DL models are exposed without secure protection.\\n8\\nCONCLUSIONS\\nIn this work, we have carried out the first empirical study to\\nunderstand how deep learning technique is adopted in real-world\\nsmartphones, as a bridge between the research and practice. By\\nmining and analyzing large-scale Android apps based on a static\\ntool, we have reached interesting and valuable findings. For exam-\\nple, we show that early adopters of mobile deep learning are top\\napps, and the role played by deep learning in those apps is critical\\nand core. Our findings also provide strong and valuable implica-\\ntions to multiple stakeholders of the mobile ecosystem, including\\ndevelopers, hardware designers and researchers.\\nREFERENCES\\n[1] 2018. Add a new op in TensorFlow. https://www.tensorflow.org/guide/extend/\\nop.\\n[2] 2018. Amazon Machine Learning. https://aws.amazon.com/machine-learning.\\n[3] 2018.\\nAn Exploration of Mobile First AI.\\nhttps://medium.com/swlh/\\nan-exploration-of-mobile-first-ai-576c944efd36.\\n[4] 2018. Android aapt. http://elinux.org/Android_aapt.\\n[5] 2018.\\nApktool: A tool for reverse engineering Android apk files.\\nhttps://\\nibotpeaches.github.io/Apktool/.\\n[6] 2018.\\nApple\\nCOO:\\nSmartphone\\nis\\na\\n’major\\nplatform’\\nfor\\nfuture\\nof\\nAI.\\nhttps://www.techrepublic.com/article/\\napple-coo-smartphone-is-a-major-platform-for-future-of-ai/.\\n[7] 2018. Arm TrustZone. https://developer.arm.com/technologies/trustzone.\\n[8] 2018.\\nArtificial\\nIntelligence\\nNext\\nKey\\nGrowth\\nArea\\nfor\\nSmart-\\nphones\\nas\\nNumbers\\nTop\\nSix\\nBillion\\nby\\n2020,\\nIHS\\nMarkit\\nSays.\\nhttps://news.ihsmarkit.com/press-release/technology/\\nartificial-intelligence-next-key-growth-area-smartphones-numbers-top-six-bi.\\n[9] 2018. Caffe2 deep learning framework. https://github.com/caffe2/caffe2.\\n[10] 2018. Chainer. https://chainer.org/.\\n[11] 2018. Converting model to C++ code. https://mace.readthedocs.io/en/latest/\\nuser_guide/advanced_usage.html.\\n[12] 2018. CoreML by Apple. https://developer.apple.com/documentation/coreml.\\n[13] 2018. Deep Learning for Java. https://deeplearning4j.org/.\\n[14] 2018. dex2jar. https://github.com/pxb1988/dex2jar.\\n[15] 2018. Dynamic shapes in TensorFlow. https://www.tensorflow.org/guide/\\ntensors.\\n[16] 2018.\\nError Ellipses.\\nhttp://www.cs.utah.edu/~tch/CS4300/resources/refs/\\nErrorEllipses.pdf.\\n[17] 2018. FeatherCNN. https://github.com/Tencent/FeatherCNN.\\n[18] 2018. Fully-connected Layers in Convolutional Neural Networks). https://\\ncs231n.github.io/convolutional-networks/.\\n[19] 2018.\\nGlobal\\nmobile\\nOS\\nmarket\\nshare\\nin\\nsales\\nto\\nend\\nusers.\\nhttps://www.statista.com/statistics/266136/\\nglobal-market-share-held-by-smartphone-operating-systems/.\\n[20] 2018. Google Prediction API. https://cloud.google.com/prediction.\\n[21] 2018. Intel Software Guard Extensions. https://software.intel.com/en-us/sgx.\\n[22] 2018. Microsoft Azure ML Studio. https://azure.microsoft.com/en-us/services/\\nmachine-learning.\\n[23] 2018. Microsoft Cognitive Toolkit (CNTK). https://github.com/Microsoft/CNTK.\\n[24] 2018. Mobile AI Compute Engine. https://github.com/XiaoMi/mace.\\n[25] 2018. Mobile deep learning. https://github.com/baidu/mobile-deep-learning.\\n[26] 2018. Open neural network exchange format. https://onnx.ai/.\\n[27] 2018. Open Source Computer Vision Library. https://opencv.org/.\\n[28] 2018.\\nOver\\nHalf\\nof\\nSmartphone\\nOwners\\nUse\\nVoice\\nAssistants.\\nhttps://voicebot.ai/2018/04/03/\\nover-half-of-smartphone-owners-use-voice-assistants-siri-leads-the-pack/.\\n[29] 2018. Protocol Buffer. https://developers.google.com/protocol-buffers/.\\n[30] 2018. pytorch. http://pytorch.org/.\\n[31] 2018. SenseTime. https://www.sensetime.com/?lang=en-us.\\n[32] 2018. Snapdragon Neural Processing Engine. https://developer.qualcomm.com/\\nsoftware/snapdragon-neural-processing-engine.\\n[33] 2018. Snapdragon performance. https://www.anandtech.com/show/12420/\\nsnapdragon-845-performance-preview/2.\\n[34] 2018.\\nStrip visible string in ncnn.\\nhttps://github.com/Tencent/ncnn/wiki/\\nhow-to-use-ncnn-with-alexnet.\\n[35] 2018. SuperID Android SDK. https://github.com/SuperID/superid-android-sdk.\\n[36] 2018. Tencent ncnn deep learning framework. https://github.com/Tencent/ncnn.\\n[37] 2018. TensorFlow. https://www.tensorflow.org/.\\n[38] 2018. TensorFlow Lite. https://www.tensorflow.org/mobile/tflite/.\\n[39] 2018.\\nThe Machine Learning Behind Android Smart Linkify.\\nhttps://ai.\\ngoogleblog.com/2018/08/the-machine-learning-behind-android.html.\\n[40] 2018. torch. http://torch.ch/.\\n[41] 2018.\\nxNN deep learning framework.\\nhttps://myrgzn.gitee.io/rgzn/news/\\npage100.html.\\n[42] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.\\n2018. Turning Your Weakness Into a Strength: Watermarking Deep Neural\\nNetworks by Backdooring. arXiv preprint arXiv:1802.04633 (2018).\\n[43] Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bartel,\\nJacques Klein, Yves Le Traon, Damien Octeau, and Patrick McDaniel. 2014.\\nFlowdroid: Precise context, flow, field, object-sensitive and lifecycle-aware taint\\nanalysis for android apps. Acm Sigplan Notices 49, 6 (2014), 259–269.\\n[44] Davide Bacciu, Stefano Chessa, Claudio Gallicchio, and Alessio Micheli. 2017.\\nOn the need of machine learning as a service for the internet of things. In\\nProceedings of the 1st International Conference on Internet of Things and Machine\\nLearning, IML 2017. 22:1–22:8.\\n[45] Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling.\\n2015. Bayesian dark knowledge. In Advances in Neural Information Processing\\nSystems. 3438–3446.\\n[46] Michael Barz and Daniel Sonntag. 2016. Gaze-guided Object Classification Using\\nDeep Neural Networks for Attention-based Computing. In Proceedings of the\\n2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing\\n(UbiComp’16). 253–256.\\n[47] Guoguo Chen, Carolina Parada, and Georg Heigold. 2014. Small-footprint\\nKeyword Spotting Using Deep Neural Networks. In IEEE International Conference\\non Acoustics, Speech and Signal Processing, (ICASSP’14). 4087–4091.\\n[48] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,\\nand Olivier Temam. 2014. DianNao: a Small-footprint High-throughput Acceler-\\nator for Ubiquitous Machine-Learning. In Architectural Support for Programming\\nLanguages and Operating Systems (ASPLOS’14). 269–284.\\n[49] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun\\nXiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. MXNet: A Flexible\\nand Efficient Machine Learning Library for Heterogeneous Distributed Systems.\\nCoRR abs/1512.01274 (2015).\\n[50] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architec-\\nture for Energy-Efficient Dataflow for Convolutional Neural Networks. In 43rd\\nACM/IEEE Annual International Symposium on Computer Architecture, (ISCA’16).\\n367–379.\\n[51] Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, and Xu-\\nanzhe Liu. 2020. A comprehensive study on challenges in deploying deep\\nlearning based software. In Proceedings of the 28th ACM Joint Meeting on Eu-\\nropean Software Engineering Conference and Symposium on the Foundations of\\nSoftware Engineering. 750–762.\\n[52] Zhenpeng Chen, Sheng Shen, Ziniu Hu, Xuan Lu, Qiaozhu Mei, and Xuanzhe\\nLiu. 2019. Emoji-powered representation learning for cross-lingual sentiment\\nclassification. In The World Wide Web Conference. 251–262.\\n[53] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binarycon-\\nnect: Training deep neural networks with binary weights during propagations.\\nIn Advances in neural information processing systems. 3123–3131.\\n[54] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks\\nfor youtube recommendations. In Proceedings of the 10th ACM Conference on\\nRecommender Systems. 191–198.\\n[55] Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.\\n2014. Exploiting Linear Structure Within Convolutional Networks for Efficient\\nEvaluation. In Advances in Neural Information Processing Systems 27: Annual\\nConference on Neural Information Processing Systems (NIPS’14). 1269–1277.\\n[56] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano\\nSoatto. 2018. Empirical study of the topology and geometry of deep networks.\\nIn IEEE CVPR.\\n[57] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion\\nattacks that exploit confidence information and basic countermeasures. In Pro-\\nceedings of the 22nd ACM SIGSAC Conference on Computer and Communications\\nSecurity. 1322–1333.\\n[58] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial\\nnets. In Advances in neural information processing systems. 2672–2680.\\n[59] Mihajlo Grbovic and Haibin Cheng. 2018. Real-time Personalization using\\nEmbeddings for Search Ranking at Airbnb. In Proceedings of the 24th ACM\\nWWW’19, May 2019, San Francisco, USA\\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu\\nSIGKDD International Conference on Knowledge Discovery & Data Mining. 311–\\n320.\\n[60] Zhongshu Gu, Heqing Huang, Jialong Zhang, Dong Su, Ankita Lamba, Dimitrios\\nPendarakis, and Ian Molloy. 2018. Securing Input Data of Deep Learning Infer-\\nence Systems via Partitioned Enclave Execution. arXiv preprint arXiv:1807.00969\\n(2018).\\n[61] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.\\n2015. Deep learning with limited numerical precision. In International Conference\\non Machine Learning. 1737–1746.\\n[62] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz,\\nand William J. Dally. 2016. EIE: Efficient Inference Engine on Compressed\\nDeep Neural Network. In 43rd ACM/IEEE Annual International Symposium on\\nComputer Architecture, (ISCA’16). 243–254.\\n[63] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Com-\\npressing deep neural networks with pruning, trained quantization and huffman\\ncoding. arXiv preprint arXiv:1510.00149 (2015).\\n[64] Seungyeop Han, Haichen Shen, Matthai Philipose, Sharad Agarwal, Alec Wol-\\nman, and Arvind Krishnamurthy. 2016. MCDNN: An Approximation-Based\\nExecution Framework for Deep Stream Processing Under Resource Constraints.\\nIn Proceedings of the 14th Annual International Conference on Mobile Systems,\\nApplications, and Services (MobiSys’16). 123–136.\\n[65] Lucjan Hanzlik, Yang Zhang, Kathrin Grosse, Ahmed Salem, Max Augustin,\\nMichael Backes, and Mario Fritz. 2018. MLCapsule: Guarded Offline Deployment\\nof Machine Learning as a Service. arXiv preprint arXiv:1808.00590 (2018).\\n[66] Johann Hauswald, Yiping Kang, Michael A. Laurenzano, Quan Chen, Cheng\\nLi, Trevor N. Mudge, Ronald G. Dreslinski, Jason Mars, and Lingjia Tang. 2015.\\nDjiNN and Tonic: DNN as a service and its implications for future warehouse\\nscale computers. In Proceedings of the 42nd Annual International Symposium on\\nComputer Architecture, Portland, OR, USA, June 13-17, 2015. 27–40.\\n[67] Ehsan Hesamifard, Hassan Takabi, Mehdi Ghasemi, and Rebecca N. Wright.\\n2018. Privacy-preserving Machine Learning as a Service. PoPETs 2018, 3 (2018),\\n123–142.\\n[68] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge\\nin a neural network. arXiv preprint arXiv:1503.02531 (2015).\\n[69] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. MobileNets:\\nEfficient Convolutional Neural Networks for Mobile Vision Applications. CoRR\\nabs/1704.04861 (2017).\\n[70] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J.\\nDally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level Accuracy with 50x\\nFewer Parameters and <1MB Model Size. arXiv preprint arXiv:1602.07360 (2016).\\n[71] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,\\nRoss B. Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convo-\\nlutional Architecture for Fast Feature Embedding. In Proceedings of the ACM\\nInternational Conference on Multimedia, MM ’14, Orlando, FL, USA, November 03\\n- 07, 2014. 675–678.\\n[72] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor N. Mudge,\\nJason Mars, and Lingjia Tang. 2017. Neurosurgeon: Collaborative Intelligence\\nBetween the Cloud and Mobile Edge. In Proceedings of the Twenty-Second Inter-\\nnational Conference on Architectural Support for Programming Languages and\\nOperating Systems (ASPLOS’17). 615–629.\\n[73] Nicholas D. Lane, Sourav Bhattacharya, Petko Georgiev, Claudio Forlivesi, Lei\\nJiao, Lorena Qendro, and Fahim Kawsar. 2016. DeepX: A Software Accelerator\\nfor Low-power Deep Learning Inference on Mobile Devices. In 15th ACM/IEEE\\nInternational Conference on Information Processing in Sensor Networks (IPSN\\n2016). 23:1–23:12.\\n[74] Nicholas D. Lane, Sourav Bhattacharya, Petko Georgiev, Claudio Forlivesi, and\\nFahim Kawsar. 2015. An Early Resource Characterization of Deep Learning\\non Wearables, Smartphones and Internet-of-Things Devices. In Proceedings\\nof the 2015 International Workshop on Internet of Things towards Applications\\n(IoT-App’15). 7–12.\\n[75] Nicholas D. Lane, Petko Georgiev, and Lorena Qendro. 2015. DeepEar: Robust\\nSmartphone Audio Sensing in Unconstrained Acoustic Environments Using\\nDeep Learning. In Proceedings of the 2015 ACM International Joint Conference on\\nPervasive and Ubiquitous Computing (UbiComp’15). 283–294.\\n[76] Seyyed Salar Latifi Oskouei, Hossein Golestani, Matin Hashemi, and Soheil\\nGhiasi. 2016. CNNdroid: GPU-Accelerated Execution of Trained Deep Con-\\nvolutional Neural Networks on Android. In Proceedings of the 2016 ACM on\\nMultimedia Conference. 1201–1205.\\n[77] Vadim Lebedev and Victor Lempitsky. 2016. Fast convnets using group-wise\\nbrain damage. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition. 2554–2564.\\n[78] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature\\n521, 7553 (2015), 436.\\n[79] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014. Efficient mini-\\nbatch training for stochastic optimization. In Proceedings of the 20th international\\nconference on Knowledge discovery and data mining (KDD’14). 661–670.\\n[80] Sicong Liu, Yingyan Lin, Zimu Zhou, Kaiming Nan, Hui Liu, and Junzhao Du.\\n2018. On-Demand Deep Model Compression for Mobile Devices: A Usage-\\nDriven Model Selection Framework. In Proceedings of the 16th Annual Inter-\\nnational Conference on Mobile Systems, Applications, and Services (MobiSys’18).\\n389–400.\\n[81] Xuanzhe Liu, Yi Hui, Wei Sun, and Haiqi Liang. 2007. Towards service composi-\\ntion based on mashup. In 2007 IEEE Congress on Services (Services 2007). IEEE,\\n332–339.\\n[82] Huynh Nguyen Loc, Youngki Lee, and Rajesh Krishna Balan. 2017. DeepMon:\\nMobile GPU-based Deep Learning Framework for Continuous Vision Appli-\\ncations. In Proceedings of the 15th Annual International Conference on Mobile\\nSystems, Applications, and Services (MobiSys’17). 82–95.\\n[83] Yun Ma, Ziniu Hu, Diandian Gu, Li Zhou, Qiaozhu Mei, Gang Huang, and\\nXuanzhe Liu. 2020. Roaming Through the Castle Tunnels: An Empirical Analysis\\nof Inter-app Navigation of Android Apps. ACM Transactions on the Web (TWEB)\\n14, 3 (2020), 1–24.\\n[84] Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, and Xuanzhe Liu. 2019.\\nMoving deep learning into web browser: How far can we go?. In The World\\nWide Web Conference. 1234–1244.\\n[85] Gaurav Mittal, Kaushal B. Yagnik, Mohit Garg, and Narayanan C. Krishnan.\\n2016. SpotGarbage: Smartphone App to Detect Garbage Using Deep Learning.\\nIn Proceedings of the 2016 ACM International Joint Conference on Pervasive and\\nUbiquitous Computing (UbiComp’16). 940–945.\\n[86] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\\nOstrovski, et al. 2015. Human-level control through deep reinforcement learning.\\nNature 518, 7540 (2015), 529.\\n[87] Yuki Nagai, Yusuke Uchida, Shigeyuki Sakazawa, and Shin’ichi Satoh. 2018. Dig-\\nital watermarking for deep neural networks. International Journal of Multimedia\\nInformation Retrieval 7, 1 (2018), 3–16.\\n[88] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017.\\nEmbedding-based news recommendation for millions of users. In Proceedings\\nof the 23rd ACM SIGKDD International Conference on Knowledge Discovery and\\nData Mining. 1933–1942.\\n[89] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Ce-\\nlik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial\\nsettings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on.\\n372–387.\\n[90] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online\\nlearning of social representations. In Proceedings of the 20th ACM SIGKDD\\ninternational conference on Knowledge discovery and data mining (KDD’14). 701–\\n710.\\n[91] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised represen-\\ntation learning with deep convolutional generative adversarial networks. arXiv\\npreprint arXiv:1511.06434 (2015).\\n[92] Valentin Radu, Nicholas D. Lane, Sourav Bhattacharya, Cecilia Mascolo, Ma-\\nhesh K. Marina, and Fahim Kawsar. 2016. Towards Multimodal Deep Learning\\nfor Activity Recognition on Mobile Devices. In Proceedings of the 2016 ACM Inter-\\nnational Joint Conference on Pervasive and Ubiquitous Computing (UbiComp’16).\\n185–188.\\n[93] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.\\nXnor-net: Imagenet classification using binary convolutional neural networks.\\nIn European Conference on Computer Vision. 525–542.\\n[94] Mauro Ribeiro, Katarina Grolinger, and Miriam A. M. Capretz. 2015. MLaaS:\\nMachine Learning as a Service. In 14th IEEE International Conference on Machine\\nLearning and Applications, ICMLA 2015. 896–902.\\n[95] Everett M Rogers. 2010. Diffusion of innovations. Simon and Schuster.\\n[96] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. [n. d.]. DeepSigns:\\nA Generic Watermarking Framework for Protecting the Ownership of Deep\\nLearning Models. ([n. d.]).\\n[97] Andrew Senior, Georg Heigold, Ke Yang, et al. 2013. An empirical study of\\nlearning rates in deep neural networks for speech recognition. In Acoustics,\\nSpeech and Signal Processing (ICASSP), 2013 IEEE International Conference on.\\nIEEE, 6724–6728.\\n[98] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.\\nMembership inference attacks against machine learning models. In Security and\\nPrivacy (SP), 2017 IEEE Symposium on. 3–18.\\n[99] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\\nHuang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,\\net al. 2017. Mastering the game of Go without human knowledge. Nature 550,\\n7676 (2017), 354.\\n[100] Karen Simonyan and Andrew Zisserman. 2014.\\nVery Deep Convolutional\\nNetworks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2014).\\n[101] SÃk, ren Sonnenburg, Mikio L Braun, Cheng Soon Ong, Samy Bengio, Leon\\nBottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert MÃžller, Fernando Pereira,\\nCarl Edward Rasmussen, et al. 2007. The need for open source software in\\nmachine learning. Journal of Machine Learning Research (JMLR) 8, Oct (2007),\\nA First Look at Deep Learning Apps on Smartphones\\nWWW’19, May 2019, San Francisco, USA\\n2443–2466.\\n[102] Ion Stoica, Dawn Song, Raluca Ada Popa, David Patterson, Michael W Mahoney,\\nRandy Katz, Anthony D Joseph, Michael Jordan, Joseph M Hellerstein, Joseph E\\nGonzalez, et al. 2017. A berkeley view of systems challenges for ai. arXiv\\npreprint arXiv:1712.05855 (2017).\\n[103] Florian Tramer and Dan Boneh. 2018. Slalom: Fast, Verifiable and Private Execu-\\ntion of Neural Networks in Trusted Hardware. arXiv preprint arXiv:1806.03287\\n(2018).\\n[104] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.\\n2016. Stealing Machine Learning Models via Prediction APIs.. In USENIX Security\\nSymposium. 601–618.\\n[105] Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. 2011. Improving the\\nspeed of neural networks on CPUs. In Proc. Deep Learning and Unsupervised\\nFeature Learning NIPS Workshop, Vol. 1. 4.\\n[106] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez-Moreno, and Javier\\nGonzalez-Dominguez. 2014. Deep Deural Detworks for Small Footprint Text-\\ndependent Speaker Verification. In IEEE International Conference on Acoustics,\\nSpeech and Signal Processing, (ICASSP’14). 4052–4056.\\n[107] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. 2016. Learning\\nstructured sparsity in deep neural networks. In Advances in Neural Information\\nProcessing Systems. 2074–2082.\\n[108] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. 2016.\\nQuantized Convolutional Neural Networks for Mobile Devices. In 2016 IEEE\\nConference on Computer Vision and Pattern Recognition, (CVPR’16). 4820–4828.\\n[109] Chenren Xu, Shuang Jiang, Guojie Luo, Guangyu Sun, Ning An, Gang Huang,\\nand Xuanzhe Liu. 2020. The Case for FPGA-based Edge Computing. IEEE\\nTransactions on Mobile Computing (2020).\\n[110] Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and\\nXuanzhe Liu. 2019. A first look at deep learning apps on smartphones. In The\\nWorld Wide Web Conference. 2125–2136.\\n[111] Mengwei Xu, Yun Ma, Xuanzhe Liu, Felix Xiaozhu Lin, and Yunxin Liu. 2017.\\nAppHolmes: Detecting and characterizing app collusion among third-party\\nAndroid markets. In Proceedings of the 26th International Conference on World\\nWide Web. 143–152.\\n[112] Mengwei Xu, Tiantu Xu, Yunxin Liu, Xuanzhe Liu, Gang Huang, and Felix Xi-\\naozhu Lin. 2020. A query engine for zero-streaming cameras. In Proceedings of\\nthe 26th Annual International Conference on Mobile Computing and Networking.\\n1–3.\\n[113] Mengwei Xu, Xiwen Zhang, Yunxin Liu, Gang Huang, Xuanzhe Liu, and Felix Xi-\\naozhu Lin. 2020. Approximate query service on autonomous iot cameras. In\\nProceedings of the 18th International Conference on Mobile Systems, Applications,\\nand Services. 191–205.\\n[114] Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xuanzhe Liu.\\n2018. DeepCache: Principled Cache for Mobile Deep Vision. In Proceedings of\\nthe 24th Annual International Conference on Mobile Computing and Networking.\\n129–144.\\n[115] Chengxu Yang, QiPeng Wang, Mengwei Xu, Shangguang Wang, Kaigui Bian,\\nand Xuanzhe Liu. 2020. Heterogeneity-aware federated learning. arXiv preprint\\narXiv:2006.06983 (2020).\\n[116] Yuanshun Yao, Zhujun Xiao, Bolun Wang, Bimal Viswanath, Haitao Zheng, and\\nBen Y. Zhao. 2017. Complexity vs. performance: empirical analysis of machine\\nlearning as a service. In Proceedings of the 2017 Internet Measurement Conference,\\nIMC 2017, London, United Kingdom, November 1-3, 2017. 384–397.\\n[117] Xiao Zeng, Kai Cao, and Mi Zhang. 2017. MobileDeepPill: A Small-Footprint\\nMobile Deep Learning System for Recognizing Unconstrained Pill Images. In\\nProceedings of the 15th Annual International Conference on Mobile Systems, Ap-\\nplications, and Services (MobiSys’17). 56–67.\\n[118] Chen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong.\\n2015. Optimizing FPGA-based Accelerator Design for Deep Convolutional Neu-\\nral Networks. In Proceedings of the 2015 ACM/SIGDA International Symposium\\non Field-Programmable Gate Arrays (FPGA’15). 161–170.\\n[119] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing\\nHuang, and Ian Molloy. 2018. Protecting Intellectual Property of Deep Neural\\nNetworks with Watermarking. In Proceedings of the 2018 on Asia Conference on\\nComputer and Communications Security. 159–172.\\n[120] Qingchen Zhang, Laurence T. Yang, and Zhikui Chen. 2016. Privacy Preserving\\nDeep Computation Model on Cloud for Big Data Feature Learning. IEEE Trans.\\nComputers (2016), 1351–1362.\\n[121] Xingzhou Zhang, Yifan Wang, and Weisong Shi. 2018. pCAMP: Performance\\nComparison of Machine Learning Packages on the Edges. (2018).\\n[122] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang.\\n2018. An Empirical Study on TensorFlow Program Bugs. (2018).\\n',\n",
       " '1901.02354v2.pdf': 'arXiv:1901.02354v2  [cs.LG]  13 Jan 2019\\n1\\nGeometrization of deep networks for the interpretability of deep\\nlearning systems\\nXiao Dong, Ling Zhou\\nFaculty of Computer Science and Engineering, Southeast University, Nanjing, China\\nHow to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the\\ngeometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation\\nand this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and\\ndeep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may\\nalso help to solve the interpretability problem of deep learning systems.\\nIndex Terms—deep networks, geometrization, physics, computation\\nI. MOTIVATION\\nAs a general tool to solve complex problems, the thriving\\ndeep learning technology is showing its power in almost all\\nresearch ﬁelds. But we are still lacking a general theoretical\\nframework to to answer the following questions: Why does\\ndeep learning work so well? What’s the relationship between\\nthe structure of a deep network and its functionality? How to\\ndesign a proper deep network structure for a given task? How\\ncan we predict and control the behaviour of a deep network\\nduring training? How can our brain construct efﬁcient network\\nstructures for different tasks with limited resources?\\nIn this work we propose a general framework to understand\\ndeep learning systems, the geometrization of deep networks.\\nA. Why geometrization\\nOur motivation to understand deep learning systems from a\\ngeometric perspective falls in three folds.\\nDeep networks are physical The reason that deep learning\\nis so powerful and universally effective in different ﬁelds is\\nthat deep networks reveal the structures of physical systems.\\nThat’s to say, deep networks are effective representations of\\nphysical systems and their evolutions. Besides the enormous\\nexamples of AI based applications on computer vision, nat-\\nural language processing and robot control, deep networks\\nare also closely related with the fundamental laws of our\\nworld, for example the effective representation of many-\\nbody quantum systems[1], renormalization group and entan-\\nglement renormalization[2][3], tensor networks and AdS/CFT\\nduality[4][5][6][7][8][9][10]. So we believe the effectiveness\\nof deep networks has a fundamental physical origin. That is,\\nthe deep network is at least a replica of our physical world so\\nthat every physical system has a correspondent deep network\\nrepresentation. Deep networks may share the same structure\\nof the physical world and they may obey the same rules. The\\ngreat success of geometrization of physics inspires our idea\\nthat geometrization may also be the ultimate framework to\\nunderstand deep networks and deep learning systems.\\nDeep networks are computational programmes From\\nthe quantum computation point of view, any physical system\\ncan be regarded as being generated from an initial simple\\nstate by an unitary operation. Similarly the evolution of any\\nphysical system is also an unitary operation or equivalently\\na computation process. As effective descriptions of physical\\nsystems and their evolutions, deep networks are essentially\\ncomputation processes and can be understood as computa-\\ntional programmes to generate and evolve physical systems.\\nThen the geometrization of quantum states and quantum\\ncomputations[11][12] also leads to the geometrization of deep\\nlearning systems. For example, quantum computation com-\\nplexity has a clear geometric picture and concrete physi-\\ncal meanings as discussed in complexity=action, complex-\\nity=volume, Hamiltonian complexity, tensor networks and the\\nemergent spacetime structure from quantum information.\\nDeep networks as optimal control and optimization\\nsystems Recently there are emerging efforts to formulate deep\\nlearning systems as either optimization or optimal control\\nproblems[13][14]. It’s well-known that these are also closely\\nrelated with geometry and physics. We will show this point\\nwith a concrete example of template image matching, which\\nhas a clear geometric picture as an optimization or an optimal\\ncontrol problem.\\nAll the above observations lead to the same conclusion,\\ngeometrization scheme may bring us new perspectives to\\nunderstand deep networks and deep learning systems. What’s\\nmore, if the geometrization of deep networks can be accom-\\nplished, this may also change our ways to understand the\\nphysical world, i.e. a physical world built by deep networks.\\nNow we show how to build the geometrization of deep net-\\nworks and how this can help us to understand deep networks\\nand deep learning.\\nB. An abstract description of deep networks\\nIn order to establish the geometric picture of deep networks,\\nwe now give an abstract description of it.\\nAs mentioned above, deep networks are programmes or\\ndata processing systems, which can achieve a transformation\\nfrom the input data space Vin to the output data space Vout.\\nNormal deep learning tasks, such as feature extraction and\\ngenerative models, are all mappings between different data\\nspaces. And usually we prefer one of them to be a vector\\n2\\nspace so that algebraic operations or classiﬁcations can be\\neasily carried out on it. From the general computation point\\nof view, a data processing or a computation system can be\\nabstracted as a mapping C : V × G →V , where V is the\\nspace of data and G is the space of operations on data. A\\ncomputation process is given by C(vin, g) = g(vin) = vout,\\nwhere vi, so ∈V are the input and output data and g ∈G\\nis an operation or transformation on data. A programme or\\nan algorithm is a realization of g, which is usually achieved\\nby a series of simple primitive operations as in both classical\\nand quantum computers. The structure of a deep network is\\nessentially a parametric realization of g and the process of\\ntraining is to ﬁnd the proper network parameters that achieve\\ng. Here we would like to note that the parameters may also\\ninclude part of the network structure so that network structure\\nitself may also be learned during training.\\nThe key feature of deep networks, the deep structure means\\nthat g is realized by a discrete time sequence of transforma-\\ntions {¯gn|n ∈[0, N], ¯g0 = Id, ¯gN = g}, where Id stands for\\nthe identity transformation. In this transformation sequence,\\nthe nth step achieves an operation ¯gn ◦¯g−1\\nn−1, which is usually\\na simple low complexity operation. To make an analytical\\nstudy of deep learning networks, we introduce a continuous\\ntime ﬂow {gt|t ∈[0, 1], g0 = Id, g1 = g}. The validity\\nof this continuous ﬂow fundamentally lies in the continuous\\nevolution of quantum states. This is to say, a discrete time\\nmodel of quantum information processing system such as the\\nquantum circuit model is essentially only an approximation\\nof a continuous time quantum evolution. Similarly a discrete\\ntime deep network is only an approximation of a continuous\\nﬂow of transformation.\\nObviously the continuous time ﬂow of transformation has\\na geometric picture. It is a continuous curve in the space\\nof transformation connecting the identity operation I and the\\ntarget operation g. Accordingly for each input data vin, there\\nis a curve in the data space given by {vi,t|t ∈0, 1, v0 =\\nvin, v1 = vout}. The purpose of deep learning systems is\\nto ﬁnd an optimal transformation ﬂow to realize the target\\ntransformation, where the correspondent collection of the\\ntrajectories of all the input data {vk\\nin,t, vk\\nin ∈Vin} should\\nshow a good shape, where a good shape means the trajectories\\nshould be smooth, stable and well distributed so that the the\\nnetwork has a good genearalization performace.\\nWe now focus on the continuous transformation curve\\ngt, t ∈[0, 1]. If we regard the space of transformation G\\nas a manifold, then we can build a Riemannian structure on\\nit. We can deﬁne the time derivative of gt as ˙gt = ut ◦gt\\nand a right invariant metric on the tangent space T G of\\nG as < ˙gt, ˙gt >T G=< ut, ut >Id. Then we can calculate\\nthe length of the curve gt, t ∈[0, 1], g0 = I, g1 = g as\\nR 1\\n0 < ut, ut > dt, which is the algorithmic complexity of\\nthe realization gt, t ∈[0, 1] of g.\\nNow we have a simple geometric picture of deep networks.\\nThe structure of a deep network and the metric determines\\nthe length of the curve. Network parameters and the metric\\ndetermines the shape of the curve. The optimal realization of\\ng under a constraint to minimize the length of the curve is the\\ngeodesic from Id to g.\\nOf course, keen readers will argue that above geometric\\npicture is too abstract for a quantitative or even a qualitative\\nunderstanding of deep learning systems. In the remaining part\\nof this paper, we will ﬁrstly give a solid example of the\\ngeometrization of deep networks by comparing deep networks\\nwith the geometry of image matching. Then we scratch a\\nbroader picture of the geometrization of deep networks by\\ncomparing deep networks with other physical systems in-\\ncluding quantum information processing, quantum many-body\\nsystems, spacetime structure and general relativity.\\nII. GEOMETRY OF IMAGE REGISTRATION\\nComputational anatomy[15] is a research ﬁeld to study\\nthe variability of anatomical shapes, where the comparison\\nbetween shapes is the key issue. Mathematically a shape can\\nbe described by a function on a spatial space I : Rn →Rm,\\nwhich we call an image I. Here n = 2, 3 stands for a 2D\\nor a 3D image. m = 1 and m > 1 mean scalar images\\nand vector/tensor images. For two different shapes represented\\nby correspondent images I0, I1, the task of image registration\\nis to ﬁnd a transformation ϕ so that the difference between\\nthe target image I1 and the transformed source image I0\\nis minimized, i.e. minϕ ∥I1 −I0 ◦ϕ∥. The details of the\\ntransformation I0 ◦ϕ depends on the type of the image I0[15].\\nA. Diffeomorphic image registration: optimization vs opti-\\nmal control\\nDiffeomorphic image registration is a framework for shape\\ncomparison by modeling transformations between shapes as a\\nsmooth invertible function ϕ : Rn →Rn. For example the\\nspace of transformations of volumetric images can be taken\\nas G = Diff(R3), which is the diffeomorphism group of\\nR3, and V = I(R3) as the space of volumetric images on R3.\\nDeforming an image I0 ∈V by a transformation ϕ ∈G is just\\nthe change of coordinate as I0 ◦ϕ. Following [16][17], image\\nregistration can be abstracted as a map G× V →V , where G\\nis the group of diffeomorphic image transformations and V is\\nthe vector space of images. Large deformation diffeomorphic\\nmetric mapping (LDDMM)[18] generates a deformation ϕ as\\na ﬂow ϕu\\nt of a time-dependent vector ﬁeld ut ∈Te(G) = g\\nso that\\n˙ϕu\\nt = ut ◦ϕu\\nt , ϕu\\n0 = Id, ϕu\\n1 = ϕ\\n(1)\\nThe diffeomorphic matching of two images I0 and I1 with\\nLDDMM is to ﬁnd a vector ﬁeld ut, t ∈[0, 1] to minimize the\\ncost function\\nE(ut) =\\nZ 1\\n0\\nl(ut)2dt+β|I1−Io◦ϕu\\n1|2, ˙ϕu\\nt = ut◦ϕu\\nt , ϕu\\n0 = Id\\n(2)\\nHere the regularity on ut is a kinetic energy term l(ut) =\\n1\\n2\\nR 1\\n0 |ut|2dt with |ut| a norm on the vector ﬁeld deﬁned\\nas |ut|2 = ⟨Lut, ut⟩L2. The operator L is a positive self-\\nadjoint differential operator, for example Lut = ut −α2∆ut.\\nObviously the norm |ut|2 = ⟨Lut, ut⟩L2 deﬁnes a Riemannian\\nmetric on the manifold of the diffeomorphic transformation\\ngroup Diff(Rn). The second term of E(ut) computes the\\ndifference between the transformed image Io ◦ϕu\\n1 and I1.\\n3\\nA necessary condition DE(ut) = 0 to minimize the cost\\nfunction is that the vector ﬁeld ut should satisfy the Euler-\\nPoincar´e (E-P) equation\\nLut = −ϕu\\n0,tI0 ⋄ϕu\\n0,tϕu\\n1,0π\\n(3)\\nwhere ϕu\\ns,t = ϕu\\nt ◦ϕu\\ns−1, π := β(ϕu\\n0,tI0 −I1)♭∈V ∗. The\\n♭operator is deﬁned as ♭: V →V ∗, ⟨u♭, v⟩V ∗×V = ⟨u, v⟩\\nand ⋄: T V ∗→g∗, ⟨I ⋄π, u⟩g∗×g = ⟨π, ζu(I)⟩V ∗×V is the\\nmomentum map.\\nThe E-P equation can also be given as\\nd\\ndt\\n∂l(ut)\\n∂ut\\n= −ad∗\\nut\\n∂l(ut)\\n∂ut\\n(4)\\nwhere ∂l(ut)\\n∂ut\\nis the momentum and ad∗: g →gl(g) is the\\ncoadjoint representation of the Lie algebra g of the Lie group\\nG. For more details please refer to [16][17].\\nIn LDDMM framework, the curve satisfying the E-P equa-\\ntion is found by a gradient descent algorithm, while the\\ngradient is given by ut + Kϕu\\n0,tI0 ⋄ϕu\\n0,tϕu\\n1,0π with K = L−1.\\nThe geometric picture of LDDMM is quite simple: LD-\\nDMM ﬁnds a minimal length curve, i.e. a geodesic given\\nby the E-P equation, in Diff(Rn) connecting Id and ψ,\\nwhich can transform the source I0 to a near neighbour of the\\ntarget image I1. Equivalently we can also induce a Riemannian\\nstructure on the image space V by the map G × V →V so\\nthat the geodesic on G leads to a geodesic on V[17].\\nHere we point out that this is exactly the same as in the\\ngeometry of quantum computation[11][19] that a Riemannian\\nmetric on the quantum operation group induces a Rieman-\\nnian metric on the Hilbert space of quantum states. Another\\ninteresting observation is that the map G × V\\n→V can\\nalso be understood as a typical computation system, where\\nV is the data representation space and G is the data operation\\nspace. So in fact image registration and quantum computation\\nessentially have the same abstract descriptions and geometric\\npictures[10].\\nLDDMM based image registration is formulated as an\\noptimization problem and solved by a gradient descent based\\noptimization. The optimal solution ϕt is parameterized by the\\ntime-dependent vector ﬁeld ut and the optimization procedure\\nis a parameter estimation of ut. We can easily see this is\\nvery similar with the abstract model of deep networks we\\nintroduced above.\\nAn alternative framework of LDDMM is to formulate it as\\nan optimal control problem[20], where the image registration\\nprocedure is regarded as a dynamical process. The state of\\nthe dynamical system is the transformed source image I0 ◦ϕt\\nand the vector ﬁeld ut is taken as the control signal to adjust\\nthe transformation ϕt. The problem is then to minimize the\\nenergy function\\nE(ut, J0\\nt , λt, γ)\\n=\\nZ 1\\n0\\nl(ut) + ⟨λt, ˙J0\\nt + ∇It · ut⟩dt(5)\\n+\\n< γ, J0\\n0 −I0 > +β|J0\\n1 −I1|\\n(6)\\nwhere J0\\nt\\n= I0 ◦ϕu\\n0,t, J1\\nt\\n= I1 ◦ϕu\\n1,t and λt, γ are the\\nLagrangian multipliers.\\nThis leads to the optimality conditions as follows\\n˙J0\\nt + ∇J0\\nt · ut\\n=\\n0\\n(7)\\n˙λt + ∇· (λt · ut)\\n=\\n0\\n(8)\\nut + K ⋆∇J0\\nt λt\\n=\\n0\\n(9)\\nJ0\\n0\\n=\\nI0\\n(10)\\nλ1\\n=\\nβ(I1 −J0\\n1)\\n(11)\\nThe optimization procedure is a bi-directional information\\nﬂow. Given the current control signal ut and initial values\\nof J0\\n0 = I0, the forward information ﬂow compute J0\\nt for\\nt ∈[0, 1]. In the backward adjoint ﬂow, we update λt starting\\nfrom λ1 = β(I1 −J0\\n1) and then ut can be updated by a\\ngradient descent using both J0\\nt and the adjoint variable λt.\\nWe note that the gradient based update of ut here is in fact\\nthe same as the updating of ut in the optimization formulation\\nto fulﬁll the E-P equation. But the idea of bi-directional adjoint\\ncomputation is a new characteristic. This is different from the\\ndirect computation of gradient in the optimization formulation.\\nThe Lagrangian multiplier based formulation can lead to more\\ngeneral strategies for parameter optimization as will be shown\\nlater.\\nB. Geodesic shooting\\nIn LDDMM, both the optimization and optimal control\\nformulations aim to ﬁnd a geodesic by ﬁnding a vector ﬁeld\\nut satisfying the E-P equation. It’s well known that for a given\\nRiemannian manifold, a geodesic is completely determined by\\nthe starting point and the initial velocity of the geodesic. So\\nif our goal is to ﬁnd a geodesic, then the vector ﬁeld ut as a\\ncontrol signal is highly redundant since it can be completely\\ndetermined by u0 and the E-P equation.\\nGeodesic shooting[21] can ﬁnd the initial vector ﬁeld u0 or\\nequivalently the initial momentum Lu0 with the E-P equation\\nas an explicit constraint. Obviously here the geodesic shoot-\\ning is also formulated as an optimal control problem. The\\ncorrespondent optimization procedure is also a bi-directional\\ninformation ﬂow. Starting from the initial momentum and\\nthe E-P equation, the forward ﬂow updates the vector ﬁeld\\nut, the transformation ϕu\\nt and the transformed source image\\nJ0\\nt = I0 ◦ϕu\\nt . The backward adjoint ﬂow updates the adjoint\\nvariables, the Lagrangian multipliers of the constraints, and\\nﬁnally the initial vector ﬁeld u0 can be updated by a gradient\\ndescent. For more details of geodesic shooting and the related\\nadjoint calculation, please refer to [21][15].\\nThe lesson we can draw from geodesic shooting is that,\\nwhen the optimal conﬁguration of a subset of the parameters\\ncan be determined as a function of all the other parameters, this\\nfunction can be regarded as a constraint and the optimization\\ncan be simpliﬁed as an optimal problem. Or in another word,\\nwhen there exists explicit constraints among parameters, the\\noptimization can be simpliﬁed. This may help to design the\\nnetwork structure in deep learning systems.\\nThis idea can be generalized to the case of a general optimal\\ncontrol, where explicit constraints among parameters should\\nbe respected. Then the Lagrangian multiplier based variational\\nmethod will lead to a similar bi-directional information passing\\n4\\nalgorithm which can be shown later to be closely related with\\ndeep learning systems.\\nC. Semiproduct group and metamorphosis\\nMetamorphosis is used to modify the original LDDMM or\\nthe geodesic shooting to support a second transformation ﬂow\\nηt, vt = ϕu\\nt ˙ηt, which is used to change the image appearance\\nof the template image I0 so that the image transformation is\\na composition of both the coordinate transformation and the\\nimage appearance transformation. Essentially this is to replace\\nthe Lie group G with a semiproduct group[17][22]. That’s\\nto say, we are now working with a composite operation of\\nmultiple operations.\\nUnder the composite transformation, the image is trans-\\nformed as ˙J0\\nt = vt+ut◦ϕu\\nt . This is a new constraint involving\\nboth the coordinate and image appearance transformations.\\nAccordingly the energy function to be minimized includes\\nboth the kinetic energies of ut and vt. In another word, the\\nRiemannian manifold of transformations is extended and a\\nnew composite metric is deﬁned. But basically the geometric\\npicture is similar with the original LDDMM or the geodesic\\nshooting framework. An alternative perspective of the meta-\\nmorphosis is that the transformation on the image appearance\\nηt can be regarded as introducing noise on the image. The\\nconstraint on the kinetic energy of vt can be understood as to\\nconstrain the power of the noise. For more details of the idea\\nof metamorphesis, please refer to [15][22][23].\\nD. Summary of diffeomorphic image registration\\nImage registration can be formalized by either energy based\\noptimization or an optimal control problem. It has a clear\\ngeometric picture, where the optimal solution is a geodesic\\non the Riemannian manifold on the transformation space.\\nThe geodesic is represented by a parameterized model and is\\nobtained by a parameter estimation procedure. What’s more,\\nthe image registration problem is closely related with the\\ngeometric mechanics in that they share lots of geometric\\nstructures.\\nA complete description of the geometric structure of image\\nregistration is given in [16][17][21][22][23][15] and references\\ntherein.\\nIII. GEOMETRIC PICTURE OF DEEP LEARNING SYSTEMS\\nTo show the validness of the geometrization of deep net-\\nworks, we now compare the diffeomorphic image registration\\nand deep learning systems to build a dictionary between\\ncorrespondent concepts in both ﬁelds. Since image registra-\\ntion has both a clear geometric and a physical (geometric\\nmechanical) picture, we hope the dictionary will give us a\\nnew understanding of deep networks from both the geometric\\nand physical points of view. Here we directly give a list of the\\ncontent of the dictionary with a brief explaination. Interested\\nreaders can check the details by themselves.\\nA. A dictionary between image registration and deep net-\\nworks\\n(1)Network structure and G: Geometrically the network\\nstructure deﬁnes the space of possible solutions, which is a set\\nof curves in the transformation space. Also the network struc-\\nture deﬁnes in which way this space is explored as explained in\\nthe relational inductive bias[24]. Due to the limited complexity\\nof the network and limited allowed operations, the network\\nstructure only represents a subset of all possible curves that\\ncan reach the target transformation ϕ in image registration or\\ng in deep learning from the identical transformation Id. In\\nfact the deep network structure deﬁnes the operation group G\\nand the network parameters θ falls in its Lie algebra g of G.\\nNormal CNNs are just discretisized transformation curves and\\nthe norms of network parameters θ along the curve can be\\nroughly regarded as the non-uniform discretization step sizes.\\n(2)Constraints and Riemannian metric: The network\\nstructure only deﬁnes the space of possible solutions. To ﬁnd\\nthe optimal solution by solving an optimization problem, we\\nneed to introduce constraints on the parameters of the network.\\nGeometrically constraints can be regarded as Riemannian\\nmetrics on G deﬁned on the manifold of possible solutions\\nencoded in the network structure and network parameters.\\nCarefully adjusting constraints can change the curvature dis-\\ntribution of the solution manifold and generally we prefer to\\nwork on a ﬂat manifold so that the optimal solution can be\\neasily found.\\n(3)Supervised training and landmark registration: Given\\nthe parametric description of the manifold of possible solutions\\nand the Riemannian metric deﬁned by constraint, supervised\\ntraining on a set of N labeled training data estimates the\\nparameters ut(θ) to ﬁnd the optimal transformation curve gt to\\nreach the desired target transformation. This can be understood\\nto achieve a diffeomorphic image registration based on N\\npairs of landmarks on the image or to simultaneously match\\nN images using the same diffeomorphic transformation.\\n(4)Optimal deep networks and geodesics: In LDDMM,\\nthe optimal transformation is achieved by a geodesic on G\\ndetermined by the E-P equation. In deep networks, an optimal\\nnetwork should also exist as a geodesic on the Riemannian\\nmanifold deﬁned by the network structure and constraints,\\nwhich is the deep network with a minimal complexity. Accord-\\ningly, if the E-P equation of deep networks can be explicitly\\ndescribed as in LDDMM, then geodesic shooting can also\\nbe implemented on the optimization of deep networks. Since\\ngeodesic shooting only optimize the initial momentum of\\nthe geodesic, the training of an optimal deep network has a\\nmuch smaller degree of freedom than a normal non-optimal\\ndeep network. Network distillation and network pruning are\\nessentially both efforts to ﬁnd the optimal deep networks.\\n(5)Back propagation and LDDMM: The back propagation\\nbased optimization of deep networks are essentially the same\\nas the gradient descent based LDDMM optimization[18].\\n(6)Neural ODE and optimal control framework: The\\noptimal control based optimization procedure of LDDMM is\\nessentially the same as the optimization used in neural ODE\\nand other related works[13][14].\\n5\\n(7)Equilibrium propagation and geodesic shooting: Ben-\\ngio’s equilibrium propagation[25] share the same structure as\\ngeodesic shooting used in LDDMM framework[21].\\n(8)Attention mechanism and semiproduct group: Es-\\nsentially attention mechanism is a composition of multiple\\ndeep networks. It shares lots of similarity with the semiprod-\\nuct group and metamorphoses in LDDMM framework since\\nsemiproduct group also plays with composite operations. In the\\nsemiproduct group case of LDDMM, the multiple operations\\nare coupled and a generalized E-P equation can obtained to\\nrepresent the optimal ﬂow. This indicates that theoretically we\\nalso have an optimal attention mechanism and the geodesic\\nshooting scheme can be applied.\\n(9)Generalization and Riemannian curve length: The\\ngeneralization capability of deep networks is a key issue of\\nthe performance of deep networks. Usually generalization is\\ndescribed by a norm based factor, which can be understood\\nas the complexity of the network[26]. It has been found that\\ndeep networks have the tendency to reduce complexity during\\ntraining and a lower complexity means a better generalization\\ncapability. In LDDMM, the complexity of the registration\\ntransformation is the length of the transformation curve evalu-\\nated using the Riemannian metric on G. In deep networks, we\\nalso have a correspondent network complexity using a special\\nRiemannian metric, the Fisher-Rao metric[26]. In fact this\\nmetric is closely related with general relativity[27][28][29],\\nwhich is another evidence that deep networks have a deep\\nphysical origin. We will give more details on this point in the\\ndiscussion section.\\n(10) Batch normalization and geodesics: It’s well known\\nthat batch normalization can help the convergence of deep\\nnetworks. From the geometrical point of view, since CNNs\\nare just discretisized transformaition curves and the norms\\nof θ are the discretization step sizes, BN can be regarded\\nas an operation to adaptively adjust the ratio of thrown-away\\ninformation (energy) along the curve. This is because CNNs\\nare the same as the entanglement renormalization algorithm,\\nwhich extracts global information by iteratively throwing away\\nlocal information[2]. By normalizing the data, BN aims to\\nkeep a constant speed of throwing away information along the\\nnetwork. In entanglement renormalization algorithms, this is\\nto throw away a ﬁxed percentage of low amplitude states and\\nkeep only those high amplitude states so that the strong global\\ninformation patterns are kept. This will result in a transforma-\\ntion curve with an isometric-like property, which coincides the\\nproperty of geodesics. So geometrically BN can be understood\\nas a constraint to force the network to be a geodesic-alike\\ncurve. This geometric picture is the same as the conclusion of\\n[30]. In [30] the Hessian matrix was introduced, which is in\\nfact the Fisher-Rao metric to evaluate the complexity of the\\ndeep network or the length of the transformation curve. If the\\nnetwork can be constrained by BN to form a geodesic-like\\ncurve, then it has the minimal curve length or equivalently\\nthe minimal deformation energy as in the geometry of image\\nregistration problem. Or BN forces the Fisher-Rao metric to\\nvary smoothly along the network. Obviously a transformation\\nwith a minimal deformation energy or a smooth curve will\\nhave a smooth loss landscape, which coincides with a key\\nconclusion of [30].\\n(11)Training convergence and curvature: The conver-\\ngence of deep networks highly depends on the back propa-\\ngation of gradients along deep networks. From the geometric\\npicture of LDDMM, we know that this is related with the\\ncurvature of the manifold since the curvature determines the\\nstability of geodesics. In deep learning ﬁelds, random matrix\\nbased analysis[31] shows that when the network has a dynamic\\nisometric property, the forward and backward information can\\nﬂow freely along the network so that a better convergence\\ncan be achieved. In fact, isometry is exactly the property of a\\ngeodesic. So the dynamic isometric property of a deep network\\nis essentially to say, the network is a geodesic on Euclidean\\nmanifold, i.e. a straight line. Similarly, batch normalization is\\nessentially to adjust the curvature of the manifold by adjusting\\nthe Fisher-Rao metric along the network.\\n(12)GAN and current based shape matching: The key\\ngoal of GAN is to approximate a distribution density. The main\\nchallenge of GAN is to ﬁnd a proper metric to measure the\\ndifference of distributions. This is why WGAN emerges as a\\nbreak-through since it provides an efﬁcient metric for distribu-\\ntions without one-to-one correspondence between samples of\\ndistributions. In LDDMM, there is also a way to compare two\\nshapes without position correspondence using current based\\nshape representation[32]. It will be interesting to ﬁnd if there\\nexists a correspondence between WGAN and currents.\\n(13)Dropout and stachastic shape evolutions: As a so-\\nlution to enhance the robustness of deep networks, dropout\\nachieves its goal by adding perturbations on the network,\\neither on the operation group G (dropout of neurons) or\\non the data space V by adding perturbation layers[33]. In\\nLDDMM framework, there are also similar shape registration\\nmethods by adding perturbations on either the momentum or\\nthe positions of landmarks on shapes[34]. Obviously dropout\\naims to ﬁnd a curve that is robust against perturbations on\\neither G or V. But how about a perturbation on the Lie algebra\\ng? We will also address this issue in the following section.\\n(14)ResNet, Lie algebra,curvature and reparameteriza-\\ntion of curves: ResNet as the most successful deep network\\nstructure shows a superior performance than normal CNNs.\\nFrom a geometric point of view, the success of ResNet falls\\nin that ResNet is a network running on the Lie algebra g,\\nwhile normal CNNs run on G. This can be understood by\\ntaking the quantum computation as an analogue of CNNs[10],\\nwhere normal CNNs try to construct an quantum algorithm by\\ncomposing elementary unitary gates and ResNets achieve the\\nsame algorithm by ﬁnding the proper Hamiltonian. It’s well\\nknown that ResNet is essentially a differential equation, which\\nperfectly matches the structure of LDDMM. For ResNets, the\\ncurvature along the network is much smoother than normal\\nCNNs since the network parameters of ResNets are only weak\\nperturbations and therefore can not lead to rough curvature\\nchange along the network. ResNets can also easily achieve\\nreparameterization of the curve gt by just adjusting the ampli-\\ntudes of the weights. In another word, compared with normal\\nCNNs, ResNets run on a much smoother manifold and can\\napproach a smooth geodesic much easier than normal CNNs.\\n(15) Geometric structure of deep networks and Rie-\\n6\\nmannian structure on V : In deep learning ﬁelds, there\\nare also works to explore the geometric structure of deep\\nnetworks[35][36]. These works are closely related with the\\ngeometrization of deep networks. But both of them are work-\\ning on the Riemannian geometry on V as described in [17]\\ninstead of on the Riemannian geometry on G. Since the\\ngeometry on V is induced by a projection from the geometry\\nof G, a complete geometrization of deep networks should be\\naccomplished on G and only the geometry of G can fully\\nexplore the dynamics of deep networks.\\nThis is only a partial list of the correspondence between the\\ngeometry of image registration and deep learning systems. We\\nhope we have convinced readers to believe the geometrization\\nof deep networks is a promising candidate for the interpreta-\\ntion of deep learning systems.\\nB. A concrete example\\nNow we will give a concrete sample on how we can\\nunderstand deep learning using the geometrization framework.\\nIn [37] the problem of how the training data will inﬂuence the\\nprediction of a deep network was addressed. They considered\\na supervised training with n data points zi = xi, yi, i = 1....n\\nand the cost function is L(z, θ) 1\\nnΣn\\ni=1L(zi, θ) with θ as the\\nnetwork parameters. The optimal network conﬁguration is\\ngiven by\\nˆ\\ntheta = arg min mintheta L(z, θ).\\nThe key results of [37] are two items to evaluate how the\\nperturbation on the training data will inﬂuence the parameter\\nˆ\\ntheta and the loss at a test point ztest given by\\nIup,params(z) = −H−1\\nˆθ ∇θL(z, ˆθ)\\n(12)\\nIup,loss(z, ztest) = −∇θL(ztest, ˆθ)H−1\\nˆθ ∇θL(z, ˆθ)\\n(13)\\nwhere Hˆθ = 1\\nn\\nPn\\ni=1 ∇2\\nˆθL(zi, ˆθ) is the Hessian.\\nTo interpret the results using our geometrization framework,\\nwe can regard the supervised network training as an optimiza-\\ntion problem following the formulation of image registration\\nwith a cost function\\nE(ut) =\\nZ 1\\n0\\nl(θ)dt + L(z, θ)\\n(14)\\nwhere l(θ)\\n=\\n⟨θI(θ)θ⟩, I(θ)\\n=\\nPn\\ni=1[∇θL(zi, θ) ⊗\\n∇θL(zi, θ)] is the Fisher-Rao metric used in [26] to describe\\ncomplexity of deep networks. Obviously the Fisher-Rao metric\\nis essentially the same as the Hessian Hˆθ since I(θ) = −Hˆθ\\nfrom information geometry.\\nThis can be understood as either to match n pairs of\\nimages simultaneously using diffeomorphic transformations on\\na higher dimensional space (due to the overparameterization\\nof deep networks) or a landmark based image registration\\ntaking all the training data as paired landmarks on an image.\\nThe goal of the optimization is to ﬁnd a proper transforma-\\ntion that can match the training data and also show good\\ngeneralization performance. The Riemannian metric on the\\nRiemannian manifold to measure the deformation energy of\\nthe transformation or the curve length or equivalently the\\ncomplexity of the deep network is the Fisher-Rao metric of\\nthe deep network. From a physical point of view, it’s obvious\\nto see that why the Fisher-Rao norm is used in [26] to\\nrepresent the generalization capability. This is because a lower\\ncomplexity network means a lower deformation energy and\\ntherefore a smoother image deformation ﬁeld. Of course for a\\nlandmark based image registration, a smooth deformation will\\nhave a better generalization performance.\\nComparing (14) with (2)(3)(4), we can observe that\\n∇θL(z, ˆθ) in (12) is exactly the momentum in g∗of E-P\\nequation and (12) is the correspondent vector in g. 13 is related\\nwith the angle between two vectors in g using the Fisher-Rao\\nmetric. We can easily draw a clear physical or a geometric\\npicture of (12)(13). If the deep network is a geodesic under\\nthe Fisher-Rao metric, then roughly (12) indicates how the\\ndirection of the geodesic will be shifted with a perturbation\\nof a landmark. (13) shows under a perturbation of a training\\nlandmark, how the direction of the trajectory of a test data\\nztest transformed by the perturbed geodesic will be shifted.\\nOf course generally deep networks are not geodesics. Then\\nthe networks in [37] are just normal non-geodesic curves. But\\nstill the above geometric picture holds approximately.\\nThe drawback of [37] is that it only consider a perturbation\\naround the current conﬁguration ˆˆθ so that the Fisher-Rao\\nmetric is ﬁxed by the network conﬁguration. Another more\\ninteresting work [38] tried to explore the complete dynamics\\nof the Fisher-Rao metric by iteratively updating the weighting\\nof training data and the network conﬁguration ˆθ. Similar to\\n[37] this can be formulated as an image registration problem\\ngive by\\nE(ut) =\\nZ 1\\n0\\nl( ˆ\\nθ(ǫ))dt + L(z, θ, )\\n(15)\\nwith ǫi, i = 1, ..., n are the weights of the training data and\\nl( ˆ\\nθ(ǫ)) are deﬁned on the Fisher-Rao metric determined by\\nthe optimal network parameters\\nˆ\\nθ(ǫ) which are ǫ dependent.\\nWhat’s new here? The main difference with the image\\nregistration problem is that the Fisher-Rao metric on G is\\nnow data dependent! In another word, the Riemannian metric\\nis not a ﬁxed background metric as in the image registration\\nproblem, instead the metric is emergent from the deep network\\nitself. Readers with a physics background can immediately\\nsee that we have an analogue of this in physics. The data\\nindependent image registration is the Neutonian mechanics\\nwith a ﬁxed spacetime background and the data dependent\\ndeep network systems correspond to general relativity with a\\ndynamic spacetime. What’s more, the Fisher-Rao metric used\\nhere is in fact closely related with general relativity since\\ngravitation equation can be derived from it[27][28][29]. So in\\n(15) the network structure and data (information) are coupled\\njust as spacetime and matter are coupled in general relativity.\\nFollowing John Wheeler, in our physical world, spacetime tells\\nmatter how to move, matter tells spacetime how to curve. In\\ndeep networks, network tells data (information) how to move,\\ndata (information) tells network how to curve. We believe this\\nis not just an analogue between the physical world and deep\\nnetworks, this should be regarded as a general principle to\\ndesign and understand deep networks. The key component\\nof interpret deep networks is to understand how the network\\n7\\nstructure and data information interact. That’s to say to ﬁnd\\nthe gravitation equation for deep networks. Here we point out,\\nsince the Fisher-Rao metric is data dependent, the optimal\\nsolution can not be written as the E-P equation with a ﬁxed\\nRiemannian metric any more since the metric is also dynamic.\\nIn [38], the solution is approximated by a two-level gradient\\ndescent algorithm which updates the network parameter ˆθ and\\nsample weights ǫ iteratively. The ﬁnal solution is a critical\\npoint that ˆθ is stable with respect to the perturbation of ǫ. At\\nthe critical point, the solution still satisﬁes the E-P equation\\nwith a Fisher-Rao metric determined by the network parameter\\nˆθ.\\nAs a conclusion of this section, the geometrization frame-\\nwork tells us that (1)The Fisher-Rao based network complexity\\nmeasure is an effective signature for the generalization prop-\\nerty for deep networks since it’s a measure for the network\\ncomplexity; (2)The network structure and data information\\nare coupled just as matter and spacetime are coupled in\\ngeneral relativity and the ultimate law of deep networks is\\na gravitational equation of deep networks; (3)The optimal\\nsolution is a result of the competition between the two terms,\\nthe network complexity and training error, in (15).\\nIV. DISCUSSION\\nTill now we have seen the validness of the geometrization\\nframework on the interpretability of deep learning systems by\\nshowing that deep networks can correspond to geometrical\\nmechanics and general relativity. The basic idea of geometriza-\\ntion is that deep networks have correspondence in the physical\\nworld. Therefore we can regard deep networks as physical\\nsystems and ask the following questions:\\n(1) Is there a GUT (grand uniﬁed theory) of deep networks?\\nIf there is a correspondence between deep networks and\\nour physical world, then the ultimate interpretability of deep\\nnetworks lies in ﬁnding the GUT of deep networks just as the\\ninterpretability of the physical world lies in the GUT of the\\nphysical world. It’s a common sense that the physical GUT is\\ndeﬁnitely a geometrical theory. So we believe geometrization\\nshould be the right roadmap for the interpretability problem\\nof deep learning systems. Also the GUT of deep networks\\nshould have the same structure as the GUT of our physical\\nworld. Therefore even the GUT of our physical world is\\nnot available yet, exploring the similarity between physical\\nsystems and deep networks can provide guidelines for us to\\nbetter understand deep networks.\\n(2) Real physical systems obey a least action principle,\\nis this also true for deep networks? We have seen that the\\ngeometry of image registration results in an optimal solution\\ngiven by the E-P equation. But for deep networks, generally we\\nare working with systems far from optimal. But still usually\\nthese non-optimal systems work well in practice. There are\\nalso works taking deep networks as general dynamic systems\\nsuch as in neural ODE[13]. Shall we investigate non-optimal\\ndeep networks as general information processing systems or\\nshould we stick to optimal deep networks since they are more\\nphysical? Our geometrization framework will deﬁnitely work\\nbetter on the optimal deep networks. But non-optimal systems\\nmight not be properly geometrized. So maybe we should\\nﬁrst focus on understanding the optimal systems with clear\\ngeometric pictures.\\n(3) What can we learn from the geometrization of physics?\\nTill now we are only working with Riemannian structures as in\\nthe geometry of image registration. In fact the geometrization\\nof physics is far beyond Riemannian structures. A natural\\nextension is the ﬁbre bundle structure which plays a key\\nrole in gauge theory. Can we describe deep networks using\\nﬁbre bundles? Good candidates in deep networks that may be\\ndescribed by ﬁbre bundles are transfer learning, meta learning,\\nneural Turing machines (NTM) and differentiable neural com-\\nputers(DNC). They all aim to ﬁnd some kind of reconﬁgurable\\nsystems. In the language of Riemannian geometry, this usually\\nmeans to reconﬁgure the metric of the system so that the\\noptimal geodesic curves can be reconﬁgurable. A natural way\\nto achieve this is to reconﬁgure the connection form on ﬁbre\\nbundles. Roughly transfer learning can be understood as to\\ntransfer (part of) a geodesic to another task. Meta learning\\naims to ﬁnd some universal descriptions of different but similar\\ngeodesics. NTM and DNC mainly achieve the reﬁguration of\\nsystems by carefully changing their memories. We can see the\\nmemory can be understood as the ﬁbre bundle above the base\\nspace of the LSTM states. It’s interesting to check if NTM\\nand DNC can be written as deﬁning a connection on their\\nﬁbre bundles.\\nAnother possibility is that ﬁbre bundles can be used to\\ndescribe the coupling of multiple deep networks. It’s get-\\nting more and more obvious that complex tasks can only\\nbe achieved by coupling multiple deep networks just as in\\nour human brains. In AI systems, typical coupled composite\\nsystems are GANs and attention. The coupling of systems\\nleads to interactions between subsystems, just as interactions\\n(forces) between physical systems. In physics, interactions are\\ndescribed by ﬁbre bundles. Accordingly interactions between\\ncoupled deep networks should also be described by ﬁbre\\nbundles.\\nThe last but not the least, the coupling of multiple deep\\nnetworks might be related with the existence of consciousness.\\nWe hypothesize that when multiple neural networks in our\\nbrains are coupled, the coupling may be achieved by an inde-\\npendent coupling system, which not only couples the multiple\\nneural subsystems but also has its own latent state space and\\na stable dynamics. This independent coupling system may be\\nthe origin of our consciousness. If this is the case, then can\\nour consciousness also be geometrized?\\n(4) How to geometrize reinforcement (RF) learning sys-\\ntems? Geometrically RF is essentially to learn the metric\\nof G from the interaction with the system and then ﬁnd\\ngeodesics using the learned metric. Imitation learning can\\nbe understood to design a metric so that the expert’s action\\nbecomes a geodesic. Can we formulate a geometrization of\\nthese procedures?\\n(5) What’s the curvature of the emergent Fisher-Rao metric\\nin deep networks? If deep networks can be formulated as a\\ndynamic system using emergent Fisher-Rao metric, we need\\nto check what’s the curvature of this metric. Because the\\ncurvature will determine the stability of the geodesic. And the\\n8\\nmetric is dependent on both the structure and the parameters of\\nthe network. Just as in general relativity, the solution spacetime\\ncan have either positive or negative curvature, deep networks\\nmay have the same problem. Taking CNN as an example,\\nin quantum information ﬁeld the correspondent system is\\nMERA or the entanglement renormalization algorithm which\\nshow a similar structure as CNN. We know that MERA\\nbuilds a negative curvature geometry and is related with the\\nfamous AdS/CFT duality. Similarly the geometry of quantum\\ncomputation, which is another analogues of image registration\\nand CNNs also has an almost negative curvature[19], where\\nthe Riemannian metric used here is static just as in image\\nregistration. It’s reasonable to guess that CNN may also have a\\nsimilar negative curvature. This might be an explaination of the\\nexistence of adversarial examples in CNN based classiﬁcation\\nnetworks.\\n(6) How to understand the overparameterization of deep\\nnetworks? Overparameterization plays a key role in nowadays\\ndeep networks. It’s closely related with the training conver-\\ngence, generalization and adversarial attacks. Geometrically\\nthis means to choose a higher dimensional group G to accom-\\nplish the transformation. In physics we also meet overparam-\\neterization problems. For example, in quantum computation\\noverparameterization means to achieve a quantum algorithm\\nusing auxiliary qubits[19]. In tensor network representation of\\nquantum states, overparameterization is closely related with\\nthe concepts of parent and uncle Hamiltonians[39]. Overpa-\\nrameterization not only brings a higher dimensional G but\\nalso a potentially more ﬂexible network structure. As we\\nsee above, the structure of deep networks will inﬂuence the\\ncurvature of the geometry built by the Fisher-Rao metric of\\nnetworks. A complete understanding of the consequence of\\noverparameterization is still needed.\\nV. CONCLUSIONS\\nIn this work, inspired by the geometrization of physics, we\\nproposed a geometrization framework for the interpretabil-\\nity of deep learning systems. By comparing the geometry\\nof image registration with deep networks, we showed that\\ngeometrization does bring us new pictures of deep networks.\\nUnder this framework, we also discussed some key problems\\nfor the understanding of deep learning systems. Our future\\nwork will be then to answer these questions.\\nAs a ﬁnal remark, besides the geometrization of physics to\\nconnect physics and geometry, currently there is a trend to\\nunderstand physical laws from the computation point of view\\nso that computational complexity starts to play a key role in\\nphysics. If we further bring deep networks into this game, we\\nhope the interactions among physics, geometry, computation\\nand deep networks may completely change our understanding\\nof the world. A possible picture of our world may be: The\\nworld is an information processing (computation) system\\nthat generates our universe by a deep network of basic\\ncomputational operators. The structure of the deep network\\nis determined by the information structure of our universe.\\nThat’s to say the deep network is the optimal network to gen-\\nerate the information pattern of our universe, i.e. a geodesic\\naccording to a certain Riemannian metric to measure the\\ncomputational complexity. Physical laws are encoded in\\nthe correspondence between the geometric structure of the\\nnetwork and the information pattern of our universe. So still\\nour world obeys a least action principle with the action is\\ngiven by the computational complexity of the physical world.\\nREFERENCES\\n[1] X. Gao and L. M. Duan. Efﬁcient representation of quantum many-body\\nstates with deep neural networks. Nature Communications, 8(1):662,\\n2017.\\n[2] Glen Evenbly.\\nAlgorithms\\nfor tensor network\\nrenormalization.\\nPhys.rev.b, 95(4), 2017.\\n[3] Cdric\\nBny.\\nDeep\\nlearning\\nand\\nthe\\nrenormalization\\ngroup.\\narxiv:1301.3124, 2013.\\n[4] G. Evenbly and G. Vidal. Tensor network states and geometry. Journal\\nof Statistical Physics, 145(4):891–918, 2011.\\n[5] Brian Swingle. Entanglement renormalization and holography. Physical\\nReview D Particles and Fields, 86(6):–, 2009.\\n[6] Patrick Hayden, Sepehr Nezami, Xiao Liang Qi, Nathaniel Thomas,\\nMichael Walter, and Zhao Yang.\\nHolographic duality from random\\ntensor networks. Journal of High Energy Physics, 2016(11):9, 2016.\\n[7] Brian Swingle. Constructing holographic spacetimes using entanglement\\nrenormalization. Physics, 2012.\\n[8] Xiao Liang Qi. Exact holographic mapping and emergent space-time\\ngeometry. Physics, arXiv:1309.6282v1, 2013.\\n[9] Wen Cong Gan and Fu Wen Shu.\\nHolography as deep learning.\\nInternational Journal of Modern Physics D, 26:1743020, 2017.\\n[10] J.S. Wu X. Dong and L. Zhou. How deep learning works –the geometry\\nof deep learning. arXiv:1710.10784, 2017.\\n[11] M. Gu M. A. Nielsen, M. R. Dowling and A. C. Doherty. Quantum\\ncomputation as geometry. Science 311,1133, 2006.\\n[12] H.\\nHeydari.\\nGeometric\\nformulation\\nof\\nquantum\\nmechanics.\\narXiv:1503.00238, 2015.\\n[13] Qi Chen Tian, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.\\nNeural ordinary differential equations. arxiv:1806.07366, 2018.\\n[14] E Weinan, Jiequn Han, and Qianxiao Li. A mean-ﬁeld optimal control\\nformulation of deep learning. arxiv:1807.01083v1, 2018.\\n[15] Laurent Younes. Shapes and Diffeomorphisms. 2010.\\n[16] M. Bruveris, F. Gay-Balmaz, D. D. Holm, and T. S. Ratiu.\\nThe\\nmomentum map representation of images. Journal of Nonlinear Science,\\n21(1):115–150, 2011.\\n[17] Martins Bruveris and Darryl D. Holm. Geometry of image registration:\\nThe diffeomorphism group and momentum maps.\\nFields Institute\\nCommunications, 73:19–56, 2013.\\n[18] Mirza Faisal Beg, Michael I. Miller, Alain Trouve, and Laurent Younes.\\nComputing large deformation metric mappings via geodesic ﬂows. 2004.\\n[19] M. R. Dowling and M. A. Nielsen.\\nThe geometry of quantum\\ncomputation. Quantum Information and Computation, 8(10):861–899,\\n2008.\\n[20] G. L. Hart, C. Zach, and M. Niethammer. An optimal control approach\\nfor deformable registration. In IEEE Computer Society Conference on\\nComputer Vision and Pattern Recognition Workshops, 2013.\\n[21] Vialard, FranoisXavier, Risser, Laurent, Rueckert, Daniel, Cotter, and\\nJ Colin.\\nDiffeomorphic 3d image registration via geodesic shooting\\nusing an efﬁcient adjoint calculation. International Journal of Computer\\nVision, 97(2):229–241, 2012.\\n[22] Darryl D. Holm, Alain Trouve, and Laurent Younes. The euler-poincare\\ntheory of metamorphosis. Quarterly of Applied Mathematics, 67(4):661–\\n685, 2008.\\n[23] Darryl D. Holm, Tanya Schmah, and Cristina Stoica.\\nGeometric\\nmechanics and symmetry. Oxford University Press Oxford, (2):xvi+515,\\n2009.\\n[24] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-\\nGonzalez, Vin´ıcius Flores Zambaldi, Mateusz Malinowski, Andrea Tac-\\nchetti, David Raposo, Adam Santoro, Ryan Faulkner, C¸ aglar G¨ulc¸ehre,\\nFrancis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish\\nVaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer,\\nNicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick,\\nOriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,\\ndeep learning, and graph networks. arxiv:1806.01261, 2018.\\n[25] Benjamin Scellier and Yoshua Bengio.\\nEquilibrium propagation:\\nBridging the gap between energy-based models and backpropagation.\\nFrontiers in Computational Neuroscience, 11:24–, 2017.\\n9\\n[26] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James\\nStokes. Fisher-rao metric, geometry, and complexity of neural networks.\\narxiv:1711.01530, 2017.\\n[27] Hiroaki Matsueda. Emergent general relativity from ﬁsher information\\nmetric. arXiv:1310.1831v2, 2013.\\n[28] Hiroaki Matsueda.\\nDerivation of gravitational ﬁeld equation from\\nentanglement entropy. arXiv:1408.5589v2, 70, 2014.\\n[29] Hiroaki Matsueda. Geodesic distance in ﬁsher information space and\\nholographic entropy formula. arXiv:1408.6633v1, 2014.\\n[30] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander\\nMadry. How does batch normalization help optimization? In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-\\nnett, editors, Advances in Neural Information Processing Systems 31,\\npages 2488–2498. Curran Associates, Inc., 2018.\\n[31] Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoen-\\nholz, and Jeffrey Pennington.\\nDynamical isometry and a mean ﬁeld\\ntheory of cnns: How to train 10,000-layer vanilla convolutional neural\\nnetworks. arxiv:1806.05393, 2018.\\n[32] Stanley Durrleman, Xavier Pennec, Alain Trouv, and Nicholas Ayache.\\nStatistical models of sets of curves and surfaces based on currents.\\nMedical Image Analysis, 13(5):793–808, 2009.\\n[33] Zhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, and Ping Wang.\\nAdversarial noise layer: Regularize neural network by adding noise.\\n2018.\\n[34] Alain Trouv and Franoisxavier Vialard. Shape splines and stochastic\\nshape evolutions: A second order point of view. Quarterly of Applied\\nMathematics, 70(2):219–251, 2012.\\n[35] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein,\\nand Surya Ganguli. Exponential expressivity in deep neural networks\\nthrough transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg,\\nI. Guyon, and R. Garnett, editors, Advances in Neural Information\\nProcessing Systems 29, pages 3360–3368. 2016.\\n[36] Michael Hauser and Asok Ray. Principles of riemannian geometry in\\nneural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 2807–2816. 2017.\\n[37] Pang Wei Koh and Percy Liang. Understanding black-box predictions\\nvia inﬂuence functions. In Doina Precup and Yee Whye Teh, editors,\\nProceedings of the 34th International Conference on Machine Learning,\\nvolume 70 of Proceedings of Machine Learning Research, pages 1885–\\n1894, International Convention Centre, Sydney, Australia, 06–11 Aug\\n2017. PMLR.\\n[38] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning\\nto reweight examples for robust deep learning. In Jennifer Dy and An-\\ndreas Krause, editors, Proceedings of the 35th International Conference\\non Machine Learning, volume 80 of Proceedings of Machine Learning\\nResearch, pages 4334–4343, Stockholmsmssan, Stockholm Sweden, 10–\\n15 Jul 2018. PMLR.\\n[39] C. Fernndez-Gonzlez, N. Schuch, M. M. Wolf, J. I. Cirac, and D. Prez-\\nGarca. Frustration free gapless hamiltonians for matrix product states.\\nCommunications in Mathematical Physics, 333(1):299–333, 2015.\\n',\n",
       " '1901.04195v1.pdf': 'Integrating Learning and Reasoning with\\nDeep Logic Models\\nGiuseppe Marra1,2, Francesco Giannini2\\nMichelangelo Diligenti2 and Marco Gori2\\n1Department of Information Engineering\\nUniversity of Florence, ITALY\\n2Department of Information Engineering and Mathematics\\nUniversity of Siena, ITALY\\ne-mail: g.marra@uniﬁ.it, {fgiannini,diligmic,marco}@diism.unisi.it\\nDecember 25, 2021\\nAbstract\\nDeep learning is very eﬀective at jointly learning feature representa-\\ntions and classiﬁcation models, especially when dealing with high dimen-\\nsional input patterns. Probabilistic logic reasoning, on the other hand,\\nis capable to take consistent and robust decisions in complex environ-\\nments. The integration of deep learning and logic reasoning is still an\\nopen-research problem and it is considered to be the key for the develop-\\nment of real intelligent agents. This paper presents Deep Logic Models,\\nwhich are deep graphical models integrating deep learning and logic rea-\\nsoning both for learning and inference. Deep Logic Models create an end-\\nto-end diﬀerentiable architecture, where deep learners are embedded into\\na network implementing a continuous relaxation of the logic knowledge.\\nThe learning process allows to jointly learn the weights of the deep learn-\\ners and the meta-parameters controlling the high-level reasoning.\\nThe\\nexperimental results show that the proposed methodology overtakes the\\nlimitations of the other approaches that have been proposed to bridge\\ndeep learning and reasoning.\\n1\\nIntroduction\\nArtiﬁcial Intelligence (AI) approaches can be generally divided into symbolic\\nand sub-symbolic approaches.\\nSub-symbolic approaches like artiﬁcial neural\\nnetworks have attracted most attention of the AI community in the last few\\nyears. Indeed, sub-symbolic approaches have got a large competitive advantage\\nfrom the availability of a large amount of labeled data in some applications.\\nIn these contexts, sub-symbolic approaches and, in particular, deep learning\\nones are eﬀective in processing low-level perception inputs [3, 18]. For instance,\\n1\\narXiv:1901.04195v1  [cs.LG]  14 Jan 2019\\ndeep learning architectures have been achieved state-of-the-art results in a wide\\nrange of tasks, e.g. speech recognition, computer vision, natural language pro-\\ncessing, where deep learning can eﬀectively develop feature representations and\\nclassiﬁcation models at the same time.\\nOn the other hand, symbolic reasoning [7, 16, 23], which is typically based\\non logical and probabilistic inference, allows to perform high-level reasoning\\n(possibly under uncertainty) without having to deal with thousands of learn-\\ning hyper-parameters. Even if recent work has tried to gain insight on how a\\ndeep model works [21], sub-symbolic approaches are still mostly seen as black-\\nboxes, whereas symbolic approaches are generally more easier to interpret, as\\nthe symbol manipulation or chain of reasoning can be unfolded to provide an\\nunderstandable explanation to a human operator.\\nIn spite of the incredible success of deep learning, many researchers have\\nrecently started to question the ability of deep learning to bring us real AI,\\nbecause the amount and quality of training data would explode in order to\\njointly learn the high-level reasoning that is needed to perform complex tasks [2].\\nFor example, forcing some structure to the output of a deep learner has been\\nshown to bring beneﬁts in image segmentation tasks, even when simple output\\ncorrelations were added to the enforced contextual information [6].\\nBlending symbolic and sub-symbolic approaches is one of the most challeng-\\ning open problem in AI and, recently, a lot of works, often referred as neuro-\\nsymbolic approaches [10], have been proposed by several authors [6, 14, 22, 27].\\nIn this paper, we present Deep Logic Models (DLMs), a uniﬁed framework to\\nintegrate logical reasoning and deep learning. DLMs bridge an input layer pro-\\ncessing the sensorial input patterns, like images, video, text, from a higher level\\nwhich enforces some structure to the model output. Unlike in Semantic-based\\nRegularization [8] or Logic Tensor Networks [9], the sensorial and reasoning lay-\\ners can be jointly trained, so that the high-level weights imposing the output\\nstructure are jointly learned together with the neural network weights, process-\\ning the low-level input. The bonding is very general as any (set of) deep learners\\ncan be integrated and any output structure can be expressed. This paper will\\nmainly focus on expressing the high-level structure using logic formalism like\\nﬁrst–order logic (FOL). In particular, a consistent and fully diﬀerentiable relax-\\nation of FOL is used to map the knowledge into a set of potentials that can be\\nused in training and inference.\\nThe outline of the paper is the following.\\nSection 2 presents the model\\nand the integration of logic and learning.\\nSection 3 compares and connects\\nthe presented work with previous work in the literature and Section 4 shows\\nthe experimental evaluation of the proposed ideas on various datasets. Finally,\\nSection 5 draws some conclusions and highlights some planned future work.\\n2\\nModel\\nWe indicate as θ the model parameters, and X the collection of input sensorial\\ndata. Deep Logic Models (DLMs) assume that the prediction of the system\\n2\\nx\\nf\\ny\\nw\\nλ\\nFigure 1: The DLM graphical model assumes that the output variables y depend\\non the output of ﬁrst stage f, processing the input X. This corresponds to the\\nbreakdown into a lower sensorial layer and a high level semantic one.\\nis constrained by the available prior knowledge.\\nTherefore, unlike standard\\nNeural networks which compute the output via a simple forward pass, the out-\\nput computation in DLM can be decomposed into two stages: a low-level stage\\nprocessing the input patterns, and a subsequent semantic stage, expressing con-\\nstraints over the output and performing higher level reasoning. We indicate by\\ny = {y1, . . . , yn} and by f = {f1, . . . , fn} the two multivariate random vari-\\nables corresponding to the output of the model and to the output of the ﬁrst\\nstage respectively, where n > 0 denotes the dimension of the model outcomes.\\nAssuming that the input data is processed using neural networks, the model\\nparameters can be split into two independent components θ = {w, λ}, where\\nw is the vector of weights of the networks fnn and λ is the vector of weights of\\nthe second stage, controlling the semantic layer and the constraint enforcement.\\nFigure 1 shows the graphical dependencies among the stochastic variables that\\nare involved in our model. The ﬁrst layer processes the inputs returning the\\nvalues f using a model with parameters w. The higher layer takes as input f\\nand applies reasoning using a set of constraints, whose parameters are indicated\\nas λ, then it returns the set of output variables y.\\nThe Bayes rule allows to link the probability of the parameters to the pos-\\nterior and prior distributions:\\np(θ|y, X) ∝p(y|θ, X)p(θ) .\\nAssuming the breakdown into a sensorial and a semantic level, the prior may\\nbe decomposed as p(θ) = p(λ)p(w), while the posterior can be computed by\\nmarginalizing over the assignments for f:\\np(y|θ, X) =\\nZ\\nf\\np(y|f, λ) · p(f|w, X)df .\\n(1)\\nA typical choice is to link p(f|w, X) to the outputs of the neural architectures:\\np(f|w, X) =\\n1\\nZ(f) exp\\n\\x12\\n−(f −f nn)2\\n2σ2\\n\\x13\\n,\\n3\\nwhere the actual (deterministic) output of the networks fnn over the inputs is\\nindicated as f nn. Please note that there is a one-to-one correspondence among\\neach element of y, f and f nn, such that |y| = |f| = |f nn|.\\nHowever, the integral in Equation (1) is too expensive to compute and, as\\ncommonly done in the deep learning community, only the actual output of the\\nnetwork is considered, namely:\\np(f|w, X) ≈δ(f −f nn) ,\\nresulting in the following approximation of the posterior:\\np(y|θ, X) ≈p(y|f nn, λ) .\\nA Deep Logic Model assumes that p(y|f nn, λ) is modeled via an undirected\\nprobabilistic graphical model in the exponential family, such that:\\np(y|f nn, λ) ≜\\n1\\nZ(y) exp\\n \\nΦr(y, f nn) +\\nX\\nc\\nλcΦc(y)\\n!\\n,\\n(2)\\nwhere the Φc are potential functions expressing some constraints on the output\\nvariables, λ = {λ1, λ2, . . . , λC} are parameters controlling the conﬁdence for the\\nsingle constraints where a higher value corresponds to a stronger enforcement\\nof the corresponding constraint, Φr is a potential favoring solutions where the\\noutput closely follows the predictions provided by the neural networks (for in-\\nstance Φr(y, f nn) = −1\\n2||y −f nn||2) and Z(y) is a normalization factor (i.e.\\nthe partition function):\\nZ(y) =\\nZ\\ny\\nexp\\n \\nΦr(y, f nn) +\\nX\\nc\\nλcΦc(y)\\n!\\ndy.\\n2.1\\nMAP Inference\\nMAP inference assumes that the model parameters are known and it aims at\\nﬁnding the assignment maximizing p(y|f nn, λ). MAP inference does not require\\nto compute the partition function Z which acts as a constant when the weights\\nare ﬁxed. Therefore:\\nyM = argmax\\ny\\nlog p(y|f nn, λ) = argmax\\ny\\n\"\\nΦr(y, f nn) +\\nX\\nc\\nλcΦc(y)\\n#\\n.\\nThe above maximization problem can be optimized via gradient descent by\\ncomputing:\\n∇y log p(y|f nn, λ) = ∇yΦr(y, f nn) +\\nX\\nc\\nλc∇yΦc(y)\\n4\\n2.2\\nLearning\\nTraining can be carried out by maximizing the likelihood of the training data:\\nargmax\\nθ\\nlog p(θ|yt, X) = log p(yt|θ, X) + log p(w) + log p(λ) .\\nIn particular, assuming that p(yt|θ, X) follows the model deﬁned in equation\\n(2) and the parameter priors follow Gaussian distributions, we get:\\nlog p(θ|yt, X)=−α\\n2 ||w||2 −β\\n2 ||λ||2 −Φr(yt, f nn) +\\nX\\nc\\nλcΦc(yt) −log Z(y)\\nwhere α, β are meta-parameters determined by the variance of the selected Gaus-\\nsian distributions. Also in this case the likelihood may be maximized by gradient\\ndescent using the following derivatives with respect to the model parameters:\\n∂log p(θ|yt,X)\\n∂λc\\n=\\n−βλc + Φc(yt) −Ep [Φc]\\n∂log p(θ|yt,X)\\n∂wi\\n=\\n−αwi + ∂Φr(yt,f nn)\\n∂wi\\n−Ep\\nh\\n∂Φr\\n∂wi\\ni\\nUnfortunately, the direct computation of the expected values in the above\\nderivatives is not feasible. A possible approximation [12, 13] relies on replacing\\nthe expected values with the corresponding value at the MAP solution, assum-\\ning that most of the probability mass of the distribution is centered around it.\\nThis can be done directly on the above expressions for the derivatives or in the\\nlog likelihood:\\nlog p(yt|f nn, X) ≈Φr(yt, f nn) −Φr(yM, f nn) +\\nX\\nc\\nλc (Φc(yt) −Φc(yM))\\nFrom the above approximation, it emerges that the likelihood tends to be\\nmaximized when the MAP solution is close to the training data, namely if\\nΦr(yt, f nn) ≃Φr(yM, f nn) and Φc(yt) ≃Φc(yM) ∀c. Furthermore, the proba-\\nbility distribution is more centered around the MAP solution when Φr(yM, f nn)\\nis close to its maximum value. We assume that Φr is negative and have zero as\\nupper bound: Φr(y, f nn) ≤0 ∀y, f nn, like it holds for example for the already\\nmentioned negative quadratic potential Φr(y, f nn) = −1\\n2||y−f nn||2. Therefore,\\nthe constraint Φr(yt, f nn) ≃Φr(yM, f nn) is transformed into the two separate\\nconstraints Φr(yt, f nn) ≃0 and Φr(yM, f nn) ≃0.\\nThis means that, given the current MAP solution, it is possible to increase\\nthe log likelihood by computing the gradient and weight updates using the\\nfollowing cost function:\\nlog p(w) + log p(λ) + Φr(yt, f nn) + Φr(yM, f nn) +\\nX\\nc\\nλc [Φc(yt) −Φc(yM)]\\nIn this paper, a quadratic form for the priors and the potentials is selected, but\\nother choices are possible. For example, Φr(·) could instead be implemented as\\n5\\nData: Input data X, output targets yt, function models with weights w\\nResult: Trained model parameters θ = {λ, w}\\nInitialize i = 0, λ = 0, random w;\\nwhile not converged ∧i < max_iterations do\\nCompute function outputs f nn on X using current function weights\\nw;\\nCompute MAP solution yM = argmaxy log p(y|f nn, λ);\\nCompute gradient ∇θCθ(yt, yM, X);\\nUpdate θ via gradient descent: θi+1 = θi −λlr · ∇θCθ(yt, yM, X);\\nSet i=i+1;\\nend\\nAlgorithm 1: Iterative algorithm to train the function weights w and the\\nconstraint weights λ.\\na negative cross entropy loss. Therefore, replacing the selected forms for the po-\\ntentials and changing the sign to transform a maximization into a minimization\\nproblem, yields the following cost function, given the current MAP solution:\\nCθ(yt, yM, X)\\n=\\nα\\n2 ||w||2 + β\\n2 ||λ||2 + 1\\n2||yt −f nn||2 + 1\\n2||yM −f nn||2 +\\n+\\nX\\nc\\nλc [Φc(yt) −Φc(yM)] .\\n(3)\\nMinimizing the cost function Cθ(yt, yM, X) is just a local approximation of\\nthe full likelihood maximization for the current MAP solution. Therefore, the\\ntraining process alternates the computation of the MAP solution, the computa-\\ntion of the gradient for Cθ(yt, yM, X) and one weight update step. Algorithm 1\\nsummarizes this iterative training algorithm.\\nPlease note that, for any con-\\nstraint c, the parameter λc admits also a negative value. This is in case the c-th\\nconstraint turns out to be too satisﬁed by the actual MAP solution with respect\\nto the satisfaction degree on the training data.\\n2.3\\nMapping Constraints into a Continuous Logic\\nThe DLM model is absolutely general in terms of the constraints that can be\\nexpressed on the outputs. However, this paper mainly focuses on constraints\\nexpressed in the output space y by means of ﬁrst–order logic formulas. There-\\nfore, this section focuses on deﬁning a methodology to integrate prior knowledge\\nexpressed via FOL into a continuous optimization process.\\nIn this framework we only deal with closed FOL formulas, namely formu-\\nlas where any variable occurring in predicates is quantiﬁed. In the following,\\ngiven an m-ary predicate p and a tuple (a1, . . . , am) ∈Dom(p), we say that\\np(a1, . . . , am) ∈[0, 1] is a grounding of p. Given a grounding of the variables\\noccurring in a FOL formula (namely a grounding for all the predicates involved\\nin the formula), the truth degree of the formula for that grounding is computed\\nusing the t-norm fuzzy logic theory as proposed in [24]. The overall degree of\\n6\\noperation\\nt-norm\\nProduct\\nMinimum\\nŁukasiewicz\\na ∧b\\na · b\\nmin(a, b)\\nmax(0, a + b −1)\\na ∨b\\na + b −a · b\\nmax(a, b)\\nmin(1, a + b)\\n¬a\\n1 −a\\n1 −a\\n1 −a\\na ⇒b\\nmin(1, b\\na)\\na ≤b?1 : b\\nmin(1, 1 −a + b)\\nTable 1: The Operations performed by the single units of an expression tree\\ndepending on the inputs a, b and the used t-norm.\\nsatisfaction of a FOL formula is obtained by grounding all the variables in such\\nformula and aggregating the values with diﬀerent operators depending on the\\noccurring quantiﬁers. The details of this process are explained in the following\\nof the section.\\nGrounded Expressions.\\nAny fully grounded FOL rule corresponds to an\\nexpression in propositional logic and we start showing how a propositional logic\\nexpression may be converted into a diﬀerentiable form. In particular, one expres-\\nsion tree is built for each considered grounded FOL rule, where any occurrence\\nof the basic logic connectives (¬, ∧, ∨, ⇒) is replaced by a unit computing its cor-\\nresponding fuzzy logic operation according to a certain logic semantics. In this\\nregard, some recent work shows how to get convex (or even linear) functional\\nconstraints exploiting the convex Łukasiewicz fragment [11].\\nThe expression\\ntree can take as input the output values of the grounded predicates and then re-\\ncursively compute the output values of all the nodes in the expression tree. The\\nvalue obtained on the root node is the result of the evaluation of the expression\\ngiven the input grounded predicates.\\nTable 1 shows the algebraic operations corresponding to the logic operators\\nfor diﬀerent selections of the t-norms.\\nPlease note that the logic operators\\nare always monotonic with respect of any single variable, but they are not\\nalways diﬀerentiable (nor even continuous). However, the sub-space where the\\noperators are non-diﬀerentiable has null-Lebesgue measure, therefore they do\\nnot pose any practical issue, when used as part of a gradient descent optimization\\nschema as detailed in the following.\\nWe assume that the input data X can be divided into a set of sub-domains\\nX = {X1, X2, . . .}, such that each variable vi of a FOL formula ranges over the\\ndata of one input domain, namely vi ∈Xdi, where di is the index of the domain\\nfor the variable vi.\\nFor example, let us consider the rule ∀v1∀v2 ¬A(v1, v2) ∧B(v1). For any as-\\nsignment to v1 and v2, the expression tree returns the output value [1−A(v1, v2)]·\\nB(v1), assuming to exploit the product t-norm to convert the connectives.\\nQuantiﬁers.\\nThe truth degree of a formula containing an expression with a\\nuniversally quantiﬁed variable vi is computed as the average of the t-norm truth\\ndegree of the expression, when grounding vi over its domain. The truth degree\\n7\\nof the existential quantiﬁer is the maximum of the t-norm expression grounded\\nover the domain of the quantiﬁed variable. When multiple quantiﬁed variables\\nare present, the conversion is performed from the outer to the inner variable.\\nWhen only universal quantiﬁers are present the aggregation is equivalent to the\\noverall average over each grounding.\\nIn the previous example, this yields the following expression:\\nΦ(X, A, B) =\\n1\\n|Xd1| · |Xd2|\\nX\\nv1∈Xd1\\nX\\nv2∈Xd2\\n[1 −A(v1, v2)] · B(v1) .\\n(4)\\n2.4\\nPotentials expressing the logic knowledge\\nIt is now possible to explain how to build the potentials from the prior knowl-\\nedge. In any learning task, each unknown grounded predicate corresponds to\\none variable in the vector y. In the above example, the number of groundings is\\n|Xd1|×|Xd2| (i.e. the size of the cartesian product of the domains of A) and |Xd1|\\n(i.e. the size of the domain of B). Therefore, assuming that both predicates A, B\\nare unknown, |y| = |f| = |Xd1|×|Xd2|+|Xd1|. The vector f nn is built similarly\\nby replacing a generic predicate with its neural implementation and then emplac-\\ning the function values for the groundings in the vector. Again for the considered\\nexample: f nn = {fA(v11, v21), . . . , fA(v1|Xd1|, v2|Xd2|), fB(v11), . . . , fB(v1|d2|)},\\nwhere vij is the j-th grounding for the i-th variable and fA, fB are the learned\\nneural approximations of A and B, respectively. Finally, the diﬀerentiable po-\\ntential for the example formula is obtained by replacing in Equation (4) each\\ngrounded predicate with the corresponding stochastic variable in y.\\nFigure 2 shows the undirected graphical model corresponding to the DLM\\nfor the running example rule used in this section, assuming that v1 can assume\\nvalues over the constants {Mary, John} and v2 over {Munich, London}. Each\\nstochastic node yi approximates one grounded predicate, while the fi nodes are\\nthe actual output of a neural network getting as input the pattern represen-\\ntations of the corresponding grounding. The vertical connections between two\\nyi and fi nodes correspond to the cliques over the groundings for which the\\nΦr potential can be decomposed. The links between the yi nodes corresponds\\nto the cliques over the groundings of the rule for which the corresponding Φc\\npotential can be decomposed. The structure of these latter cliques follows a\\ntemplate determined by the rule, that is repeated for the single groundings.\\nThe graphical model is similar to the ones built by Probabilistic Soft Logic [1]\\nor Markov Logic Networks [26], but enriched with the nodes corresponding to\\nthe output of the neural networks.\\n3\\nRelated Works\\nDLMs have also their roots in Probabilistic Soft Logic (PSL) [1], a probabilistic\\nlogic using an undirected graphical model to represent a grounded FOL knowl-\\nedge base, and employing a similar diﬀerentiable and convex approximation of\\n8\\ny1 ≈A(Mary, Munich)\\nf1 = fA(xMary, xMunich)\\nw\\ny2 ≈A(Mary, London)\\nf2 = fA(xMary, xLondon)\\nw\\ny3 ≈A(John, Munich)\\nf3 = fA(xJohn, xMunich)\\nw\\ny4 ≈A(John, London)\\nf4 = fA(xJohn, xLondon)\\nw\\ny5 ≈B(Mary)\\nf5 = fB(xMary)\\nw\\ny6 ≈B(John)\\nf6 = fB(xJohn)\\nw\\nFigure 2:\\nThe undirected graphical model built by a DLM for the rule\\n∀v1∀v2 ¬A(v1, v2) ∧B(v1) where v1 can assume values over the constants\\n{Mary, John} and v2 over {Munich, London}. Each stochastic node yi ap-\\nproximates one grounded predicate, while the fi nodes are the actual output of\\na network getting the pattern representations of a grounding.\\nFOL. PSL, similar to a DLM, allows to learn the weight of each formula in the\\nKB by maximizing the log likelihood of the training data. However in PSL, rule\\nweights are restricted to only positive values denoting how far the rule is from\\nbeing satisﬁed. On the other hand, in DLMs the rule weights denote the needed\\nconstraint reactions to match the degree satisfaction of the training data. In\\naddition, unlike DLMs, PSL focuses on logic reasoning without any integration\\nwith deep learners, beside a simple stacking with no joint training.\\nThe integration of learning from data and symbolic reasoning [10] has re-\\ncently attracted a lot of attention. Hu at al. [15], Semantic-based regularization\\n(SBR) [8] for kernel machines and Logic Tensor Networks (LTN) [9] for neural\\nnetworks share the same basic idea of integrating logic reasoning and learning\\nusing a similar continuous relaxation of logic to the one presented in this paper.\\nHowever, this class of approaches considers the reasoning layer as frozen, with-\\nout allowing to jointly train its parameters. This is a big limitation, as these\\n9\\nmethods work better only with hard constraints, while they are less suitable in\\npresence of reasoning under uncertainty.\\nThe integration of deep learning with Conditional Random Fields (CRFs) [20]\\nis also an alternative approach to enforce some structure on the network output.\\nThis approach has been proved to be quite successful on sequence labeling for\\nnatural language processing tasks. This methodology can be seen as a special\\ncase of the more general methodology presented in this paper, when the poten-\\ntial functions are used to represent the correlations among consecutive outputs\\nof a recurrent deep network.\\nDeepProbLog [22] extends the popular ProbLog [7] probabilistic program-\\nming framework with the integration of deep learners. DeepProbLog requires\\nthe output from the neural networks to be probabilities and an independence\\nassumption among atoms in the logic is required to make inference tractable.\\nThis is a strong restriction, since the sub-symbolic layer often consists of several\\nneural layers sharing weights.\\nA Neural Theorem Prover (NTP) [27, 28] is an end-to-end diﬀerentiable\\nprover based on the Prolog’s backward chaining algorithm. An NTP constructs\\nan end-to-end diﬀerentiable architecture capable of proving queries to a KB\\nusing sub-symbolic vector representations. NTPs have been proven to be ef-\\nfective in tasks like entity linking and knowledge base completion. However,\\nan NTP encodes relations as vectors using a frozen pre-selected function (like\\ncosine similarity). This can be ineﬀective in modeling relations with a complex\\nand multifaceted nature (for example a relation friend(A,B) can be triggered\\nby diﬀerent relationships of the representations in the embedding space). On\\nthe other hand, DLMs allow a relation to be encoded by any selected function\\n(e.g. any deep neural networks), which is co-trained during learning. Therefore,\\nDLMs are capable of a more powerful and ﬂexible exploitation of the represen-\\ntation space. On the other end, DLMs require to fully ground a KB (like SBR,\\nLTN, PSL and most of other methods discussed here), while NTPs expands\\nonly the groundings on the explored frontier, which can be more eﬃcient in\\nsome cases.\\nDeep Structured Models [6, 19] use a similar graphical model to bridge the\\nsensorial and semantic levels. However, they have mainly focused on imposing\\ncorrelations on the output layer, without any focus on logic reasoning. Fur-\\nthermore, DLMs transform the training process into an iterative constrained\\noptimization problem, which is very diﬀerent from the approximation of the\\npartition function used in Deep Structured Models.\\nDLMs also open up the possibility to iteratively integrate rule induction\\nmechanisms like the ones proposed by the Inductive Logic Programming com-\\nmunity [17, 25].\\n10\\nFigure 3: A sample of the data used in the PAIRS experiment, where each\\ncolumn is a pair of digits.\\n4\\nExperimental Results\\n4.1\\nThe PAIRS artiﬁcial dataset\\nConsider the following artiﬁcial task. We are provided with 1000 pairs of hand-\\nwritten digits images sampled from the MNIST dataset.\\nThe pairs are not\\nconstructed randomly but they are compilied according to the following struc-\\nture:\\n1. pairs with mixed even-odd digits are not allowed;\\n2. the ﬁrst image of a pair represents a digit randomly selected from a uniform\\ndistribution;\\n3. if the ﬁrst image is an even (resp.\\nodd) digit, the second image of a\\npair represents one of the ﬁve even (resp. odd) digits with probabilities\\np1 ≥p2 ≥p3 ≥p4 ≥p5, with p1 the probability of being an image of\\nthe same digit, p2 the probability of being an image of the next even/odd\\ndigit, and so on.\\nFor example, if the ﬁrst image of a pair is selected to be a two, the second image\\nwill be a two with probability p1, it will be a four with probability p2, a six\\nwith probability p3 and so on, in a circular fashion. An example is shown in\\nFigure 3. A correct classiﬁcation happens when both digit in a pair are correctly\\npredicted.\\nTo model a task using DLMs there are some common design choices regard-\\ning these two features that one needs to take.\\nWe use the current example\\nto show them. The ﬁrst choice is to individuate the constants of the problem\\nand their sensorial representation in the perceptual space. Depending on the\\nproblem, the constants can live in a single or multiple separate domains. In the\\npairs example, the images are constants and each one is represented as a vector\\nof pixel brightnesses like commonly done in deep learning.\\nThe second choice is the selection of the predicates that should predict some\\ncharacteristic over the constants and their implementation. In the pairs experi-\\nment, the predicates are the membership functions for single digits (e.g. one(x),\\ntwo(x), etc.). A single neural network with 1 hidden layer, 10 hidden neurons\\nand 10 outputs, each one mapped to a predicate, was used in this toy experi-\\nment. The choice of a small neural network is due to the fact that the goal is\\nnot to get the best possible results, but to show how the prior knowledge can\\nhelp a classiﬁer to improve its decision. In more complex experiments, diﬀerent\\n11\\nModel\\nNN\\nSBR\\nDLM-NN\\nDLM\\nAccuracy\\n0.62\\n0.64\\n0.65\\n0.76\\nTable 2: Comparison of the accuracy metric on the PAIRS dataset using diﬀer-\\nent models.\\nnetworks can be used for diﬀerent sets of predicates, or each use a separate\\nnetwork for each predicate.\\nFinally, the prior knowledge is selected.\\nIn the pairs dataset, where the\\nconstants are grouped in pairs, it is natural to express the correlations among\\ntwo images in a pair via the prior knowledge. Therefore, the knowledge consists\\nof 100 rules in the form ∀(x, y) D1(x) →D2(y), where (x, y) is a generic pair of\\nimages and (D1, D2) range over all the possible pairs of digit classes.\\nWe performed the experiments with p1 = 0.9, p2 = 0.07, p3 = p4 = p5 = 0.01.\\nAll the images are rotated with a random degree between 0 and 90 anti-clockwise\\nto increase the complexity of the task. There is a strong regularity in having\\ntwo images representing the same digit in a pair, even some rare deviations from\\nthis rule are possible. Moreover, there are some inadmissible pairs, i.e. those\\ncontaining mixed even-odd digits. The train and test sets are built by sampling\\n90% and 10% image pairs.\\nThe results provided using a DLM have been compared against the following\\nbaselines:\\n• the neural network (NN) with no knowledge of the structure of the prob-\\nlem;\\n• the Semantic Based Regularization [8] (SBR) framework, which also em-\\nploys logical rules to improve the learner. However, the rule weights are\\ntreated as ﬁxed parameters, which are not jointly trained during learning.\\nSince searching in the space of these parameters via cross-validation is not\\nfeasible, a strong prior was provided to make SBR prefers pairs with the\\nsame image using 10 rules of the form ∀(x, y) D(x) →D(y), for each digit\\nclass D. These rules hold true in most cases and improve the baseline\\nperformance of the network.\\nTable 4.1 shows how the neural network output of a DLM (DLM-NN) already\\nbeats both the same neural model trained without prior knowledge and SBR.\\nThis happens because the neural network in DLM is indirectly adjusted to\\nrespect the prior knowledge in the overall optimization problem. When reading\\nthe DLM output from the MAP solution (DLM), the results are signiﬁcantly\\nimproved.\\n4.2\\nLink Prediction in Knowledge Graphs\\nNeural-symbolic approaches have been proved to be very powerful to perform\\napproximated logical reasoning [29]. A common approach is to assign to each\\nlogical constant and relation a learned vectorial representation [4]. Approximate\\n12\\nreasoning is then carried out in this embedded space. Link Prediction in Knowl-\\nedge Graphs is a generic reasoning task where it is requested to establish the\\nlinks of the graph between semantic entities acting as constants. Rocktaschel et\\nal. [28] shows state-of-the-art performances on some link prediction benchmarks\\nby combining Prolog backward chain with a soft uniﬁcation scheme.\\nThis section shows how to model a link prediction task on the Countries\\ndataset using a Deep Logic Models, and compare this proposed solution to the\\nother state-of-the-art approaches.\\nDataset.\\nThe Countries dataset [5] consists of 244 countries (e.g. germany),\\n5 regions (e.g. europe), 23 sub-regions (e.g. western europe, northern america,\\netc.), which act as the constants of the KB. Two types of binary relations among\\nthe constant are present in the dataset: locatedIn(c1, c2), expressing that c1\\nis part of c2 and neighborOf(c1, c2), expressing that c1 neighbors with c2. The\\nknowledge base consists of 1158 facts about the countries, regions and sub-\\nregions, expressed in the form of Prolog facts (e.g. locatedIn(italy,europe)).\\nThe training, validation and test sets are composed by 204, 20 and 20 countries,\\nrespectively, such that each country in the validation and test sets has at least\\none neighbor in the training set. Three diﬀerent tasks have been proposed for\\nthis dataset with an increasing level of diﬃculty. For all tasks, the goal is to\\npredict the relation locatedIn(c, r) for every test country c and all ﬁve regions r,\\nbut the access to training atoms in the KB varies, as explained in the following:\\n• Task S1: all ground atoms locatedIn(c, r) where c is a test country and\\nr is a region are removed from the KB. Since information about the sub-\\nregion of test countries is still contained in the KB, this task can be solved\\nexactly by learning the transitivity of the locatedIn relation.\\n• Task S2: like S1 but all grounded atoms locatedIn(c, s), where c is a test\\ncountry and s is a sub-region, are removed. The location of test countries\\nneeds to be inferred from the location of its neighbors. This task is more\\ndiﬃcult than S1, as neighboring countries might not be in the same region.\\n• Task S3: like S2, but all ground atoms locatedIn(c, r), where r is a\\nregion and c is a training country with either a test or validation country\\nas a neighbor, are removed. This task requires multiple reasoning steps\\nto determine an unknown link, and it strongly exploits the sub-symbolic\\nreasoning capability of the model to be eﬀectively solved.\\nModel.\\nEach country, region and sub-region corresponds to a constant. Since\\nthe constants are just symbols, each one is assigned to an embedding, which\\nis learned together with the other parameters of the model.\\nThe predicates\\nare the binary relations locatedIn and neighborOf, which connect constants\\nin the KB. Each relation is learned via a separate neural network with a 50\\nneuron hidden layer taking as input the concatenation of the embeddings of\\nthe constants. In particular, similarly to [4], the constants are encoded into a\\n13\\nTask\\nComplEx\\nNTP\\nNTPλ\\nDLM\\nS1\\n99.37\\n90.83\\n100.00\\n100.00\\nS2\\n87.95\\n87.40\\n93.04\\n97.79\\nS3\\n48.44\\n56.68\\n77.26\\n91.93\\nTable 3: Comparison of the accuracy provided by diﬀerent methods on link\\nprediction on the Countries dataset. Bold numbers are the best performers for\\neach task.\\none-hot vector, which is processed by the ﬁrst layer of the network, outputting\\nan embedding composed by 50 real number values. As commonly done in link\\nprediction tasks, the learning process is performed in a transductive mode. In\\nparticular, the input X consists of all possible constants for the task, while\\nthe train examples yt will cover only a subset of all the possible grounded\\npredicates, leaving to the joint train and inference process the generalization of\\nthe prediction to the other unknown grounded relations. Indeed, the output of\\nthe train process in this case is both the set of model parameters and the MAP\\nsolution predicting the unknown grounded relations that hold true.\\nMulti-step dependencies among the constants are very important to predict\\nthe existence of a link in this task. For example in task S1, the prediction of a\\nlink among a country and a region can be established via the path passing by a\\nsub-region, once the model learns a rule stating the transitivity of the locatedIn\\nrelation (i.e. locatedIn(x, y) ∧locatedIn(y, z) →locatedIn(x, z)). Exploit-\\ning instead the rule neighborOf(x, y)∧locatedIn(y, z) →locatedIn(x, z), the\\nmodel should be capable of approximately solving task S2.\\nAll 8 rules ∀x ∀y ∀z A(x, y) ∧B(y,z) →C(y, z), where A, B and C are either\\nneighborOf or locatedIn are added to the knowledge base for this experiment.\\nThese rules represent all the 2-steps paths reasoning that can be encoded, and\\nthe strength of each rule needs to be estimated as part of the learning process for\\neach task. The training process will iteratively minimize Equation 3 by jointly\\ndetermining the embeddings and the network weights such that network outputs\\nand the MAP solution will correctly predict the training data, while respecting\\nthe constraints on the MAP solution at the same level as on the train data.\\nResults.\\nTable 4.2 compares DLM against the state-of-the-art methods used\\nby Rocktaschel et al. [28], namely ComplEx, NTP and NTPλ.\\nTask S1 is\\nthe only one that can be solved exactly when the transitive property of the\\nlocatedIn relation has been learned to always hold true. Indeed, most methods\\nare able to perfectly solve this task, except for the plain NTP model. DLM is\\ncapable perfectly solving this task by joining the logical reasoning capabilities\\nwith the discriminative power of neural networks. DLMs perform better than\\nthe competitors on tasks S2 and S3, thanks to additional ﬂexibility obtained by\\njointly training the relation functions using neural networks, unlike the simple\\nvectorial operations like the cosine similarity employed by the competitors.\\n14\\n5\\nConclusions and future work\\nThis paper presents Deep Logic Models that integrate (deep) learning and logic\\nreasoning into a single fully diﬀerentiable architecture. The logic can be ex-\\npressed with unrestricted FOL formalism, where each FOL rule is converted\\ninto a diﬀerentiable potential function, which can be integrated into the learn-\\ning process. The main advantage of the presented framework is the ability to\\nfully integrate learning from low-level representations and semantic high-level\\nreasoning over the network outputs. Allowing to jointly learn the weights of\\nthe deep learners and the parameters controlling the reasoning enables a pos-\\nitive feedback loop, which is shown to improve the accuracy of both layers.\\nFuture work will try to bridge the gap between fully grounded methodologies\\nlike current Deep Logic Models and Theorem Provers which expand only the\\ngroundings needed to expand the frontier of the search space.\\nReferences\\n[1] Bach, S.H., Broecheler, M., Huang, B., Getoor, L.: Hinge-loss markov\\nrandom ﬁelds and probabilistic soft logic. arXiv preprint arXiv:1505.04406\\n(2015)\\n[2] Battaglia, P.W., Hamrick, J.B., Bapst, V., Sanchez-Gonzalez, A., Zam-\\nbaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner,\\nR., et al.: Relational inductive biases, deep learning, and graph networks.\\narXiv preprint arXiv:1806.01261 (2018)\\n[3] Bengio, Y., et al.: Learning deep architectures for ai. Foundations and\\ntrends R⃝in Machine Learning 2(1), 1–127 (2009)\\n[4] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., Yakhnenko, O.:\\nTranslating embeddings for modeling multi-relational data. In: Advances\\nin neural information processing systems. pp. 2787–2795 (2013)\\n[5] Bouchard, G., Singh, S., Trouillon, T.: On approximate reasoning capa-\\nbilities of low-rank vector spaces. AAAI Spring Syposium on Knowledge\\nRepresentation and Reasoning (KRR): Integrating Symbolic and Neural\\nApproaches (2015)\\n[6] Chen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured\\nmodels. In: International Conference on Machine Learning. pp. 1785–1794\\n(2015)\\n[7] De Raedt, L., Kimmig, A., Toivonen, H.: Problog: A probabilistic pro-\\nlog and its application in link discovery. In:\\nProceedings of the 20th\\nInternational Joint Conference on Artiﬁcal Intelligence. pp. 2468–2473.\\nIJCAI’07, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA\\n(2007), http://dl.acm.org/citation.cfm?id=1625275.1625673\\n15\\n[8] Diligenti, M., Gori, M., Sacca, C.: Semantic-based regularization for learn-\\ning and inference. Artiﬁcial Intelligence 244, 143–165 (2017)\\n[9] Donadello, I., Seraﬁni, L., Garcez, A.d.: Logic tensor networks for semantic\\nimage interpretation. arXiv preprint arXiv:1705.08968 (2017)\\n[10] Garcez, A.S.d., Broda, K.B., Gabbay, D.M.: Neural-symbolic learning sys-\\ntems: foundations and applications. Springer Science & Business Media\\n(2012)\\n[11] Giannini, F., Diligenti, M., Gori, M., Maggini, M.: On a convex logic\\nfragment for learning and reasoning. IEEE Transactions on Fuzzy Systems\\n(2018)\\n[12] Goodfellow, I., Bengio, Y., Courville, A., Bengio, Y.: Deep learning, vol. 1.\\nMIT press Cambridge (2016)\\n[13] Haykin, S.: Neural Networks: A Comprehensive Foundation. Prentice Hall\\nPTR, Upper Saddle River, NJ, USA, 1st edn. (1994)\\n[14] Hazan, T., Schwing, A.G., Urtasun, R.: Blending learning and inference\\nin conditional random ﬁelds. The Journal of Machine Learning Research\\n17(1), 8305–8329 (2016)\\n[15] Hu, Z., Ma, X., Liu, Z., Hovy, E., Xing, E.: Harnessing deep neural net-\\nworks with logic rules. arXiv preprint arXiv:1603.06318 (2016)\\n[16] Kimmig, A., Bach, S., Broecheler, M., Huang, B., Getoor, L.: A short\\nintroduction to probabilistic soft logic. In: Proceedings of the NIPS Work-\\nshop on Probabilistic Programming: Foundations and Applications. pp.\\n1–4 (2012)\\n[17] Lavrac, N., Dzeroski, S.: Inductive logic programming. In: WLP. pp. 146–\\n160. Springer (1994)\\n[18] LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning\\napplied to document recognition. Proceedings of the IEEE 86(11), 2278–\\n2324 (1998)\\n[19] Lin, G., Shen, C., Van Den Hengel, A., Reid, I.: Eﬃcient piecewise training\\nof deep structured models for semantic segmentation. In: Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition. pp.\\n3194–3203 (2016)\\n[20] Ma, X., Hovy, E.: End-to-end sequence labeling via bi-directional lstm-\\ncnns-crf. In: Proceedings of the 54th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers). pp. 1064–1074.\\nAssociation for Computational Linguistics (2016), http://aclweb.org/\\nanthology/P16-1101\\n16\\n[21] Mahendran, A., Vedaldi, A.: Understanding deep image representations by\\ninverting them. In: Proceedings of the IEEE conference on computer vision\\nand pattern recognition. pp. 5188–5196 (2015)\\n[22] Manhaeve, R., Dumančić, S., Kimmig, A., Demeester, T., De Raedt,\\nL.: Deepproblog: Neural probabilistic logic programming. arXiv preprint\\narXiv:1805.10872 (2018)\\n[23] Muggleton, S., De Raedt, L.: Inductive logic programming: Theory and\\nmethods. The Journal of Logic Programming 19, 629–679 (1994)\\n[24] Novák, V., Perﬁlieva, I., Mockor, J.: Mathematical principles of fuzzy logic,\\nvol. 517. Springer Science & Business Media (2012)\\n[25] Quinlan, J.R.: Learning logical deﬁnitions from relations. Machine learning\\n5(3), 239–266 (1990)\\n[26] Richardson, M., Domingos, P.: Markov logic networks. Machine learning\\n62(1-2), 107–136 (2006)\\n[27] Rocktäschel, T., Riedel, S.: Learning knowledge base inference with neu-\\nral theorem provers. In: Proceedings of the 5th Workshop on Automated\\nKnowledge Base Construction. pp. 45–50 (2016)\\n[28] Rocktäschel, T., Riedel, S.: End-to-end diﬀerentiable proving. In: Ad-\\nvances in Neural Information Processing Systems. pp. 3788–3800 (2017)\\n[29] Trouillon, T., Welbl, J., Riedel, S., Gaussier, É., Bouchard, G.: Complex\\nembeddings for simple link prediction. In: International Conference on Ma-\\nchine Learning. pp. 2071–2080 (2016)\\n17\\n',\n",
       " '1901.09388v2.pdf': 'Moving Deep Learning into Web Browser: How Far Can We Go?\\nYun Ma1,2, Dongwei Xiang1, Shuyu Zheng1, Deyu Tian1, Xuanzhe Liu1\\n1Key Lab of High-Confidence Software Technology, MoE (Peking University), Beijing, China\\n2Tsinghua University, Beijing, China\\n{mayun,xdw,zhengshuyu,tiandeyu,xzl}@pku.edu.cn\\nABSTRACT\\nRecently, several JavaScript-based deep learning frameworks have\\nemerged, making it possible to perform deep learning tasks directly\\nin browsers. However, little is known on what and how well we can\\ndo with these frameworks for deep learning in browsers. To bridge\\nthe knowledge gap, in this paper, we conduct the first empirical\\nstudy of deep learning in browsers. We survey 7 most popular\\nJavaScript-based deep learning frameworks, investigating to what\\nextent deep learning tasks have been supported in browsers so\\nfar. Then we measure the performance of different frameworks\\nwhen running different deep learning tasks. Finally, we dig out\\nthe performance gap between deep learning in browsers and on\\nnative platforms by comparing the performance of TensorFlow.js\\nand TensorFlow in Python. Our findings could help application\\ndevelopers, deep-learning framework vendors and browser vendors\\nto improve the efficiency of deep learning in browsers.\\nKEYWORDS\\nDeep learning; Web browser; Web applications; Measurement\\n1\\nINTRODUCTION\\nIn the past decade, the advance of deep learning (DL) technique\\nhas significantly promoted the artificial intelligence (AI). Numer-\\nous AI applications, e.g., image processing, object tracking, speech\\nrecognition, and natural language processing, have raised urgent\\nrequirements to adopt the DL. As a result, various libraries and\\nframeworks, such as TensorFlow [18], Caffe [2], and CNTK [3],\\nhave been proposed and applied in practice.\\nHowever, developing AI applications powered by the popular\\nDL frameworks and libraries is a non-trivial task. Usually, these\\nframeworks and libraries are leveraged by native applications that\\ncan run on heterogeneous development environments such as Win-\\ndows, Linux, MacOS/iOS, and Android. The applications are devel-\\noped by various imperative programming languages, i.e., C/C++ on\\nWindows, Objective-C on iOS and MacOS, and Java on Android.\\nDeveloping AI applications that is portable to multiple platforms is\\nindeed not easy. The development is particularly complicated for\\nmobile applications, as the app vendors usually need to develop\\nand maintain both iOS and Android versions. In addition, the de-\\nployment is also non-trivial, as most current platforms come with\\nan appstore, some of which require manual testing of submitted\\napplications by the appstore provider before being published-a pro-\\ncess that can take several weeks-and applications can be rejected\\nfor seemingly arbitrary reasons.\\nWWW ’19, May 13–17, 2019, San Francisco, CA, USA\\n2019. ACM ISBN 978-1-4503-6674-8/19/05.\\nhttps://doi.org/10.1145/3308558.3313639\\nCompared to the native applications, Web applications can in-\\ndeed make the cross-platform portability issues much simpler. The\\nsame implementation of a DL-powered Web application can be de-\\nployed in the browser on all platforms regardless of the underlying\\nhardware device types (PC, smartphones, and wearable devices)\\nand the operating systems (Windows, Mac, iOS, and Android). Ad-\\nvancements in HTML5, CSS3, and especially JavaScript language,\\nstarted to enable the creation of DL-powered Web applications that\\noffer a comparable experience to native applications, especially for\\nthe popular Web game applications [38][34]. In particular, bene-\\nfited from the development of WebGL [32][19][20], current major\\nbrowsers such as Google Chrome, Mozilla FireFox, and Apple Sa-\\nfari, can better utilize the integrated graphics card to accelerate DL\\ntasks, without the need of standalone graphics card like NVIDIA\\nwhich is required by native DL frameworks.\\nRunning DL-powered Web applications in browsers has drawn\\nthe attention from various research communities including AI, soft-\\nware engineering, Web browsers, and even computer architecture.\\nAs a result, various JavaScript-based DL development frameworks\\nand libraries have been published. In 2015, Karpathy presented the\\nConvNetJS [4], known as the first JavaScript library for DL in Web\\nbrowsers to date. Other efforts such as WebDNN [15], Keras.js [5],\\nand Mind [7], were proposed to support DL in browsers. In early\\n2018, Google released the TensorFlow.js [14], which is a significant\\nstep for promoting the DL in browsers.\\nAlthough the preceding efforts along with some on-going efforts\\nseem to make running DL tasks in browsers possible, little is known\\non what DL tasks we can do and how well DL works in browsers.\\nMore importantly, considering the long debate of performance of\\nWeb applications compared with that of native applications, the\\nsame issue also exists in developing DL-powered Web applications.\\nHence, it is urgent to address such a knowledge gap in terms of the\\nfeasibility and usability for running DL in Web browsers.\\nIn this paper, we make the first empirical study of DL in browsers\\nby answering the following research questions.\\n• RQ1: What features do existing frameworks provide to im-\\nplement various kinds of DL tasks in the browser?\\n• RQ2: How well do existing frameworks perform over differ-\\nent DL tasks?\\n• RQ3: How big is the performance gap between running DL\\nin the browser and on the native platform?\\nWe select 7 popular JavaScript-based frameworks that support\\nrunning DL in browsers, and conduct a characteristic study over\\nthem. We develop a browser extension to measure the performance\\nas well as the utilization of system resources when running different\\nDL tasks. We choose the TensorFlow.js and native TensorFlow in\\nPython to compare the performance of DL in browsers with that\\non native platforms.\\narXiv:1901.09388v2  [cs.SE]  24 Mar 2019\\nThe key findings of our study includes:\\n• DL in browsers is still at dawn. Most frameworks of DL in\\nbrowsers support only a specific subset of DL tasks. Among all the\\nframeworks, TensorFlow.js provides the most number of function-\\nalities to realize various kinds of DL tasks.\\n• Support of training in browsers is not fledged. In most frame-\\nworks, inference has drawn more attention compared with training.\\nFor training tasks, the number of neurons per layer dominates the\\nperformance variation considering the complexity of DL models\\nsince the browser is limited in complex matrix calculation.\\n• Model loading dominates the computation for inference\\ntasks. Loading and warming up the DL model costs more time\\nthan running the inference task itself. The CPU backend performs\\nbetter than the GPU backend when the browser run inference tasks\\nfor small-size models.\\n• Integrated graphics card helps browsers to beat native plat-\\nforms when standalone graphics card is not available. For\\npopular pre-trained models like MobileNet and Inception, Tensor-\\nFlow.js is just 1x to 2x slower than native TensorFlow in Python\\nwhen running inference tasks on the standalone graphics card. Ten-\\nsorFlow.js on the integrated graphics card outperforms the native\\nTensorFlow on CPU when running the same inference task.\\n• System resources can be further exploited for in-browser\\nDL tasks. For TensorFlow.js, the CPU is not fully utilized (about\\n80%) when DL tasks run on the CPU backend. The memory allocated\\nto WebGL is limited by the browser, leading to the crash of some\\nDL tasks.\\nBased on the findings, We have drawn some practical recommen-\\ndations for application developers as well as DL-framework and\\nbrowser vendors. Application developers who aim to develop DL-\\npowered Web applications, need to better control the the number\\nof neurons per layer in DL models, pre-load the model file in ad-\\nvance, and employ the CPU backend rather than the GPU backend\\nwhen running inference tasks on small DL models. DL-framework\\nvendors should consider encoding the model file in binary format\\nrather than JSON to reduce the file size as well as improve the\\ncomputation time, and leverage compiler optimization techniques\\nto reduce the call stack. Browser vendors should consider support-\\ning multi-process and scheduling over multi-core in the JavaScript\\nengines.\\nThe remainder of this paper is organized as follows. Section 2\\nshows some background knowledge of deep learning in browsers.\\nSections 3 to 5 describe the results, including the analysis of frame-\\nwork functionality, performance measurement, and comparison\\nwith native DL frameworks. Section 6 presents the implications\\nand recommendations drawn from the findings. Section 7 surveys\\nrelated work and Section 8 concludes the paper with future work.\\n2\\nBACKGROUND\\nIn this section, we give some background of deep learning and then\\ndiscuss how browsers support deep learning tasks.\\n2.1\\nDeep Learning\\nDeep learning (DL) is a class of machine learning algorithms that\\nuse a cascade of multiple layers of nonlinear processing units (called\\nneurons) for feature extraction and transformation. Each successive\\nlayer uses the output from the preceding layer as input. In recent\\nyears, DL has gained great success in many areas such as computer\\nvision, speech recognition and natural language processing.\\nThere are many types of DL models, among which deep neural\\nnetwork (DNN) [23], convolutional neural network (CNN) [28],\\nand recurrent neural network (RNN) [35] are three basic structures.\\nDNN is a typically feedforward network with multiple layers be-\\ntween the input and output layers, in which data flows from the\\ninput layer to the output layer without looping back. CNN uses a\\nvariation of multi-layer perceptrons designed to require minimal\\npreprocessing, usually applied to analyzing visual imagery. RNN\\nhas connections between nodes forming a directed graph along a\\ntemporal sequence, allowing it to exhibit temporal behaviors.\\nDL consists of two phases: training phase where the input data\\nare used to calculate the parameters of the model, and inference\\nphase where the trained model outputs the value given a specific\\ninput sample.\\n2.2\\nDeep Learning in Browsers\\nRecently, there is a trend that applications perform DL tasks directly\\non the clients for better privacy and timely response. As a cross-\\nplatform client-side computation target, Web browsers have drawn\\nthe attention to AI communities to support client-side DL. Sev-\\neral applications of in-browser DL are implemented and published,\\nsuch as 1) TensorFlow playground [13], which is an interactive\\nplatform to learn the principle of DL; 2) Teachable Machine [12],\\nwhich gives the users an experience of teaching the machine how\\nto response when they pose a gesture, using camera in the browser;\\n3) MLitB [33], which is capable of performing distributed learning\\nwith heterogeneous classes of devices using Web browsers; 4) Mor-\\nphCast [9], which combines interactive video and face recognition\\nwith emotion, gender and age analysis to create adaptive-media.\\nDL in browsers is implemented by JavaScript and rely on the\\nbrowser engine to execute. Fortunately, the advancement of latest\\nbrowsers provides APIs to access GPU, which can be used to ac-\\ncelerate matrix calculations of DL. These APIs are: 1) WebGL [16],\\nwhich is a JavaScript API for rendering interactive 2D and 3D graph-\\nics within any compatible Web browser; 2) WebGPU [17], which is\\nthe fastest among existing JavaScript APIs for accelerating graphics\\nand computation. Currently, WebGPU API is supported only in\\nSafari Technology Preview. We should mention that WebGL and\\nWebGPU can run on both integrated graphics cards and standalone\\ngraphics cards.\\n3\\nSUPPORTED FEATURES OF DEEP\\nLEARNING IN BROWSERS\\nIn this section, we make a characteristic study to answer the first\\nresearch question, i.e., what features do existing frameworks pro-\\nvide to implement various kinds of DL tasks in the browser? We\\nfirst introduce the frameworks selected for the study. Then we com-\\npare the features of these frameworks from two aspects: provided\\nfunctionality and developer support. For provided functionality,\\nwe mainly examine whether each framework supports some basic\\nfunctionalities that are commonly used in the development of DL\\napplications. For developer support, we take a look at some factors\\nwhich may affect the efficiency of developing and deploying DL\\napplications. Table 1 summarizes all the results as of Nov. 2018.\\n3.1\\nSelected Frameworks\\nTo select the state-of-the-art frameworks of supporting DL in browsers,\\nwe search on the GitHub with the keyword “deep learning frame-\\nwork” and filter the results in JavaScript language. Then we choose\\nthe top 7 frameworks of which the number of stars exceeds 1,000\\non GitHub. We introduce each framework as follows.\\nTensorFlow.js [14], released by Google in Mar. 2018, is an in-\\nbrowser machine learning library that supports defining, training,\\nand running models entirely in the browser using JavaScript. It is\\nthe successor to deeplearn.js which is now called TensorFlow.js\\nCore. TensorFlow.js is powered by WebGL and provides high-level\\nAPIs for defining models. TensorFlow.js support all the Keras layers\\n(including Dense, CNN, LSTM, and so on). Therefore, it is easy to\\nimport models pre-trained by the native TensorFlow and Keras into\\nthe browser and run with Tensorflow.js.\\nConvNetJS [4] is a Javascript library originally written by Andrej\\nKarpathy at Stanford. The entire library is based on transforming\\n3-dimensional volumes of numbers. ConvNetJS currently supports\\ncommon neural network models and cost functions for classifica-\\ntion and regression. Furthermore, it supports convolutional net-\\nworks, and an experimental reinforcement learning. Unfortunately,\\nalthough ConvNetJS might be the most famous framework before\\nTensorFlow.js, it is no longer maintained after Nov. 2016.\\nKeras.js [5] abstracts away a number of frameworks as backends\\nincluding TensorFlow, CNTK, etc. It supports importing models\\npre-trained by Keras for inference. In the GPU mode, computation\\nis performed by WebGL. However, this project is no longer active.\\nWebDNN [15], released by the University of Tokyo, claims to be\\nthe fastest DNN execution framework in browsers. It supports only\\nthe inference tasks. The framework supports 4 execution backends:\\nWebGPU, WebGL, WebAssembly, and fallback pure JavaScript im-\\nplementation. WebDNN optimizes DNN models by compressing\\nthe model data to accelerate the execution. Empirical evaluations\\nshowed that it achieved more than 200x acceleration [6].\\nbrain.js [1] is a JavaScript library for neural networks replacing\\nthe deprecated “brain” library. It provides DNN, RNN, LSTM and\\nGRU for training tasks. The library supports serializing and loading\\nthe state of a trained DL model with JSON.\\nsynaptic [11] is a JavaScript architecture-free neural network li-\\nbrary, supporting basically any type of first order or even second\\norder RNN. This library also includes a few built-in DL architectures,\\nincluding multi-layer perceptrons, LSTM, liquid state machines and\\nHopfield networks.\\nMind [7] is a flexible neural network library. The core framework\\nhas only 247 lines of code, which uses a matrix implementation\\nto process training data. It supports customization of the network\\ntopology and plugins to configure pre-trained models created by\\nthe mind community. However, this framework is no longer active.\\n3.2\\nProvided Functionality\\nSupport for training. Most frameworks support training and in-\\nference tasks in the browser. However, Keras.js and WebDNN do not\\nsupport training DL models in browsers. They support only load-\\ning pre-trained models to perform inference tasks. Therefore, the\\nnumber is not available for the types of layer/activation/optimizer\\nsupported by Keras.js and WebDNN in Table 1.\\nSupported network types. Some frameworks are not for general-\\npurpose DL tasks, so they differ in the supported network types.\\nSpecifically, TensorFlow.js, Keras.js and WebDNN support three\\nnetwork types: DNN, CNN and RNN. However, ConvNetJS mainly\\nsupports CNN tasks and does not support RNN. brain.js and synap-\\ntic mainly support RNN tasks, and do not support convolution and\\npooling operations used in CNN networks. Mind supports only the\\nbasic DNN.\\nSupported layer types. All frameworks support building neural\\nnetworks using units of layers. The layer API of TensorFlow.js\\nsupports 49 different layers, including dense, convolution, pooling,\\nRNN, normalization, and so on. Other frameworks support a smaller\\nvariety of layers, which are also related to the network types they\\nsupport. It should be noted that the core API of TensorFlow.js is\\nimplemented in a way similar to the native TensorFlow which com-\\nbines various operations to build computational graphs. synaptic is\\nan architecture-free framework that supports building any type of\\nfirst order or even second order RNN networks.\\nSupported activation/optimizer types. In general, TensorFlow.js\\nprovides developers with the most kinds of choices. For activation\\nfunctions, other frameworks support only basic sigmoid or ReLU.\\nFor optimizers, other frameworks mainly support basic stochastic\\ngradient descent (SGD).\\nSupport for GPU acceleration (WebGL). TensorFlow.js is the\\nonly framework that supports GPU-accelerated training tasks. Ten-\\nsorFlow.js, Keras.js, and WebDNN support using GPU to accelerate\\ninference tasks. WebDNN also supports a more advanced tech-\\nnology, WebGPU, but WebGPU has been supported by only the\\ntechnology preview version of Safari.\\n3.3\\nDeveloper Support\\nDocuments. Documents provided by TensorFlow.js, ConvNetJS,\\nWebDNN and synaptic are completed and in detail. The document\\nof Keras.js is not complete and brain.js has only a few tutorials.\\nDemos. All the frameworks provide demos for developers to get\\nstart. TensorFlow.js offers the richest demos covering a wide range\\nof use cases.\\nImporting models from other frameworks. TensorFlow.js, Keras.js\\nand WebDNN support importing models from native DL frame-\\nworks in Python and all of them provide Python scripts for convert-\\ning models. TensorFlow.js supports models trained by TensorFlow\\nand Keras. Keras.js supports Keras models. WebDNN supports im-\\nporting models from TensorFlow, Keras, Caffe and Pytorch. With\\nthe support of using pre-trained models from other DL frameworks,\\nthe development effort can be significantly reduced.\\nAPI to save/load model. All frameworks that support training\\ntasks in the browser have APIs for saving models. All frameworks\\nhave APIs for loading models.\\nSupport for server side (Node.js). All frameworks are supported\\nfor Node.js. Such a feature makes it possible to offload computation\\ninside browsers onto remote servers.\\nTable 1: Characteristics of JavaScript-based frameworks that support deep learning in browsers.\\nTensorFlow.js\\nConvNetJS\\nKeras.js\\nWebDNN\\nbrain.js\\nsynaptic\\nMind\\nBasic Information\\nGithub Stars\\n9453\\n9364\\n4348\\n1464\\n6366\\n6315\\n1333\\nMain Contributor\\nGoogle\\nStanford\\nUniversity\\nLeon Chen\\nThe University\\nof Tokyo\\nRobert\\nPlummer\\nJuan\\nCazala\\nSteven\\nMiller\\nLast Commit Date\\nOct 30, 2018\\nNov 25, 2016\\nAug 17, 2018\\nOct 25, 2018\\nNov 5, 2018\\nMar 25, 2018\\nJul 7, 2017\\nStatus\\nActive\\nNot Active\\nNot Active\\nActive\\nActive\\nActive\\nNot Active\\nFunctionality\\nSupport for Training\\nY\\nY\\nN\\nN\\nY\\nY\\nY\\nSupported\\nNetwork Types\\nDNN\\nY\\nY\\nY\\nY\\nY\\nY\\nY\\nCNN\\nY\\nY\\nY\\nY\\nN\\nN\\nN\\nRNN\\nY\\nN\\nY\\nY\\nY\\nY\\nN\\nSupported Layer Types\\n49\\n7\\nNA\\nNA\\n7\\n1\\n1\\nSupported Activation Types\\n16\\n4\\nNA\\nNA\\n4\\n5\\n2\\nSupported Optimizer Types\\n7\\n3\\nNA\\nNA\\n1\\nNA\\nNA\\nSupport for GPU Accelaration (WebGL)\\nY\\nN\\nY\\nY\\nN\\nN\\nN\\nDeveloper Support\\nDocuments\\nY\\nY\\nNot finished\\nY\\nOnly tutorials\\nY\\nY\\nDemos\\n20\\n10\\n9\\n8\\n7\\n7\\n4\\nImporting Models from\\nOther Frameworks\\nTensorFlow\\nY\\nN\\nN\\nY\\nN\\nN\\nN\\nKeras\\nY\\nN\\nY\\nY\\nN\\nN\\nN\\nCaffe&Pytorch\\nN\\nN\\nN\\nY\\nN\\nN\\nN\\nAPI to Save/Load Model\\nSave\\nY\\nY\\nN\\nN\\nY\\nY\\nY\\nLoad\\nY\\nY\\nY\\nY\\nY\\nY\\nY\\nSupport for Server Side (Node.js)\\nY\\nY\\nY\\nY\\nY\\nY\\nY\\nLibrary Size\\n732KB\\n33KB\\n650KB\\n130KB\\n819KB\\n106KB\\nNA\\nLibrary size. We list the size of the library files that need to be\\nloaded into browsers. ConvNetJS is the smallest, which is just 33KB.\\nTensorFlow.js and brain.js have very large size of files, which are\\n732KB and 819KB, respectively. Small-size libraries are better for\\nloading applications in browsers since all the files have to be down-\\nloaded on demand.\\n4\\nPERFORMANCE OF DEEP LEARNING IN\\nBROWSERS\\nIn this section, we conduct a measurement study to investigate the\\nsecond research question, i.e., how well do existing frameworks\\nperform over different DL tasks? We study the influence of model\\ncomplexity and backend processor (CPU or GPU) on the perfor-\\nmance when the browser runs training and inference tasks.\\n4.1\\nExperiment Setup\\nDL model. As explained before, the network types supported by\\ndifferent frameworks are not the same. So we adopt the most basic\\nfully connected neural network as the model in the experiment. For\\nthe dataset to run the DL tasks, we use the classic MNIST handwrit-\\nten digit recognition database [8]. The model to be trained has 784\\ninput nodes and 10 output nodes. To study the influences of model\\ncomplexity on the performance, we choose different configurations\\nof the model. The parameters include 1) the number of the hidden\\nlayers (depth) of the neural network, which ranges in [1, 2, 4, 8],\\nand 2) the number of neurons (width) in each hidden layer, which\\nranges in [64, 128, 256]. The range of depth and width is set based\\non the assumption that client-side DL models should be of small\\nsize, being able to run on the client. In the training process, the\\nbatch size is always set to 64.\\nHardware. In order to study the performance difference between\\nCPU and GPU backend, we use a Hasee T97E laptop computer,\\nwhich has a standalone graphics card, Nvidia 1070 Max-Q (with\\n8GB GPU memory). The CPU is Intel i7-8750H, which includes an\\nIntel HD Graphics 630, enabling us to measure the performance\\nusing integrated graphics card. In the following, we use nGPU and\\niGPU to denote the GPU backend on the standalone Nvidia graphics\\ncard and the integrated Intel graphics card, respectively.\\nSoftware. All the experiments run on the Chrome browser (version:\\n71.0.3578.10 dev 64-bit) on Ubuntu 18.04.01 LTS (64-bit). For the\\nframeworks, we use their latest published version.\\nPerformance measurement. For each DL task, we implement a\\nWeb page where the configurations of DL models can be varied\\nthrough the parameters in the URL. We run each DL task on the\\nChrome browser, and measure the time spent on finishing the task.\\nSince each experiment usually requires running dozens of tasks\\nunder different configurations, we developed a Chrome extension\\nto iterate all the pages and change the configuration after one\\ntask is performed. This browser extension is also responsible for\\nmonitoring the system resource usage of the Web page. At the same\\ntime, a local server records the experimental statistics uploaded by\\nthe extension.\\n4.2\\nTraining Performance\\nWe select four JavaScript frameworks, brain.js, ConvNetJS, synaptic,\\nand TensorFlow.js, which support training in browsers, to compare\\nFigure 1: Average training time (ms) on one batch under different model complexities. The y-axis is on log scale.\\ntheir performance of running training tasks. All the four frame-\\nworks can train models on the CPU backend except that Tensor-\\nFlow.js is also able to use the GPU backend via WebGL. We train the\\ndefined model using each framework and obtain the average time\\nspent on training one batch. Figure 1 shows the results under differ-\\nent model complexities. Since the training time of synaptic is about\\ntens to hundreds of times longer than that of other frameworks, we\\nomit the result of synaptic in the figure for better presentation but\\nthe findings are similar to other frameworks.\\nIn general, the training time increases with the increase of the\\nnetwork size since more computation is needed to complete the\\ntraining process for larger networks. Comparing the training time\\nof different frameworks on the CPU backend, we can see that Con-\\nvNetJS is the fastest among all the frameworks for all network\\nconfigurations. The possible reason may be that ConvNetJS is de-\\nsigned to be simpler, which can be reflected by its small library file\\nsize. Brain.js is closely behind, with a performance gap of about two\\ntimes (2x) with ConvNetJS. Tensorflow.js has a performance gap\\nof two to three times (2x-3x) with ConvNetJS. When comparing\\nthe training time ratio of ConvNetJS over TensorFlow.js, we find\\nthat the performance gap is gradually reduced when the depth and\\nwidth increase, indicating that compared with ConvNetJS, Tensor-\\nFlow.js has relatively large overhead beyond calculation. In addition,\\nthe performance gap is larger as the network width increases than\\nas the network depth increases, implying that TensorFlow.js deals\\nbetter with large-scale matrix calculation than ConvNetJS.\\nGPU benefits. The training time on the CPU backend becomes\\nlonger with the increase of network size, but the results on the\\nGPU backend are not the same. For both the iGPU with weaker\\ncomputation power and the nGPU which can satisfy larger-scale\\nmatrix calculations, the training time does not increase significantly.\\nBut in the process from (4 hidden layers, 128 neurons per layer) to\\n(8 hidden layers, 256 neurons per layer), the training time of iGPU\\nincreases significantly. The reason may be that under the network\\nsize set in this experiment, the training process does not reach\\nthe GPU’s capability bottleneck. Although the matrix computation\\ncapability of nGPU is better than that of iGPU, the training time\\non nGPU is even longer than iGPU. Such a result is caused by the\\nexcessive time overhead to call the WebGL for accessing GPU. The\\nreal computation time of GPU should be much shorter.\\nSystem resource utilization. We show the statistics of CPU uti-\\nlization of each framework during the training process in Table 2.\\n110% is the upper bound of CPU utilization. The capability of multi-\\ncore processor cannot be used since the JavaScript engine is single-\\nthreaded. As a result, it can only maximize the usage of a single\\nTable 2: CPU utilization (%) in the training process.\\nFramework\\nBackend\\nMax\\nMin\\nAverage\\nbrain.js\\nCPU\\n104.0\\n99.9\\n101.2\\nConvNetJS\\nCPU\\n108.0\\n101.9\\n104.1\\nsynaptic\\nCPU\\n113.9\\n88.7\\n102.8\\nTensorFlow.js\\nCPU\\n108.0\\n61.0\\n82.1\\niGPU\\n82.0\\n54.9\\n65.9\\nnGPU\\n75.9\\n48.0\\n60.0\\ncore. The reason why the CPU utilization is over 100% is that other\\nkernel and user space components occasionally run simultaneously\\nin other threads.\\nOn the CPU backend, TensorFlow.js sometimes cannot maximize\\nthe utilization of a single core and its average CPU utilization is\\nonly 82.1%. Meanwhile, we can find that when running training\\ntasks on the GPU backend, CPU is not fully utilized since most\\ncomputation is on the GPU. Training on iGPU has about 5-7%\\nhigher CPU utilization than that on nGPU.\\n4.3\\nInference Performance\\nWe select 6 JavaScript frameworks to compare their performance\\nof running inference tasks. TensorFlow.js, Keras.js, and WebDNN\\nsupport using GPU for acceleration, but brain.js, ConvNetJS, and\\nsynaptic support using only CPU for inference. In terms of model\\nusage, brain.js, ConvNetJS, synaptic and TensorFlow.js support sav-\\ning their own trained models, while Keras.js and WebDNN only\\nsupport importing pre-trained models from other deep learning\\nframeworks. Therefore, for brain.js, ConvNetJS, synaptic and Ten-\\nsorFlow.js, we use the models saved by the frameworks themselves.\\nFor Keras.js and WebDNN, we use the models trained by Keras and\\nthen convert the models to the corresponding format. Theoretically,\\nthe parameter values of the trained DL models should be different,\\nbut the absolute value does not affect the inference time. So we\\njust assign the same parameter values to all the models of different\\nframeworks.\\nThe inference task involves loading a pre-trained model and\\nthen given a sample input, the model outputs the result. In addition,\\non the GPU backend, there is a warmup process where the first\\nsample for inference is usually used to activate the GPU processor.\\nTherefore, we break down the inference process into three phases:\\nmodel loading, warming up, and inference, and study the fine-\\ngrained performance. Due to the space limitation, we omit the\\nresults where the model depth is 8 in the following analysis because\\nthe trend is similar as the depth increases. Besides, since the model\\nFigure 2: Model loading time (ms) under different model complexities. The y-axis is on log scale.\\nTable 3: Size of model files (MB).\\nDepth\\nWidth\\nbrain.js\\nConvNetJS\\nsynaptic\\nTensorFlow.js\\n1\\n64\\n1.4\\n1.3\\n3.4\\n0.2\\n128\\n2.7\\n2.7\\n6.7\\n0.4\\n256\\n5.5\\n5.4\\n13.3\\n0.8\\n2\\n64\\n1.5\\n1.5\\n3.7\\n0.2\\n128\\n3.2\\n3.1\\n7.8\\n0.5\\n256\\n7.2\\n7.1\\n17.7\\n1.1\\n4\\n64\\n1.7\\n1.7\\n4.2\\n0.3\\n128\\n4.0\\n4.0\\n10.1\\n0.6\\n256\\n10.7\\n10.5\\n26.5\\n1.6\\nloading time and inference time of synaptic are still much longer\\nthan those of other frameworks, we do not depict the results of\\nsynaptic in the figures for better presentation.\\nModel file size. We first investigate the size of the model file used\\nby different frameworks. As models for inference usually should\\nbe downloaded from the remote server, smaller size of model files\\nmeans shorter downloading time. Table 3 shows the size of model\\nfiles that are used in all inference experiments. ConvNetJS and\\nbrain.js use similar JSON encoding, so the size of their model files\\nare nearly the same. The model file of synaptic uses JSON encoding\\nas well but its size is the largest among all the frameworks. As\\nthe model files used by TensorFlow.js, Keras.js and WebDNN are\\nall converted from Keras models, their model files are of the same\\nsize. So we just show TensorFlow.js in the table. Since the model\\nconverted from Keras is compressed and saved as a binary file, the\\nsize can be greatly reduced, just about 1/7 of the model file in JSON.\\nModel loading time. We then compare the time spent on loading\\nthe model of different frameworks, as shown in Figure 2. For the\\nCPU backend, the loading time of different models of the same\\nframework is proportional to the size of the model files described in\\nTable 3. However, the model loading time of different frameworks\\nis significantly different. ConvNetJS is the fastest. Model loading\\ntime of brain.js, TensorFlow.js and Keras.js are consistent in terms\\nof magnitude. Interestingly, the increase of loading time of Con-\\nvNetJS, brain.js and synaptic is particularly noticeable when the\\nwidth increases. The result is caused by their choice of using JSON\\nto encode models. The model loading time of synaptic is slowest\\namong all the frameworks, which are more than 100x to 1000x\\nlonger than ConvNetJS. The model loading time of TensorFlow.js\\nis almost unchanged regardless of the model size.\\nThe loading time on the GPU backend does not change much un-\\nder different model complexities. However, the difference is still sig-\\nnificant between different frameworks. TensorFlow.js is the fastest.\\nCompared with loading models on the CPU backend, Keras.js speeds\\nup loading large models, but the loading time of WebDNN is longer.\\nIn addition, it can be seen that there is no difference in the model\\nloading time between iGPU and nGPU.\\nWarmup time. Next, we examine the difference of warmup time\\non the GPU backend. As shown in Figure 3, Keras.js is still far ahead,\\nand can complete the warmup in 3ms on all tasks. Tensorflow.js is\\nthe second, and WebDNN is the worst. On the whole, the warmup\\ntime on iGPU backend is shorter than that on nGPU.\\nInference time. Figure 4 shows the average time of doing inference\\non one sample. Almost all the inference tasks can finish within 1.5ms\\n(except synaptic, of which the shortest is 6.68ms). In the range\\nof the model sizes we set, the powerful computation capability\\nof GPU does not make a difference. Among all the model sizes,\\nConvNetJS occupies all the first place, followed by WebDNN on\\nthe CPU backend. The inference time of WebDNN on the GPU\\nbackend is longer than the inference time on the CPU backend.\\nAs for TensorFlow.js, running on the CPU backend is faster for\\ninference on smaller models, while the GPU backend is faster for\\ninference on larger models. Inference times of Keras.js on the CPU\\nand GPU backend are basically the same.\\nWe can observe that for all the frameworks on the CPU backend,\\nthe inference time increases when the model becomes complex. In\\nparticular, when the width increases, the time increases sharply\\n(about two times as the model width doubles). Similar to the train-\\ning tasks, such a result also reflects that these frameworks do not\\noptimize the large-scale matrix operations in the process of forward\\npropagation on the CPU backend. TensorFlow.js and WebDNN on\\nFigure 3: Model warmup time (ms) on GPU under different model complexities. The y-axis is on log scale.\\nFigure 4: Average inference time (ms) on one sample under different model complexities.\\nthe GPU backend do not exhibit this problem, but Keras.js on the\\nGPU still suffers from this problem.\\n4.4\\nTakeaway\\nBased on the above results, we can see that in small-scale fully-\\nconnected neural network which the browser is capable of, Con-\\nvNetJS performs the best for both training and inference. However,\\nsince ConvNetJS is no longer maintained and has fewer functional-\\nities, developers may need to choose some alternatives.\\nTensorflow.js is the only framework that can take advantage\\nof GPU to accelerate training processes. It is feature-rich and has\\ncomparable performance with ConvNetJS. So TensorFlow.js is a\\ngood choice for both training and inference. We do not recommend\\nusing GPU as the backend on small models because the advantage\\nof GPU’s computation power is not fully exploited.\\nFinally, we are interested in why ConvNetJS has the best perfor-\\nmance for all the tasks among these frameworks. Given the same\\nmodel of which the process logic is the same, the performance\\ndifference is likely to be accounted by the different implementation\\ndetails. To this end, we compare the function call stack of ConvNetJS\\nwith that of TensorFlow.js when doing the same training task. It is\\nsurprising to find that the depth of the call stack of ConvNetJS is\\nonly 3 while TensorFlow.js is 48! Such a result suggests that one\\npossible reason for the performance difference among different\\nframeworks is the deep call stack that costs a lot of computation\\nresources.\\n5\\nCOMPARISON WITH NATIVE\\nFRAMEWORK\\nIn this section, we study the third research question, i.e., how big\\nis the performance gap between running DL in the browser and\\non the native platform? To this end, we compare the performance\\nof TensorFlow.js and the native TensorFlow in Python, both of\\nwhich are released and maintained by Google and have similar\\nAPIs, making the comparison fair enough.\\nWe study the performance gap from two aspects. On one hand,\\nwe leverage well-known pre-trained models to compare the perfor-\\nmance when running inference tasks on TensorFlow.js and native\\nTensorFlow. On the other hand, we use decision tree analysis to\\ndistinguish the factors contributing to the performance gap. We\\nuse the same laptop as the one used in the experiments of the last\\nsection. We install the latest TensorFlow in Python (version 1.11.0)\\non the laptop.\\n5.1\\nInference Based on Pre-Trained Models\\nWe use the pre-trained models officially provided by the Keras to\\nmeasure the performance of TensorFlow.js and native TensorFlow\\nwhen doing inference tasks on these classical models.\\n5.1.1\\nLimitations of TensorFlow.js and browser constraints. Keras\\nofficially provides 11 pre-trained models. Although these models\\ncan work using native TensorFlow, we encountered a series of errors\\nwhen we run them using TensorFlow.js in the browser. These errors\\nTable 4: Selected Keras pre-trained models.\\nModel Name\\nPre-trained\\nModel Size\\nTrainable\\nParameters\\nComputation\\n(FLOPs)\\nMobileNetV2\\n14MB\\n3.5M\\n7.2M\\nDenseNet121\\n33MB\\n8.0M\\n16.3M\\nXception\\n88MB\\n22.9M\\n46.0M\\nInceptionV3\\n92MB\\n23.8M\\n47.8M\\nResNet50\\n99MB\\n25.6M\\n51.4M\\nFigure 5: Inference time on pre-trained Keras models. The\\ny-axis is on log scale.\\nimply the limitations of TensorFlow.js itself as well as constraints\\nimposed by the browser.\\nFor the model of NasNet Large, the browser throws out the error\\nmessage “truncatedNormal is not a valid Distribution”. For the\\nmodel of ResNet V3, the browser throws out the error message\\n“Unknown layer: Lambda”. The reason for these two errors is that\\nTensorFlow.js is still under development and so far has offered only\\na limited number of support for the converted model. Many user-\\ndefined operations are not supported by TensorFlow.js, e.g., models\\nwith control flow operations in RNNs are not yet supported.\\nWhen we try to use VGG16 or VGG19, the browser throws out\\nthe error message “GL OUT OF MEMORY”, meaning that the GPU\\nmemory is overfilled. The reason is that the VGG16 model applies\\nfor more than 1GB GPU memory. However, it should not be an\\nissue since the GPU memory of our experiment laptop is 8GB. As a\\nresult, such an error is due to the browser constraints.\\nAfter trying all the models, we finally have 5 models that can\\nbe correctly converted and run on the browser. The information\\nof these models are listed in Table 4. The number of trainable pa-\\nrameters is obtained by the build-in summary() method of tensor-\\nflow.keras, and the computation complexity (Floating Operations)\\nare obtained by tensorflow.propfiler.profile() method.\\n5.1.2\\nResults. Figure 5 shows the inference time for each model.\\nIt can be seen that the inference time of TensorFlow.js on nGPU\\nis comparable (1x-2x slower) to native TensorFlow’s. The most en-\\ncouraging result is that the performance of TensorFlow.js on the\\niGPU backend is better than that of native TensorFlow on the CPU\\nbackend. This result is not surprising considering the computation\\ncapability of integrated graphics card and CPU. However, since\\ntraditional native DL frameworks do not support integrated graph-\\nics card for acceleration, DL tasks can benefit a lot from browsers\\nin such a case with the help of integrated graphics card that is\\ncommon on current devices.\\nUnder the real-time requirement of client-side DL, if users want\\nto achieve the experience of 10FPS (frame per second), they need\\nTable 5: Contributing factors to the performance gap.\\nNetwork Type\\nFactor\\nRange\\nDNN\\nBackend\\nCPU, GPU\\nTask Type\\ntraining, inference\\nDepth\\n1, 2, 4, 8, 16\\nWidth\\n64, 128, 256, 512\\nCNN\\nBackend\\nCPU, GPU\\nTask Type\\ntraining, inference\\nDepth\\n6, 9, 15, 27\\nWidth\\n200, 400, 800\\nRNN\\nBackend\\nCPU, GPU\\nTask Type\\ntraining, inference\\nDepth\\n1, 2, 3\\nWidth\\n4, 8, 16, 32, 64, 256\\nto consider using a more powerful standalone graphics card. The\\nMobile Net model accelerated by iGPU can also meet the require-\\nment. If the requirement is 1FPS, iGPU is also fully capable. But if\\nonly CPU can be used, then these common models are too heavy\\nto run in browsers.\\n5.2\\nDecision Tree Analysis\\nIn order to deeply reveal how different factors of DL tasks influ-\\nence the performance gap between DL in browsers and on native\\nframeworks, we build a predictive model based on decision tree\\nanalysis to study the factor importance.\\n5.2.1\\nExperiment Setup. We consider 4 factors that influence the\\nperformance gap between DL in browsers and on native platforms\\nas shown in Table 5, including backend (CPU or GPU), task type\\n(training or inference), as well as depth and width that represent\\nthe model complexity. In the DNN and RNN models, width refers\\nto the number of neurons of each layer. In the CNN models, width\\nrefers to the number of kernels used in the convolution layer. For\\neach of DNN, CNN and RNN, we choose one model from the Ten-\\nsorflow.js official examples. The DNN and CNN models are used to\\nrecognize handwritten digits on the MNIST dataset, and the RNN\\nmodel is to perform text generation from Nietzsche’s writings. The\\nrange of depth and width is selected according to the values set in\\nTensorflow.js official examples.\\nIn our experiment, we build and run the DNN, CNN and RNN\\nmodels under different configurations using TensorFlow.js and na-\\ntive TensorFlow, respectively. Each configuration is a combination\\nof values for the factors above. We measure the execution time of\\neach configuration as the average time per batch for training tasks\\nand average time per sample for inference tasks on two platforms.\\nWe use the ratio of the execution time on TensorFlow.js over that\\non native TensorFlow to quantify the performance gap.\\n5.2.2\\nMethodology. We run the decision tree algorithm with sklearn [10]\\nto predict the ratio of execution time between TensorFlow.js and\\nnative TensorFlow. The decision tree depicts the relative impor-\\ntance of contributing factors. Intuitively, factors close to the root\\nof the decision tree affect the time ratio more than those near the\\nleaves. This is because the decision tree chooses to do the splitting\\nof the nodes according to the Entropy-Information Gain criterion.\\n>192\\n<=3\\n<=12\\n>12\\nCPU\\ntraining\\n<=192\\n<=192\\n<=12\\n>192\\n<=192\\n>3\\n<=1.5 >1.5\\n<=384\\n>384\\n<=12\\n>12\\n>12\\n>192\\ntraining\\ninference\\ninference\\nGPU\\n5.2\\n35.3\\n20.9\\n28.3\\n4.8\\n7.6\\n3.7\\n4.4\\n1.6\\n2.1\\n3.1\\n3.4\\n44.7\\nBackend\\nTask\\nWidth\\nWidth\\nWidth\\nDepth\\nDepth\\nWidth\\nTask\\nDepth\\nDepth\\nDepth\\nDepth\\n(a) DNN\\n>600\\n>7.5\\n>12\\n<=7.5\\n<=7.5 >7.5\\nCPU\\ntraining\\n<=600\\n<=600\\n<=600\\n<=21\\n>7.5\\n<=12\\n<=7.5\\n>21\\n<=12\\n<=600\\n>600\\n>600\\n>12\\n>600\\ntraining\\ninference\\ninference\\nGPU\\n7.3\\nDepth\\n609.2\\n334.0\\n2268.4\\n81.6\\n4.9\\n53.9\\n6.0\\n1.5\\n1.0\\n1.7\\n5.5\\n4.8\\n14.6\\nBackend\\nTask\\nWidth\\nWidth\\nDepth\\nWidth\\nDepth\\nWidth\\nDepth\\nTask\\nDepth\\nDepth\\n(b) CNN\\n>96\\n>1.5\\n>1.5\\n<=192\\nCPU\\ntraining\\n\\x03\\x04\\x02\\x01\\n<=48\\n<=1.5\\n<=96\\n>192\\n<=1.5\\n<=1.5\\n>96\\n<=1.5\\n>1.5\\n>1.5\\n>48\\ntraining\\ninference\\ninference\\nGPU\\n4.9\\n119.7\\n30.8\\n42.4\\n7.8\\n4.5\\n2.3\\n2.6\\n3.3\\n2.0\\n2.9\\n7.6\\nBackend\\nTask\\nWidth\\nWidth\\nWidth\\nDepth\\nDepth\\nWidth\\nTask\\nDepth\\nDepth\\nDepth\\nDepth\\n(c) RNN\\nFigure 6: Decision tree to analyze the time ratio of TensorFlow.js over native TensorFlow on DNN, CNN, and RNN Models.\\nIn other words, the decision tree places the important factors near\\nthe root to gain the best prediction of splits.\\nBased on the results, we first produce a fully grown and unpruned\\ndecision tree for all the factors. In this way, each leaf contains only\\none configuration. Then we set the depth of the tree to the number\\nof factors, in order to prevent using a factor several times on one\\npath. Figure 6 shows the decision trees for DNN, CNN, and RNN.\\n5.2.3\\nResults. The execution time of TensorFlow.js is longer than\\nnative TensorFlow in almost every configuration.\\nBackend is the most important factor contributing to the per-\\nformance gap. The ratio of execution time on the CPU backend is\\nmuch higher than that on the GPU backend. For example, the ratio\\ndecreases from 44.7 to 4.4 for training tasks when the DNN model\\nwith depth over 3 and width over 192 runs on the GPU backend\\ninstead of on the CPU backend. The extreme case happens on the\\nCNN. On the CPU backend, there is a wide range of the ratio from\\nbelow 5 to over 2200 (when depth is less than 7.5 and width is over\\n600). However, when doing inference task on the GPU backend\\nwith depth over 12 and width over 600, TensorFlow.js performs\\nas fast as native TensorFlow. This is because CNN makes use of\\nthe powerful computation capability of GPU when the model is\\nlarge enough, yet not exceeding the upper bound of the browser\\nmemory.\\nThe second most important factor is task type for all the three\\nmodels. Performing training tasks exhibits a higher ratio, while the\\nperformance gap on inference tasks is small. For example, for the\\nDNN model on the CPU backend, training tasks of TensorFlow.js\\nperform 33.9 times slower than native TensorFlow on average, and\\ninference tasks of TensorFlow.js perform 5.8 times slower than\\nnative TensorFlow on average.\\nThe decision trees of DNN and RNN both suggest that the im-\\nportance of depth and width depends on which backend the task\\nis taken on. On the CPU backend, the importance of width out-\\nweighs that of depth, while depth plays a more important role on\\nthe GPU backend. However, in the case of CNN, width plays a more\\nimportant role to the performance gap than depth for training tasks.\\n6\\nIMPLICATIONS\\nTable 6 summarizes the findings and implications of our study.\\nSpecifically, we draw implications for three stakeholders of DL\\nin browsers: application developers, DL-framework vendors, and\\nbrowser vendors. For application developers, we give recommen-\\ndations on how to choose frameworks for DL in browsers, how\\nto optimize the model, as well as how to select the backend. For\\nDL-framework vendors, we present some advice on encoding of\\nmodel files and optimizing the call stack. For browser vendors, we\\nsuggest on the utilization of system resources.\\n7\\nRELATED WORK\\nTo the best of our knowledge, this paper is the first study to char-\\nacterize the DL in browsers. So we survey related work on general\\nclient-side DL and performance measurement of DL systems.\\n7.1\\nClient-side Deep Learning\\nWith the emphasis on privacy, personalization and timely response,\\nit is a trend to conduct DL directly on the clients where the data\\nis generated, especially on mobile devices. Lane et al. [27] studied\\nthe feasibility of using DL for typical mobile sensing tasks, such\\nas activity recognition. Yao et al. [40] proposed DeepSense, a uni-\\nfied DL framework for processing time-series mobile sensing data.\\nDespite of the increasing computation power of mobile devices,\\ntypical DL tasks are still of heavy workload for these resource-\\nconstraint devices. Several optimization methods were proposed\\nto improve the performance of client-side DL. One line of work\\nfocuses on the DL models. Han et al. [22] proposed deep compres-\\nsion to compress the DNN through a three-stage method: pruning,\\ntrained quantization and Huffman coding, which showed a consid-\\nerable reduction in terms of the storage requirements of DNNs. The\\nother line of work leverages the cloud and edge environment to\\noffload the computation-intensive tasks to powerful computation\\nnodes [29]. Kang et al. [26] proposed Neurosurgeon, a lightweight\\nscheduler to automatically partition DNN computation between\\nmobile devices and data centers at the granularity of neural net-\\nwork layers. Wang et al. [39] designed Arden, a cloud-based deep\\nlearning framework for mobile devices. The framework partitions\\nthe DNN and offloads the resource-hungry training and complex\\ninferences tasks to the cloud.\\nAnther usage scenario of client-side DL is to support the dis-\\ntributed deep learning. Teerapittayanon et al. [37] proposed dis-\\ntributed deep neural networks (DDNNs) over distributed computing\\nhierarchies, consisting of the cloud, the edge (fog) and end devices.\\nIchinose et al. [24] proposed a pipelined method for distributed\\nDL processing between mobile devices and the cloud to reduce\\nthe amount of data sent to the cloud and protect the privacy of\\nusers. Meeds et al. [33] designed MLitB, a prototype DL frame-\\nwork capable of performing large-scale distributed computing with\\nheterogeneous classes of devices using Web browsers.\\nTable 6: Major findings and implications of DL in browsers.\\nNo.\\nName\\nFinding\\nImplication\\nStakeholder\\n1\\nSpecific DL Tasks\\nSupport\\nFrameworks supporting DL in browsers are emerging and being actively\\nmaintained. Most of them are not for general purpose and support only a\\nspecific subset of DL tasks.\\nIt is better for developers to use general-purpose DL\\nframeworks like TensorFlow.js to implement their DL-\\npowered Web applications.\\nApplication\\nDeveloper\\n2\\nModel Complex-\\nity\\nThe width of DL models dominates the performance variation of both\\ntraining and inference tasks considering the complexity of DL models.\\nDevelopers should pay attention to the width of their\\nmodels, and balance the width and required perfor-\\nmance if possible.\\nApplication\\nDeveloper\\n3\\nModel Loading\\nFor inference tasks, loading and warming up the DL model accounts for\\nmuch longer time than running the inference task itself. The warmup\\ntime on the integrated graphics card is generally shorter than that on the\\nstandalone graphics card.\\nDevelopers should pre-load and warm up the model\\nbefore using it for inference.\\nApplication\\nDeveloper\\n4\\nBenefits\\nfrom\\nGPU\\nFor popular pre-trained models like MobileNet and Inception, TensorFlow.js\\nhas comparable performance with native TensorFlow when running infer-\\nence on the standalone graphics card.\\nIt is possible to develop Web applications rather than\\nnative applications for these tasks.\\nApplication\\nDeveloper\\n5\\nBenefits from In-\\ntegared Graphics\\nCard\\nTensorFlow.js running on the integrated graphics card works better than\\nnative TensorFlow running on CPU backend.\\nFor devices without standalone GPUs, developers can\\nuse the browser for DL tasks, leveraging integrated\\ngraphics card for acceleration.\\nApplication\\nDeveloper\\n6\\nModel File Encod-\\ning and Size\\nModel file encoded in JSON is much bigger (7x) in size than that encoded\\nin binary, and significantly increases the model loading time.\\nIt is better to encode DL models in binary files.\\nDL-\\nFramework\\nVendor\\n7\\nFramework Call\\nStack\\nThe call stack of TensorFlow.js is much deeper than that of ConvNetJS,\\npulling down the performance.\\nFramework vendors could leverage compiler optimiza-\\ntion techniques to reduce the call stack when the DL\\nmodels are used in the production environment.\\nDL-\\nFramework\\nVendor\\n8\\nSystem Resource\\nUtilization\\nThe capability of multi-core CPU cannot be utilized when running DL tasks\\non the CPU backend in browsers since the JavaScript program is single-\\nthreaded. GPU memory usage is limited in 1GB, failing to load and run\\nlarger models.\\nJavaScript engine should take into account the support\\nof multi-process or scheduling among multi cores for\\nbetter performance of DL tasks in browsers. The GPU\\nmemory should be configurable for DL tasks.\\nBrowser\\nVendor\\n7.2\\nPerformance Measurement of Deep\\nLearning\\nIn recent years, researchers have conducted studies to measure the\\nperformance for various kinds of deep learning tasks. Liu et al. [30]\\nevaluated the performance of leading DL methods for object detec-\\ntion. Guignard et al. [21] presented detailed characterization results\\nof a set of archetypal state-of-the-art DL workloads to identify the\\nperformance bottlenecks and to guide the design of prospective\\nacceleration platforms in a more effective manner. Shi et al. [36]\\nevaluated the performance of four state-of-the-art distributed DL\\nframeworks over different GPU hardware environments. They built\\nperformance models of standard processes in training DNNs with\\nSGD, and then benchmark the performance of the frameworks with\\nthree neural networks (i.e., AlexNet, GoogleNet and ResNet-50). As\\nfor DL on mobile devices, Ignatov et al. [25] studied state-of-the-art\\nDL in the Android ecosystem and described available frameworks,\\nprogramming models and the limitations of running AI on smart-\\nphones.\\nAlthough many JavaScript-based frameworks have been pub-\\nlished to support DL in browsers, there is no comprehensive study to\\nunderstand their characteristics and performance. Some researchers\\nfocus on the possibility of supporting DL in browsers by measur-\\ning the low-level browser capabilities. Malle et al. [31] presented a\\ncomparison study between native code and different browser-based\\nimplementations: JavaScript, ASM.js as well as WebAssembly on a\\nrepresentative mix of algorithms. However, these algorithms are not\\nDL tasks. Their goal is just to show that the browsers performance\\nis now comparable to and even exceeds native binary performance.\\n8\\nCONCLUSION\\nThis paper made the first study on understanding the feasibility and\\nperformance of deep learning in Web browsers. We chose 7 recently\\nemerging JavaScript-based DL frameworks and comprehensively\\nrevealed which type of DL tasks have been supported. We measured\\nthe performance of different frameworks when doing different DL\\ntasks in browsers, and compared with the native DL framework to\\ninvestigate the performance gap. Although the in-browser DL is\\nstill at the early stage, some interesting findings, e.g., the compara-\\nble performance of JavaScript frameworks to that of native ones on\\nsome types of DL tasks and the benefits gained from the integrated\\ngraphics card, can be useful and help guide the DL-powered Web\\napplications. Additionally, we have also found that there are some\\npotential space of improvement for currently in-browser DL frame-\\nworks, and plan to realize some practical solutions. We believe that\\nour work can shed a light on the future of Web applications in the\\nAI era.\\nACKNOWLEDGMENTS\\nThis work was supported by the National Key R&D Program of\\nChina under the grant number 2018YFB1004800, the National Natu-\\nral Science Foundation of China under the grant number 61725201,\\nthe Beijing Municipal Science and Technology Project under the\\ngrant number Z171100005117002, and China Postdoctoral Science\\nFoundation.\\nREFERENCES\\n[1] 2018. brain.js. https://github.com/BrainJS.\\n[2] 2018. Caffe. http://caffe.berkeleyvision.org/.\\n[3] 2018. CNTK. https://www.microsoft.com/en-us/cognitive-toolkit/.\\n[4] 2018. ConvNetJS. https://cs.stanford.edu/people/karpathy/convnetjs/.\\n[5] 2018. Keras.js. https://github.com/transcranial/keras-js.\\n[6] 2018.\\nMIL WebDNN Benchmark.\\nhttps://mil-tokyo.github.io/webdnn/\\n#benchmar.\\n[7] 2018. Mind. https://github.com/stevenmiller888/mind.\\n[8] 2018. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/\\nmnist/.\\n[9] 2018. MorphCast. https://www.morphcast.com/.\\n[10] 2018. Sklearn DecisionTreeRegressor. https://scikit-learn.org/stable/modules/\\ngenerated/sklearn.tree.DecisionTreeRegressor.html.\\n[11] 2018. synaptic.js. https://github.com/cazala/synaptic.\\n[12] 2018. Teachable Machine. https://teachablemachine.withgoogle.com/.\\n[13] 2018. TensorFlow Playgournd. http://playground.tensorflow.org.\\n[14] 2018. TensorFlow.js. https://js.tensorflow.org/.\\n[15] 2018. WebDNN. https://github.com/mil-tokyo/webdnn.\\n[16] 2018. WebGL. https://www.khronos.org/webgl/.\\n[17] 2018. WebGPU. https://www.w3.org/community/gpu/.\\n[18] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manju-\\nnath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\\nYuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale\\nMachine Learning. In Proceedings of the 12th USENIX Symposium on Operating\\nSystems Design and Implementation, (OSDI). 265–283.\\n[19] Michael Auer. 2012. Real-time Web GIS Analysis Using WebGL. International\\nJournal of 3-D Information Modeling (IJ3DIM) 1, 3 (2012), 49–61.\\n[20] Bijin Chen and Zhiqi Xu. 2011. A Framework for Browser-Based Multiplayer\\nOnline Games Using WebGL and WebSocket. In Proceedings of 2011 International\\nConference on Multimedia Technology (ICMT). 471–474.\\n[21] Mauricio Guignard, Marcelo Schild, Carlos S. Bederián, Nicolás Wolovick, and\\nAugusto J. Vega. 2018. Performance Characterization of State-Of-The-Art Deep\\nLearning Workloads on an IBM \"Minsky\" Platform. In Proceedings of the 51st\\nHawaii International Conference on System Sciences, HICSS 2018.\\n[22] Song Han, Huizi Mao, and William J. Dally. 2015. Deep Compression: Compress-\\ning Deep Neural Network with Pruning, Trained Quantization and Huffman\\nCoding. CoRR abs/1510.00149 (2015). http://arxiv.org/abs/1510.00149\\n[23] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A Fast Learning\\nAlgorithm for Deep Belief Nets. Neural computation 18, 7 (2006), 1527–1554.\\n[24] Ayae Ichinose, Atsuko Takefusa, Hidemoto Nakada, and Masato Oguchi. 2018.\\nPerformance Evaluation of Pipeline-Based Processing for the Caffe Deep Learning\\nFramework. IEICE Transactions 101-D (2018), 1042–1052.\\n[25] Andrey Ignatov, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley,\\nand Luc Van Gool. 2018. AI Benchmark: Running Deep Neural Networks on\\nAndroid Smartphones. In Computer Vision - ECCV 2018 Workshops. 288–314.\\n[26] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor N. Mudge,\\nJason Mars, and Lingjia Tang. 2017. Neurosurgeon: Collaborative Intelligence\\nBetween the Cloud and Mobile Edge. In Proceedings of the Twenty-Second Inter-\\nnational Conference on Architectural Support for Programming Languages and\\nOperating Systems, (ASPLOS). 615–629.\\n[27] Nicholas D Lane and Petko Georgiev. 2015. Can Deep Learning Revolutionize\\nMobile Sensing?. In Proceedings of the 16th International Workshop on Mobile\\nComputing Systems and Applications (HotMobile). 117–122.\\n[28] Yann LeCun et al. 1989. Generalization and Network Design Strategies. Connec-\\ntionism in perspective (1989), 143–155.\\n[29] Weiqing Liu, Jiannong Cao, Lei Yang, Lin Xu, Xuanjia Qiu, and Jing Li. 2017.\\nAppBooster: Boosting the Performance of Interactive Mobile Applications with\\nComputation Offloading and Parameter Tuning. IEEE Transactions on Parallel\\nand Distributed Systems 28, 6 (2017), 1593–1606.\\n[30] Y. Liu, P. Sun, M. R. Highsmith, N. M. Wergeles, J. Sartwell, A. Raedeke, M.\\nMitchell, H. Hagy, A. D. Gilbert, B. Lubinski, and Y. Shang. 2018. Performance\\nComparison of Deep Learning Techniques for Recognizing Birds in Aerial Images.\\nIn 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC).\\n317–324.\\n[31] Bernd Malle, Nicola Giuliani, Peter Kieseberg, and Andreas Holzinger. 2018.\\nThe Need for Speed of AI Applications: Performance Comparison of Native\\nvs. Browser-based Algorithm Implementations. CoRR abs/1802.03707 (2018).\\nhttp://arxiv.org/abs/1802.03707\\n[32] Chris Marrin. 2011. WebGL specification. Khronos WebGL Working Group (2011).\\n[33] Edward Meeds, Remco Hendriks, Said Al Faraby, Magiel Bruntink, and Max\\nWelling. 2015. MLitB: machine learning in the browser. PeerJ Computer Science 1\\n(2015), e11.\\n[34] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder P. Singh.\\n2015. Action-Conditional Video Prediction using Deep Networks in Atari Games.\\nIn NIPS.\\n[35] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning\\nrepresentations by back-propagating errors. nature 323, 6088 (1986), 533.\\n[36] Shaohuai Shi, Qiang Wang, and Xiaowen Chu. 2018. Performance Modeling and\\nEvaluation of Distributed Deep Learning Frameworks on GPUs. In Proceedings of\\nIEEE DASC/PiCom/DataCom/CyberSciTech 2018. 949–957.\\n[37] Surat Teerapittayanon, Bradley McDanel, and HT Kung. 2017. Distributed Deep\\nNeural Networks over the Cloud, the Edge and End Devices. In Proceedings of\\nIEEE 37th International Conference on Distributed Computing Systems (ICDCS).\\n328–339.\\n[38] Aaron Tucker, Adam Gleave, and Stuart Russell. 2018. Inverse Reinforcement\\nLearning for Video Games. arXiv preprint arXiv:1810.10593 (2018).\\n[39] Ji Wang, Jianguo Zhang, Weidong Bao, Xiaomin Zhu, Bokai Cao, and Philip S.\\nYu. 2018. Not Just Privacy: Improving Performance of Private Deep Learning in\\nMobile Cloud. In Proceedings of the 24th ACM SIGKDD International Conference\\non Knowledge Discovery & Data Mining, (KDD). 2407–2416.\\n[40] Shuochao Yao, Shaohan Hu, Yiran Zhao, Aston Zhang, and Tarek Abdelzaher.\\n2017. DeepSense: A Unified Deep Learning Framework for Time-Series Mobile\\nSensing Data Processing. In Proceedings of the 26th International Conference on\\nWorld Wide Web. 351–360.\\n',\n",
       " '1908.02130v1.pdf': 'Deep learning research landscape & roadmap in a nutshell:\\npast, present and future - Towards deep cortical learning\\nAras R. Dargazany\\nDecember 30, 2021\\nAbstract\\nThe past, present and future of deep learning is presented in this work. Given this landscape\\n& roadmap, we predict that deep cortical learning will be the convergence of deep learning &\\ncortical learning which builds an artiﬁcial cortical column ultimately.\\n1\\nPast: Deep learning inspirations\\nDeep learning horizon, landscape and research roadmap in nutshell is presented in this ﬁgure 1.\\nThe historical development and timeline of deep learning & neural network is separately illustrated\\nFigure 1: Deep learning research landscape & roadmap: past, present, future.\\nThe future is\\nhighlighted as deep cortical learning.\\nin ﬁgure 2. The Origin of neural nets [WR17] is thoroughly reviewed in terms of the evolutionary\\nhistory of deep learning models. Vernon Mountcastle discovery of cortical columns in somatosen-\\nsory cortex [Mou97] was a breakthrough in brain science. The big bang was the discovery of Hubel\\n& Wiesel of simple cells and complex cell in visual cortex [HW59] which won the Nobel prize for\\nthis discovery in 1981. This work was heavily founded on Vernon Mountcastle discovery of cortical\\ncolumns in somatosensory cortex [Mou97]. After the discovery of Hubel & Wiesel, Fukushima\\nproposed a pattern recognition architecture based on the simple cell and complex cell discovery,\\nknown as NeoCognitron [FM82]. In this work, a deep neural network was proposed using simple\\ncell layer and complex cell layer repeatedly. In 80s and maybe a bit earlier backpropagation have\\nbeen proposed by multiple people but the ﬁrst time it was well-explained and applied for learning\\nneural nets was done by Hinton and his colleagues in 1987 [RHW86].\\n1\\narXiv:1908.02130v1  [cs.NE]  30 Jul 2019\\nFigure 2: Neural nets origin, timeline & history made by Favio Vazquez\\n2\\nPresent: Deep learning by LeCun, Bengio and Hinton\\nConvolutional nets was invented by LeCun [LBD+89] which led to deep learning conspiracy which\\nalso started by the three founding fathers of the ﬁeld: LeCun, Bengio and Hinton [LBH15]. The\\nmain hype in deep learning happened in 2012 when the state-of-the-art result in Imagenet classi-\\nﬁcation and TIMIT speech recognition task were dramatically reduced using an end-to-end deep\\nconvolutional network [KSH12] and deep belief net [HDY+12].\\nThe power of deep learning is scalability and the ability to learn in an end-to-end fashion.\\nIn this sense, deep learning architectures are capable of learning big datasets such as Imagenet\\n[KSH12, GDG+17] and TIMIT using multiple GPUs in an end-to-end fashion meaning directly\\nfrom raw inputs, all the way the desired outputs. Alexnet [KSH12] used two GPUs for Imagenet\\nclassiﬁcation which is a very big dataset of images, almost 1.5 million images of size 215x215.\\nKaiming He et al. [GDG+17] proposed a highly scalable approach for training on Image using\\n256 GPUs for almost an hour which shows an amazingly powerful approach based stochastic\\ngradient descent for applying big cluster of GPUs on huge datasets. Very many application domains\\nhave been revolutionized using deep learning architectures such as image classiﬁcations [KSH12],\\nmachine translation [WSC+16, JSL+16], speech recognition [HDY+12], and robotics [MKS+15].\\nThe Nobel Prize in Physiology or Medicine 2014 was given to John O’Keefe, May-Britt Moser\\nand Edvard I. Moser “for their discoveries of cells that constitute a positioning system in the\\nbrain.” [Bur14].\\nThis study of cognitive neuroscience shed light on how the world is repre-\\nsented within the brain. Hinton’s Capsule network [SFH17] and Hawkins’ cortical learning al-\\ngorithm [HAD11] are highly inspired by this Nobel-prize winning work [Bur14].\\n3\\nFuture: Brain-plausible deep learning & cortical learning\\nalgorithms\\nThe main direction and inclination in the deep learning for future is the ability to bridge the gap\\nbetween the cortical architecture and deep learning architectures, speciﬁcally convolutional nets.\\nIn this quest, Hinton proposed capsule network [SFH17] as an eﬀort to get rid of pooling layers\\nand replace it with capsules which are highly inspired bu cortical mini-columns in cortical columns\\nand layers and include the location information or pose information of parts.\\nAnother important quest in deep learning is understanding the biological root of learning in our\\nbrain, speciﬁcally in our cortex. Backpropagation is not biologically inspired and plausible. Hinton\\nand the other founding fathers of deep learning have been trying to understand how backprop\\n2\\nmight be feasible biologically in brain. Feedback alignment [LCTA16] and spike time-dependent\\nplasticity or STDP-based backprop [BSR+18] are some of the works which have been done by\\nTimothy Lillicrap, Blake Richards, and Hinton in order to model backprop biologically based on\\nthe pyramidal neuron in the cortex.\\nIn the far future, the main goal should be the merge of two very independent quest to build\\ncortical structure in our brain: The ﬁrst one is heavily target by the big and active deep learning\\ncommunity; The second one is targeted independently and neuroscientiﬁcally by Numenta and\\nGeoﬀHawkins [HAD11]. These people argue that the cortical structure and our neocortex is the\\nmain source of our intelligence and for building a true intelligent machine, we should be able to\\nreconstruct the cortex and to do so, we should ﬁrst focus more on the cortex and understand what\\ncortex is made out of.\\n4\\nFinale: Deep cortical learning as the merge of deep learning\\nand cortical learning\\nBy merging deep learning and cortical learning, a very more focused and detailed architectures,\\nnamed deep cortical learning might be created. We might be able to understand and reconstruct\\nthe cortical structure with much more accuracy and have a better idea what the true intelligence is\\nand how artiﬁcial general intelligence or AGI might be reproducible. Deep cortical learning might\\nbe the algorithm behind one cortical column in the neocortex.\\nReferences\\n[BSR+18]\\nSergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoﬀrey E Hinton,\\nand Timothy Lillicrap. Assessing the scalability of biologically-motivated deep learning\\nalgorithms and architectures. In Advances in Neural Information Processing Systems,\\npages 9368–9378, 2018.\\n[Bur14]\\nNeil Burgess. The 2014 nobel prize in physiology or medicine: a spatial model for\\ncognitive neuroscience. Neuron, 84(6):1120–1125, 2014.\\n[FM82]\\nKunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network\\nmodel for a mechanism of visual pattern recognition. In Competition and cooperation\\nin neural nets, pages 267–285. Springer, 1982.\\n[GDG+17] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo\\nKyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch\\nsgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\\n[HAD11]\\nJeﬀ\\nHawkins,\\nSubutai\\nAhmad,\\nand\\nD\\nDubinsky.\\nHierarchi-\\ncal\\ntemporal\\nmemory\\nincluding\\nhtm\\ncortical\\nlearning\\nalgorithms.\\nTechnical\\nreport,\\nNumenta,\\nInc.\\nhttp://www.numenta.com/htm-\\noverview/education/HTM/CorticalLearningAlgorithms.pdf, 2011.\\n[HDY+12] Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep\\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep\\nneural networks for acoustic modeling in speech recognition: The shared views of four\\nresearch groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.\\n[HW59]\\nDavid H Hubel and Torsten N Wiesel. Receptive ﬁelds of single neurones in the cat’s\\nstriate cortex. The Journal of physiology, 148(3):574–591, 1959.\\n[JSL+16]\\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng\\nChen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al.\\nGoogle’s multilingual neural machine translation system: Enabling zero-shot trans-\\nlation. arXiv preprint arXiv:1611.04558, 2016.\\n[KSH12]\\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\\ndeep convolutional neural networks.\\nIn Advances in neural information processing\\nsystems, pages 1097–1105, 2012.\\n3\\n[LBD+89] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard,\\nWayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip\\ncode recognition. Neural computation, 1(4):541–551, 1989.\\n[LBH15]\\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\\nDeep learning.\\nNature,\\n521(7553):436–444, 2015.\\n[LCTA16] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Ran-\\ndom synaptic feedback weights support error backpropagation for deep learning. Nature\\ncommunications, 7:13276, 2016.\\n[MKS+15] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\\nOstrovski, et al. Human-level control through deep reinforcement learning. Nature,\\n518(7540):529–533, 2015.\\n[Mou97]\\nVernon B Mountcastle. The columnar organization of the neocortex. Brain: a journal\\nof neurology, 120(4):701–722, 1997.\\n[RHW86]\\nDavid E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representa-\\ntions by back-propagating errors. Nature, 323:533–536, 1986.\\n[SFH17]\\nSara Sabour, Nicholas Frosst, and Geoﬀrey E Hinton. Dynamic routing between cap-\\nsules. In Advances in Neural Information Processing Systems, pages 3859–3869, 2017.\\n[WR17]\\nHaohan Wang and Bhiksha Raj.\\nOn the origin of deep learning.\\narXiv preprint\\narXiv:1702.07800, 2017.\\n[WSC+16] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural\\nmachine translation system: Bridging the gap between human and machine translation.\\narXiv preprint arXiv:1609.08144, 2016.\\n4\\n',\n",
       " '2002.05658v1.pdf': '1 \\n \\nTen Research Challenge Areas in Data Science \\n \\nJeannette M. Wing \\nAvanessians Director of the Data Science Institute and Professor of Computer Science \\nColumbia University \\n \\nDecember 30, 2019 \\nAlthough data science builds on knowledge from computer science, mathematics, statistics, and other \\ndisciplines, data science is a unique field with many mysteries to unlock: challenging scientific questions \\nand pressing questions of societal importance. \\nIs data science a discipline? \\nData science is a field of study: one can get a degree in data science, get a job as a data scientist, and get \\nfunded to do data science research.  But is data science a discipline, or will it evolve to be one, distinct \\nfrom other disciplines?  Here are a few meta-questions about data science as a discipline. \\n• \\nWhat is/are the driving deep question(s) of data science?  Each scientific discipline (usually) has \\none or more “deep” questions that drive its research agenda: What is the origin of the universe \\n(astrophysics)?  What is the origin of life (biology)?  What is computable (computer science)?  \\nDoes data science inherit its deep questions from all its constituency disciplines or does it have \\nits own unique ones?   \\n• \\nWhat is the role of the domain in the field of data science?  People (including this author) (Wing, \\nJaneia, Kloefkorn, & Erickson 2018) have argued that data science is unique in that it is not just \\nabout methods, but about the use of those methods in the context of a domain—the domain of \\nthe data being collected and analyzed; the domain for which a question to be answered comes \\nfrom collecting and analyzing the data.  Is the inclusion of a domain inherent in defining the field \\nof data science?  If so, is the way it is included unique to data science? \\n• \\nWhat makes data science data science?  Is there a problem unique to data science that one can \\nconvincingly argue would not be addressed or asked by any of its constituent disciplines, e.g., \\ncomputer science and statistics?   \\nTen research areas \\nWhile answering the above meta-questions is still under lively debate, we can ask an easier question, \\none that also underlies any field of study: What are the research challenge areas that drive the study of \\ndata science?  Here is a list of ten.  They are not in any priority order, and some of them are related to \\neach other.  They are phrased as challenge areas, not challenge questions.  They are not necessarily the \\n2 \\n \\n“top ten” but they are a good ten to start the community discussing what a broad research agenda for \\ndata science might look like.1 \\n1. Scientific understanding of learning, especially deep learning algorithms.  As much as we admire \\nthe astonishing successes of deep learning, we still lack a scientific understanding of why deep \\nlearning works so well.  We do not understand the mathematical properties of deep learning \\nmodels.  We do not know how to explain why a deep learning model produces one result and \\nnot another.  We do not understand how robust or fragile they are to perturbations to input \\ndata distributions.  We do not understand how to verify that deep learning will perform the \\nintended task well on new input data.  Deep learning is an example of where experimentation in \\na field is far ahead of any kind of theoretical understanding. \\n2. Causal reasoning.  Machine learning is a powerful tool to find patterns and examine \\ncorrelations, particularly in large data sets. While the adoption of machine learning has opened \\nmany fruitful areas of research in economics, social science, and medicine, these fields require \\nmethods that move beyond correlational analyses and can tackle causal questions. A rich and \\ngrowing area of current study is revisiting causal inference in the presence of large amounts of \\ndata.  Economists are already revisiting causal reasoning by devising new methods at the \\nintersection of economics and machine learning that make causal inference estimation more \\nefficient and flexible (Athey, 2016), (Taddy, 2019).  Data scientists are just beginning to explore \\nmultiple causal inference, not just to overcome some of the strong assumptions of univariate \\ncausal inference, but because most real-world observations are due to multiple factors that \\ninteract with each other (Wang & Blei, 2018).  \\n3. Precious data.  Data can be precious for one of three reasons: the dataset is expensive to collect; \\nthe dataset contains a rare event (low signal-to-noise ratio); or the dataset is artisanal—small \\nand task-specific.   A good example of expensive data comes from large, one-of, expensive \\nscientific instruments, e.g., the Large Synoptic Survey Telescope, the Large Hadron Collider, the \\nIceCube Neutrino Detector at the South Pole.  A good example of rare event data is data from \\nsensors on physical infrastructure, such as bridges and tunnels; sensors produce a lot of raw \\ndata, but the disastrous event they are used to predict is (thankfully) rare.   Rare data can also \\nbe expensive to collect.  A good example of artisanal data is the tens of millions of court \\njudgments that China has released online to the public since 2014 (Liebman, Roberts, Stern, & \\nWang, 2017) or the 2+ million US government declassified documents collected by Columbia’s \\nHistory Lab (Connelly, Madigan, Jervis, Spirling, & Hicks, 2019).   For each of these different kinds \\nof precious data, we need new data science methods and algorithms, taking into consideration \\nthe domain and intended uses of the data. \\n4. Multiple, heterogeneous data sources.  For some problems, we can collect lots of data from \\ndifferent data sources to improve our models.  For example, to predict the effectiveness of a \\n                                                           \\n1 Based on an NSF workshop, the recent report by the statistics community, “Statistics at a Crossroads: \\nWho is for the Challenge?,” gives an overlapping list of seven foundational challenges in data science \\n(Berger et al., 2019). \\n \\n3 \\n \\nspecific cancer treatment for a human, we might build a model based on 2-D cell lines from \\nmice, more expensive 3-D cell lines from mice, and the costly DNA sequence of the cancer cells \\nextracted from the human. State-of-the-art data science methods cannot as yet handle \\ncombining multiple, heterogeneous sources of data to build a single, accurate model.  Since \\nmany of these data sources might be precious data, this challenge is related to the third \\nchallenge.  Focused research in combining multiple sources of data will provide extraordinary \\nimpact. \\n5. Inferring from noisy and/or incomplete data.  The real world is messy and we often do not have \\ncomplete information about every data point.  Yet, data scientists want to build models from \\nsuch data to do prediction and inference.  A great example of a novel formulation of this \\nproblem is the planned use of differential privacy for Census 2020 data (Garfinkel, 2019), where \\nnoise is deliberately added to a query result, to maintain the privacy of individuals participating \\nin the census. Handling “deliberate” noise is particularly important for researchers working with \\nsmall geographic areas such as census blocks, since the added noise can make the data \\nuninformative at those levels of aggregation. How then can social scientists, who for decades \\nhave been drawing inferences from census data, make inferences on this “noisy” data and how \\ndo they combine their past inferences with these new ones? Machine learning’s ability to better \\nseparate noise from signal can improve the efficiency and accuracy of those inferences. \\n6. Trustworthy AI.  We have seen rapid deployment systems using artificial intelligence (AI) and \\nmachine learning in critical domains such as autonomous vehicles, criminal justice, healthcare, \\nhiring, housing, human resource management, law enforcement, and public safety, where \\ndecisions taken by AI agents directly impact human lives. Consequently, there is an increasing \\nconcern if these decisions can be trusted to be correct, reliable, robust, safe, secure, and fair, \\nespecially under adversarial attacks. One approach to building trust is through providing \\nexplanations of the outcomes of a machine learned model.  If we can interpret the outcome in a \\nmeaningful way, then the end user can better trust the model.  Another approach is through \\nformal methods, where one strives to prove once and for all a model satisfies a certain property.  \\nNew trust properties yield new tradeoffs for machine learned models, e.g., privacy versus \\naccuracy; robustness versus efficiency. There are actually multiple audiences for trustworthy \\nmodels: the model developer, the model user, and the model customer.  Ultimately, for \\nwidespread adoption of the technology, it is the public who must trust these automated \\ndecision systems. \\n7. Computing systems for data-intensive applications.  Traditional designs of computing systems \\nhave focused on computational speed and power: the more cycles, the faster the application \\ncan run.  Today, the primary focus of applications, especially in the sciences (e.g., astronomy, \\nbiology, climate science, materials science), is data.  Also, novel special-purpose processors, e.g., \\nGPUs, FPGAs, TPUs, are now commonly found in large data centers. Even with all these data and \\nall this fast and flexible computational power, it can still take weeks to build accurate predictive \\nmodels; however, applications, whether from science or industry, want real-time predictions.  \\nAlso, data-hungry and compute-hungry algorithms, e.g., deep learning, are energy hogs \\n(Strubell, Ganesh, & McCallum, 2019).  Not only should we consider space and time, but energy \\nconsumption, in our performance metrics.  In short, we need to rethink computer systems \\ndesign from first principles, with data (not compute) the focus.  New computing systems designs \\n4 \\n \\nneed to consider: heterogeneous processing; efficient layout of massive amounts of data for fast \\naccess; the target domain, application, or even task; and energy efficiency. \\n8. Automating front-end stages of the data life cycle.  While the excitement in data science is due \\nlargely to the successes of machine learning, and more specifically deep learning, before we get \\nto use machine learning methods, we need to prepare the data for analysis.  The early stages in \\nthe data life cycle (Wing, 2019) are still labor intensive and tedious.  Data scientists, drawing on \\nboth computational and statistical methods, need to devise automated methods that address \\ndata cleaning and data wrangling, without losing other desired properties, e.g., accuracy, \\nprecision, and robustness, of the end model. One example of emerging work in this area is the \\nData Analysis Baseline Library (Mueller, 2019), which provides a framework to simplify and \\nautomate data cleaning, visualization, model building, and model interpretation.  The Snorkel \\nproject addresses the tedious task of data labeling (Ratner et al., 2018). \\n9. Privacy.  Today, the more data we have, the better the model we can build.  One way to get \\nmore data is to share data, e.g., multiple parties pool their individual datasets to build \\ncollectively a better model than any one party can build.  However, in many cases, due to \\nregulation or privacy concerns, we need to preserve the confidentiality of each party’s dataset.  \\nAn example of this scenario is in building a model to predict whether someone has a disease or \\nnot. If multiple hospitals could share their patient records, we could build a better predictive \\nmodel; but due to Health Insurance Portability and Accountability Act (HIPAA) privacy \\nregulations, hospitals cannot share these records. We are only now exploring practical and \\nscalable ways, using cryptographic and statistical methods, for multiple parties to share data \\nand/or share models to preserve the privacy of each party’s dataset.  Industry and government \\nare exploring and exploiting methods and concepts, such as secure multi-party computation, \\nhomomorphic encryption, zero-knowledge proofs, and differential privacy, as part of a point \\nsolution to a point problem. \\n10. Ethics.  Data science raises new ethical issues. They can be framed along three axes: (1) the \\nethics of data: how data are generated, recorded, and shared; (2) the ethics of algorithms: how \\nartificial intelligence, machine learning, and robots interpret data; and (3) the ethics of practices: \\ndevising responsible innovation and professional codes to guide this emerging science (Floridi & \\nTaddeo, 2016) and for defining Institutional Review Board (IRB) criteria and processes specific \\nfor data (Wing, Janeia, Kloefkorn, & Erickson 2018). Example ethical questions include how to \\ndetect and eliminate racial, gender, socio-economic, or other biases in machine learning models. \\nClosing remarks \\nAs many universities and colleges are creating new data science schools, institutes, centers, etc. (Wing, \\nJaneia, Kloefkorn, & Erickson 2018), it is worth reflecting on data science as a field.  Will data science as \\nan area of research and education evolve into being its own discipline or be a field that cuts across all \\nother disciplines?  One could argue that computer science, mathematics, and statistics share this \\ncommonality: they are each their own discipline, but they each can be applied to (almost) every other \\ndiscipline. What will data science be in 10 or 50 years? \\n \\n5 \\n \\nAcknowledgments \\nI would like to thank Cliff Stein, Gerad Torats-Espinosa, Max Topaz, and Richard Witten for their \\nfeedback on earlier renditions of this paper.  Many thanks to all Columbia Data Science faculty who have \\nhelped me formulate and discuss these tend (and other) challenges during our Fall 2019 DSI retreat.   \\nReferences \\nAthey, S. (2016). “Susan Athey on how economists can use machine learning to improve policy,”  \\nRetrieved from https://siepr.stanford.edu/news/susan-athey-how-economists-can-use-machine-\\nlearning-improve-policy \\nBerger, J., He, X., Madigan, C., Murphy, S., Yu, B., & Wellner, J. (2019), Statistics at a Crossroad: Who is \\nfor the Challenge? NSF workshop report.  Retrieved from https://hub.ki/groups/statscrossroad \\nConnelly, M., Madigan, D., Jervis, R., Spirling, A., & Hicks, R. (2019). The History Lab.  Retrieved from  \\nhttp://history-lab.org/ \\nFloridi, L. & Taddeo, M. (2016). What is Data Ethics? Philosophical Transactions of the Royal Society A, \\nvol. 374, issue 2083, December 2016. \\nGarfinkel, S. (2019). Deploying Differential Privacy for the 2020 Census of Population and Housing. \\nPrivacy Enhancing Technologies Symposium, Stockholm, Sweden.  Retrieved from \\nhttp://simson.net/ref/2019/2019-07-\\n16%20Deploying%20Differential%20Privacy%20for%20the%202020%20Census.pdf \\nLiebman, B.L., Roberts, M., Stern, R.E., & Wang, A. (2017). Mass Digitization of Chinese Court Decisions: \\nHow to Use Text as Data in the Field of Chinese Law. UC San Diego School of Global Policy and Strategy, \\n21st Century China Center Research Paper No. 2017-01; Columbia Public Law Research Paper No. 14-551. \\nRetrieved from https://scholarship.law.columbia.edu/faculty_scholarship/2039 \\nMueller, A. (2019). Data Analysis Baseline Library. Retrieved from \\nhttps://libraries.io/github/amueller/dabl \\nRatner, A., Bach, S., Ehrenberg, H., Fries, J., Wu, S, & Ré, C. (2018). Snorkel: Rapid Training Data Creation \\nwith Weak Supervision. Proceedings of the 44th International Conference on Very Large Data Bases.  \\nStrubell E., Ganesh, A., & McCallum, A. (2019),”Energy and Policy Considerations for Deep Learning in \\nNLP.  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL). \\nTaddy, M. (2019).  Business Data Science: Combining Machine Learning and Economics to Optimize, \\nAutomate, and Accelerate Business Decisions, Mc-Graw Hill. \\nWang, Y. & Blei, D.M. (2018). The Blessings of Multiple Causes, Retrieved from \\nhttps://arxiv.org/abs/1805.06826 \\n6 \\n \\nWing, J.M. (2019), The Data Life Cycle, Harvard Data Science Review, vol. 1, no. 1. \\n \\nWing, J.M., Janeia, V.P., Kloefkorn, T., & Erickson, L.C. (2018). Data Science Leadership Summit, \\nWorkshop Report, National Science Foundation.  Retrieved from \\nhttps://dl.acm.org/citation.cfm?id=3293458 \\n \\n',\n",
       " '2007.03606v1.pdf': '1\\nData Science: A Comprehensive Overview\\nLONGBING CAO, University of Technology Sydney, Australia\\nThe twenty-ﬁrst century has ushered in the age of big data and data economy, in which data DNA, which\\ncarries important knowledge, insights and potential, has become an intrinsic constituent of all data-based\\norganisms. An appropriate understanding of data DNA and its organisms relies on the new ﬁeld of data\\nscience and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz,\\nand data science is still in a very early phase, signiﬁcant challenges and opportunities are emerging or have\\nbeen inspired by the research, innovation, business, profession, and education of data science. This paper\\nprovides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from\\ndata analysis to data science, the data science concepts, a big picture of the era of data science, the major\\nchallenges and directions in data innovation, the nature of data analytics, new industrialization and service\\nopportunities in the data economy, the profession and competency of data education, and the future of data\\nscience. This article is the ﬁrst in the ﬁeld to draw a comprehensive big picture, in addition to offering rich\\nobservations, lessons and thinking about data science and analytics.\\nAdditional Key Words and Phrases: Big data, Data Analysis, Data Analytics, Advanced Analytics, Big Data\\nAnalytics, Data Science, Data Engineering, Data Scientist, Statistics, Computing, Informatics, Data DNA,\\nData Innovation, Data Economy, Data Industry, Data Service, Data Profession, Data Education\\nACM Reference Format:\\nLongbing Cao, 2016. Data Science: A Comprehensive Overview. ACM Computing Surveys 1, 1, Article 1\\n(January 2016), 42 pages.\\nDOI: http://dx.doi.org/10.1145/0000000.0000000\\n1. INTRODUCTION\\nWe are living in the age of big data, advanced analytics, and data science. The trend\\nof “big data growth” [Laney 2001; CSC 2012; Beyer and Laney 2012; McKinsey 2011;\\nVesset et al. 2012], or “data deluge” [Hey and Trefethen 2003], has not only triggered\\ntremendous hype and buzz, but more importantly presents enormous challenges which\\nin turn bring incredible innovation and economic opportunities. Big data has attracted\\nintensive and growing attention, initially from giant private data-oriented enterprise\\nand lately from major governmental organizations and academic institutions. Typical\\nexamples include large data-centric projects in Google, Facebook and IBM, and strate-\\ngic initiatives in the United Nations [UN 2010; USNSF 2012], EU [Commission 2014]\\nand China [Government 2015].\\nFrom the disciplinary development perspective, recognition of the signiﬁcant chal-\\nlenges, opportunities and values of big data is fundamentally reshaping the tradi-\\ntional data-oriented scientiﬁc and engineering ﬁelds. It is also reshaping those non-\\ntraditional data engineering domains such as social science, business and manage-\\nment [Yiu 2012; Labrinidis and Jagadish 2012; Chen et al. 2012; Khan et al. 2014].\\nThis reshaping and paradigm shifting is driven not just by data itself but all other as-\\nThis work is partially supported by the Australian Research Council Discovery Grant, under grant\\nDP130102691.\\nAuthor’s addresses: L. Cao, Advanced Analytics Institute, University of Technology Sydney, Australia.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted\\nwithout fee provided that copies are not made or distributed for proﬁt or commercial advantage and that\\ncopies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned\\nby others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or repub-\\nlish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request\\npermissions from permissions@acm.org.\\nc⃝2016 ACM. 1539-9087/2016/01-ART1 $15.00\\nDOI: http://dx.doi.org/10.1145/0000000.0000000\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\narXiv:2007.03606v1  [cs.CY]  1 Jul 2020\\n1:2\\nL. CAO\\npects that could be created, transformed and/or adjusted by understanding, exploring\\nand utilizing data.\\nThe above trend and its potential have triggered new debate about data-intensive\\nscientiﬁc discovery as a new paradigm, the so-called “fourth science paradigm”, which\\nuniﬁes experiment, theory and computation (corresponding to “empirical” or “experi-\\nmental”, “theoretical” and “computational” science) [Gray 2007; Hey et al. 2009]. Data\\nis regarded as the new Intel Inside [O’Reilly 2005], or the new oil and strategic as-\\nset, and drives or even determines the future of science, technology, the economy, and\\npossibly everything in our world today and tomorrow.\\nIn 2005 in Sydney, we were asked a critical question at a brainstorming meeting\\nabout data science and data analytics by several local industry representatives from\\nmajor analytics software vendors: “Information science has been there for so long, why\\ndo we need data science?” Related fundamental questions often discussed in the com-\\nmunity include “What is data science?” [Loukides 2012], and “Is data science old wine\\nin new bottles?” [Agarwal and Dhar 2014]. Data science and relevant topics have be-\\ncome the key concern in panel discussions at conferences in statistics, data mining, and\\nmachine learning, and more recently in big data, advanced analytics, and data science.\\nTypical topics such as “grand challenges in data science”, “data-driven discovery”, and\\n“data-driven science” have frequently been visited and continue to attract wide and\\nincreasing attention and debate. These questions are mainly posted from research and\\ndisciplinary development perspectives; while there are many other important ques-\\ntions, such as those relating to data economy and competency, these perspectives are\\nless well considered in the conferences referred to above.\\nA fundamental trigger for the above questions and many others not mentioned here\\nis the exploration of new or more complex challenges and opportunities [Jagadish et al.\\n2014; Cao 2016b; 2010a; Khan et al. 2014] in data science and engineering. Such chal-\\nlenges and opportunities apply to existing ﬁelds, including statistics and mathematics,\\nartiﬁcial intelligence, and other relevant disciplines and domains that have never been\\naddressed, or have not been adequately addressed, in the classic methodologies, theo-\\nries, systems, tools, applications and economy of relevant areas. Such challenges and\\nopportunities cannot be effectively accommodated by the existing body of knowledge\\nand capability set without the development of a new discipline. On the other hand,\\ndata science is at a very early stage and is engendering enormous hype and even be-\\nwilderment; issues and possibilities that are unique to data science and big data ana-\\nlytics are not clear, speciﬁc or certain. Different views, observations, and explanations\\n– some of them controversial – have thus emerged from a wide range of perspectives.\\nThere is no doubt, nevertheless, that the potential of data science and analytics\\nto enable data-driven theory, economy, and professional development is increasingly\\nbeing recognized. This involves not only core disciplines such as computing, infor-\\nmatics, and statistics, but also the broad-based ﬁelds of business, social science, and\\nhealth/medical science. Although very few people today would ask the question we\\nwere asked 10 years ago, a comprehensive and in-depth understanding of what data\\nscience is, and what can be achieved with data science and analytics research, educa-\\ntion, and economy [Cao 2017], has yet to be commonly agreed.\\nMotivated by the above concerns and observations, this article shares the ﬁndings\\nfrom a comprehensive survey of the journey from statistics and data analysis to data\\nscience. It constructs an overview of data science as a ﬁeld in terms of its research,\\ninnovation, economy, profession, and education1. This is built on (1) our observations\\nand experience in providing real-life data innovation and practices to large government\\n1Interested readers may refer to a monograph: L. Cao, Understanding Data Science, to be published by\\nSpringer soon for comprehensive discussions about data science.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:3\\nTable I. Some key terms in data science.\\nKey terms\\nDescription\\nAdvanced analytics\\nRefers to theories, technologies, tools and processes that enable an in-depth\\nunderstanding and discovery of actionable insights in big data, which\\ncannot be achieved by traditional data analysis and processing theories,\\ntechnologies, tools and processes.\\nBig data\\nRefers to data that are too large and/or complex to be effectively and/or\\nefﬁciently handled by traditional data-related theories, technologies and tools.\\nData analysis\\nRefers to the processing of data by traditional (e.g., classic statistical,\\nmathematical or logical) theories, technologies and tools for obtaining\\nuseful information and for practical purposes.\\nData analytics\\nRefers to the theories, technologies, tools and processes that enable an\\nin-depth understanding and discovery of actionable insight into data.\\nData analytics consists of descriptive analytics, predictive analytics,\\nand prescriptive analytics.\\nData science\\nIs the science of data.\\nData scientist\\nRefers to those people whose roles very much center on data.\\nDescriptive analytics\\nRefers to the type of data analytics that typically uses statistics to\\ndescribe the data used to gain information, or for other useful purposes.\\nPredictive analytics\\nRefers to the type of data analytics that makes predictions about unknown future\\nevents and discloses the reasons behind them, typically by advanced analytics.\\nPrescriptive analytics\\nRefers to the type of data analytics that optimizes indications and recommends\\nactions for smart decision-making.\\nExplicit analytics\\nFocuses on descriptive analytics typically by reporting, descriptive\\nanalysis, alerting and forecasting.\\nImplicit analytics\\nFocuses on deep analytics, typically by predictive modeling, optimization,\\nprescriptive analytics, and actionable knowledge delivery.\\nDeep analytics\\nRefers to data analytics that can acquire an in-depth understanding of why\\nand how things have happened, are happening or will happen, which cannot\\nbe addressed by descriptive analytics.\\nand industry organizations; (2) the education and training opportunities that have\\nbeen created for professionals at various levels; and (3) reﬂection on our view on critical\\nissues, future directions and strategic opportunities in data science and analytics.\\nFocusing on data science (rather than big data), it is clear that only a few articles\\nand references have discussed its history and contents, such as in Press [Press 2013],\\nDonoho [Donoho 2015], and Galetto [Galetto 2016]. A comprehensive review of data\\nscience was provided in Donoho [Donoho 2015], which focuses on the evolution of data\\nscience from statistics. To the best of our knowledge, this paper is the ﬁrst in the ﬁeld\\nto present such a comprehensive and in-depth survey and overview. Unlike studies\\nwhich focus on evolution and speciﬁc disciplinary perspectives, this paper provides\\nan introduction to the major aspects and domains of data science research, economy,\\nprofession, disciplinary development, and education. This overview complements our\\nother contributions on speciﬁc data science issues and perspectives, i.e., the realities\\nand pitfalls in Cao [Cao 2016c], the challenges and disciplinary directions in Cao and\\nFayyad [Cao 2016b], the profession and education of data science in Cao [Cao 2016d],\\nand the book on understanding data science in Cao [Cao 2017].\\nThere are several key terms, such as data analysis, data analytics, advanced analyt-\\nics, big data, data science, deep analytics, descriptive analytics, predictive analytics,\\nand prescriptive analytics, which are highly connected and easily confusing. Table I\\nlists and explains them, which are also the key terms widely used in this review. A list\\nof data science terminology is available at www.datasciences.org.\\nThe paper is organized as follows. Section 2 tracks the progression from data analy-\\nsis to data science, and addresses the fundamental question: What is data science? In\\nSection 3, the main features, initiatives, activities, and status of the era of data sci-\\nence are summarized. The evolution, state-of-the-art, paradigm shift, and major tasks\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:4\\nL. CAO\\nof deep analytics, as the keystone of data science, are discussed in Section 4. Major\\nchallenges and directions of data-driven innovation are presented in Section 5. Sec-\\ntion 6 summarizes new data-driven industrialization and service opportunities. The\\ndata science profession, competency, role of data scientists, and course framework are\\nsummarized in Section 7. The future of data science is brieﬂy discussed in Section 8,\\nfollowed by the conclusion of this work.\\n2. FROM DATA ANALYSIS TO DATA SCIENCE\\nThis section summarizes the ﬁndings of a comprehensive survey, including ours in\\nCao [Cao 2016c], Cao and Fayyad [Cao 2016b], Cao [Cao 2016d] and others such as in\\nPress [Press 2013], Donoho [Donoho 2015] and Galetto [Galetto 2016]), of the journey\\nfrom data analysis to data science and the evolution of the interest in data science.\\nSubsequently, the question “What is data science?” is addressed.\\n2.1. The Data Science Journey\\nIt is likely that the ﬁrst appearance of “data science” as a term in literature was in the\\npreface to Naur’s book “Concise Survey of Computer Methods” [Naur 1974] in 1974.\\nIn that preface, data science was deﬁned as “the science of dealing with data, once\\nthey have been established, while the relation of the data to what they represent is\\ndelegated to other ﬁelds and sciences.” Another term, “datalogy”, had previously been\\nintroduced in 1968 as “the science of data and of data processes” [Naur 1968]. These\\ndeﬁnitions are clearly more speciﬁc than those we discuss today. However, they have\\ninspired today’s signiﬁcant move to the comprehensive exploration of scientiﬁc content\\nand development.\\nThe evolutionary journey from data analysis [Huber 2011] to data science started in\\nthe statistics and mathematics community in 1962. It was stated that “data analysis is\\nintrinsically an empirical science” [Tukey 1962]2. Typical original work on promoting\\ndata processing included information processing [Morrell 1968] and exploratory data\\nanalysis [Tukey 1977]. It was suggested that more emphasis needed to be placed on\\nusing data to suggest suitable hypotheses to test. This contributed to the later term of\\n“data-driven discovery” in 1989 [KDD89 1989]. In 2001, an action plan was suggested\\nin Cleveland [Cleveland 2001] that would expand the technical areas of statistics to-\\nward data science.\\nPlaying a major role in statistics, descriptive analytics (also called descriptive statis-\\ntics in the statistics community) [Stewart and McMillan 1987] quantitatively summa-\\nrizes or describes the characteristics and measurements of a data sample or set. To-\\nday, descriptive analytics forms the foundation for the default analytical and reporting\\ntasks and tools in typical data analysis and business intelligence projects and systems.\\nOur understanding of the roles of data analysis in those early years extended beyond\\ndata exploration and processing to the aspiration to “convert data into information\\nand knowledge” in 1977 [IASC 1977]. More than 20 years later, this desire fostered\\nthe origin of the currently popular community of the ACM SIGKDD conference, specif-\\nically the ﬁrst workshop on Knowledge Discovery in Databases (KDD for short) with\\nIJCAI’1989 [KDD89 1989]. In KDD and other data mining conferences, “data-driven\\ndiscovery” was adopted as one of key themes of these events. Since then, key terms\\nsuch as “data mining”, “knowledge discovery” [Fayyad et al. 1996] and data analytics\\n[Renae 2011] have been increasingly recognized not only in computer science but also\\nin other areas and disciplines. Data mining and knowledge discovery is the process\\nof discovering hidden and interesting knowledge from data. Today, in addition to the\\n2On this basis, David Donoho argued that data science had existed for 50 years and questioned how/whether\\ndata science really differs from statistics [Donoho 2015].\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:5\\nFig. 1.\\nData science journey (w.r.t. typical events).\\nwell-recognized events KDD, ICML, NIPS and JSM, many regional and international\\nconferences and workshops on data analysis and learning have been created. The lat-\\nest development is the creation of global and regional conferences on data science,\\nespecially the IEEE International Conference on Data Science and Advanced Analyt-\\nics [DSAA 2014]. DSAA has received joint support from IEEE, ACM and the American\\nStatistics Association, in addition to industry sponsorship. The above efforts have os-\\ntensibly made data science the fastest growing and most popular computing, statistics\\nand interdisciplinary communities.\\nThe development of data mining, knowledge discovery, and machine learning, to-\\ngether with the original data analysis and descriptive analytics from the statistical\\nperspective, forms the general concept of “data analytics”. The initial data analysis\\nfocused on processing data. Data analytics is the multi-disciplinary science of quanti-\\ntatively and qualitatively examining data for the purpose of drawing new conclusions\\nor insights (exploratory or predictive), or for extracting and proving (conﬁrmatory or\\nfact-based) hypotheses about that information for decision making and action.\\nAnalytics has also become more business-oriented [Kohavi et al. 2002]. It now ex-\\ntends to a variety of data and domain-speciﬁc analytical tasks, such as business an-\\nalytics, risk analytics, behavior analytics [Cao et al. 2012], social analytics, and web\\nanalytics (also generally termed “X-analytics”). Domain-speciﬁc analytics fundamen-\\ntally drives the innovation and application of data science. Both domain-speciﬁc and\\ndata-speciﬁc analytics and theoretical data analytics have together formed the key-\\nstone of data science.\\nFig. 1 summarizes the data science journey. It presents the evolution in terms of\\nrepresentative moments, events and major aspects of disciplinary development, gov-\\nernment initiatives, scientiﬁc agendas, typical socio-economic events, and education.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:6\\nL. CAO\\nFig. 2.\\nOnline search interest trends on data science-related keywords by Google.\\nNote: The data was collected on 15 November 2016.\\n2.2. Online Search Interest Trends\\nAccording to Google Trends [Google 2016d], the online search interest over time in\\n“data science” is similar to the interest in “data analytics”, but is 50% to 100% less\\nthan the interest in “big data”. However, the historical search interest in data science\\nand analytics is roughly double the interest shown in big data about 10 years ago.\\nCompared to the smooth growth of interest in data science and analytics, the interest\\nin big data has experienced a more rapid increase since 2012. When we googled “data\\nscience”, 83.8M records were returned, compared to 365M on “big data”, and 81.8M on\\n“data analytics”.\\nAlthough they do not reﬂect the full picture, the Google search results in the last 10\\nyears, shown in Fig. 2, indicate that: (1) Data science, data analysis, and data analyt-\\nics have much richer histories and stronger disciplinary foundations than big data. (2)\\nThe signiﬁcant boom in big data has been fundamentally business-related, while data\\nscience has been highly linked with research and innovation. (3) Data analysis has\\nalways been a top concern, although search interest has been ﬂattened and diversiﬁed\\ninto other hot topics, including big data, data science and data analytics. (4) Interest-\\ningly, the word “advanced analytics” has received much less attention than all other\\nterms, reﬂecting the fact that knowledge of, and interest in, more general terms like\\ndata analytics is greater than it is for more speciﬁc terms such as advanced analyt-\\nics. (5) Compared to 10 years ago, scrutiny of the search trends in the past four years\\nwould ﬁnd that big data has seen signiﬁcantly increasing interest from 2012 to 2015\\nand then less movement; however, the interest in data science and data analytics has\\nconsistently increased, although it has grown at a much lower rate (some one third\\nof big data). Data analysis has maintained a relatively stable attraction to searchers\\nduring these 10 years.\\n2.3. What Is Data Science\\nThe art of data science [Graham 2012] has attracted increasing interest from a wide\\nrange of domains and disciplines. Accordingly, communities or proposers from diverse\\nbackgrounds, with contrasting aspirations, have presented very different views or foci.\\nSome examples are that data science is the new generation of statistics, is a consolida-\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:7\\ntion of several interdisciplinary ﬁelds, or is a new body of knowledge. Data science also\\nhas implications for providing capabilities and practices for the data profession, or for\\ngenerating business strategies.\\nStatisticians have had much to say about data science, since it is they who actually\\ncreated the term “data science” and promoted the upgrading of statistics to data sci-\\nence as a broader discipline. This is reﬂected in a series of earlier actions, such as the\\nfollowing.\\n— Jeff Wu questioned in 1997 whether “Statistics = Data Science?” and suggested that\\nstatistics should be renamed “data science” and statisticians should be known as\\n“data scientists” [Wu 1997]. The intention was to shift the focus of statistics from\\n“data collection, modeling, analysis, problem understanding/resolving, decision mak-\\ning” to future directions on “large/complex data, empirical-physical approach, repre-\\nsentation and exploitation of knowledge”.\\n— William S. Cleveland suggested in 2001 that it would be appropriate to alter the\\nstatistics ﬁeld to data science and “to enlarge the major areas of technical work of the\\nﬁeld of statistics” by looking to computing and partnering with computer scientists\\n[Cleveland 2001].\\n— Leo Breiman suggested in 2001 that it was necessary to “move away from exclusive\\ndependence on data models (in statistics) and adopt a more diverse set of tools” such\\nas algorithmic modeling, which treats the data mechanism as unknown [Breiman\\n2001].\\n— In 2015, a statement about the role of statistics in data science was released by a\\nnumber of ASA leaders [van Dyk et al. 2015], saying that “statistics and machine\\nlearning play a central role in data science.” Many other relevant discussions are\\navailable in AMSTATNEWS [ASA 2015] and IMS [Yu 2014].\\nA large proportion of the conceptual arguments are derived from the data-centric\\nview. For example, data-driven science is mainly interpreted in terms of the reuse of\\nopen data [Murray-Rust 2007; OECD 2007]; data science comprises the numbers of\\nour lives [Miller 2013]; or data science enables the creation of data products [Loukides\\n2012; 2011]. In Jagadish [Jagadish 2015], six myths were discussed: (1) size is all\\nthat matters, (2) the central challenge of big data is that of devising new computing\\narchitectures and algorithms, (3) analytics is the central problem of big data, (4) data\\nreuse is low hanging fruit, (5) data science is the same as big data, and (6) big data is\\nall hype. This illustrates the constituents of the ecosystem, but also shows the divided\\nviews within the communities.\\nIntensive discussions have taken place within the research and academic commu-\\nnity about creating data science as an academic discipline [Smith 2006]. This involves\\nnot only statistics, but also a multi-disciplinary body of knowledge that includes com-\\nputing, communication, management and decision. The concept of data science is cor-\\nrespondingly deﬁned from the perspective of disciplinary and course development: for\\nexample, treating data science as a mixture of statistics, mathematics, computer sci-\\nence, graphic design, data mining, human-computer interaction, and information vi-\\nsualization [Yau 2009].\\nBelow, we present several deﬁnitions of data science from high-level and disciplinary\\nperspectives, building on the observations and insights we have gained from this re-\\nview and relevant experience3.\\nDeﬁnition 2.1 (Data Science1). A high-level statement is: “data science is the sci-\\nence of data” or “data science is the study of data”.\\n3Interested readers may refer to Cao [Cao 2016c] for another deﬁnition from the process perspective.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:8\\nL. CAO\\nDeﬁnition 2.2 (Data Science2). From the disciplinary perspective, data science is a\\nnew interdisciplinary ﬁeld that synthesizes and builds on statistics, informatics, com-\\nputing, communication, management and sociology to study data and its environments\\n(including domains and other contextual aspects, such as organizational and social\\naspects) in order to transform data to insights and decisions by following a data-to-\\nknowledge-to-wisdom thinking and methodology.\\nAccordingly, a discipline-based data science formula is given below:\\ndata science = statistics + informatics + computing + communication +\\nsociology + management|data + environment + thinking\\n(1)\\nwhere “|” means “conditional on.”\\nThe outputs of data science are data products [Loukides 2011; 2012]. We deﬁne data\\nproducts below.\\nDeﬁnition 2.3 (Data Products). A data product is a deliverable from data, or is en-\\nabled or driven by data, and can be a discovery, prediction, service, recommendation,\\ndecision-making insight, thinking, model, mode, paradigm, tool or system. The ulti-\\nmate data products of value are knowledge, intelligence, wisdom and decision.\\nThe above deﬁnition of data product goes beyond technical product-based types and\\nforms in the business and economic domain, such as social network platforms like\\nFacebook, recommender systems like Netﬂix, and mobile apps like Uber. Data science\\nenables us to explore new data-driven or data-enabled personalized, organizational,\\neducational, ethical, societal, cultural, economic, political, cyber-physical forms, modes,\\nparadigms, innovations, directions and ecosystems, or even thinking, strategies and\\npolicies. For example, there is a good possibility that large scale data will enable and\\nenhance the transfer of subjective autonomy to objective autonomy, beneﬁcence and\\njustice in the social sciences [Fairﬁelda and Shteina 2014]. It can enable the discovery\\nof indicators like Google Flu [Lazer et al. 2014] which may not be readily predicted by\\ndomain-driven hypothesis and professionals.\\nThese platforms deliver data products in various forms, ways, channels, and do-\\nmains that are fundamentally transforming our academic, industrial, governmental,\\nand socio-economic life and world. With the development of data science and engi-\\nneering theories and technologies, new data products will be created. This creation\\nis likely to take place at a speed and to an extent that greatly exceeds our imagina-\\ntion and thinking, as shown in the evolution of Internet-based products and artiﬁcial\\nintelligence systems.\\n3. THE ERA OF DATA SCIENCE\\nIn this section, we summarize the main characteristics of data science-related govern-\\nment initiatives, disciplinary development, economy, and profession, as well as activi-\\nties in these ﬁelds, and the progress made to date. Dataﬁcation [Ayankoya et al. 2014],\\nthe quantiﬁed self (QS) [Swan 2013; Clay 2013; Wolf 2012; Duncan 2009; Smarr 2012;\\nFawcett 2016], initiatives by governments and research institutions and open data are\\ndiscussed as the key drivers of the era of big data and data science.\\n3.1. Dataﬁcation and Data Quantiﬁcation\\nData is ubiquitous because dataﬁcation [Ayankoya et al. 2014] and data quantiﬁca-\\ntion are ubiquitous. In addition to the commonly seen data transactions acquired from\\nbusiness and operational information systems, increasingly popular and widespread\\ndataﬁcation and data quantiﬁcation systems and services are signiﬁcantly strength-\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:9\\nening the data deluge and big data realm. Such systems and services include but are\\nnot limited to wearables, Internet of Things (IoT), mobile and social applications.\\nAs we have seen and can predict, dataﬁcation and data quantiﬁcation take place\\nat any time and any place by anybody in any form in any way in a non-traditional\\nmanner, extent, depth, variety and speed.\\n— Quantiﬁcation timing: anytime quantiﬁcation, from working to studying, day-to-day\\nliving, relaxing, enjoying entertainment and socializing;\\n— Quantiﬁcation places: anyplace quantiﬁcation, from biological systems to physical,\\nbehavioral, emotional, cognitive, cyber, environmental, cultural, economic, sociologi-\\ncal and political systems and environments;\\n— Quantiﬁcation bodies: anybody quantiﬁcation, from selves to others, connected\\nselves, exo-selves [Kelly 2012] and the world, and from individuals to groups, or-\\nganizations and societies;\\n— Quantiﬁcation forms: anyform quantiﬁcation, from observation to drivers, from ob-\\njective to subjective, from physical to philosophical, from explicit to implicit, and from\\nqualitative to quantitative forms and aspects;\\n— Quantiﬁcation ways: anysource quantiﬁcation, such as sources and tools that in-\\nclude information systems, digitalization, sensors, surveillance and tracking systems,\\nthe IoT, mobile devices and applications, social services and network platforms, and\\nwearable [Viseu and Suchman 2010] and Quantiﬁed Self (QS) devices and services;\\nand\\n— Quantiﬁcation speed: anyspeed quantiﬁcation, from static to dynamic, from ﬁnite to\\ninﬁnite, and from incremental to exponential generation of data objects, sets, ware-\\nhouses, lakes and clouds.\\nExamples of fast developing quantiﬁcation areas are the health and medical do-\\nmains. We are datafying both traditional medical and health care data and “omics”\\ndata (genomics, proteomics, microbiomics, metabolomics, etc.) and increasingly over-\\nwhelming QS-based tracking data [Swan 2013] on personal, family, group, community,\\nand/or cohort levels.\\n3.2. Data Initiatives by Governments\\nTo effectively understand and utilize everywhere data, data DNA and its potential, in-\\ncreasing numbers of regional and global government initiatives [Security 2015] are be-\\ning introduced at different levels and on different scales in this age of big data and data\\nscience to promote data science research, innovation, funding support, policy making,\\nindustrialization, and economy. Table II summarizes the major initiatives of several\\ncountries and regions.\\n— The Australian Public Service Big Data Strategy [UN 2010] aims to “provide an op-\\nportunity to consider the range of opportunities presented to agencies in relation\\nto the use of big data, and the emerging tools that allow us to better appreciate\\nwhat it tells us, in the context of the potential concerns that this might raise”. It\\naddresses the identiﬁed big data strategy issues [AGIMO 2013]. Australia’s whole-\\nof-government Centre of Excellence in Data Analytics [AU 2016] coordinates relevant\\ngovernment activities. The Australian Research Council has granted approval to the\\nAustralian Research Council (ARC) Centre of Excellence for Mathematical and Sta-\\ntistical Frontiers [ACEMS 2014] to conduct research on big data-based mathematical\\nand statistical foundations. Another recent effort made by the Australian govern-\\nment was the establishment of Data61 [Data61 2016], which consolidated the rele-\\nvant data-related human resources in the original National ICT Australia (NICTA)\\n[NICTA 2016] and CSIRO and aims for a uniﬁed platform for data research and in-\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:10\\nL. CAO\\nnovation, engagement with industry and government and academia, and software\\ndevelopment.\\n— Canada’s policy framework Capitalizing on Big Data [CA 2016] aims at “establishing\\na culture of stewardship ... coordination of stakeholder engagement ... developing\\ncapacity and future funding parameters.”\\n— China’s Guidelines [Government 2015] are aimed at boosting the development of big\\ndata research and applications, to “set up an overall coordination mechanism for big\\ndata development and application, speed up the establishment of relevant rules, and\\nencourage cooperation between the government, enterprises and institutions.” China\\nhas also set up a national strategic plan for the IoT and big data [Government 2015].\\nMany states and cities in China have launched national big data strategies and ac-\\ntion plans for big data and cloud computing [CBDIO 2016; Agency 2016], such as\\nin Beijing [Government 2016]. Probably, a very early example in China was the Lu-\\noyang City-sponsored consulting project in 2011, for which we developed a strategic\\nplan for the City’s industrial transformation to a “data industry” [Cao 2011].\\n— The European Union’s communication Towards a Thriving Data-driven Economy\\n[EU 2014] is “an action plan to bring about the data-driven economy of the future”.\\nIt outlines “a new strategy on Big Data, supporting and accelerating the transition\\ntowards a data-driven economy in Europe. The data-driven economy will stimulate\\nresearch and innovation on data while leading to more business opportunities and\\nan increased availability of knowledge and capital, in particular for SMEs, across\\nEurope.” In 2015, the European Data Science Academy, EDSA [EU-DSA 2016] was\\nformed.\\n— The United Kingdom’s Big Data and Energy Efﬁcient Computing initiative funded by\\nthe Research Councils UK [UK 2016] aims to “create a foundation where researchers,\\nusers and industry can work together to create enhanced opportunities for scientiﬁc\\ndiscovery and development.”\\n— The United Nations (UN) Global Pulse Project is “a ﬂagship innovation initiative of\\nthe United Nations Secretary-General on big data. Its vision is a future in which big\\ndata is harnessed safely and responsibly as a public good. Its mission is to accelerate\\nthe discovery, development and scaled adoption of big data innovation for sustainable\\ndevelopment and humanitarian action.” [UN 2010]\\n— The United States (US) Big Data Research Initiative [USNSF 2012] is directed to-\\nward “supporting the fundamental science and underlying infrastructure enabling\\nthe big data revolution.” In 2005, the US National Science Board set the goal that it\\n“should act to develop and mature the career path for data scientists” in its report\\n“Long-lived Digital Data Collections: Enabling Research and Education in the 21st\\nCentury” [NSB 2005]. In 2009, the Committee on Science of the National Science\\nand Technology Council formed an Interagency Working Group on Digital Data. It\\npublished a report [CSNSTC 2009] outlining the strategy to “create a comprehen-\\nsive framework of transparent, evolvable, extensible policies and management and\\norganizational structures that provide reliable, effective access to the full spectrum\\nof public digital scientiﬁc data”, which “will serve as a driving force for American\\nleadership in science and in a competitive, global information society.” In addition,\\nthe Defence Advanced Research Projects Agency (DARPA) launched its XDATA Pro-\\ngram [DARPA 2016], which aims to develop computational techniques and software\\ntools for processing and analyzing large, imperfect and incomplete data. In 2012, the\\nNational Institute of Standards and Technology (NIST) introduced a new data sci-\\nence initiative [Dorr et al. 2015], and in 2013, the US National Consortium for Data\\nScience was established [USD2D 2016].\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:11\\nTable II. Government initiatives in big data and data science.\\nGovernment\\nRepresentative Initiatives\\nAustralia\\nPublic Service Big Data Strategy [UN 2010], Whole-of-Government Centre\\nof Excellence on Data Analytics [AU 2016]\\nCanada\\nCapitalizing on Big Data [CA 2016]\\nChina\\nBig Data Guideline [Government 2015], China Computer Federation Task Force on Big\\nData [CCF-BDTF 2013], China National Science Foundation big data program [CNSF 2015]\\nEU\\nData-driven Economy [EU 2014], European Commission Horizon 2020\\nBig Data Private Public Partnership [Horizon 2014]\\nUK\\nUK’s Big Data and Energy Efﬁcient Computing [UK 2016]\\nUN\\nUN Global Pulse Project [UN 2010]\\nUS\\nUS Big Data Research Initiative [USNSF 2012], Interagency Working Group\\non Digital Data [CSNSTC 2009], DARPA’s XDATA Program [DARPA 2016], USA NSF Big\\nData Research Fund [USNSF 2012]\\n3.3. The Scientiﬁc Agenda of Data Science\\nAn increasing number of new scientiﬁc initiatives, activities and programs have been\\ncreated by governments, research institutions, and educational institutions to promote\\ndata science as a new ﬁeld of science.\\nThe original scientiﬁc agenda of data science has been driven by both government\\ninitiatives and academic recommendations. This was built on the strong promotion of\\nconverting statistics to data science, and blending statistics with computing science in\\nthe statistics community [Wu 1997; Cleveland 2001; Iwata 2008; Hardin et al. 2015;\\nHand 2015; Diggle 2015; Graham 2012; Finzer 2013]. Today, many regional and global\\ninitiatives have been taken in data science research, disciplinary development and\\neducation, as strategic matters and agenda in the digital era. Several examples are\\ngiven below.\\n— In Australia, a Go8 report [Brown 2009] suggested the incorporation of data as a key-\\nstone in K-12 education through statistics and science by such methods as creating\\ndata games for children.\\n— In China, the Ministry of Science and Technology very recently announced the es-\\ntablishment of national key labs in big data research as part of a strategic national\\nagenda [CMIST 2016].\\n— In the EU, the HLSG report “Riding the Wave” [HLSG 2010] and “The Data Harvest”\\n[HLSG 2014] urged the European Commission to implement the vision of creating\\n“scientiﬁc e-infrastructure that supports seamless access, use, re-use, and trust of\\ndata” and foster the development of data science university programs and discipline.\\n— In the US, a National Science Board report [NSB 2005] recommended that the Na-\\ntional Science Foundation (NSF) “should evaluate in an integrated way the impact\\nof the full portfolio of programs of outreach to students and citizens of all ages that\\nare ‘or could be’ implemented through digital data collections.” Different roles and re-\\nsponsibilities were discussed for individuals and institutions, including data authors,\\nusers, managers and scientists as well as funding agencies. The report [CSNSTC\\n2009] from the US Committee on Science of the National Science and Technology\\nCouncil suggested the development of necessary knowledge and skill sets by initiat-\\ning new educational programs and curricula, such as “some new specializations in\\ndata tools, infrastructures, sciences, and management.”\\nAn increasing number of research streams, strengths and focused projects have been\\nannounced in major countries and regions, including\\n— The US NSF Big Data Research Fund [USNSF 2012],\\n— The European Commission Horizon 2020 Big Data Private Public Partnership [Hori-\\nzon 2014; EU 2014], and\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:12\\nL. CAO\\n— The China NSF big data special fund [CNSF 2015].\\nEach of these supports theoretical, basic and applied data science research and de-\\nvelopment in big data and analytics through respective scientiﬁc foundations, high-\\ntech programs and domain-speciﬁc funds such as heath and medical funds. Signiﬁcant\\ninvestment has been made to create even faster high performance computers.\\nMany universities and institutions have either established or are creating research\\ncenters or institutes in data science, analytics, big data, cloud computing, and IoT.\\nFor example, in Australia, the author created the ﬁrst data science lab: the Data\\nScience and Knowledge Discovery Lab at UTS in 2007 [DSKD 2007], and the ﬁrst\\nAustralian institute: the Advanced Analytics Institute [UTSAAI 2011; AGIMO 2013]\\nin 2011 which implements the RED model of Research, Education and Development\\n(RED) of big data analytics for many major government and business organizations. In\\nthe US, top universities have worked on building data science initiatives, such as the\\nInstitute for Advanced Analytics at North Carolina State University in 2007 [NCSU\\n2007a], the Stanford Data Science Initiatives in 2014 [Stanford 2014], and the Data\\nScience Initiatives at University of Michigan in 2015 [UMichi 2015].\\n3.4. Data Science Disciplinary Development\\nIn contrast to big data that has been driven by data-oriented business and private en-\\nterprise, researchers and scientists also play a driving role in the data science agenda.\\nMigrating from the original push in the statistics communities, various disciplines\\nhave been involved in promoting the disciplinary development of data science. This\\ninvolves the disciplinary structure, intrinsic challenges and directions, course struc-\\nture and curriculum design, and qualiﬁcations for next-generation data scientists [Cao\\n2016b; 2016c; 2016d].\\nIn Borne et al. [Borne et al. 2010], the authors highlight the need to train the next\\ngeneration of specialists and non-specialists to derive intelligent understanding from\\nthe increased vastness of data from the Universe, “with data science as an essen-\\ntial competency” in astronomy education “within two contexts: formal education and\\nlifelong learners”. The aim is to manage “a growing gap between our awareness of\\nthat information and our understanding of it.” In several researches [Fox and Hendler\\n2014; Bailer et al. 2012; Rudin 2014; Anderson et al. 2014; Baumer 2015; Bussaban\\nand Waraporn 2015], discussions focus on the needs, variations and addenda of data\\nscience-oriented subjects for undergraduate and postgraduate students majoring in\\nmathematics and computing. Case studies of relevant subjects at seven institutions\\nwere introduced in Hardin [Hardin et al. 2015], with the syllabi collected in Hardin\\n[Hardin 2016].\\nIn addition to the promotion activities in core analytics disciplines such as statis-\\ntics, mathematics, computing and artiﬁcial intelligence, the extended recognition and\\nundertaking of domain-speciﬁc data science seems to repeat the evolutionary history\\nof the computer and computer-based applications. Data science is warmly embraced\\nby more and more disciplines and domains in which it was traditionally irrelevant,\\nsuch as law, history and even nursing [Clancy et al. 2014]. Its core driving forces come\\nfrom data-intensive and data-rich areas such as astronomy [Borne et al. 2010], neu-\\nrobiology [Dierick and Gabbiani 2015], climate change [Faghmous and Kumar 2014],\\nresearch assessment [Siart et al. 2015], media and entertainment [Gold et al. 2013],\\nsupply chain management (SCM) [Hazena et al. 2014] and SCM predictive analytics\\n[Schoenherr and Speier-Pero 2015], advanced hierarchical/multiscale materials [Ka-\\nlidindi 2015; Gupta et al. 2015], and cyberinfrastructure [NSF 2007]. The era of data\\nscience presents signiﬁcant interdisciplinary opportunities [Rudin 2014], as evidenced\\nby the transformation from traditional statistics and computing-independent research\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:13\\nto cross-disciplinary data-driven discovery combining statistics, mathematics, comput-\\ning, informatics, sociology and management. Data science drives the disciplinary shift\\nof artiﬁcial intelligence (AI) from its origins in logics, reasoning and planning-driven\\nmachine intelligence to meta-synthesizing ubiquitous X-intelligence-enabled complex\\nintelligent systems and services [Qian 1991; Qian et al. 1993; Cao et al. 2009; Cao\\n2015b].\\nA very typical inter-, multi- and cross-disciplinary evolutionary trend is the adop-\\ntion and adaptation of data-driven discovery and science in classic disciplines from\\nan informatics perspective. This has resulted in the phenomenon of X-informatics for\\ntransforming and reforming the body of knowledge. Typical examples include astroin-\\nformatics, behavior informatics [Cao 2010b; Cao and (Eds) 2012], bioinformatics, bio-\\nstatistics, brain informatics, health informatics, medical informatics, and social infor-\\nmatics, to name a few [Wikipedia 2016b]. Hence, it is not surprising to see courses\\nand subjects being offered in speciﬁc areas such as biomedical informatics, healthcare\\ninformatics, and even urban informatics.\\nFollowing the creation of the world ﬁrst coursework Master of Science in Analytics\\n[NCSU 2007b] created at North Carolina State University in 2007, and the world ﬁrst\\nMaster of Analytics by Research and PhD in Analytics launched at the University of\\nTechnology Sydney in 2011 [UTS 2011; WIRED 2014], more than 150 universities and\\ninstitutions have now either created or are planning courses in data science, big data\\nand analytics [Silk 2016]. The majority of these course initiatives focus on training\\npostgraduate specialists and certiﬁcate-based trainees in business disciplines, followed\\nby the disciplines of computer science and statistics.\\nSeveral repositories [Silk 2016; DSC 2016a; Github 2016a; Classcentral 2016; US-\\nDSC 2016] collect information about courses and subjects related to analytics, data\\nscience, information systems and management, statistics, and decision science. For ex-\\nample, according to DSC [DSC 2016a] and Github [Github 2016a], there are currently\\nabout 500 general or speciﬁc subjects or courses that relate to data analytics, informa-\\ntion processing, data mining and machine learning, of which 78% are offered in the\\nUS. Seventy-two percent are offered at Master’s level, with only 7% at bachelor level,\\nand 3.6% at doctoral level. About 30% are online courses. From the disciplinary per-\\nspective, some 43% of courses speciﬁcally encompass “Analytics”, compared to 18% on\\n“Data Science” and only 9% on “Statistics”. Approximately 40% focus on business and\\nsocial science aspects. In Classcentral [Classcentral 2016], 138 courses and subjects\\nare available. A number of US programs are listed in USDSC [USDSC 2016], most\\nbeing created in business and management disciplines.\\nMore than 85% of courses [DSC 2016a] cover a broad scope of big data, analytics,\\nand data science and engineering. Some courses only offer training in very speciﬁc\\ntechnical skills, capabilities and technologies, such as artiﬁcial intelligence, data min-\\ning, predictive analytics, machine learning, visualization, business intelligence, com-\\nputational modeling, cloud computing, information quality, and analytics practices. It\\nis very rare to ﬁnd courses that are dedicated to analytics project management and\\ncommunication skill training [Faris et al. 2011], although several courses on decision\\nscience are offered.\\nOnline data science courses signiﬁcantly complement traditional education and typ-\\nically offer a successful Internet-based data business model. The corporate training\\nmarket has seen increasing competition as vendors and universities invest more re-\\nsources in this area: the SAS training courses are one such example. Online courses\\nsuch as those offered as a massive open online course (MOOC) and by open universities\\nare quickly feeding the market.\\nToday, an increasing number of courses are offered in the MOOC mode [Fox et al.\\n2015; Boyer et al. 2015], such as through Class Central [Classcentral 2016], Coursera\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:14\\nL. CAO\\n[Coursera 2016], edX [Edx 2016], Udacity [Udacity 2016] and Udemy [Udemy 2016].\\nThe MOOC model is fundamentally changing the way courses are offered by utilizing\\nonline, distributed and open data, curriculum development resources and expertise,\\nand delivery channels and services. Course development technologies such as Google\\nCourse Builder [Google 2016c] and Open edX [OPENedX 2016] are used to create on-\\nline courses and their operations.\\nMost of the available courses focus on classic subjects, in particular statistics, data\\nmining, machine learning, prediction, business intelligence, information management,\\nand database management. New programming languages including R and Python, and\\ncloud infrastructure MapReduce and Hadoop are highlights in these courses. Tech-\\nniques related to off-the-shelf software and tools are often emphasized. Very few sub-\\njects are speciﬁed for advanced analytics, real-life analytics practices, communication,\\nproject management, and decision-support. An increasing number of courses are cre-\\nated to address domain-speciﬁc demands, such as incorporating statistics, business\\nanalytics, web and social network analytics into SCM predictive analytics [Schoenherr\\nand Speier-Pero 2015].\\n3.5. New Data Economy and Industry Transformation\\nThe recognition of the values and potential of data science and analytics and its rapid\\ngrowth have also been driven and promoted by the evolution of a new data economy\\nand industry transformation, such as large private data enterprise. The advancement\\nof data science and big data analytics is conversely signiﬁcantly inﬂuencing and driv-\\ning the development of a new data economy, industry transformation and increase in\\nproductivity. This wave of data economy upgrading and industry transformation fea-\\ntures the revolution of advanced artiﬁcial intelligence-enabled technologies and busi-\\nnesses, and the complementary advances in AI and the AI-driven data economy are\\nlargely propelled by data science and analytics. They include inventing, commercializ-\\ning, and applying infrastructures, tools, systems, services, applications, and consulta-\\ntions for managing, discovering, and utilizing deep data intelligence and synthesizing\\nX-intelligences and X-complexities [Cao 2016b].\\nA typical indicator is the 2010 IBM Global CEO study, from which the resultant\\nreport [IBM 2010] draws the following conclusions: “Yet the emergence of advanced\\ntechnologies like business analytics can help uncover previously hidden correlations\\nand patterns, and provide greater clarity and certainty when making many business\\ndecisions.” To manage the increasing complexity, the CEOs in this study believe that\\n“a better handle on information and mastery of analytics to predict consequences of\\ndecisions could go a long way in reducing uncertainty and in forging answers that\\nare both swift and right.” This leads to their desire to “translate data into insight\\ninto action that creates business results” and to “take advantage of the beneﬁts of\\nanalytics.”\\nToday, it can safely be said that data science has enabled the so-called “new econ-\\nomy” as evidenced by large private enterprises such as Facebook, Google and Alibaba.\\nThis new data economy is data product-based and data technology-driven. An increas-\\ning number of organizations recognize the value of data as a strategic asset and invest\\nin building infrastructure, resources, talent, and teams to support enterprise innova-\\ntion, and to create differentiators that will lift competition and productivity. Leading\\nInternet-based data-driven businesses [Dhar 2013], such as Google, Facebook, SAS,\\nAlibaba, Baidu and Tencent, have overtaken traditional enterprise giants.\\nClassic manufacturing-focused and core business-oriented companies, including\\nIBM, Intel [Stonebraker et al. 2013] and Huawei, have also all launched correspond-\\ning initiatives and strategic actions for big data, IoT, and/or cloud computing, and are\\npursuing the strategy of data product-based transformation, productivity growth and\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:15\\ninnovation. Data science has been their new innovation engine for productivity and\\ncompetition upgrade. Core businesses, including banks, capital market ﬁrms, telecom-\\nmunication service providers, and insurance companies, are leading the way in datafy-\\ning, quantifying, analyzing and using data. It is encouraging to see that other tradi-\\ntional business sectors, such as agriculture, tourism, retail, property and education,\\nare also investing in data analytics to transform their productivity and competitive\\nadvantage.\\nMany new start-ups and spin-offs have emerged rapidly in recent years and have fo-\\ncused on data-based business, products and services. This is reﬂected in a fast-evolving\\nbig data landscape [BDL 2016a], which covers data sources & API, infrastructure, an-\\nalytics, cross-infrastructure/analytics, open source initiatives, and applications. Every\\nyear, this changing landscape sees signiﬁcant, swift growth. As a result of dataﬁcation\\nand data quantiﬁcation, new platforms, products, applications, services and economic\\nmodels such as Spark and Cloudera have quickly emerged in analytics and big data.\\n3.6. Data Professional Community Formation\\nThe growth and recognition of an emerging ﬁeld can be effectively measured in terms\\nof the formation width, depth and speed of its professional communities. The data\\nscience and analytics community is growing incredibly quickly.\\nThe ﬁrst indicator is the emergence of dedicated publication venues in this area.\\nSeveral journals on data science have been established. These include the Journal of\\nData Science [JDS 2002], launched in 2002, which is devoted to applications of statis-\\ntical methods at large; the electronic Data Science Journal [DSJ 2014] relaunched by\\nCODATA in 2014; the EPJ Data Science [EPJDS 2012] launched in 2012; the Interna-\\ntional Journal of Data Science and Analytics (JDSA) [JDSA 2015] in 2015 by Springer;\\nIEEE Transactions on Big Data [TOBD 2015] in 2015; and the Springer Series on Data\\nScience [SSDS 2015] and the Data Analytics Book Series [DABS 2016].\\nOther publications are in development by various regional and domain-speciﬁc pub-\\nlishers and groups. Some examples are the International Journal of Data Science\\n[IJDS 2016], Data Science and Engineering [DSE 2015] published on behalf of the\\nChina Computer Federation (CCF) [DSE 2015], the International Journal of Research\\non Data Science [IJRDS 2017], and the Journal of Finance and Data Science [JFDS\\n2016].\\nThe second indicator can be found in the creation of a data science community which\\nis signiﬁcantly enhanced by conferences, workshops and forums dedicated to the pro-\\nmotion of data science and analytics. There are also many well-established venues\\nwhich either focus on speciﬁc aspects such as KDD and ICML or have adjusted their\\nprevious non-data and/or analytics focus, such as the traditional AI conferences IJCAI\\nand AAAI.\\n— The ﬁrst conference to adopt “data science” as a topic was the 1996 IFCS Conference\\non Data Science, Classiﬁcation, and Related Methods [IFSC-96 1996], which included\\npapers on general data analysis issues.\\n— The IEEE International Conference on Data Science and Advanced Analytics (DSAA)\\n[DSAA 2014] launched in 2014, was probably the ﬁrst conference series dedicated\\nto both data science and analytics research and practice. Co-sponsored by ACM\\nSIGKDD, IEEE CIS and the American Statistics Association (ASA), it attracted wide\\nand signiﬁcant interest from statistics, industry, business, IT and professional bod-\\nies. The IEEE Conference on Big Data is an event dedicated to broad areas of big\\ndata.\\n— Several other domain-speciﬁc and regional initiatives have emerged, such as the\\nthree initiatives in India, i.e., the Indian Conference on Data Sciences, the Interna-\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:16\\nL. CAO\\ntional Conference on Big Data Analytics, and the International Conference on Data\\nScience and Engineering.\\n— Several other conference series have been renamed and repositioned from their orig-\\ninal focus on topics such as software engineering and service-based computing to\\nconnect with big data and data science, drawing mainly on key topics of interest and\\nparticipants from their original areas.\\n— Data analytics, machine learning, and big data have eclipsed the original topics of\\ninterest in many traditionally non-data and/or analytics conferences, such as IJCAI,\\nAAAI, VLDB, SIGMOD and ICDE. Not surprisingly, some of these venues now fre-\\nquently incorporate more than 50% of papers on data science matters.\\nThe third indicator is the growth and development of professional (online) communi-\\nties and organizations established publicly or privately to promote big data, analytics\\nand data science research, practices and education, and interdisciplinary communica-\\ntions. For example:\\n— The IEEE Big Data Initiative [IEEEBD 2014] aims to “provide a framework for col-\\nlaboration throughout IEEE”, and states that “Plans are under way to capture all the\\ndifferent perspectives via in depth discussions, and to drive to a set of results which\\nwill deﬁne the scope and the direction for the initiative.”\\n— The IEEE Task Force on Data Science and Advanced Analytics (TF-DSAA) [TFDSAA\\n2013] was launched in 2013 to promote relevant activities and community building,\\nincluding the annual IEEE Conference on Data Science and Advanced Analytics.\\n— The International Institute of Data & Analytics [IDA 2014] aims to bridge the gaps\\nbetween academia and industry through the promotion of data and analytics re-\\nsearch, education and development.\\n— The China Computer Federation Task Force on Big Data [CCF-BDTF 2013] consists\\nof a network of representatives from academia, industry and government, and orga-\\nnizes its annual big data conference with participants from industry and government.\\n— Several groups and initiatives promote dedicated activities of analytics and data\\nscience. For instance, Datasciences.org [Datasciences.org 2005] collects relevant in-\\nformation about data science research, courses, funding opportunities, professional\\nactivities, and platforms for collaborations and partnership. The Data Science Com-\\nmunity [DSC 2016b] claims to be the European Knowledge Hub for Bigdata and\\nDatascience. Data Science Central [DSCentral 2016] aims to be the industry’s online\\nresource for big data practitioners. The Data Science Association [DSA 2016] aims\\nto be a “professional group offering education, professional certiﬁcation, conferences\\nand meetups” [Galetto 2016], and even offers a “Data Science Code of Professional\\nConduct.”\\n— Many existing consulting and servicing organizations have adjusted their scope to\\ncover analytics, where they previously focused on other disciplinary matters. Inter-\\ndisciplinary efforts have been made to promote cross-domain and cross-disciplinary\\nactivities and growth opportunities. Examples include INFORMS [INFORMS 2016],\\nGartner, McKinsey, Deloitte, PricewaterhouseCoopers, KPMG, and Bloomberg.\\nLastly, multinational vendors, online and new economy giants, and service providers\\nplay a critical driving role in community outreach. Each of these has launched relevant\\ninitiatives, such as those by SAS [SAS 2016], IBM [IBM 2016a], Google [Google 2016a]\\nand Facebook [Facebook 2016]. Many professional interest groups have been set up\\nin social media, including Google groups, LinkedIn, Facebook and Twitter, and are\\namong the most attractive and popular venues for big data, data science and analytics\\nprofessionals to share and network.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:17\\n3.7. The Open Model and Open Data\\nA key feature differentiating the data science era from the previous era lies in the\\noverwhelming adoption and acceptance of the open model rather than a closed one.\\nThe open model enables free, distributed, and collaborative modes in every aspect of\\neconomy, society, research, and living. It supports the innovation of social media like\\nFacebook and LinkedIn, the migration of mobile to smart phone-embedded applica-\\ntions, and industrial transformation such as the migration of physical shop centre-\\nbased commerce to online businesses like Taobao.\\nTypically, open data and data sharing programs have been announced in many coun-\\ntries and domains, such as the US Government open data site [US-OD 2016], the UK\\nopen data project [UK-OD 2016; UK-HM 2012], the Australian Government open gov-\\nernment data site [AU 2010; 2013] and Data Matching program [AU 1990], and the\\nEuropean Union Open Data Portal [EU-OD 2016] and data sharing projects [HLSG\\n2014]. In addition, many Open Access schemes are increasingly being accepted by aca-\\ndemic journals.\\nEfforts have also been made in diverse societies to create shareable data reposito-\\nries, especially for science and research. Examples of open repositories are the global\\nclimate data [Tutiempo 2016], the global terrorism database [GTD 2016], the Yahoo\\nFinance data [Yahoo 2016], the Gene Expression Omnibus [GEO 2016], mobile data\\n[Google 2016e], the UCI repositories for machine learning [UCI 2016], the Linguistic\\nData Consortium data for Natural Language Processing [LDC 2016], the TREC data\\nfor text retrieval [NIST 2015], Kaggle competition data [Kaggle 2016], and the VAST\\nchallenge [Vast 2016] for visual analytics, to name a few.\\n4. DATA ANALYTICS: A KEYSTONE OF DATA SCIENCE\\nIn the age of analytics, what is to be analyzed, what constitutes the analytics spec-\\ntrum for understanding data, and what form the paradigm shift of analytics takes are\\ncritical questions to be answered. We address these issues in this section.\\nData and analytics form a comprehensive map that covers\\n— the whole life cycle of the data from the past to the present and the future,\\n— the analytics from explicit (known) analytics and reactive understanding to implicit\\n(unknown) analytics and proactive early prediction and intervention, and\\n— the journey from data exploration (by descriptive and predictive analytics) to the\\ndelivery of actionable insights and decisions through prescriptive analytics and ac-\\ntionable knowledge delivery [Cao et al. 2010].\\n4.1. Data-to-Insight-to-Decision Whole-of-Life Analytics\\nAs shown in Fig. 3, the data-to-insight-to-decision transfer at different time periods\\nand analytic stages is embodied along the whole-of-life analytics. This can be further\\nrepresented in terms of a variety of analytics goals (G) and approaches (A) to achieve\\nthe data-to-decision goal.\\n— Past data: the main focus of historical analytics is to explore “what happened” in\\nthe data and business, and to gain insights into “how and why it happened” through\\nmodeling and experimental design, etc. This stage focuses on “we know what we\\nknow” to conduct a reactive understanding of what took place.\\n— Present data: detection at this stage is mainly focused on exploring “what is hap-\\npening”, to generate insights about “how and why it happens”. This stage addresses\\n“we know what we do not know” with alerts generated about suspicious events, or\\ninteresting groups or patterns presented in the data and business. The insights are\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:18\\nL. CAO\\nFig. 3.\\nData-to-insight-to-decision whole-of-life analytics.\\nextracted for decision-making purposes, such as real-time risk management and in-\\ntervention, to address the question “what are the key driving factors?”\\n— Future data: predictive analytics is undertaken to investigate “what will happen” in\\nthe future, and to achieve insights into “how and why it will happen” by estimating\\nthe occurrence of future events, grouping and patterns. The aim of this stage is to\\nsolve the problem that “we do not know what we do not know” by achieving proactive\\nunderstanding, forecasting and prediction, and early prevention.\\n— Actionable decision: prescriptive analytics and actionable knowledge delivery are un-\\ndertaken to investigate “what best action to take” to interpret ﬁndings from the past,\\npresent or future data. This achieves insights into “what is the next best action” and\\nenables the corresponding optimal actions and recommendations to be undertaken\\nbased on the ﬁndings. The aim of this stage is to solve the problem of “how to actively\\nand optimally manage the problems identiﬁed” by making optimal recommendations\\nand actionable interventions.\\n4.2. Explicit-to-Implicit Analytics Evolution\\nAs discussed in Section 2, the last four decades have seen the transfer of data analysis\\non small and simple data, along with hypothesis testing, to data analytics on large\\nand complex data for hypothesis-free knowledge and insight discovery. Today, the sig-\\nniﬁcance and innovation of analytics are better recognized than at any previous time.\\nCorrespondingly, a critical question to ask is What is the conceptual map and evolu-\\ntion of data analytics? Fig. 4 shows a high-level conceptual view of the spectrum and\\nevolution of analytical components and tasks in terms of two major dimensions.\\n— Dimension 1: Levels of visibility, automation and state-of-the-art capabilities: i.e., the\\nlevel of data and analytics complexity that is visible to users, the level of automated\\ndata analytics, and the level of available capability to handle the complexity and\\nsupport the automation. With the upgrading of analytics, the visibility of data and\\nanalytics becomes lower and the level of automated data analytics is lower too. As\\ndata complexity increases, the available capability is weakened. The goal of analytics\\nis to increase the visibility, automation and capability levels of data understanding,\\nproduction and application.\\n— Dimension 2: Degree of X-complexities, X-intelligence and value: i.e., the degree of\\ndata complexity and X-intelligence involved in data and analytics are increased with\\nthe movement from lower-level analytics to higher-level analytics. During this pro-\\ncess, the level of learned intelligence and value resulting from the corresponding\\nanalytics is increased.\\nAs shown in Fig. 4, there are many typical analytical approaches and components\\nthat may be involved in executing analytics tasks. They include reporting, statistical\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:19\\nanalysis, alerting, forecasting, predictive modeling, optimization, prescriptive analyt-\\nics, and actionable knowledge delivery (delivering insights-based actions for business\\ndecision-making and operations) [Cao et al. 2010]. The listed approaches may be used\\nfor non-analytical purposes, and the corresponding analytical tasks may be addressed\\nby non-analytical approaches. An example is optimization, which may be used for ana-\\nlytics to select the best options as an analytics approach or may be achieved by ﬁndings\\nfrom analytics approaches as an analytics objective. There may be different foci and\\nconnections between the listed analytics approaches. For example, forecasting may be\\nused as an approach for prediction when it focuses on probabilistic estimates of possi-\\nble futures, while prediction may involve broad techniques and objectives for estimat-\\ning outcomes.\\nWe also roughly categorize the multiple components and tasks in analytics evolution\\ninto two main eras from the perspective of the disciplinary development of analytics:\\n— Era 1: The era of explicit analytics: which focuses on descriptive analytics. Typical\\nanalytics approaches consist of reporting, statistical analysis, alerting and forecast-\\ning.\\n— Era 2: The era of implicit analytics: which focuses on deep analytics. Typical ana-\\nlytics approaches are predictive modeling, optimization, prescriptive analytics, and\\nactionable knowledge delivery.\\nNote that Fig. 4 only shows an evolution path of the analytics family. It does not\\nindicate the path of analytics within a speciﬁc organization that conducts and uti-\\nlizes analytics. It does not indicate a linear path of analytics evolution either. Often,\\na back-and-forth iterative approach is taken in an analytics team, and multiple ana-\\nlytics components may be involved in parallel for exploring multifaceted observations\\nand understandings.\\n4.2.1. The Era of Explicit Analytics: Descriptive Analytics. Typical elements and tasks have\\nin the past focused on explicit descriptive analysis and have the following features:\\n— Goal: we know what we know, and therefore aim to identify and describe the distri-\\nbution, generation and trends of data and business problems;\\n— Nature of problem: similar to sighted people recognize an elephant, we know what is\\nto be analyzed by hypothesis-based approaches, and for what purposes;\\n— Approach: domain-driven analysis for which hypotheses are available from domain-\\nspeciﬁc knowledge and experts; data analysis tests such hypotheses, and the data\\nveriﬁes and explains the hypotheses;\\n— Outcome: focused methods are available from mathematics and statistics as well as\\nfrom computing. Such methods describe and present what has happened, is happen-\\ning or will happen in usually small or highly manipulated data.\\n4.2.2. The Era of Implicit Analytics: Deep Analytics. The limitation of explicit analytics has\\nrecently been more widely recognized in the analytics community, such as in handling\\nlatent, uncertain and non-IID data [Cao 2014; 2015a]. As a result, the focus has re-\\ncently shifted to implicit analytics, and towards deep analytics. Deep analytics gains\\nan in-depth understanding of why and how things have happened, are happening or\\nwill happen. Such whys and hows cannot be addressed by descriptive analytics and\\ncan determine the next best or worst situation, as well as devise optimal intervention\\nstrategies.\\n— Goal: we do not know what we do not know, and therefore aim to gain a latent but gen-\\nuine understanding of data and business problems from visible and invisible sources;\\n— Nature of problem: similar to blind people recognize an elephant, we do not know\\nwhat is to be analyzed, or even why and what we can obtain;\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:20\\nL. CAO\\nFig. 4.\\nExplicit-to-implicit analytics spectrum and evolution.\\nTable III. Explicit-to-implicit analytics.\\nCategories\\nExplicit analytics\\nImplicit analytics\\nNature\\nSighted people recognize an elephant\\nBlind people do not recognize an elephant\\nGoal\\nWe know what we know\\nWe do not know what we do not know\\nApproach\\nHypothesis + data\\nData + environment (incl. domain)\\nOutcome\\nDescription of data\\nIn-depth representation of data\\n— Approach: data-driven discovery by which interesting but hidden insights are learned\\nfrom data; data creates a view invisible to us and explains the unseen reasons or\\nindicators, to complement domain-driven hypotheses and observations;\\n— Outcome: the focus is on gaining an in-depth, intrinsic and complete understanding\\nof invisible insights, knowledge and wisdom from data, behaviors and environment\\nabout what has happened, is happening or will happen in data and business.\\nTable III summarizes the key categories and features of explicit analytics vs. implicit\\nanalytics.\\n4.3. Descriptive-to-predictive-to-prescriptive Analytics Paradigm Shift\\nThe paradigm shift from data analysis to data science constitutes the so-called “new\\nparadigm” [Nelson 2009; Hey et al. 2009], i.e., data-driven discovery. The history of\\nanalytics from the spectrum and dynamics perspective spans two main eras of ana-\\nlytics, as shown in Fig. 3. Analytics practices have seen a signiﬁcant paradigm shift\\nacross three major stages: (1) Stage 1: descriptive analytics and reporting, (2) Stage 2:\\npredictive analytics and business analytics, and (3) Stage 3: prescriptive analytics and\\ndecision making.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:21\\nWe brieﬂy discuss these three stages below.\\n— Stage 1: Descriptive analytics and business reporting: the major effort is on explicit\\nanalytics, which focuses on descriptive analytics and regular and ad hoc reporting.\\nLimited effort is made on implicit analytics for hidden knowledge discovery, which is\\nmainly achieved by using off-the-shelf tools and built-in algorithms. Business reports\\n(often analytical reports) generated by dashboards and automated processes are the\\nmeans for carrying ﬁndings from analytics to management.\\n— Stage 2: Predictive analytics and business analytics: the major effort is on implicit an-\\nalytics, which focuses on predictive modeling and business analytics (here business\\nanalytics refers to an in-depth understanding of business through deep analytics.\\nNote that this meaning differs from the broad meaning widely adopted in business\\nand management), with more effort being made to apply forecasting, data mining and\\nmachine learning tools for business understanding and prediction. Patterns, scoring\\nand ﬁndings are presented through dashboards and analytical reports to manage-\\nment.\\n— Stage 3: Prescriptive analytics and decision making: the major effort is on the de-\\nlivery of recommended optimal (next best) actions for business decisions by discov-\\nering invisible knowledge and actionable insights from complex data, behavior and\\nenvironment. This is achieved by developing innovative and effective customized al-\\ngorithms and tools to deeply and genuinely understand domain-speciﬁc data and\\nbusiness. Consequently, prescriptive decision-taking strategies, business rules, ac-\\ntions and recommendations are disseminated to decision-makers for the purpose of\\ntaking corresponding actions. By contrast, relatively limited effort is made on explicit\\nanalytics since they are conducted through automated processes and systems.\\nDuring the paradigm shift (as shown in Fig. 5), a signiﬁcant decrease is seen in the\\neffort made in routine explicit analytics, which is increasingly undertaken by auto-\\nmated analytics services. By contrast, a signiﬁcant increase in effort is seen in implicit\\nanalytics and actionable knowledge delivery [Cao et al. 2010]. The shift from a lower\\nstage to a higher stage accommodates an increasingly higher degree of knowledge,\\nintelligence and value to an organization.\\n5. DATA INNOVATION: CHALLENGES AND OPPORTUNITIES\\nIn this section, we summarize the major challenges and opportunities relating to data\\nscience in the relevant communities.\\nThere are two ways of exploring major research challenges: one is to summarize the\\nconcerns of the relevant communities, and the other is to scrutinize the issues from\\nthe perspective of the intrinsic complexity and nature of data science problems as\\ncomplex systems [Cao and Dai 2008; Cao 2015b]. The ﬁrst approach summarizes the\\nmain topics and issues identiﬁed in the statistics [Chambers 1993; Wu 1997; van Dyk\\net al. 2015], informatics and computing [Rudin 2014; Cao 2016b] communities, vendors\\n[Stonebraker et al. 2013] government initiatives [USNSF 2012; UN 2010; Government\\n2015; Commission 2014; UK 2016] and research institutions [UTSAAI 2011; NCSU\\n2007a] which focus on data science and analytics. This can result in a picture of the\\nmain research challenges. The second approach is much more challenging. It requires\\nus to explore the nature of complex data science problems, and the unknown space of\\nthe complexities and comprehensive intelligence in complex data systems.\\nFig. 6 presents a comprehensive conceptual map of data science as a complex sys-\\ntem. It summarizes some of the main challenges faced by the data science community\\nin addressing big data complexities [Cao 2016b]. We categorize the challenges facing\\ndomain-speciﬁc data applications and problems in terms of ﬁve major areas:\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:22\\nL. CAO\\nFig. 5.\\nDescriptive-to-predictive-to-prescriptive analytics paradigm shift.\\nFig. 6.\\nData science conceptual map.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:23\\n— Challenges in data/business understanding: The challenges here are to identify, spec-\\nify, represent and quantify comprehensive complexities, known as X-complexities\\n[Cao 2016b; 2015b]) and intelligence, known as X-intelligence [Cao 2016b; 2015b]).\\nSuch X-complexities and X-intelligence cannot be managed well by existing theo-\\nries and techniques. However, they nonetheless exist and are embedded in domain-\\nspeciﬁc data and business problems. The issue is to understand in what form, at what\\nlevel, and to what extent they exist, and to understand how the respective complex-\\nities and intelligence interact and integrate with one another. An in-depth under-\\nstanding of X-complexities and X-intelligence would subsequently result in devising\\neffective methodologies and technologies for incorporating them into data science\\ntasks and processes.\\n— Challenges in mathematical and statistical foundations: The challenges here are to\\ndiscover and explore whether, how and why existing theoretical foundations are in-\\nsufﬁcient, missing, and problematic in disclosing, describing, representing, and cap-\\nturing the above complexities and intelligence and obtaining actionable insights.\\n— Challenges in X-analytics and data/knowledge engineering: The challenge is to de-\\nvelop domain-speciﬁc analytic theories, tools and systems that are not yet available\\nin the body of knowledge. They will represent, discover, implement and manage the\\nrelevant and resultant data, knowledge and intelligence, and support the engineer-\\ning of big data storage and management, behavior and event processing.\\n— Challenges in social issues: This challenge is to identify, specify and respect social\\nissues related to the domain-speciﬁc data and business understanding and data sci-\\nence processes, including processing and protecting privacy, security and trust and\\nenabling social issues-based data science tasks, which have not so far been handled\\nwell.\\n— Challenges in data value, impact and usability: This challenge is to identify, specify,\\nquantify and evaluate the value, impact, utility and usability of domain-speciﬁc data\\nthat cannot be addressed by existing theories and systems, from technical, business,\\nsubjective, and objective perspectives.\\n— Challenges in data-to-decision and actions: The challenge recognized here is the need\\nto develop decision-support theories and systems that will enable data-driven deci-\\nsion generation, insight-to-decision transformation, as well as decision-making ac-\\ntion generation, and data-driven decision management and governance. These can-\\nnot be managed by existing technologies.\\nThe challenges in X-analytics and data/knowledge engineering involve many speciﬁc\\nresearch issues that have not been properly addressed, for example:\\n— Behavior and event processing: how to capture, store, model, match, query, visualize\\nand manage behaviors and events and their properties, behavior sequences/streams,\\nand the impact and evolution of behaviors and events of individuals and groups in\\nthe physical world.\\n— Data storage and management systems: how to design effective and efﬁcient storage\\nand management systems that can handle big data with high volume, velocity and\\nvariety, and support real-time, online, and on-the-ﬂy processing and analytics; and\\nhow to house such data in an Internet-based (including cloud) environment.\\n— Data quality enhancement: how to handle both existing data quality issues, such\\nas noise, uncertainty, missing values and imbalance which may be present at very\\ndifferent levels due to the signiﬁcantly increased scale, extent and complexity of data.\\nAt the same time, how to handle new data quality issues emerging in the big data\\nand Internet-based data/business environment, such as cross-organizational, cross-\\nmedia, cross-cultural, and cross-economic mechanism data science problems.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:24\\nL. CAO\\n— Data modeling, learning and mining: how to model, learn, analyze and mine data\\nthat is embedded with comprehensive complexity and intelligence.\\n— Deep analytics, learning and discovery: how to discover unknown knowledge and in-\\ntelligence hidden in the space D in Fig. 1 (unknown complexities, knowledge and\\nintelligence, see Can and Fayyad [Cao 2016b]) through inventing new theories and\\nalgorithms for implicit and deep analytics that cannot be handled by existing la-\\ntent learning and descriptive and predictive analytics. Also, how to integrate data-\\ndriven and model-based problem-solving which balances common learning mod-\\nels/frameworks and domain-speciﬁc data complexity and intelligence-driven evidence\\nlearning.\\n— Simulation and experimental design: how to simulate the complexity and intelli-\\ngence, working mechanisms, processes, dynamics and evolution in data and busi-\\nness, and how to design experiments and explore the subsequent impact if certain\\ndata-driven decisions and actions are undertaken in a business.\\n— High-performance processing and analytics: how to support large scale, real-time,\\nonline, high frequency, Internet-based (including cloud-based) cross-organizational\\ndata processing and analytics while balancing local and global resource involvement\\nand objectives. This requires new batch, array, memory, disk storage and processing\\ntechnologies and systems, and massive parallel processing and distributed/parallel\\nand high-performance processing infrastructure, as well as cloud-based processing\\nand storage. It also requires large and complex matrix calculation, mixed data struc-\\ntures and management systems, and data-to-knowledge management.\\n— Analytics and computing architectures and infrastructure: how to facilitate the above\\ntasks and processes by inventing efﬁcient analytics and computing architectures and\\ninfrastructure based on memory, disk, cloud and Internet-based resources and facili-\\nties.\\n— Networking, communication and interoperation: how to support the networking, com-\\nmunication and inter-operation between different data science roles in a distributed\\ndata science team and during the whole-of-cycle of data science problem-solving. This\\nrequires the distributed cooperative management of projects, data, goals, tasks, mod-\\nels, outcomes, workﬂows, task scheduling, version control, reporting and governance.\\nSystematic and interdisciplinary approaches and methodologies are required to ad-\\ndress the above issues in data science and analytics. These may involve developing a\\nsynergy of several research disciplines and areas, including data representation, pre-\\nprocessing and preparation, information processing, parallel processing, distributed\\nsystems, high performance computing, data management, data warehousing, cloud\\ncomputing, evolutionary computation, neural networks, fuzzy systems, enterprise in-\\nfrastructure, system architecture, communication and networking, integration and\\ninteroperation, machine learning, data modeling, analytics and mining, service com-\\nputing, system simulation, experimental design, and evaluation. It may also involve\\nbusiness and social aspects, including industry transformation, enterprise information\\nsystems, business intelligence, business process management, project management,\\ninformation security, trust and reputation, privacy processing, business impact model-\\ning, business value, and utility evaluation [DSAA 2014; Cao 2016a]. Inter-disciplinary\\ninitiatives are necessary to bridge the gaps between the respective disciplines, and to\\ncreate new opportunities for the invention and development of new technologies, the-\\nories and tools to address critical complexities in complex data science problems that\\ncannot be addressed by singular disciplinary efforts.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:25\\n6. DATA ECONOMY: DATA INDUSTRIALIZATION AND SERVICES\\nData science and big data analytics have led to next-generation economy innovation,\\ncompetition and productivity [McKinsey 2011], as shown by the rapidly updated Big\\nData Landscape [BDL 2016b]. Signiﬁcant new business opportunities and previously\\nimpossible prospects have become possible through the creation of data products, data\\neconomy and data industrialization and services [Yiu 2012; CSC 2012; IBM 2016a;\\nLoukides 2012]. In this section, we discuss such opportunities.\\n6.1. Data Industry\\nIf data is viewed in the same way as oil, as the new international currency, then\\nclearly the global economy is experiencing a revolutionary change from data poor to\\ndata rich and data-driven. On one hand, data industrialization creates new business,\\nwhere companies, organizations and even countries compete over how to best use data\\nto create new data products. On the other hand, core businesses, including retail busi-\\nness and manufacturing, are giving way to a new economy that is centered on the data\\nindustry and digital economy. This is evidenced by the domination of data-enabled\\ncompanies listed in the top 10 global companies, especially the largest data company,\\nGoogle, and the largest Initial Public Offering Alibaba.\\nThe data industry is taking shape and gaining signiﬁcance as a driving force in the\\nnew global economy. Without loss of generality, Fig. 7 illustrates those aspects in which\\nnew data business and the resultant areas of data business may grow. The main driv-\\ning forces of the data industry come from the following six core areas: data/analytics\\ndesign, data/analytics content, data/analytics software, data/analytics infrastructure,\\ndata/analytics services, and data/analytics education.\\n— Data/analytics design includes the invention of new methods and ways of design-\\ning and producing digital and data products, services, business models, engagement\\nmodels, communication models, pricing modeling, economic forms, value-added data\\nproducts/services, decision support systems, automation systems and tools;\\n— Data/analytics content includes acquiring, producing, maintaining, publicizing, dis-\\nseminating, recommending and presenting data-centered content through online,\\nmobile, social media platforms and other channels;\\n— Data/analytics software refers to the creation of software, platforms, architectures,\\nservices, tools, systems and applications that acquire, organize, manage, analyze,\\nvisualize, use and present data for speciﬁc business and scientiﬁc purposes, and pro-\\nvide quality assurance to support these aspects;\\n— Data/analytics infrastructure relates to creating infrastructure and devices for data\\nstorage, backup, server revenue, data centers, data management and storage, cloud,\\ndistributed and parallel computing infrastructure, high performance computing in-\\nfrastructure, networking, communications, and security;\\n— Data/analytics services focus on providing strategic and tactical thinking leadership,\\ntechnical and practical consulting services, problem-oriented solutions and applica-\\ntions, outsourcing, and speciﬁc services for data auditing and quality enhancement,\\ndata collection, extraction, transformation and loading, recommendation, data cen-\\nter/infrastructure hosting, data analytics and more;\\n— Data/analytics education enables the building of corporate competency and training,\\nas well as offering online/ofﬂine/degree-based courses, workshops, materials and ser-\\nvices that will allow the gaps in the supply of qualiﬁed data professionals to be ﬁlled,\\nthus contributing to building and enhancing the community of this discipline.\\nThe above six core data/analytics areas will see the growth of new data business in\\nterms of the following core aspects and procedures: data storage and management, un-\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:26\\nL. CAO\\nFig. 7.\\nData & analytics-enabled industry and business transformation.\\nderstanding, processing, optimization, value-added opportunities, transport and com-\\nmunication, servicing and decision support.\\nThe respective core data/analytics sectors and core procedures can be developed\\nin any data-related sectors, especially data-intensive domains and sectors such as\\ntelecommunication, government, ﬁnance, banking, capital markets, lifestyle and ed-\\nucation. Core business including manufacturing and living business will see increased\\nopportunities for better collection, management and use of data. Analytics will be un-\\ndertaken to improve productivity, effectiveness, and efﬁciency, and create new value-\\nadded growth in the economy.\\nInterestingly, as we have seen, the data economy occupies the top 10 largest capital\\nentities. The data industry continues to create new business models, products, services,\\noperationalization modes, and workforce models. Data economy will further change the\\nway we live, work, learn and are entertained, as new facilities and environments are\\ncreated in which data plays a critical role.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:27\\n6.2. Data Services\\nData services form part of the whole landscape of data and analytics which, as noted\\nabove, is changing every aspect of the way we live. Data services can be differentiated\\nfrom traditional services by the fact that they are not traditional physical material- or\\nenergy-oriented services.\\n— Data services act as the core business rather than the auxiliary business of an econ-\\nomy;\\n— Data-driven production and decision-making emerges as the core function in large or-\\nganizations for complex decision-making and strategic planning, rather than adjunct\\nfacilities;\\n— Data services are online, mobile and socially based, embedded in our activities and\\nagenda;\\n— Data business is global and 24/7, offered at any place at any time on demand or in a\\nsupply-driven mode;\\n— The provision of data services does not require traditional production elements such\\nas intensive workshops, factories and ofﬁce facilities;\\n— Data-driven services offer real-time public service data management, high perfor-\\nmance processing, analytics and decision-making;\\n— Data-driven services support full life-cycle analysis, from descriptive, predictive and\\nprescriptive analytics for the prediction, detection, and prevention of risk, to innova-\\ntion and optimization;\\n— Data-analytical services are intelligent or can enhance the intelligence of generous\\ndata and information services;\\n— Data services enable cross-media, cross-source and cross-organization innovation\\nand practice; and\\n— Data services demonstrate signiﬁcant savings and efﬁciency improvement through\\nthe delivery of actionable knowledge/insights.\\nSome typical data services delivered through analytics for both core business and\\nnew economy are listed below as examples:\\n— Credit scoring: to establish the credit worthiness of a customer requesting a loan;\\n— Fraud detection: to identify fraudulent transactions and suspicious behavior;\\n— Healthcare: to detect over-service, under-service, fraud, and events like epidemics;\\n— Insurance: to detect fraudulent claims and assess risk;\\n— Manufacturing process analysis: to identify the causes of manufacturing problems\\nand to optimize the processes;\\n— Marketing and sales: to identify potential customers and establish the effectiveness\\nof campaigns;\\n— Portfolio trading: to optimize a portfolio of ﬁnancial instruments by maximizing re-\\nturns and minimizing risk;\\n— Surveillance: to detect intrusion, objects, persons and linkages from multi-sensor\\ndata and remote sensing;\\n— Understanding customer behaviors: to model churn, afﬁnities, propensities, and next\\nbest actions on intervention behaviors;\\n— Web analytics: to model user preferences from data to devise and provide personalized\\nand targeted services.\\nA major challenge and increasing need in the data industry is to provide global or\\nInternet-based data services for a collection of organizations, such as multi-national\\ncompanies and whole-of-government. Such services need to\\n— Deﬁne global data analytics objectives and beneﬁts;\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:28\\nL. CAO\\n— Support good data governance, security, privacy and accountability to enable smarter\\ndata use and sharing;\\n— Support data matching and sharing in the context of cross-organizational, cross-\\nplatform, cross-format and cross-analytical goals;\\n— Prepare global and organization-speciﬁc local/departmental data;\\n— Foster global and local analytics capabilities, capacity and competency;\\n— Enable sharing and collaboration in data and analytics skills, infrastructure, tools,\\ntechniques and outcomes;\\n— Support crowdsourcing, collaborative and parallel analytic tasks and analytic work-\\nﬂow management;\\n— Support analytic capability and package sharing;\\n— Support data and data software versioning management and control [Bhardwaj et al.\\n2015] at a global and collaborative level;\\n— Support the visualization and dissemination of outcomes to targeted audiences and\\nin personalized preferences.\\nData-driven industry and service are forming new trends in data science for busi-\\nness, for instance:\\n— Advanced analytics is no longer just for analysts [Ghodke 2015]; dummy analytics is\\nbecoming the default setting of management and operational systems;\\n— Cloud data management, storage and cloud-based analytics are gaining popularity\\n[Ghodke 2015] and are replacing traditional management information systems, busi-\\nness support systems, and operational support systems;\\n— Data science on scale from multiple sources of data is becoming feasible; Internet-\\nbased services are a strongly growing area of the new economy;\\n— Analytics as a service is becoming feasible with appropriate social issue manage-\\nment, as analytics becomes a reality everywhere and is embedded in business, mo-\\nbile, social and online services;\\n— Visual analytics is becoming a common language;\\n— Data services can be mixed with virtual reality and presented in a way that combines\\nphysical and virtual worlds, resources and intelligence;\\n— Services on matched and mixed data are streamlined into a one-world process, with\\nboth local and global objectives addressed.\\n7. DATA EDUCATION: CAPABILITIES AND COMPETENCY\\nData innovation and economy is dependent on the corresponding data and analytics\\ncapabilities and competencies and the ability to handle related social issues. These are\\nweak areas, and there are signiﬁcant gaps in the current body of knowledge, organi-\\nzational maturity [Paulk et al. 1993; Crowston and Qin 2011], education and training.\\nThe requirement is to “think with data”, “manage data”, “compute with data”, “mine\\non data”, “communicate with data”, “deliver with data”, and “take action on data” [Cao\\n2016c]. This section discusses these important matters.\\nMore and more industry and government organizations recognize the value of data\\nfor decision-making and have set up general and speciﬁc data scientist roles to support\\ndata science and engineering, e.g., Chief Data Ofﬁcer, Chief Analytics Ofﬁcer, data\\nmodelers and data miners, in addition to data engineers and business analysts.\\n7.1. Data Scientists in a Sexy Profession\\nThe role of the data scientist was recognized 10 years ago, and it has become a sexy\\nprofession in the job market. The next-generation data scientists will be mostly wel-\\ncomed in the increasingly important data economy and data-to-decision.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:29\\nIn 2004, Dr Usama Fayyad was appointed as the Chief Data Ofﬁcer of Yahoo, which\\nopened the door to a new career possibility: the data science professional [Manieri et al.\\n2015; Harris et al. 2013] or more speciﬁcally data scientist, for those people whose role\\nvery much centers on data. In 2015, the White House appointed the ﬁrst U.S. Chief\\nData Scientist [Whitehouse 2015]. This role “will shape policies and practices to help\\nthe U.S. remain a leader in technology and innovation, foster partnerships to help\\nresponsibly maximize the nation’s return on its investment in data, and help to recruit\\nand retain the best minds in data science to join us in serving the public.” [Whitehouse\\n2015].\\nToday, the role of data scientist [Patil 2011] is regarded as “the sexiest job of the\\n21st century” [Davenport and Patil 2012]. It is reported that data scientists earn much\\nhigher salaries than those in other data-related jobs, with a median salary of US$120k\\nfor data scientists and US$160k for managers, according to the 2014 Burtchworks sur-\\nvey [Burtch 2014]. This is attributed to the fact that 88% of respondents in this survey\\nhave at least a Master’s degree, while 46% also hold a Doctorate compared to only 20%\\nof other Big Data professionals. In the 2015 O’Reilly survey [King and Magoulas 2015],\\n23% were found to hold a doctorate, while another 44% had a Master’s. The median\\nannual base salary of this survey sample was US$91,000 globally, and among US re-\\nspondents was US$104,000, compared to US$150k for “upper management” (higher\\nthan project and product managers).\\n7.2. What Does a Data Scientist Do\\nSo, what are the roles and responsibilities of data scientists? Here we summarize the\\nﬁndings from several documents on government initiatives:\\n— The US National Science Board deﬁnes data scientists as “the information and com-\\nputer scientists, database and software engineers and programmers, disciplinary ex-\\nperts, curators and expert annotators, librarians, archivists, and others, who are cru-\\ncial to the successful management of a digital data collection.” [NSB 2005]\\n— In a report from the US Committee on Science of the National Science and Technol-\\nogy Council, data scientists are deﬁned as “Scientists who come from information or\\ncomputer science backgrounds but learn a subject area and may become scientiﬁc\\ndata curators in disciplines and advance the art of data science. Focus on all parts of\\nthe data life cycle.” [CSNSTC 2009]\\n— The Joint Information Systems Committee deﬁnes data scientists as “people who\\nwork where the research is carried out, or, in the case of data centre personnel, in\\nclose collaboration with the creators of the data and may be involved in creative\\ninquiry and analysis, enabling others to work with digital data, and developments in\\ndata base technology.” [Swan and Brown 2008]\\nIn business, immense interest has been expressed by multinational vendors, so-\\ncial media and online communities, and information providers, such as IBM [IBM\\n2016b], LinkedIn [LinkedIn 2016], KDnuggets [Kdnuggets 2016], Facebook [Facebook\\n2016] and SIAM [SIAM 2016] about the roles and responsibilities of data scientists\\nand what makes a good data scientist. For instance, a data scientist metromap was\\ncreated in Chandrasekaran [Chandrasekaran 2013]. The metromap covers 10 areas\\nand domains: Fundamentals, Statistics, Programming, Machine Learning, Text Min-\\ning/Natural Language Processing, Data Visualization, Big Data, Data Ingestion, Data\\nMunging and Toolbox. Each area/domain is represented as a “metro line”, with the\\nstations depicting the topics to be learned and understood in a progressive fashion.\\nIn addition, INFORMS summarizes the following seven job tasks for data scientists\\n[INFORMS 2014]: Business Problem (Question) Framing, Analytics Problem Fram-\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:30\\nL. CAO\\ning, Data, Methodology (Approach) Selection, Model Building, Deployment, and Model\\nLifecycle Management.\\nAn increasing number of academic and research institutions are working on deﬁning\\nthe certiﬁcation and accreditation of next-generation data scientists. This is reﬂected\\nin general and domain-speciﬁc data science curricula for Masters and PhD qualiﬁca-\\ntions, such as a PhD in Analytics [UTS 2011] and Master’s degree in SCM predictive\\nanalytics [INFORMS 2014].\\nWithout the loss of generality, typical domain-free and problem-neutral responsi-\\nbilities and requirements for jobs announced in social media channels (e.g., Google\\nGroups, Facebook and LinkedIn) and what we have experienced in the past 15 years\\nin large governmental and business organizations can be summarized as follows:\\n— Learn the business problem domain, talk to business experts and decision-makers to\\nunderstand the business objectives, requirements and preferences, issues and con-\\nstraints facing an organization; understand the organizational maturity; identify,\\nspecify and deﬁne the problems, boundaries and environment, as well as the chal-\\nlenges; generate business understanding reports;\\n— Identify and specify social and ethical issues such as privacy and security; develop\\nethical reasoning plans to address social and ethical issues;\\n— Understand data characteristics and complexities; identify the problems and con-\\nstraints of the data; develop a data understanding report; specify and scope analyti-\\ncal goals and milestones by developing respective project plans to set up an agenda\\nand create governance and management plans;\\n— Set up engineering and analytical processes corresponding to analytical goals for\\nturning business and data into information, turning information into insight, and\\nturning insight into business decision-making actions by developing technical plans\\nfor the discovery, upgrade and deployment of relevant data intelligence;\\n— Transform business problems into analytical tasks, and conduct advanced analytics\\nby developing corresponding techniques, models, methods, algorithms, tools and sys-\\ntems, experimental design and evaluation of data science, generating better practices\\nexperience, performing descriptive, predictive and prescriptive analytics, conducting\\nsurvey research, and supporting visualization and presentation;\\n— Based on the understanding of data characteristics and complexities, extract, an-\\nalyze, construct, mine and select discriminative features, constantly optimize and\\ninnovate new variables for best possible problem representation and modeling; when\\nnecessary, conduct data quality enhancement [Hazena et al. 2014];\\n— Combine analytical, statistical, algorithmic, engineering and technical skills to mine\\nrelevant data by involving contextual information; invent novel and effective mod-\\nels, and constantly improve modeling techniques to optimize and boost model perfor-\\nmance and seek to achieve best practice;\\n— Maintain, manage and reﬁne projects and milestones, and their processes, deliver-\\nables, evaluation, risk and reporting to build active, lifecycle management;\\n— Develop corresponding services, solutions and products or modules to feed into a\\nsystem package on top of user-speciﬁed programming languages, frameworks and\\ninfrastructure, or open source tools and frameworks;\\n— Maintain the privacy, security and veracity of data and deliverables;\\n— Engage in frequent client interaction during the whole lifecycle; tell clear and concise\\nstories and draw simple conclusions from complex data or algorithms; provide clients\\nwith situational analyses and deep insights into areas requiring improvement; trans-\\nlate into business improving actions in the ﬁnal deployment;\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:31\\n— Write coherent reports and make presentations to specialists and non-specialists;\\npresent executive summaries with precise and evidence-based recommendations and\\nrisk management strategies, especially for decision-makers and business owners.\\n7.3. What Makes a Good Data Scientist\\nTo satisfy the above position requirements, data scientist candidates need to have cer-\\ntain qualiﬁcations in addition to the analytic skills that are the foundation of this role.\\nThese qualiﬁcations and abilities include:\\n— Thinking, mindset and ability to think analytically, creatively, critically and inquisi-\\ntively;\\n— Methodologies and knowledge of complex systems and approaches for conducting\\nboth top-down and bottom-up problem-solving;\\n— Master’s or PhD degree in computer science, statistics, mathematics, analytics, data\\nscience, informatics, engineering, physics, operations research, pattern recognition,\\nartiﬁcial intelligence, visualization, information retrieval or related ﬁelds;\\n— A deep understanding of common statistics, data mining and machine learning\\nmethodologies and models;\\n— Ability to implement, maintain, and troubleshoot big data infrastructure, such as\\ncloud computing, high performance computing infrastructure, distributed processing\\nparadigms, stream processing and databases;\\n— Knowledge of human-computer interactions, visualization and knowledge represen-\\ntation and management;\\n— Background in software engineering (including systems design and analysis), quality\\nassurance;\\n— Experience working with large datasets, and mixed data types and sources in a net-\\nworked and distributed environment;\\n— Experience in data extraction and processing, feature understanding and relation\\nanalysis;\\n— Active interest and knowledge in multi-disciplinary and trans-disciplinary studies\\nand methods in scientiﬁc, technical, and social and life sciences;\\n— Substantial experience with state-of-the-art analytics-oriented scripting, data struc-\\ntures, programming languages, and development platforms in a Linux, cloud or dis-\\ntributed environment;\\n— Theoretical background and domain knowledge for the evaluation of the technical\\nand business merits of analytic ﬁndings;\\n— Excellent written and verbal communication [Matsudaira 2015] and organizational\\nskills, ability to write and edit analytical materials and reports for different audi-\\nences, and capacity to transform analytical concepts and outcomes into business-\\nfriendly interpretations; ability to communicate actionable insights to non-technical\\naudiences, and experience in data-driven decision making.\\nWhile there is signiﬁcant role overlap between data scientists and business intel-\\nligence (BI) professionals [SAS 2013], different research works show that data sci-\\nence professionals are generally much more data and technology-savvy rather than\\nbusiness-oriented, with most holding a Master’s or PhD degree in statistics or com-\\nputer science.\\nAn EMC data science community survey [EMC 2011] shows that (1) data scientists\\ncan open up new possibilities; (2) compared to 37% of BI professionals trained in busi-\\nness, 24% of data science professionals are in computer science, 17% are in engineering\\nand 11% are in hard science; (3) compared to BI toolkits, data science toolkits are more\\ntechnically sophisticated and more diversiﬁed; (4) the number of data scientists un-\\ndertaking data experiments is almost double that of BI professionals; (5) data science\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:32\\nL. CAO\\nprofessionals more frequently interact with diverse technical and business roles in\\nan organization (such as data scientists, strategic planners, statisticians, marketing\\nstaff, sales people, graphic designers, business management and IT administration,\\nprogrammers, and HR personnel) than BI professionals; (6) compared to working on\\nnormal data, big data manipulators tend to tackle more sophisticated data complex-\\nities; and (7) data science professionals spend almost double the time they spend on\\nnormal data on big data manipulation (e.g., data parsing, organization, mining, algo-\\nrithms, visualization, story-telling, dynamics, and decisions).\\nAs a data-centric expert, a good data scientist is also expected to know the underly-\\ning domain well. Without an in-depth understanding of the domain, the actionability\\nof the data deliverables and products by data scientists may be low. However, a data\\nscientist is no substitute for domain experts in complex data science problem solving\\n[Cao 2016b]. Similar to any other disciplinary specialists, data scientists work more ef-\\nfectively by collaborating with domain-speciﬁc specialists and subject matter experts\\nto achieve broader impact. This is similar to the requirements of domain-driven, ac-\\ntionable knowledge discovery [Cao et al. 2010].\\n7.4. Tools for Data Scientists\\nIn the above sections, the respective responsibilities and qualiﬁcations of data scien-\\ntists have been discussed. In Section 5, relevant research challenges and issues have\\nbeen listed. To support the data services listed in Section 6.2, the corresponding roles\\nand qualiﬁcations discussed in Sections 7.2 and 7.3 are necessary.\\nIn this section, we discuss tools that may be used by data scientists to address the\\nabove aspects. Tools are categorized in terms of cloud infrastructure, data and appli-\\ncation integration, data preparation and processing, analytics, visualization, program-\\nming, master data management, high performance processing, business intelligence\\nreporting, and project management. A data scientist may use one or more of these\\ntools on demand for data science problem-solving.\\n— Cloud infrastructure: Such as Apache Hadoop, Spark, Cloudera, Amazon Web Ser-\\nvices, Unix shell/awk/gawk, 1010data, Hortonworks, Pivotal, and MapR. Most tradi-\\ntional IT vendors have migrated their services and platforms to support cloud.\\n— Data/application integration: Including Ab Initio, Informatica, IBM InfoSphere\\nDataStage, Oracle Data Integrator, SAP Data Integrator, Apatar, CloverETL, Infor-\\nmation Builders, Jitterbit, Adeptia Integration Suite, DMExpress Syncsort, Pentaho\\nData Integration, and Talend [Review 2016].\\n— Master data management: Typical software and platforms include IBM InfoSphere\\nMaster Data Management Server, Informatica MDM, Microsoft Master Data Ser-\\nvices, Oracle Master Data Management Suite, SAPNetWeaver Master Data Manage-\\nment tool, Teradata Warehousing, TIBCO MDM, Talend MDM, Black Watch Data.\\n— Data preparation and processing: In Today [Today 2016], 29 data preparation tools\\nand platforms were listed, such as Platfora, Paxata, Teradata Loom, IBM SPSS,\\nInformatica Rev, Omniscope, Alpine Chorus, Knime, and Wrangler Enterprise and\\nWrangler.\\n— Analytics: In addition to well-recognized commercial tools including SAS Enterprise\\nMiner, IBM SPSS Modeler and SPSS Statistics, MatLab and Rapidminer [Rapid-\\nMiner 2016], many new tools have been created, such as DataRobot [DataRobot\\n2016], BigML [BigML 2016], MLBase [Lab 2016], and APIs including Google Cloud\\nPrediction API [Google 2016b].\\n— Visualization: Many free and commercial software are listed in KDnuggets [KD-\\nnuggets 2015] for visualization, such as Interactive Data Language, IRIS Explorer,\\nMiner3D, NETMAP, Panopticon, ScienceGL, Quadrigram, and VisuMap.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:33\\n— Programming: In addition to the main languages R, SAS, SQL, Python and Java,\\nmany others are used for analytics, including Scala, JavaScript, .net, NodeJS, Obj-C,\\nPHP, Ruby, and Go [Davis 2016].\\n— High performance processing: In Wikipedia [Wikipedia 2016a], about 40 computer\\ncluster software are listed and compared in terms of their technical performance,\\nsuch as Stacki, Kubernetes, Moab Cluster Suite, and Platform Cluster Manager.\\n— Business intelligence reporting: There are many reporting tools available [Capterra\\n2016b; Wikipedia 2016c], typical of which are Excel, IBM Cognos, MicroStrategy, SAS\\nBusiness Intelligence, and SAP Crystal Reports.\\n— Project management: In Capterra [Capterra 2016a], more than 500 software and\\ntools were listed for project management, including Microsoft Project, Atlassian, Po-\\ndio, Wrike, Basecamp, and Teamwork.\\n— Social network analysis: In Desale [Desale 2015], 30 tools were listed for SNA and vi-\\nsualization, such as Centrifuge, Commetrix, Cuttleﬁsh, Cytoscape, EgoNet, InFlow,\\nJUNG, Keynetiq, NetMiner, Network Workbench, NodeXL, and SocNetV (Social Net-\\nworks Visualizer).\\n— Other tools: Increasing numbers of tools have been developed and are under devel-\\nopment for domain-speciﬁc and problem-speciﬁc data science, such as Alteryx and\\nTableau for tablets; SuggestGrid and Mortar Recommendation Engine for recom-\\nmender systems [Github 2016b]; OptumHealth, Verisk Analytics, MedeAnalytics,\\nMcKesson and Truven Health Analytics [Technavio 2016] for healthcare analytics;\\nBLAST, EMBOSS, Staden, THREADER, PHD and RasMol for bioinformatics.\\n8. THE FUTURE OF DATA SCIENCE\\nThere is continuing debate about how data science will evolve in the next 50 years\\nand what it will ultimately look like. With the joint efforts to be made by the entire\\nscientiﬁc community, data science will build its systematic scientiﬁc foundations, dis-\\nciplinary structure, theoretical systems, technological families, and engineering tool\\nsets as an independent science.\\nThe last 50 years since the proposal of the concept “data science” has contributed to\\nthe progressive and now widespread acceptance of the need for a new science and its\\ninitial conceptualization through its transition and transformation from statistics to\\nthe merger with existing disciplines and ﬁelds. The next 50 years of data science will\\nextend beyond statistics to identify, discover, explore, and deﬁne speciﬁc foundational\\nscientiﬁc problems and grand challenges. It will build a systematic family of scientiﬁc\\nmethodologies and methods and self-contained disciplinary systems and curricula that\\nare not merely a relabeled ’salad’ created by mixing existing disciplinary components.\\nBased on the understanding of the intrinsic challenges and nature of data science\\n[Cao 2016b; 2016c], the development of data science may seek to:\\n— Design and develop data brain that can autonomously mimic human brain working\\nmechanisms while recognize, understand, analyze and learn data and environment,\\ninfer and reason about knowledge and insight, and correspondingly decide actions;\\n— Deepen our understanding of data invisibility (i.e., invisible data characteristics, com-\\nplexities, intelligence and value), in particular, to understand their X-complexities\\nand X-intelligence (see Cao and Fayyad [Cao 2016b]). The exploration of what we\\ndo not know about what we do not know will strengthen our understanding of the\\ncapabilities, limitations, and future directions of data science.\\n— Broaden conceptual, theoretical and technological systems for data science by en-\\nabling cross-disciplinary and trans-disciplinary research, innovation and education.\\nThis will address existing issues such as the variations in statistics hypotheses and\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:34\\nL. CAO\\nwill discover and propose problems that are currently invisible to broad science or\\nspeciﬁc ﬁelds;\\n— Invent new data representation capabilities, including designs, structures, schemas\\nand algorithms to make invisible data complexities and unknown characteristics in\\ncomplex data more visible and explicit, and more easily understood or explored;\\n— Design new storage, access and management mechanisms, including memory, disk\\nand cloud-based mechanisms, to enable the acquisition, storage, access, sampling,\\nand management of richer characteristics and properties in the physical world that\\nhave been simpliﬁed and ﬁltered by existing systems, and to support scalable, trans-\\nparent, ﬂexible, interpretable and personalized data manipulation and analytics in\\nreal time;\\n— Create new analytical and learning capabilities, including original mathematical,\\nstatistical and analytical theories, algorithms and models, to disclose the unknown\\nknowledge in unknown space;\\n— Build new intelligent systems and services, including corporate and Internet-based\\ncollaborative platforms and services, to support the automated, or human-data-\\ncooperative, collaborative and collective exploration of invisible and unknown chal-\\nlenges in unknown space;\\n— Train the next-generation data scientists and data professionals who are qualiﬁed\\nfor data science problem-solving, with data literacy, thinking, competency, conscious-\\nness, curiosity, communication and cognitive intelligence, to work on the above data\\nscience agenda;\\n— Assure cross-domain and trans-disciplinary cooperation, collaborations and alliance\\nin complex data science problem-solving. This requires the education of competent\\ndata scientists who are multi-disciplinary experts, as well as collaboration between\\ndata scientists and domain-speciﬁc experts; and\\n— Discover and invent data power as yet unknown to current understanding and imagi-\\nnation, such as new data economy, mobile applications, social applications, and data-\\ndriven business.\\n9. CONCLUSIONS\\nData science, big data and advanced analytics have been increasingly recognized as\\nmajor driving forces for next-generation innovation, economy, and education. Although\\nthey are at an early stage of development, strategic discussions about the big picture,\\ntrends, major challenges, future directions, and prospects are critical for the healthy\\ndevelopment of the ﬁeld and the community. The purpose of this article has been to\\nshare an overview of the conceptualization, development, observations and thinking\\nabout the age of data science initiatives, research, innovation, industrialization, pro-\\nfession, competency and education.\\nWe are witnessing a highly evolving data world that seamlessly connects to our\\ndaily life, work, learning, economy and entertainment. New efforts are increasingly\\nbeing made by government, industry, academia and even private institutions on ways\\nto convert data for decision-making, and promote the research and development of\\ndata science and analytics. The next generation of data science, encompassing a broad\\nrange of disciplines, science and economy, relies heavily on the strategic planning and\\nvisionary actions that will be undertaken in prioritized data research areas and start-\\nups. Without any doubt, today’s questions such as “why do we need data science” will\\nbe replaced by a family of scientiﬁc theories and tools to address the visible grand\\nchallenges and signiﬁcant problems facing tomorrow’s big data, science, business, so-\\nciety, and the economy. We will be greatly amazed by the surprising developments and\\npotential changes that will take place in the next 50 years.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:35\\nACKNOWLEDGMENTS\\nThis work is partially sponsored by the Australian Research Council Discovery Grant (DP130102691).\\nREFERENCES\\nACEMS. 2014. The Australian Research Council (ARC) Centre of Excellence for Mathematical and Statisti-\\ncal Frontiers. (2014). Available at acems.org.au/.\\nRitu Agarwal and Vasant Dhar. 2014. Editorial-Big Data, Data Science, and Analytics: The Opportunity\\nand Challenge for IS Research. Information Systems Research 25, 3 (2014), 443–448.\\nXinhua News Agency. 2016. The 13th Five-Year Plan for the National Economic and Social Development of\\nthe People’s Republic of China. (2016). Available at http://news.xinhuanet.com/politics/2016lh/2016-03/\\n17/c 1118366322.htm.\\nAGIMO. 2013. AGIMO Big Data Strategy - Issues Paper. (2013). Available at www.ﬁnance.gov.au/ﬁles/2013/\\n03/Big-Data-Strategy-Issues-Paper1.pdf.\\nPaul E. Anderson, James F. Bowring, Rene McCauley, George Pothering, and Christopher W. Starr. 2014.\\nAn undergraduate degree in data science: Curriculum and a decade of implementation experience. In\\nComputer Science Education: Proceedings of the 45th ACM Technical Symposium (SIGCSE’14). 145–\\n150.\\nASA. 2015. ASA views on data science. (2015). Available at http://magazine.amstat.org/?s=data+science&x=\\n0&y=0.\\nAU. 1990. Data-matching Program. (1990). Available at http://www.comlaw.gov.au/Series/C2004A04095.\\nAU.\\n2010.\\nDeclaration\\nof\\nOpen\\nGovernment.\\n(2010).\\nAvailable\\nat\\nhttp://agimo.gov.au/2010/07/16/\\ndeclaration-of-open-government/.\\nAU.\\n2013.\\nAttorney-General’s\\nDepartment.\\n(2013).\\nAvailable\\nat\\nhttp:\\n//www.attorneygeneral.gov.au/Mediareleases/Pages/2013/Second\\\\%20quarter/\\n22May2013-AustraliajoinsOpenGovernmentPartnership.aspx.\\nAU. 2016. Australia Big Data. (2016). Available at http://www.ﬁnance.gov.au/big-data/.\\nK. Ayankoya, A. Calitz, and J. Greyling. 2014. Intrinsic relations between data science, big data, business\\nanalytics and dataﬁcation. ACM International Conference Proceeding Series 28 (2014), 192–198.\\nJ. Bailer, R. Hoerl, D. Madigan, J. Montaquila, and T. Wright. 2012. Report of the ASA Workgroup on Mas-\\nter’s Degrees. (2012).\\nBen Baumer. 2015. A data science course for undergraduates: Thinking with data. The American Statistician\\n69, 4 (2015), 334–342.\\nBDL. 2016a. Big Data Landscape. (2016). Available at www.bigdatalandscape.com.\\nBDL. 2016b. Big Data Landscape 2016 (Version 3.0). (2016). Available at http://mattturck.com/2016/02/01/\\nbig-data-landscape/.\\nMark A. Beyer and Douglas Laney. 2012. The Importance of ‘Big Data’: A Deﬁnition. (2012). Available at\\nhttps://www.gartner.com/doc/2057415.\\nAnant Bhardwaj, Souvik Bhattacherjee, Amit Chavan, Amol Deshp, Aaron J. Elmore, Samuel Madden, and\\nAditya Parameswaran. 2015. Datahub: Collaborative data science & dataset version management at\\nscale. In In CIDR.\\nBigML. 2016. BigML. (2016). Available at https://bigml.com/.\\nKirk D. Borne, Suzanne Jacoby, Karen Carney, Andy Connolly, Timothy Eastman, M. Jordan Raddick, J. A.\\nTyson, and John Wallin. 2010. The revolution in astronomy education: Data science for the masses.\\n(2010). Available at http://arxiv.org/pdf/0909.3895v1.pdf.\\nSebastien Boyer, Ben U. Gelman, Benjamin Schreck, and Kalyan Veeramachaneni. 2015. Data science\\nfoundry for MOOCs. In IEEE International Conference on Data Science and Advanced Analytics\\n(DSAA). 1–10.\\nLeo Breiman. 2001. Statistical modeling: The two cultures. Statist. Sci. 16, 3 (2001), 199–231.\\nGavin Brown. 2009. Review of Education in Mathematics, Data Science and Quantitative Disci-\\nplines: Report to the Group of Eight Universities. (2009). Available at https://go8.edu.au/publication/\\ngo8-review-education-mathematics-data-science-and-quantitative-disciplines.\\nLinda Burtch. 2014. The Burtch Works Study: Salaries of Data Scientists. (April 2014). Available at http:\\n//www.burtchworks.com/ﬁles/2014/07/Burtch-Works-Study DS ﬁnal.pdf.\\nKanyarat Bussaban and Phanu Waraporn. 2015. Preparing undergraduate students majoring in computer\\nscience and mathematics with data science perspectives and awareness in the age of big data. In 7th\\nWorld Conference on Educational Sciences, Vol. 197. 1443–1446.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:36\\nL. CAO\\nCA.\\n2016.\\nCanada\\nCapitalizing\\non\\nBig\\nData.\\n(2016).\\nAvailable\\nat\\nhttp://www.sshrc-crsh.gc.ca/\\nnews room-salle de presse/latest news-nouvelles recentes/big data consultation-donnees massives\\nconsultation-eng.aspx.\\nLongbing Cao. 2010a. Domain driven data mining: Challenges and prospects. IEEE Trans. on Knowledge\\nand Data Engineering 22, 6 (2010), 755–769.\\nLongbing Cao. 2010b. In-depth behavior understanding and use: The behavior informatics approach. Infor-\\nmation Science 180, 17 (2010), 3067–3085.\\nLongbing Cao. 2011. Strategic Recommendations on Advanced Data Industry and Services for the Yanhuang\\nScience and Technology Park. (2011).\\nLongbing Cao. 2014. Non-IIDness learning in behavioral and social data. Comput. J. 57, 9 (2014), 1358–\\n1370.\\nLongbing Cao. 2015a. Coupling learning of complex interactions. J. Information Processing and Management\\n51, 2 (2015), 167–186.\\nLongbing Cao. 2015b. Metasynthetic Computing and Engineering of Complex Systems. Springer.\\nLongbing Cao. 2016a. Data Science and Analytics: A New Era. International Journal of Data Science and\\nAnalytics 1, 1 (2016), 1–2.\\nLongbing Cao. 2016b. Data science: Challenges and directions. (2016). Technical Report, UTS Advanced\\nAnalytics Institute.\\nLongbing Cao. 2016c. Data Science: Nature and Pitfalls. (2016). Technical Report, UTS Advanced Analytics\\nInstitute.\\nLongbing Cao. 2016d. Data Science: Profession and Education. (2016). Technical Report, UTS Advanced\\nAnalytics Institute.\\nLongbing Cao. 2017. Understand Data Science (to be published). Springer.\\nLongbing Cao and Ruwei Dai. 2008. Open Complex Intelligent Systems. Post & Telecom Press.\\nLongbing Cao, Ruwei Dai, and Mengchu Zhou. 2009. Metasynthesis: M-Space, M-Interaction and M-\\nComputing for Open Complex Giant Systems. IEEE Trans. On Systems, Man, and Cybernetics–Part\\nA 39, 5 (2009), 1007–1021.\\nLongbing Cao and Philip S Yu (Eds). 2012. Behavior Computing: Modeling, Analysis, Mining and Decision.\\nSpringer.\\nLongbing Cao, Yuming Ou, and Philip S Yu. 2012. Coupled behavior analysis with applications. IEEE Trans.\\non Knowledge and Data Engineering 24, 8 (2012), 1378–1392.\\nLongbing Cao, Philip S Yu, Chengqi Zhang, and Yanchang Zhao. 2010. Domain Driven Data Mining.\\nSpringer.\\nCapterra.\\n2016a.\\nTop\\nProject\\nManagement\\nTools.\\n(2016).\\nAvailable\\nat\\nhttp://www.capterra.com/\\nproject-management-software/.\\nCapterra.\\n2016b.\\nTop\\nReporting\\nSoftware\\nProducts.\\n(2016).\\nAvailable\\nat\\nhttp://www.capterra.com/\\nreporting-software/.\\nCBDIO. 2016. China Big Data Industrial Observation. (2016). Available at www.cbdio.com.\\nCCF-BDTF. 2013. China Computer Federation Task Force on Big Data. (2013). Available at http://www.\\nbigdataforum.org.cn/.\\nJohn M Chambers. 1993. Greater or lesser statistics: A choice for future research. Statistics and Computing\\n3, 4 (1993), 182–184.\\nSwami Chandrasekaran. 2013. Becoming a Data Scientist. (2013). Available at http://nirvacana.com/\\nthoughts/becoming-a-data-scientist/.\\nH. Chen, R. H. L. Chiang, and V. C. Storey. 2012. Business intelligence and analytics: From Big Data to big\\nimpact. MIS Quarterly 36, 4 (2012), 1165–1188.\\nThomas R. Clancy, Kathryn H. Bowles, Lillee Gelinas, Ida Androwich, Connie Delaney, Susan Matney,\\nJoyce Sensmeier, Judith Warren, John Welton, and Bonnie Westra. 2014. A call to action: Engage in big\\ndata science. Nursing Outlook 62, 1 (2014), 64–65.\\nClasscentral. 2016. Data Science and Big Data — Free Online Courses. (2016). Available at https://www.\\nclass-central.com/subject/data-science.\\nKelly Clay. 2013. CES 2013: The Year of The Quantiﬁed Self? (2013). Available at http://www.forbes.com/\\nsites/kellyclay/2013/01/06/ces-2013-the-year-of-the-quantiﬁed-self/#4cf4d2b55e74.\\nWilliam\\nS.\\nCleveland.\\n2001.\\nData\\nscience:\\nAn\\naction\\nplan\\nfor\\nexpanding\\nthe\\ntechnical\\nar-\\neas\\nof\\nthe\\nﬁeld\\nof\\nstatistics.\\nInternational\\nStatistical\\nReview\\n69,\\n1\\n(2001),\\n21–26.\\nDOI:http://dx.doi.org/10.1111/j.1751-5823.2001.tb00477.x\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:37\\nCMIST. 2016. China Will Establish A Series of National Labs. (2016). Available at http://news.sciencenet.\\ncn/htmlnews/2016/4/344404.shtm.\\nCNSF. 2015. National Science Foundation China. (2015). Available at http://www.nsfc.gov.cn/.\\nEuropean Commission. 2014. Commission urges governments to embrace potential of big data. (2014). Avail-\\nable at europa.eu/rapid/press-release IP-14-769 en.htm.\\nCoursera. 2016. Coursera. (2016). Available at www.coursera.org/data-science.\\nKevin Crowston and Jian Qin. 2011. A capability maturity model for scientiﬁc data management: Evidence\\nfrom the literatute. 48, 10 (2011), 1–9.\\nCSC. 2012. Big Data Universe Beginning to Explode. (2012). Available at http://www.csc.com/insights/ﬂxwd/\\n78931-big data growth just beginning to explode.\\nCSNSTC. 2009. Harnessing the Power of Digital Data for Science and Society. (2009). Report of the In-\\nteragency Working Group on Digital Data to the Committee on Science of the National Science and\\nTechnology Council.\\nDABS. 2016. Data Analytics Book Series. (2016). Available at http://www.springer.com/series/15063.\\nDARPA. 2016. DARPA Xdata program. (2016). Available at www.darpa.mil/program/xdata.\\nData61. 2016. Data61. (2016). Available at https://www.data61.csiro.au/.\\nDataRobot. 2016. DataRobot. (2016). Available at https://www.datarobot.com/.\\nDatasciences.org. 2005. Datasciences.org. (2005). Available at www.datasciences.org.\\nThomas H. Davenport and D.J. Patil. 2012. Data scientist: The sexiest job of the 21st century. Harvard\\nBusiness Review (2012), 70–76.\\nJessica\\nDavis.\\n2016.\\n10\\nProgramming\\nLanguages\\nAnd\\nTools\\nData\\nScientists\\nUsed.\\n(2016).\\nAvailable\\nat\\nhttp://www.informationweek.com/devops/programming-languages/\\n10-programming-languages-and-tools-data-scientists-use-now/d/d-id/1326034.\\nDevendra Desale. 2015. Top 30 Social Network Analysis and Visualization Tools. (2015). Available at http:\\n//www.kdnuggets.com/2015/06/top-30-social-network-analysis-visualization-tools.html.\\nVasant Dhar. 2013. Data science and prediction. Commun. ACM 56, 12 (2013), 64–73.\\nHerman A. Dierick and Fabrizio Gabbiani. 2015. Drosophila neurobiology: No escape from ‘Big Data’ science.\\nCurrent Biology 25, 14 (2015), 606–608.\\nPeter J. Diggle. 2015. Statistics: A data science for the 21st century. Journal of the Royal Statistical Society:\\nSeries A (Statistics in Society) 178, 4 (2015), 793–813.\\nDavid Donoho. 2015. 50 years of Data Science. (2015). Available at http://courses.csail.mit.edu/18.337/2015/\\ndocs/50YearsDataScience.pdf.\\nBonnie J. Dorr, Craig S. Greenberg, Peter Fontana, Mark A. Przybocki, Marion Le Bras, Cathryn A. Ploehn,\\nOleg Aulov, Martial Michel, E. Jim Golden, and Wo Chang. 2015. The NIST data science initiative. In\\n2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA). 1–10.\\nDSA. 2016. Data Science Association. (2016). Available at http://www.datascienceassn.org/.\\nDSAA. 2014. IEEE/ACM/ASA International Conference on Data Science and Advanced Analytics. (2014).\\nAvailable at www.dsaa.co.\\nDSC. 2016a. College & University Data Science Degrees. (2016). Available at http://datascience.community/\\ncolleges(accessedon16April2016.).\\nDSC. 2016b. The Data Science Community. (2016). Available at http://datasciencebe.com/.\\nDSCentral. 2016. Data Science Central. (2016). Available at http://www.datasciencecentral.com/.\\nDSE. 2015. Data Science and Engineering. (2015). Available at http://link.springer.com/journal/41019.\\nDSJ. 2014. Data Science Journal. (2014). Available at datascience.codata.org.\\nDSKD.\\n2007.\\nData\\nScience\\nand\\nKnowledge\\nDiscovery\\nLab,\\nUTS.\\n(2007).\\nAvailable\\nat\\nhttp:\\n//www.uts.edu.au/research-and-teaching/our-research/quantum-computation-and-intelligent-systems/\\ndata-sciences-and.\\nDavid Ewing Duncan. 2009. Experimental Man: What One Man’s Body Reveals about His Future, Your\\nHealth, and Our Toxic World. New York: Wiley & Sons.\\nEdx. 2016. EDX Courses. (2016). Available at https://www.edx.org/course?search query=data+science.\\nEMC. 2011. Data Science Revealed: A Data-Driven Glimpse into the Burgeoning New Field. (2011). Avail-\\nable at www.emc.com/collateral/about/news/emc-data-science-study-wp.pdf.\\nEPJDS. 2012. EPJ Data Science. (2012). Available at http://epjdatascience.springeropen.com/.\\nEU. 2014. EU Towards a Thriving Data-driven Economy. (2014). Available at https://ec.europa.eu/\\ndigital-single-market/en/towards-thriving-data-driven-economy.\\nEU-DSA. 2016. The European Data Science Academy. (2016). Available at edsa-project.eu.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:38\\nL. CAO\\nEU-OD. 2016. The European Union Open Data Portal. (2016). Available at https://open-data.europa.eu/.\\nFacebook. 2016. Facebook Data. (2016). Available at https://www.facebook.com/careers/teams/data/.\\nJames H. Faghmous and Vipin Kumar. 2014. A big data guide to understanding climate change: The case\\nfor theory-guided data science. Big Data 2, 3 (2014), 155–163.\\nJoshua Fairﬁelda and Hannah Shteina. 2014. Big data, big problems: Emerging issues in the ethics of data\\nscience and journalism. Journal of Mass Media Ethics 29, 1 (2014), 38–51.\\nJack Faris, Evelyne Kolker, Alex Szalay, Leon Bradlow, Ewa Deelman, Wu Feng, Judy Qiu, Donna Rus-\\nsell, Elizabeth Stewart, and Eugene Kolker. 2011. Communication and data-intensive science in the\\nbeginning of the 21st century. A Journal of Integrative Biology 15, 4 (2011), 213–215.\\nTom Fawcett. 2016. Mining the quantiﬁed self: Personal knowledge discovery as a challenge for data science.\\nBig Data (2016), 249–266.\\nUsama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth. 1996. From data mining to knowledge\\ndiscovery in databases. AI Magazine 17, 3 (1996), 37–54.\\nWilliam Finzer. 2013. The data science education dilemma. Technology Innovations in Statistics Education\\n7, 2 (2013).\\nGeoffrey Fox, Siddharth Maini, Howard Rosenbaum, and David J. Wild. 2015. Data science and online edu-\\ncation. In 2015 IEEE 7th International Conference on Cloud Computing Technology and Science (Cloud-\\nCom). 582–587.\\nP. Fox and J. Hendler. 2014. The science of data science. Big Data 2, 2 (2014), 68–70.\\nMolly Galetto. 2016. Top 50 Data Science Resources. (2016). Available at http://www.ngdata.com/\\ntop-data-science-resources/?\\nGEO. 2016. Gene Expression Omnibus. (2016). Available at http://www.ncbi.nlm.nih.gov/geo/.\\nDeepak Ghodke. 2015. Bye Bye 2015: What lies ahead for BI. (2015). Available at http://www.ciol.com/\\nbye-bye-2015-what-lies-ahead-for-bi/.\\nGithub.\\n2016a.\\nData\\nscience\\ncolleges.\\n(2016).\\nAvailable\\nat\\nhttps://github.com/ryanswanstrom/\\nawesome-datascience-colleges.\\nGithub. 2016b. List of Recommender Systems. (2016). Available at https://github.com/grahamjenson/list of\\nrecommender systems.\\nMichael Gold, Ryan McClarren, and Conor Gaughan. 2013. The lessons Oscar taught us: Data science and\\nmedia & entertainment. Big Data 1, 2 (2013), 105–109.\\nGoogle. 2016a. Google Bigquery and Cloud Platform. (2016). Available at https://cloud.google.com/bigquery/.\\nGoogle. 2016b. Google Cloud Prediction API. (2016). Available at https://cloud.google.com/prediction/docs/.\\nGoogle. 2016c. Google Online Open Education. (2016). Available at https://www.google.com/edu/openonline/.\\nGoogle. 2016d. Google Trends. (2016). Available at https://www.google.com.au/trends/explore#q=data\\\\\\n%20science\\\\%2C\\\\%20data\\\\%20analytics\\\\%2C\\\\%20big\\\\%20data\\\\%2C\\\\%20data\\\\%20analysis\\\\%2C\\\\\\n%20advanced\\\\%20analytics&cmpt=q&tz=Etc\\\\%2FGMT-11.\\nGoogle. 2016e. Open Mobile Data. (2016). Available at https://console.developers.google.com/storage/\\nbrowser/openmobiledata public/.\\nBeijing Municipal Government. 2016. Beijing Big Data and Cloud Computing Development Action Plan.\\n(2016). Available at http://zhengwu.beijing.gov.cn/gh/dt/t1445533.htm.\\nChina Government. 2015. China Big Data. (2015). Available at http://www.gov.cn/zhengce/content/2015-09/\\n05/content 10137.htm.\\nMatthew J. Graham. 2012. The art of data science. In Astrostatistics and Data Mining, Volume 2 of the series\\nSpringer Series in Astrostatistics. 47–59.\\nJim Gray. 2007. eScience – A Transformed Scientiﬁc Method. (2007). Available at http://research.microsoft.\\ncom/en-us/um/people/gray/talks/NRC-CSTB eScience.ppt.\\nGTD. 2016. Global Terrorism Database. (2016). Available at https://www.start.umd.edu/gtd/.\\nAkash Gupta, Ahmet Cecen, Sharad Goyal, Amarendra K. Singh, and Surya R. Kalidindi. 2015. Structure-\\nproperty linkages using a data science approach: Application to a non-metallic inclusion/steel composite\\nsystem. Acta Mater 91 (2015), 239–254.\\nDavid J. Hand. 2015. Statistics and computing: The genesis of data science. Statistics and Computing 25, 4\\n(2015), 705–711.\\nHardin. 2016. Github. (2016). Available at hardin47.github.io/DataSciStatsMaterials/.\\nJ. Hardin, R. Hoerl, Nicholas J. Horton, D. Nolan, B. Baumer, O. Hall-Holt, P. Murrell, R. Peng, P. Roback,\\nD. Temple Lang, and M. D. Ward. 2015. Data science in statistics curricula: Preparing students to\\n“Think with Data”. The American Statistician 69, 4 (2015), 343–353.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:39\\nHarlan Harris, Sean Murphy, and Marck Vaisman. 2013. Analyzing the Analyzers: An Introspective Survey\\nof Data Scientists and Their Work. O’Reilly Media.\\nBenjamin T. Hazena, Christopher A. Booneb, Jeremy D. Ezellc, and L. Allison Jones-Farmer. 2014. Data\\nquality for data science, predictive analytics, and big data in supply chain management: An introduc-\\ntion to the problem and suggestions for research and applications. International Journal of Production\\nEconomics 154 (2014), 72–80.\\nTony Hey, Stewart Tansley, and Kristin Tolle (Eds.). 2009. The Fourth Paradigm: Data-Intensive Scientiﬁc\\nDiscovery. Available at http://research.microsoft.com/en-us/collaboration/fourthparadigm/.\\nTony Hey and Anne Trefethen. 2003. The Data Deluge: An e-Science Perspective. John Wiley & Sons, Ltd,\\n809–824.\\nHLSG. 2010. Final Report of the High Level Expert Group on Scientiﬁc Data. (2010). Available at http:\\n//ec.europa.eu/information society/newsroom/cf/document.cfm?action=display&doc id=707.\\nHLSG. 2014. An RDA Europe Report. (2014). Available at http://www.e-nformation.ro/wp-content/uploads/\\n2014/12/TheDataHarvestReport -Final.pdf.\\nHorizon.\\n2014.\\nEuropean\\nCommission\\nHorizon\\n2020\\nBig\\nData\\nPrivate\\nPublic\\nPartner-\\nship.\\n(2014).\\nAvailable\\nat\\nhttp://ec.europa.eu/programmes/horizon2020/en/h2020-section/\\ninformation-and-communication-technologies.\\nPeter J. Huber. 2011. Data Analysis: What Can Be Learned From the Past 50 Years. John Wiley & Sons.\\nIASC. 1977. International Association for Statistical Computing. (1977). Available at http://www.iasc-isi.\\norg/.\\nIBM. 2010. Capitalizing on Complexity. (2010). Available at http://www-935.ibm.com/services/us/ceo/\\nceostudy2010/multimedia.html.\\nIBM. 2016a. IBM Analytics and Big Data. (2016). Available at http://www.ibm.com/analytics/us/en/orhttp:\\n//www-01.ibm.com/software/data/bigdata/.\\nIBM. 2016b. What is a data scientist? (2016). Available at http://www-01.ibm.com/software/data/infosphere/\\ndata-scientist/.\\nIDA. 2014. International Institute of Data & Analytics. (2014). Available at www.datasciences.org.\\nIEEEBD. 2014. IEEE Big Data Initiative. (2014). Available at http://bigdata.ieee.org/.\\nIFSC-96. 1996. Data Science, Classiﬁcation, and Related Methods. (1996). Available at http://d-nb.info/\\n955715512/04.\\nIJDS. 2016. International Journal of Data Science. (2016). Available at http://www.inderscience.com/jhome.\\nphp?jcode=ijds.\\nIJRDS. 2017. International Journal of Research on Data Science. (2017). Available at http://www.\\nsciencepublishinggroup.com/journal/index?journalid=310.\\nINFORMS.\\n2014.\\nCandidate\\nHandbook.\\n(2014).\\nAvailable\\nat\\nhttps://www.informs.org/\\nCertiﬁcation-Continuing-Ed/Analytics-Certiﬁcation/Candidate-Handbook.\\nINFORMS. 2016. Institute for Operations Research and the Management Sciences. (2016). Available at\\nhttps://www.informs.org/.\\nS Iwata. 2008. Scientiﬁc “agenda” of data science. Data Science Journal 7, 5 (2008), 54–56.\\nH.V. Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Papakonstantinou, Jignesh M. Patel,\\nRaghu Ramakrishnan, and Cyrus Shahabi. 2014. Big data and its technical challenges. Commun. ACM\\n57, 7 (2014), 86–94.\\nH. V. Jagadish. 2015. Big data and science: Myths and reality. Big Data Research 2, 2 (2015), 49–52.\\nJDS. 2002. Journal of Data Science. (2002). Available at http://www.jds-online.com/.\\nJDSA. 2015. International Journal of Data Science and Analytics (JDSA). (2015). Available at http://www.\\nspringer.com/41060.\\nJFDS. 2016. The Journal of Finance and Data Science. (2016). Available at http://www.keaipublishing.com/\\nen/journals/the-journal-of-ﬁnance-and-data-science/.\\nKaggle. 2016. Kaggle Competition Data. (2016). Available at https://www.kaggle.com/competitions.\\nSurya R. Kalidindi. 2015. Data science and cyberinfrastructure: critical enablers for accelerated develop-\\nment of hierarchical materials. International Materials Reviews 60, 3 (2015), 150–168.\\nKDD89. 1989. IJCAI-89 Workshop on Knowledge Discovery in Databases. (1989). Available at http://www.\\nkdnuggets.com/meetings/kdd89/index.html.\\nKDnuggets.\\n2015.\\nVisualization\\nSoftware.\\n(2015).\\nAvailable\\nat\\nhttp://www.kdnuggets.com/software/\\nvisualization.html.\\nKdnuggets. 2016. Kdnuggets. (2016). Available at http://www.kdnuggets.com/.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:40\\nL. CAO\\nK Kelly. 2012. The Quantiﬁed Century. In Quantiﬁed Self Conference. Available at http://quantiﬁedself.com/\\nconference/Palo-Alto-2012.\\nNawsher Khan, Ibrar Yaqoob, Ibrahim Abaker Targio Hashem, and et al. 2014. Big data: Survey, technolo-\\ngies, opportunities, and challenges. The Scientiﬁc World Journal 2014 (2014), 18.\\nJohn King and Roger Magoulas. 2015. 2015 Data Science Salary Survey. (2015). Available at http:\\n//duu86o6n09pv.cloudfront.net/reports/2015-data-science-salary-survey.pdf.\\nRon Kohavi, Neal J. Rothleder, and Evangelos Simoudis. 2002. Emerging trends in business analytics. Com-\\nmun. ACM 45, 8 (2002), 45–48.\\nAMP Lab. 2016. MLBase. (2016). Available at http://mlbase.org/.\\nA. Labrinidis and H. V. Jagadish. 2012. Challenges and opportunities with Big Data. Proceedings of the\\nVLDB Endowment 5, 12 (2012), 2032–2033.\\nDouglas Laney. 2001. 3D Data Management: Controlling Data Volume, Velocity and Variety. (2001). Techni-\\ncal Report, META Group.\\nLD. Lazer, R Kennedy, G King, and A Vespignani. 2014. The parable of Google ﬂu: Traps in big data analysis.\\nScience 343 (2014), 1203–1205.\\nLDC. 2016. Linguistic Data Consortium. (2016). Available at https://www.ldc.upenn.edu/about.\\nLinkedIn. 2016. LinkedIn Jobs. (2016). Available at https://www.linkedin.com/jobs/data-scientist-jobs.\\nMike Loukides. 2011. The Evolution of Data Products. O’Reilly, Cambridge.\\nMike Loukides. 2012. What is data science? O’Reilly Media, Sebastopol, CA.\\nAndrea Manieri, Steve Brewer, Ruben Riestra, Yuri Demchenko, Matthias Hemmje, Tomasz Wiktorski,\\nTiziana Ferrari, and Jrmy Frey. 2015. Data science professional uncovered: How the EDISON project\\nwill contribute to a widely accepted proﬁle for data scientists. In 2015 IEEE 7th International Conference\\non Cloud Computing Technology and Science (CloudCom). 588–593.\\nKate Matsudaira. 2015. The science of managing data science. Commun. ACM 58, 6 (2015), 44–47.\\nMcKinsey. 2011. Big Data: The Next Frontier for Innovation, CCompetition, and Productivity. (2011). McK-\\ninsey Global Institute.\\nClaire Cain Miller. 2013. Data Science: The Numbers of Our Lives. New York Times (2013).\\nAJH. Morrell. 1968. Information processing 68 (Ed.). In Proceedings of IFIP Congress 1968. Edinburgh, UK.\\nPeter Murray-Rust. 2007. Data-Driven Science: A Scientist’s View. In NSF/JISC 2007 Digital Repositories\\nWorkshop. Available at http://www.sis.pitt.edu/repwkshop/papers/murray.pdf.\\nPeter Naur. 1968. ‘Datalogy’, the science of data and data processes. (1968), 1383–1387.\\nPeter Naur. 1974. Concise Survey of Computer Methods. Studentlitteratur, Lund, Sweden.\\nNCSU. 2007a. Institute for Advanced Analytics, North Carolina State University. (2007). Available at http:\\n//analytics.ncsu.edu/.\\nNCSU. 2007b. Master of Science in Analytics, Institute for Advanced Analytics, North Carolina State Uni-\\nversity. (2007). Available at http://analytics.ncsu.edu/.\\nMichael L. Nelson. 2009. Data-driven science: A new paradigm? EDUCAUSE Review 44, 4 (2009), 6–7.\\nNICTA. 2016. National ICT Australia. (2016). Available at https://www.nicta.com.au/.\\nNIST. 2015. NIST Text Retrieval Conference Data. (2015). Available at http://trec.nist.gov/data.html.\\nNSB. 2005. Long-lived Digital Data Collections: Enabling Research and Education in the 21st Century.\\n(2005).\\nNSF. 2007. US NSF07-28. (2007). Available at http://www.nsf.gov/pubs/2007/nsf0728/nsf0728.pdf.\\nOECD. 2007. OECD Principles and Guidances for Access to Research Data from Public Funding. (2007).\\nAvailable at https://www.oecd.org/sti/sci-tech/38500813.pdf.\\nOPENedX. 2016. OPENedX Online education platform. (2016). Available at https://open.edx.org/.\\nTim\\nO’Reilly.\\n2005.\\nWhat\\nis\\nWeb\\n2.0.\\n(2005).\\nAvailable\\nat\\nhttp://oreilly.com/pub/a/web2/archive/\\nwhat-is-web-20.html?page=3.\\nDJ Patil. 2011. Building Data Science Teams. O’Reilly Media.\\nM. C. Paulk, B. Curtis, M. B. Chrissis, and C. Weber. 1993. Capability maturity model Version 1.1. IEEE\\nSoftware 10, 4 (1993), 18–27.\\nGil Press. 2013. A Very Short History Of Data Science. (2013). Available at http://www.forbes.com/sites/\\ngilpress/2013/05/28/a-very-short-history-of-data-science/#61ae3ebb69fd.\\nXuesen Qian. 1991. Revisiting issues on open complex giant systems. Pattern Recognit. Artif. Intell. 4, 1\\n(1991), 5–8.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\nData Science: A Comprehensive Overview\\n1:41\\nXuesen Qian, Jingyuan Yu, and Ruwei Dai. 1993. A new discipline of science-The study of open complex\\ngiant system and its methodology. Chin. J. Syst. Eng. Electron. 4, 2 (1993), 2–12.\\nRapidMiner. 2016. RapidMiner. (2016). Available at https://rapidminer.com/.\\nSamantha Renae. 2011. Data analytics: Crunching the future. Bloomberg Businessweek (2011). September\\n8.\\nSolutions Review. 2016. Data Integration and Application Integration Solutions Directory. (2016). Available\\nat http://solutionsreview.com/data-integration/data-integration-solutions-directory/.\\nC. Rudin. 2014. Discovery with Data: Leveraging Statistics with Computer Science to Transform Science\\nand Society. (2014). American Statistical Association.\\nSAS. 2013. Big Data Analytics: An Assessment of Demand for Labour and Dkills, 2012-2017. (2013). Avail-\\nable at https://www.thetechpartnership.com/globalassets/pdfs/research-2014/bigdata report nov14.pdf.\\nSAS. 2016. SAS Insights. (2016). Available at http://www.sas.com/en us/insights.html.\\nTobias Schoenherr and Cheri Speier-Pero. 2015. Data science, predictive analytics, and big data in supply\\nchain management: Current state and future potential. Journal of Business Logistics 36, 1 (2015), 120–\\n132.\\nChina Information Security. 2015. Big Data Strategies and Actions in Major Countries. (2015). Available at\\nhttp://www.cac.gov.cn/2015-07/03/c 1115812491.htm.\\nSIAM. 2016. SIAM career center. (2016). Available at http://jobs.siam.org/home/.\\nC. Siart, S. Kopp, and J. Apel. 2015. The interface between data science, research assessment and science\\nsupport - Highlights from the German perspective and examples from Heidelberg University. In 2015\\nIIAI 4th International Congress on Advanced Applied Informatics (IIAI-AAI). 472–476.\\nSilk. 2016. Data Science University Programs. (2016). Available at http://data-science-university-programs.\\nsilk.co/.\\nLarry Smarr. 2012. Quantifying your body: A how-to guide from a systems biology perspective. Biotechnology\\nJournal 7, 8 (2012), 980–991. DOI:http://dx.doi.org/10.1002/biot.201100495\\nF. Jack Smith. 2006. Data science as an academic discipline. Data Science Journal 5 (2006), 163–164.\\nSSDS. 2015. Springer Series in the Data Sciences. (2015). Available at http://www.springer.com/series/13852.\\nStanford. 2014. Stanford Data Science Initiatives, Stanford University. (2014). Available at https://sdsi.\\nstanford.edu/.\\nThomas R. Stewart and Jr. Claude McMillan. 1987. Descriptive and prescriptive models for judgment and\\ndecision making: Implications for knowledge engineering. In Expert Judgment and Expert Systems,\\nJeryl L. Mumpower, Ortwin Renn, Lawrence D. Phillips, and V. R. R. Uppuluri (Eds.) (Eds.). Springer-\\nVerlag, London, 305–320.\\nMichael Stonebraker, Sam Madden, and Pradeep Dubey. 2013. Intel ‘big data’ science and technology center\\nvision and execution plan. SIGMOD Record 42, 1 (2013), 44–49.\\nAlma Swan and Sheridan Brown. 2008. The Skills, Role & Career Structure of Data Scientists & Curators:\\nAssessment of Current Practice & Future Needs. (2008). Technical Report. University of Southampton.\\nMelanie Swan. 2013. The quantiﬁed self: Fundamental disruption in big data science and biological discov-\\nery. Big Data 1, 2 (2013), 85–99.\\nTechnavio. 2016. Top 10 Healthcare Data Analytics Companies. (2016). Available at http://www.technavio.\\ncom/blog/top-10-healthcare-data-analytics-companies.\\nTFDSAA. 2013. IEEE Task Force on Data Science and Advanced Analytics. (2013). Available at http://dsaatf.\\ndsaa.co/.\\nTOBD. 2015. IEEE Transactions on Big Data. (2015). Available at https://www.computer.org/web/tbd.\\nPredictive Analytics Today. 2016. 29 Data Preparation Tools and Platforms. (2016). Available at http://www.\\npredictiveanalyticstoday.com/data-preparation-tools-and-platforms/.\\nJohn W. Tukey. 1962. The future of data analysis. Ann. Math. Statist. 33, 1 (1962), 1–67.\\nJohn W. Tukey. 1977. Exploratory Data Analysis. Pearson.\\nTutiempo. 2016. Global Climate Data. (2016). Available at http://en.tutiempo.net/climate.\\nUCI. 2016. UCI Machine Learning Repository. (2016). Available at archive.ics.uci.edu/ml/.\\nUdacity. 2016. Udacity Courses. (2016). Available at https://www.udacity.com/courses/data-science.\\nUdemy. 2016. Udemy Courses. (2016). Available at https://www.udemy.com/courses/search/?ref=home&src=\\nukw&q=data+science&lang=en.\\nUK. 2016. UK Big Data. (2016). Available at http://www.rcuk.ac.uk/research/infrastructure/big-data/.\\nUK-HM. 2012. UK HM Government. (2012). Available at http://data.gov.uk/sites/default/ﬁles/Open data\\nWhite Paper.pdf.\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n1:42\\nL. CAO\\nUK-OD. 2016. UK Open Data. (2016). Available at http://data.gov.uk/.\\nUMichi. 2015. Michigan Institute For Data Science, University of Michigan. (2015). Available at http://\\nmidas.umich.edu/.\\nUN. 2010. United Nation Global Pulse Projects. (2010). Available at http://www.unglobalpulse.org/.\\nUS-OD. 2016. US government open data. (2016). Available at https://www.data.gov/.\\nUSD2D. 2016. US National Consortium for Data Science. (2016). Available at data2discovery.org.\\nUSDSC. 2016. US Degree Programs in Analytics and Data Science. (2016). Available at http://analytics.\\nncsu.edu/?page id=4184.\\nUSNSF. 2012. US Big Data Research Initiative. (2012). Available at http://www.nsf.gov/cise/news/bigdata.\\njsp.\\nUTS.\\n2011.\\nMaster\\nof\\nAnalytics\\n(Research)\\nand\\nDoctor\\nof\\nPhilosophy\\nThesis:\\nAnalyt-\\nics,\\nAdvanced\\nAnalytics\\nInstitute,\\nUniversity\\nof\\nTechnology\\nSydney.\\n(2011).\\nAvailable\\nat\\nhttp://www.uts.edu.au/research-and-teaching/our-research/advanced-analytics-institute/\\neducation-and-research-opportuniti-1.\\nUTSAAI. 2011. Advanced Analytics Institute, University of Technology Sydney. (2011). Available at https:\\n//analytics.uts.edu.au/.\\nDavid van Dyk, Montse Fuentes, Michael I. Jordan, Michael Newton, Bonnie K. Ray, Duncan Temple Lang,\\nand Hadley Wickham. 2015. ASA Statement on the Role of Statistics in Data Science. (2015). Available\\nat http://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/.\\nVast. 2016. Visual Analytics Community. (2016). Available at http://vacommunity.org/HomePage.\\nDan Vesset, Benjamin Woo, Henry D. Morris, Richard L. Villars, Gard Little, Jean S. Bozman, Lucinda\\nBorovick, Carl W. Olofson, Susan Feldman, Steve Conway, Matthew Eastwood, and Natalya Yezhkova.\\n2012. IDC Worldwide Big Data Technology and Services 2012-2015 Forecast. (2012).\\nAna Viseu and Lucy Suchman. 2010. Wearable Augmentations: Imaginaries of the Informed Body. Berghahn\\nBooks, New York, NY, USA, 161–184.\\nWhitehouse.\\n2015.\\nThe\\nWhite\\nHouse\\nNames\\nDr.\\nDJ\\nPatil\\nas\\nthe\\nFirst\\nU.S.\\nChief\\nData\\nScientist.\\n(2015).\\nAvailable\\nat\\nhttps://www.whitehouse.gov/blog/2015/02/18/\\nwhite-house-names-dr-dj-patil-ﬁrst-us-chief-data-scientist.\\nWikipedia. 2016a. Comparison of cluster software. (2016). Available at https://en.wikipedia.org/wiki/\\nComparison of cluster software.\\nWikipedia. 2016b. Informatics. (2016). Available at https://en.wikipedia.org/wiki/Informatics.\\nWikipedia. 2016c. List of reporting software. (2016). Available at https://en.wikipedia.org/wiki/List of\\nreporting software.\\nWIRED. 2014. How Europe can seize the starring role in big data. (2014). Available at www.wired.com/\\ninsights/2014/09/europe-big-data/.\\nG. Wolf. 2012. The Data-Driven Life. New York Times (2012).\\nJeff Wu. 1997. Statistics = Data Science? (1997). Available at http://www2.isye.gatech.edu/∼jeffwu/\\npresentations/datascience.pdf.\\nYahoo. 2016. Yahoo Finance. (2016). Available at ﬁnance.yahoo.com.\\nNathan Yau. 2009. Rise of the Data Scientist. (2009). Available at http://ﬂowingdata.com/2009/06/04/\\nrise-of-the-data-scientist/.\\nChris Yiu. 2012. The Big Data Opportunity. (2012). Available at http://www.policyexchange.org.uk/images/\\npublications/the\\\\%20big\\\\%20data\\\\%20opportunity.pdf.\\nBin Yu. 2014. IMS Presidential Address: Let us own Data Science. IMS Bulletin Online (2014). 1 Oct 2014.\\nReceived February 2007; revised March 2009; accepted June 2009\\nACM Computing Surveys, Vol. 1, No. 1, Article 1, Publication date: January 2016.\\n',\n",
       " '2010.05125v2.pdf': 'Learning Task-aware Robust Deep Learning Systems\\n1,2Keji Han, 1,2Yun Li*, 1,2Xianzhong Long, 1,2Yao Ge,\\n1Nanjing University of Posts and Telecommunications\\n2Jiangsu Key Laboratory of Big Data Security & Intelligent Processing\\nliyun@njupt.edu.cn\\nAbstract\\nMany works demonstrate that deep learning system is vulner-\\nable to adversarial attack. A deep learning system consists of\\ntwo parts: the deep learning task and the deep model. Nowa-\\ndays, most existing works investigate the impact of the deep\\nmodel on robustness of deep learning systems, ignoring the\\nimpact of the learning task. In this paper, we adopt the binary\\nand interval label encoding strategy to redeﬁne the classiﬁca-\\ntion task and design corresponding loss to improve robustness\\nof the deep learning system. Our method can be viewed as im-\\nproving the robustness of deep learning systems from both the\\nlearning task and deep model. Experimental results demon-\\nstrate that our learning task-aware method is much more ro-\\nbust than traditional classiﬁcation while retaining the accu-\\nracy.\\nIntroduction\\nDeep neural networks have been applied to many learning\\ntasks, such as image recognition, speech recognition, and\\nmachine translation (LeCun, Bengio, and Hinton 2015).\\nWith increasing real-world applications, the robustness of\\ndeep neural networks arouses increasing attention from\\nboth academia and industry. Deep neural networks are\\ndemonstrated to be vulnerable to the adversarial example\\n(Szegedy et al. 2014). The adversarial example is crafted\\nby adding imperceptible adversarial perturbation to the\\noriginal legitimate example. In essence, the existence of\\nadversarial examples is rooted in the difference between\\nhuman intelligence and machine intelligence (Cao et al.\\n2021).\\nThere are many adversarial attack methods proposed\\nto explore the vulnerability of deep learning systems.\\nAccording to the phase that adversarial attack happens, ad-\\nversarial attack methods fall into two categories: poisoning\\nattack and evasion attack (Yuan et al. 2019). In this paper,\\nwe focus on the evasion attack. According to manner to\\ncraft adversarial examples, existing evasion attack methods\\ncan be divided into two categories, namely single-step\\nattack and multi-step attack. A single-step attack explores\\nadversarial perturbation in one step or directly maps the\\noriginal example as an adversarial example. Multi-step\\n*Corresponding Author\\nFor Preview\\nattack explores adversarial perturbation iteratively. For\\nthe single-step attack, FGSM (Goodfellow, Shlens, and\\nSzegedy 2015) crafts the adversarial example with the\\ngradient sign; AdvGAN (Xiao et al. 2018) and ATN (Baluja\\nand Fischer 2018) directly map the legitimate example\\nas an adversarial example. As to the multi-step attack,\\nPGD (Kurakin, Goodfellow, and Bengio 2017) crafts the\\nadversarial example in an iterative way; CW (Carlini and\\nWagner 2017) and Deepfool (Moosavi-Dezfooli, Fawzi,\\nand Frossard 2016) formulate the attack as an optimization\\nproblem then solve it in an iterative way.\\nExisting methods to improve the robustness of deep\\nlearning systems focus on even all elements related\\nto the deep model, such as the training data/features,\\nthe model architecture, training loss/regularization loss,\\nand parameter-updating strategy. For training data-level\\nmethods, feature nulliﬁcation (Wang et al. 2017), image\\ncompress (Das et al. 2018), and feature squeezing (Xu,\\nEvans, and Qi 2018) are demonstrated to be efﬁcient to\\nimprove the robustness of the deep learning system. As to\\nthe model architecture-level method, the denoising module\\n(Xie et al. 2019), Euler skip connection (Li, He, and Lin\\n2020), and additional batch normalization module (Xie\\nand Yuille 2020) are introduced as additional modules\\nto improve the robustness of the deep model. When it\\ncomes to training loss-based methods, adversarial training\\nmethods, such as TRADES (Zhang et al. 2019) and MART\\n(Wang et al. 2020), introduce regularization loss. For the\\nparameter-updating strategy methods, Reluplex (Katz et al.\\n2017) updates model parameters with the simplex-like\\nmethod. Some generative classiﬁers (Lee et al. 2019) update\\nthe parameter of deep model with Bayesian Backpropaga-\\ntion (BBP) (Blundell et al. 2015). Moreover, the adversarial\\nexample detection methods (Meng and Chen 2017; Ma\\net al. 2019) can keep the adversarial example away from the\\ndeep model, which is also demonstrated to be efﬁcient in\\nimproving the robustness of the deep learning system.\\nActually, a deep learning system consists of the learn-\\ning task and a deep model. The learning task provides a\\nformalized deﬁnition of the problem, while the deep model\\nimplements the learning task. So we can also improve the\\nrobustness of the deep learning system by deﬁning robust\\narXiv:2010.05125v2  [cs.LG]  2 Dec 2021\\nlearning tasks. The traditional classiﬁcation is so simplistic\\nthat the model can be lazy. It just needs to remember instead\\nof learning (Feldman and Zhang 2020). Moreover, in the\\ntraditional classiﬁcation, the accuracy and robustness are\\nat odds (Tsipras et al. 2019). Above all, the learning task-\\naware method may be promising to address the adversarial\\nissue for deep learning systems.\\nMoreover, there are some works that focus on the\\nlearning task to improve the accuracy of the deep learning\\nsystem. For instance, DeepBE (Li et al. 2016) adopts\\nbinary encoding labels to improve the accuracy of the\\ndeep learning system. In this paper, we explore methods\\nthat improve the robustness of the deep learning system\\nfrom both the learning task and model loss prospects. In\\ndetail, we introduce robust binary-label classiﬁcation with\\na scaling factor to improve the accuracy and robustness.\\nMoreover, we deﬁne the interval-label classiﬁcation, which\\nmarks inputs with predeﬁned nonoverlapping intervals.\\nOur contributions can be summarized as follows.\\n• We introduce task-aware robust deep learning systems\\nbased on binary-label and interval-label classiﬁcations;\\n• Experimental results demonstrate that our method\\nachieves a lower adversarial transfer rate than traditional\\nclassiﬁcation;\\n• Both analysis and experimental results demonstrate that\\nthe accuracy and robustness are no more at odds for the\\nrobust binary-label classiﬁcation;\\n• We ﬁnd that the adversarial training does not constantly\\nimprove the robustness of robust binary-label classiﬁca-\\ntion, demonstrating that learning task affects the adver-\\nsarial robustness.\\nInterval-label and Robust Binary-label\\nclassiﬁcation Systems\\nMost existing works to improve the robustness of the deep\\nlearning system focus on the deep model. As introduced\\nin (Cao et al. 2021), the adversarial example has roots in\\nthe difference between machine intelligence and human\\nintelligence. We think that the decision process of the deep\\nlearning task may also impact the robustness of the deep\\nlearning system. That is another motivation behind propos-\\ning robust binary-label and interval-label classiﬁcation\\nsystems.\\nA robust classiﬁcation system consists of three mod-\\nules: the classiﬁer, label encoding module, and label\\ndecoding module, as shown in Fig. 1. The classiﬁer maps\\ninputs into the corresponding output vectors then turns\\noutput vectors as codewords. The codewords (Dietterich\\nand Bakiri 1995) is the encoding of decimal labels, i.e.,\\nhard labels. Label encoding translates hard labels as cor-\\nresponding codewords, while the label decoding module\\ntranslates the codewords provided by the classiﬁer as the\\nhard labels, namely the predicted label. If the codeword do\\nnot correspond to any predeﬁned hard label, the input will\\nbe marked as None, i.e., an abnormal example.\\nLabel Encoding\\nLabel Decoding\\n0\\nNone\\nClassifier\\n•\\nLabel Encoding: mapping hard \\nlabels as corresponding \\ncodewords.\\n•\\nLabel Decoding: mapping the \\ncodewords as hard labels. \\nInput: Example\\nInput: Hard Label\\n0\\nOutput Vector\\nCodeword\\nFigure 1: The workﬂow of the learning task-aware robust\\ndeep classiﬁcation system.\\nDeﬁnition\\nTraditional Classiﬁcation: the traditional deep classiﬁcation\\ncan be deﬁned as follows: ∀x ∈RD, T : x →RC, where x\\nis an example, D is the dimension of x. C is the number of\\ncategories.\\nInterval-label Classiﬁcation: the traditional deep clas-\\nsiﬁcation can be deﬁned as follows: ∀x ∈RD, T : x →R1,\\nwhere x is an example, D is the dimension of x. It prede-\\nﬁnes some nonoverlapping intervals. The interval that the\\nmodel output falls is corresponding interval label. If the\\noutput does not fall into any predeﬁend intervals, the input\\nwill be marked as an anomaly example.\\nBinary-label Classiﬁcation: the binary-label classiﬁca-\\ntion can be deﬁned as follows: ∀x ∈RD, T : x →RB,\\nwhere x is an example, D is the dimension of x. B is the\\nnumber of bits in the binary label.\\nThe decision process of traditional classiﬁcation is different\\nfrom the binary-label and interval-label classiﬁcations.\\nThe traditional classiﬁcation maximizes the probability\\ncorresponding to the ground-truth label. The binary-label\\nclassiﬁcation focuses on each element’s sign for the output\\nvector. The interval-label classiﬁcation tries to decide the\\ninterval that the output falls into. Generally speaking,\\nthe traditional classiﬁcation decides the category that the\\ninput most likely belongs to, while the binary-label and\\ninterval-label classiﬁcations have to decide the category that\\nthe input exactly belongs to. The following paragraphs will\\nintroduce details of robust deep classiﬁcation systems based\\non robust binary-label and interval-label classiﬁcations.\\nImplementation Details of Interval-label\\nClassiﬁcation System\\nAs introduced above, the interval-label classiﬁcation system\\nconsists of three modules: the interval-label classiﬁer,\\nlabel encoding module, and label decoding module. The\\ninterval-label classiﬁer can be a deep neural network whose\\noutput is a scalar. The label encoding turns the hard label\\ninto an interval label, namely codeword, while the label\\ndecoding translates the output of the classiﬁer as a hard\\nlabel.\\nLabel Encoding module of interval-label classiﬁcation\\nsystem transforms the hard label into an interval label. The\\nmap function can be formulated as follows.\\nL(y) = s0 + y · (α + β);\\nU(y) = L(y) + β,\\nwhere s0 is the smallest lower bound of interval labels.\\nα is the length of the gap between two adjacent interval\\nlabels, while β is the length of the interval label. ML(·)\\nand MU(·) are lower bound and upper bound map function,\\nrespectively. According to Eq. (), for the hard label ‘3’,\\ns0 = 0, α = 1, β = 3, the corresponding interval label is\\n[12, 15].\\nThe label decoding function can be formulated as fol-\\nlows.\\ney = ⌊I(x) −s0\\nα + β\\n⌋,\\nwhere x is the input example. I(·) is the interval-label\\nclassiﬁer. ⌊·⌋is the ﬂoor function. If I(x) does not belong\\nto any interval label, the corresponding input example will\\nviewed as an abnormal example.\\nThe\\nloss\\nfunction\\nof\\nthe\\ninterval-label\\nclassiﬁcation\\ncan be formulated as follows.\\nL(B(X, Y\\n′); θ) = ∥r(L(Y ) −I(X))∥2\\n2\\n|\\n{z\\n}\\nlower bound loss\\n+\\n∥r(I(X) −U(Y )∥2\\n2\\n|\\n{z\\n}\\nupper bound loss\\n,\\nwhere θ is the parameter set of the classiﬁer I. B(X, Y\\n′) is\\na mini batch. X and Y is the examples set and hard label\\nset, and Y\\n′ = [L(Y ), U(Y )] is the interval labels set. L(Y )\\nand U(Y ) is the lower bound set and upper bound set, re-\\nspectively. r(·) is the ReLU (Nair and Hinton 2010) activa-\\ntion function. lower bound loss and upper bound loss in\\nEq. are the distance between the output and lower and upper\\nbounds, respectively.\\nImplementation Details of Robust Binary-label\\nClassiﬁcation System\\nThere are three hyperparameters for interval-label classi-\\nﬁcation, which may limit its generalization. So we also\\nexplore another classiﬁcation for a robust deep classiﬁca-\\ntion system, namely robust binary-label classiﬁcation. As\\nintroduced above, a robust binary-label classiﬁcation (RBC)\\nsystem involves three modules: the binary-label classiﬁer,\\nthe label-encoding module, and the label decoding module.\\nThe binary-label classiﬁer is the same as the traditional\\nneural network classiﬁer, except for the output dimension.\\nThe label-encoding module converts the traditional decimal\\nlabel into a binary label, while the label decoding module\\nconverts the binary label into a decimal label.\\nIn this paper, the binary-label classiﬁer is a deep neu-\\nral network. We set the output dimension of the network as\\nB, 2B ≥C. C is the number of predeﬁned categories. If\\ndenoting the robust binary-label classiﬁer as B, the process\\nto get binary label b of the example x can be formulated as\\nfollows.\\nbi =\\n\\x1a1, B(x)i > 0;\\n0, B(x)i ≤0,\\n(0)\\nwhere i ∈0, 1, · · · , B −1. bi and B(x) are i-th element of\\nthe binary label and output of binary-label classiﬁer.\\nLabel-encoding and label-decoding modules are intro-\\nduced for conversion between the decimal and binary labels.\\nThe loss function of the RBC can be formulated as\\nfollows.\\nL(x, b; θ) = ∥r(S · 1 −B(x) · (2b −1))∥2\\n2,\\n(0)\\nwhere x and b are the input example and its correspond-\\ning binary label, respectively. θ is the parameter set of the\\nbinary-label classiﬁer. r(·) is the ReLU activation function.\\nS scaling factor is a hyperparameter to control margins be-\\ntween different categories. 1 is the all-ones vector. In detail,\\n∀x and its binary label b, when S = 100, if bi = 0, the loss\\nin Eq. will force the B(x)i ≤−100. Otherwise, if bi = 1,\\nthe loss in Eq. will force the B(x)i ≥100, i = 0, · · · , B−1.\\nIn other words, loss in Eq. scales the margin between ele-\\nments in binary label from 1 to 2S.\\nExperiment\\nExperiment Settings\\nIn this paper, two data sets are applied to evaluate the\\neffectiveness of our method, namely MNIST (Lecun et al.\\n1998), CIFAR-10, and CIFAR-100 (Krizhevsky 2009).\\nMNIST is a 10-class grayscale hand-written digit image\\ndataset, consisting of 60,000 training examples and 10,000\\ntesting examples. CIFAR-10 is a 10-class color image\\ndataset, consisting of 50,000 training examples and 10,000\\ntesting examples. CIFAR100 is a 100-class color image\\ndataset, consisting of 50,000 training examples and 10,000\\ntesting examples.\\nFor adversarial robustness evaluation, FGSM and PGD\\nare adopted. FGSM is a classical single-step attack, while\\nPGD is a multi-step attack. The two attacks are adopted\\nfor the reason that they can conveniently control the attack\\nintensity. Moreover, since the decision process of the\\nrobust binary-label and interval-label classiﬁcations are\\ndifferent from the traditional classiﬁcation, the existing\\noptimization-based attack can not attack the binary-label\\nclassiﬁcation, such as CW and Deepfool. The reason is that\\nadversary can not directly get the probability that the input\\nbelongs to a speciﬁc category.\\nConvergence of Robust Binary-label Classiﬁcation\\nSystem\\nIn this subsection, we experiment to investigate the con-\\nvergence of robust binary-label classiﬁcation system. We\\n0\\n20\\n40\\n60\\n80\\n100\\nTraining Epoch\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\nLoss\\n95\\n96\\n97\\n98\\n99\\n100\\nAccuracy (%)\\nHighest Test Accuracy: 99.49\\nLoss\\nTrain Acc\\nTest Acc\\n(a) TRA MNIST\\n0\\n20\\n40\\n60\\n80\\n100\\nTraining Epoch\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\n1750\\nLoss\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHighest Test Accuracy: 99.52\\nLoss\\nTrain Acc\\nTest Acc\\n(b) RBC MNIST\\n0\\n50\\n100\\n150\\n200\\nTraining Epoch\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\nLoss\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHighest Test Accuracy: 90.59\\nLoss\\nTrain Acc\\nTest Acc\\n(c) TRA CIFAR10\\n0\\n50\\n100\\n150\\n200\\nTraining Epoch\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\nLoss\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nHighest Test Accuracy: 91.63\\nLoss\\nTrain Acc\\nTest Acc\\n(d) RBC CIFAR10\\n0\\n50\\n100\\n150\\n200\\nTraining Epoch\\n0\\n1\\n2\\n3\\n4\\nLoss\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nHighest Test Accuracy: 68.85\\nLoss\\nTrain Acc\\nTest Acc\\n(e) TRA CIFAR100\\n0\\n100\\n200\\n300\\n400\\nTraining Epoch\\n0\\n2000\\n4000\\n6000\\n8000\\nLoss\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy (%)\\nHighest Test Accuracy: 64.89\\nLoss\\nTrain Acc\\nTest Acc\\n(f) RBC CIFAR100\\nFigure 2: Comparison of convergence between the tradi-\\ntional and robust binary-label classiﬁcations. TRA and RBC\\nrepresent the traditional and robust binary-label classiﬁca-\\ntion systems, respectively.\\ncompare the convergence of the traditional and binary-\\nlabel classiﬁcation systems on MNIST, CIFAR10, and\\nCIFAR100. B is set as 4 for MNIST and CIFAR10, while\\nB is set as 7 for CIFAR100. For the robust binary-label\\nclassiﬁcation, the scaling factor S is set as 350 for all\\ndatasets. In this paper, EfﬁcientNet is EfﬁcientNetB0\\nwith 5.3M parameters (Tan and Le 2019), and ResNet is\\nResNet18 with 9.2M parameters (He et al. 2016).\\nAs shown in Fig. 2, on MNIST and CIFAR10, robust\\nbinary-label classiﬁcation system achieves higher test\\naccuracy than the traditional classiﬁcation system. While\\non CIFAR100, the binary-label classiﬁcation system con-\\nvergences more slowly than the traditional classiﬁcation\\nsystem. The reason is that the binary-label classiﬁcation\\nis more complex than the traditional classiﬁcation. So the\\nbinary-label classiﬁcation system needs a higher model\\ncapacity to ﬁt the data distribution. The test accuracies of\\ndifferent classiﬁcations with variant models are similar,\\nwhich demonstrates that when model capacity is sufﬁcient,\\nthe robust binary-label and traditional classiﬁcation systems\\ngeneralize similarly. Moreover, that explains why the test\\naccuracy of binary-label classiﬁcation system is slightly\\nlower than the traditional classiﬁcation system in some\\ncases.\\nRobustness Evaluation for Different Classiﬁcation\\nSystems\\nIn this subsection, we experiment to investigate the robust-\\nness of different classiﬁcation systems. We compare the\\nrobustness of the traditional classiﬁcation, interval-label\\nclassiﬁcation, and binary-label classiﬁcations with Efﬁ-\\ncientNet and ResNet on MNIST, CIFAR10, and CIFAR100.\\nSince FGSM and PGD are easy to control the attack\\nintensity, we set three different attack intensities, namely I,\\nII, and III. I represents the attack intensity is 0.1 for MNIST\\nand 3/255 for CIFAR10 and CIFAR100. II represents the\\nattack intensity is 0.2 for MNIST and 6/255 for CIFAR10\\nand CIFAR100. III represents the attack intensity is 0.3 for\\nMNIST and 9/255 for CIFAR10 and CIFAR100. LEG in\\nTable 1 represents the accuracy of the legitimate examples.\\nTRA, DeepBE, INT, and RBC represent the traditional\\nclassiﬁcation, binary encoding classiﬁcation, interval-label\\nclassiﬁcation, and robust binary-label classiﬁcation, respec-\\ntively. For the binary-label classiﬁcation, the S is set 350,\\nwhile for the interval-label classiﬁcation, the interval label\\ninitial point s0 is set as 0, interval label length β is set as 16,\\nand the gap length α is set as 4.\\nAs shown in Table 1, our method achieves the highest\\naccuracies in variant evaluation scenarios. Compared to the\\ntraditional and interval-label classiﬁcation, our method can\\nbalance the adversarial robustness and accuracy better. We\\nalso note that the binary-label and interval-label classiﬁ-\\ncations are insensitive to the variant in adversarial attack\\nintensity, which is different from the traditional classiﬁca-\\ntion. The reason is that the traditional classiﬁcation depends\\non the relative magnitude for the values of elements of\\nthe output vector, while the binary-label and interval-label\\nclassiﬁcation depend on the actual magnitude for the values\\nof elements in the outputs vector. Different adversarial\\nattack intensities have a variant impact on the relative\\nmagnitude for the values of elements in the output vector,\\nnamely the logits. However, only when the adversarial\\nattack intensity exceeds a speciﬁc threshold, the adversarial\\nattack can change the sign of elements in the output vector\\nor cause the values of elements in the output vector to vary\\nfrom one interval to another.\\nWe also experiment to investigate the robustness of our\\nmethod combining with the adversarial training or retraining\\nwith Gaussian Noise perturbed examples. The adversar-\\nial training method in this paper is Madry adversarial\\ntraining (Madry et al. 2018). The intensity of adversarial\\nexamples and Gaussian Noise for the adversarial training\\nand retraining is set as I, as mentioned above. In Table\\n2, the TRA+AT and RBC+AT represent the traditional\\nclassiﬁcation with Madry adversarial training and robust\\nbinary-label classiﬁcation with Madry adversarial training,\\nrespectively. RBC+GN represents the robust binary-label\\nclassiﬁcation retrained with Gaussian Noise perturbed ex-\\namples. ↑represents that corresponding retraining method\\nTable 1: Comparison of adversarial robustness and accuracy among different classiﬁcation systems.\\nData set\\nModel\\nMethod\\nFGSM\\nPGD\\nLEG\\nI\\nII\\nIII\\nI\\nII\\nIII\\nMNIST\\nEfﬁcientNetB0\\nTRA\\n20.38\\n14.03\\n8.10\\n6.50\\n4.54\\n2.96\\n99.49\\nDeepBE\\n86.87\\n69.89\\n48.95\\n71.33\\n32.69\\n11.56\\n99.26\\nINT\\n99.12\\n99.12\\n99.13\\n99.04\\n98.71\\n99.11\\n99.18\\nRBC\\n99.26\\n99.27\\n99.26\\n99.20\\n98.89\\n97.90\\n99.52\\nResNet18\\nTRA\\n68.52\\n14.88\\n10.03\\n34.59\\n4.40\\n2.19\\n99.65\\nDeepBE\\n90.26\\n56.51\\n39.14\\n64.54\\n9.43\\n6.47\\n99.51\\nINT\\n99.25\\n99.21\\n99.21\\n99.16\\n98.96\\n98.23\\n99.25\\nRBC\\n99.21\\n99.22\\n99.21\\n99.50\\n95.19\\n74.55\\n99.55\\nCIFAR10\\nEfﬁcientNetB0\\nTRA\\n62.95\\n38.50\\n25.55\\n61.05\\n31.07\\n15.12\\n90.59\\nDeepBE\\n52.48\\n38.43\\n33.64\\n51.59\\n36.44\\n30.19\\n76.82\\nINT\\n80.66\\n80.37\\n80.46\\n80.48\\n80.19\\n80.30\\n83.94\\nRBC\\n85.43\\n85.40\\n85.38\\n85.52\\n85.30\\n85.23\\n91.63\\nResNet18\\nTRA\\n75.26\\n65.15\\n52.84\\n73.17\\n52.13\\n38.36\\n93.67\\nDeepBE\\n71.65\\n66.64\\n62.95\\n70.33\\n64.37\\n60.33\\n89.08\\nINT\\n86.47\\n86.55\\n86.56\\n86.40\\n86.55\\n86.47\\n87.54\\nRBC\\n90.92\\n90.91\\n90.94\\n90.86\\n90.99\\n90.93\\n93.55\\nCIFAR100\\nEfﬁcientNetB0\\nTRA\\n40.86\\n22.38\\n14.21\\n39.39\\n17.90\\n8.21\\n68.85\\nDeepBE\\n9.80\\n5.56\\n3.65\\n9.76\\n5.31\\n3.22\\n17.90\\nINT\\n6.10\\n6.12\\n6.21\\n6.04\\n6.06\\n6.09\\n8.24\\nRBC\\n54.19\\n54.12\\n54.37\\n53.88\\n53.76\\n53.75\\n64.68\\nResNet18\\nTRA\\n46.52\\n31.87\\n24.06\\n42.94\\n23.16\\n12.37\\n72.05\\nDeepBE\\n16.68\\n11.06\\n8.76\\n16.40\\n10.32\\n7.98\\n27.99\\nINT\\n17.59\\n17.61\\n17.65\\n17.43\\n17.52\\n17.62\\n20.13\\nRBC\\n58.33\\n58.35\\n58.45\\n58.35\\n58.26\\n57.94\\n69.09\\ncan improve the adversarial robustness, while ↓represents\\nthe retraining suppresses the adversarial robustness.\\nAs shown in Table 2, the robust binary-label classiﬁcation\\nis still more robust than the traditional classiﬁcation, even\\nwith the adversarial training. We note that the adversarial\\ntraining with a speciﬁc adversarial attack intensity can not\\nendow the deep model robustness against variant-intensity\\nattacks, especially when the attack intensity is bigger than\\nthat in the adversarial training. However, the variant of the\\nattack intensity has a limited impact on the robustness of\\nthe RBC. Different from the traditional classiﬁcation, we\\nnote that adversarial training can not constantly improve\\nthe robustness of RBC, but retraining with Gaussian Noise\\nperturbed examples can. Here we deﬁne a stability metric\\nof robust accuracy.\\nR = acca\\nacct\\n,\\n(0)\\nwhere acca and acct are robust accuracy and test accuracy,\\nwhen adopting speciﬁc defense strategy. R can be employed\\nto evaluate the sufﬁciency of defense method. For instance,\\nif the R is smaller than 1 for the adversarial training, we\\nmay need to increase the training epochs or the attack inten-\\nsity. According to Table 2, adversarial training improves the\\nrobustness of RBC, when R is abnormally small. However,\\nwhen R is large enough, adversarial training will inhibit the\\nrobustness of RBC.\\nTo further intestigate robustness of variant classiﬁca-\\ntion systems, we also experiment to visualize features with\\nt-SNE (Maaten and Geoffrey 2008). Since the Efﬁcient-\\nNetB0 has been employed as the backbone model for many\\nlearning tasks, we apply t-SNE visualization to its penul-\\ntimate layer features. We randomly select 1,000 examples\\nfrom the MNIST for the visualization. The scaling factor S\\nis set as 350 for the robust binary-label classiﬁcation.\\nAs shown in Fig. 3, EfﬁcientNetB0 can learn a reasonable\\nfeature representation for the traditional, binary-label, and\\ninterval label classiﬁcation. There is even no intersection for\\nessential features of different examples belong to different\\ncategories. Moreover, we note that features of examples\\nfrom the same category are distributed in long and narrow\\nareas for the robust binary-label and interval-label classiﬁ-\\ncation. The reason is that robust binary-label and interval\\nlabel classiﬁcation depends on actual values of the outputs,\\nand the loss function constraints their outputs into some\\nspeciﬁc regions. We also note that the distances between\\nfeatures of different categories for robust binary-label and\\ninterval-label classiﬁcation are more signiﬁcant than that\\nof traditional classiﬁcation or DeepBE. Increasing the\\nmargin between the boundaries of different categories can\\nimprove the robustness of deep learning systems (Tanay and\\nGrifﬁn 2016). That is the reason why robust binary-label\\nand interval-label classiﬁcations are more robust than the\\ntraditional classiﬁcation.\\nFig. 3 also can explain different impacts of adversarial\\nTable 2: Comparison of adversarial robustness and accuracy between traditional and robust bianry-label classiﬁcation systems\\nwith Madry Adversarial Training.\\nData set\\nModel\\nMethod\\nFGSM\\nPGD\\nLEG\\nI\\nII\\nIII\\nI\\nII\\nIII\\nMNIST\\nEfﬁcientNetB0\\nTRA+AT\\n98.48↑\\n96.89↑\\n88.65↑\\n98.37↑\\n80.51↑\\n10.36↑\\n99.44↓\\nRBC+AT\\n99.21↓\\n99.20↓\\n99.20↓\\n99.19↓\\n99.18↑\\n99.14↑\\n99.54↑\\nRBC+GN\\n99.34↑\\n99.33↑\\n99.35↑\\n99.36↑\\n99.21↑\\n98.78↑\\n99.44↓\\nResNet18\\nTRA+AT\\n98.97↑\\n98.00↑\\n85.03↑\\n98.98↑\\n94.87↑\\n30.25↑\\n99.57↓\\nRBC+AT\\n99.04↓\\n98.87↓\\n98.88↓\\n98.96↓\\n98.89↑\\n98.77↑\\n99.53↓\\nRBC+GN\\n99.30↑\\n99.30↑\\n99.30↑\\n99.33↓\\n99.00↑\\n94.28↑\\n99.56↑\\nCIFAR10\\nEfﬁcientNetB0\\nTRA+AT\\n80.33↑\\n69.25↑\\n58.78↑\\n80.30↑\\n69.41↑\\n57.81↑\\n88.58↓\\nRBC+AT\\n83.31↓\\n79.81↓\\n79.22↓\\n83.25↓\\n79.60↓\\n79.18↓\\n91.30↓\\nRBC+GN\\n88.43↑\\n88.48↑\\n88.49↑\\n88.46↑\\n88.53↑\\n88.27↑\\n92.09↑\\nResNet18\\nTRA+AT\\n86.81↑\\n80.11↑\\n72.96↑\\n84.74↑\\n79.66↑\\n71.41↑\\n92.63↓\\nRBC+AT\\n86.94↓\\n86.10↓\\n86.03↓\\n86.92↓\\n86.07↓\\n85.95↓\\n93.29↓\\nRBC+GN\\n91.75↑\\n91.77↑\\n91.78↑\\n91.72↑\\n91.70↑\\n91.52↑\\n93.78↑\\nCIFAR100\\nEfﬁcientNetB0\\nTRA+AT\\n53.66↑\\n43.16↑\\n34.03↑\\n53.63↑\\n42.78↑\\n33.14↑\\n64.70↓\\nRBC+AT\\n50.91↓\\n49.60↓\\n49.21↓\\n50.52↓\\n49.03↓\\n48.86↓\\n63.36↓\\nRBC+GN\\n57.30↑\\n57.18↑\\n57.44↑\\n56.85↑\\n56.75↑\\n56.67↑\\n64.30↓\\nResNet18\\nTRA+AT\\n59.07↑\\n50.38↑\\n42.32↑\\n58.87↑\\n49.69↑\\n40.87↑\\n68.96↓\\nRBC+AT\\n52.81↓\\n51.17↓\\n51.08↓\\n52.54↓\\n51.18↓\\n51.00↓\\n66.94↓\\nRBC+GN\\n61.22↑\\n61.29↑\\n61.40↑\\n61.24↑\\n61.09↑\\n60.95↑\\n68.56↓\\n40\\n20\\n0\\n20\\n40\\n40\\n20\\n0\\n20\\n40\\n5\\n0\\n4\\n1\\n9\\n2\\n3\\n6\\n7\\n8\\n(a) TRA\\n40\\n20\\n0\\n20\\n40\\n40\\n20\\n0\\n20\\n40\\n5\\n0\\n4\\n1\\n9\\n2\\n3\\n6\\n7\\n8\\n(b) DeepBE\\n40\\n20\\n0\\n20\\n40\\n60\\n40\\n20\\n0\\n20\\n40\\n5\\n0\\n4\\n1\\n9\\n2\\n3\\n6\\n7\\n8\\n(c) RBC\\n75\\n50\\n25\\n0\\n25\\n50\\n75\\n60\\n40\\n20\\n0\\n20\\n40\\n5\\n0\\n4\\n1\\n9\\n2\\n3\\n6\\n7\\n8\\n(d) INT\\nFigure 3: The t-SNE visualization of features in penultimate layer of EfﬁcientNetB0 for traditional classiﬁcation (TRA),\\nDeepBE, robust binary-label classiﬁcation (RBC), and interval-label classiﬁcation (INT) on MNIST. The ‘red’, ‘green’, ‘blue’,\\n‘orange’, ‘purple’, ‘brown’, ‘gray’, ‘pink’, ‘olive’, and ‘cyan’ colorful points represent examples of ‘0’, ‘1’, ‘2’, ‘3’, ‘4’, ‘5’,\\n‘6’, ‘7’, ‘8’, and ‘9’ category, respectively.\\ntraining on the traditional classiﬁcation and robust binary\\nclassiﬁcaiton. The feature distribution manifold for tra-\\nditional classiﬁcation approximates a hypersphere. The\\nadversarial example is near to the decision boundary, which\\ncan make the decision boundary more smooth and decrease\\nthe dimension of adversarial space (Tram`er et al. 2017).\\nHowever, features of RBC distribute in a long and narrow\\nregions. Moreover, RBC depends on actual values of ele-\\nments in output vector. The adversarial training will extend\\nthe feature manifold and decrease the margin between\\ndifferent categories. The feature manifold extension has a\\nlimited impact on traditional classiﬁcation since it depends\\non the relative values of elements in the output vector.\\nImpact of S on Accuracy and Robustness of\\nRobust Binary-label Classiﬁcation System\\nIn this subsection, we experiment to investigate the impact\\nof S in Eq. on the accuracy and robustness of the robust\\nbinary-label classiﬁcation system. In detail, we set variant S\\nfor RBC to check its accuracy and robustness on CIFAR100.\\nThe deep model to implement RBC is the EfﬁcientNetB0.\\nMoreover, the FGSM and PGD-5 are adopted, and the\\nadversarial intensity is set as 9/255.\\nAs shown in Fig. 4, with increasing S, the robust binary-\\nlabel classiﬁcation achieves higher test accuracy and\\nrobustness. The reason is that S determines the margin\\nbetween different categories. Bigger S endows RBC with a\\nbigger margin between different categories. Furthermover,\\nbigger margin endows the robust binary-label classiﬁcation\\nto generalize better. However, we also note that when\\nFGSM\\nPGD\\nLEG\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nAccuracy(%)\\n9.11\\n7.91\\n26.22\\n27.34\\n19.71\\n61.33\\n44.88\\n42.44\\n63.08\\n54.37\\n53.79\\n64.68\\n59.53\\n58.97\\n64.47\\nS=1\\nS=150\\nS=250\\nS=350\\nS=500\\nFigure 4: Comparison of accuracy and adversarial robust-\\nness between robust binary-label classiﬁcations with variant\\nS.\\nS exceeds a threshold, the generalization of the robust\\nbinary-label classiﬁcation improves slowly. Moreover, we\\nnote that the increasing S will slow down the convergence.\\nWhen S\\n=\\n350, the robust binary-label classiﬁcation\\nachieves reasonable convergence. According to Fig. 4, we\\nconclude that the accuracy and robustness for the robust\\nbinary-label classiﬁcation are consistent, which is different\\nfrom the traditional classiﬁcation (Tsipras et al. 2019).\\nInvestigation of Adversarial Transferability for\\nDifferent Classiﬁcation Systems\\nIn this subsection, we experiment to investigate the ad-\\nversarial\\ntransferability\\nbetween\\ndifferent\\nmodels\\nfor\\nthe traditional classiﬁcation, interval-label, and robust\\nbinary-label classiﬁcation systems. In detail, CIFAR100 is\\nemployed evaluation dataset to investigate the adversarial\\ntransferability. The adopted adversarial attack method is\\nPGD-5, and the adversarial attack intensity is set as 9/255.\\nIn Fig. 5, the model in the row is the threat model, while the\\nmodel in the column is the victim model. The threat model\\ncrafts adversarial examples to attack victim models. In order\\nto eliminate the impact of test accuracy of the threat model,\\nwe remove examples that are misclassiﬁed by the threat\\nmodel from the testing set of CIFAR100.\\nAs shown in Fig. 5, the adversarial transfer rate of the\\ntraditional classiﬁcation is higher than that of the robust\\nbinary-label and interval-label classiﬁcation. We also\\nattribute it to the reason that the traditional classiﬁcation\\ndepends on the relative magnitude for the values of elements\\nin the output vector, while the binary-label and interval-label\\nclassiﬁcation depend on the actual magnitude for the values\\nof elements in the outputs vector. The adversarial attack\\ncan change the relative magnitude for element values but\\ndifferent magnitude of actual values in the output vector\\n(Baluja and Fischer 2018).\\n1.0\\n0.93\\n0.95\\n0.91\\n1.0\\n0.87\\n0.93\\n0.9\\n1.0\\nD\\nR\\nE\\nE\\nR\\nD\\n0.90\\n0.95\\n1.00\\n(a) TRA\\n1.0\\n0.34\\n0.27\\n0.13\\n1.0\\n0.26\\n0.21\\n0.47\\n1.0\\nD\\nR\\nE\\nE\\nR\\nD\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(b) INT\\n1.0\\n0.64\\n0.63\\n0.71\\n1.0\\n0.7\\n0.74\\n0.75\\n1.0\\nS=500\\nS=350\\nS=250\\nS=250\\nS=350\\nS=500\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\n(c) RBC S\\n1.0\\n0.58\\n0.51\\n0.64\\n1.0\\n0.65\\n0.72\\n0.75\\n1.0\\nD\\nR\\nE\\nE\\nR\\nD\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n(d) RBC ARC\\nFigure 5: The adversarial transferability of different classiﬁ-\\ncation systems. The values in the ﬁgure represent the success\\nattack rate. E, R, and D represent EfﬁcientNetB0, ResNet18,\\nand DenseNet121, respectively. Models in the row are threat\\nmodels, while models in the column are victim models. Fig.\\n5(c) investigates the adversarial transfer rate for RBC with\\nvariant S, and the adopted deep model is EfﬁcientNetB0.\\nFig. 5(d) investigates the adversarial transfer rate for RBC\\nimplemented by different deep models, and S is set as 350.\\nDiscussion\\nThe DeepBE can be viewed as a special case of RBC. The\\nloss of DeepBE (Li et al. 2016) can be reformulated as\\n∥r(0 −(B(x) −0.5)(2b −1))∥2\\n2. So the DeepBE can be\\nviewed as RBC with S = 0. Here B and r the binary-label\\nclassiﬁer and ReLU function, respectively. x is an example,\\nand b is its corresponding binary label. The relationship\\nbetween our method and regularization methods, such as\\nadversarial training, is parallel. They focus on different\\nelements of the deep learning system. As introduced above,\\nour method with adversarial training can further improve\\nthe robustness of the deep learning system in some cases.\\nThe interval-label classiﬁcation and RBC adopt differ-\\nent label encoding strategies to redeﬁne the classiﬁcation\\ntask. Actually, with an increasing numerical system, we\\ncan adopt fewer bits to represent a number. For instance,\\ndecimal number 9 is only a bit in decimal encoding,\\nwhile it needs four bit in binary encoding. Interval-label\\nclassiﬁcation can be viewed as adopting a larger numerical\\nsystem than binary-label classiﬁcation to encoding the label.\\nThey both mark the input depending on the actual values of\\nelements in the output vector.\\nConclusion\\nIn this paper, we improve the robustness of deep learn-\\ning systems from both the learning task and deep model\\nprospects. The interval-label classiﬁcation can reduce the\\nadversarial transferability of adversarial examples. Unlike\\nthe traditional classiﬁcation, the robustness of robust binary-\\nlabel classiﬁcation is no more at odds with the accuracy. The\\nscaling factor of the robust binary-label classiﬁcation con-\\nstrains the minima boundary margin. Moreover, the experi-\\nmental results demonstrate that adversarial training does not\\nconstantly improve the adversarial robustness of the deep\\nlearning system, demonstrating that the learning task im-\\npacts the robustness of deep learning systems.\\nReferences\\nBaluja, S.; and Fischer, I. 2018. Learning to Attack: Ad-\\nversarial Transformation Networks. In Proceedings of the\\nThirty-Second AAAI Conference on Artiﬁcial Intelligence,\\n(AAAI-18), the 30th innovative Applications of Artiﬁcial In-\\ntelligence (IAAI-18), and the 8th AAAI Symposium on Edu-\\ncational Advances in Artiﬁcial Intelligence (EAAI-18), New\\nOrleans, Louisiana, USA, February 2-7, 2018, 2687–2695.\\nBlundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra,\\nD. 2015. Weight Uncertainty in Neural Networks. CoRR,\\nabs/1505.05424.\\nCao, X.; Shi, Y.; Yu, H.; Wang, J.; Wang, X.; Yan, Z.; and\\nChen, Z. 2021. DEKR: Description Enhanced Knowledge\\nGraph for Machine Learning Method Recommendation. In\\nDiaz, F.; Shah, C.; Suel, T.; Castells, P.; Jones, R.; and Sakai,\\nT., eds., SIGIR ’21: The 44th International ACM SIGIR Con-\\nference on Research and Development in Information Re-\\ntrieval, Virtual Event, Canada, July 11-15, 2021, 203–212.\\nACM.\\nCarlini, N.; and Wagner, D. A. 2017. Towards Evaluating the\\nRobustness of Neural Networks. In 2017 IEEE Symposium\\non Security and Privacy, SP 2017, San Jose, CA, USA, May\\n22-26, 2017, 39–57.\\nDas, N.; Shanbhogue, M.; Chen, S.; Hohman, F.; Li, S.;\\nChen, L.; Kounavis, M. E.; and Chau, D. H. 2018. SHIELD:\\nFast, Practical Defense and Vaccination for Deep Learn-\\ning using JPEG Compression. In Guo, Y.; and Farooq, F.,\\neds., Proceedings of the 24th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, KDD\\n2018, London, UK, August 19-23, 2018, 196–204. ACM.\\nDietterich, T. G.; and Bakiri, G. 1995. Solving Multiclass\\nLearning Problems via Error-Correcting Output Codes. J.\\nArtif. Intell. Res., 2: 263–286.\\nFeldman, V.; and Zhang, C. 2020. What Neural Networks\\nMemorize and Why: Discovering the Long Tail via Inﬂu-\\nence Estimation. In Larochelle, H.; Ranzato, M.; Hadsell,\\nR.; Balcan, M.; and Lin, H., eds., Advances in Neural Infor-\\nmation Processing Systems 33: Annual Conference on Neu-\\nral Information Processing Systems 2020, NeurIPS 2020,\\nDecember 6-12, 2020, virtual.\\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-\\ning and Harnessing Adversarial Examples. In 3rd Interna-\\ntional Conference on Learning Representations, ICLR 2015,\\nSan Diego, CA, USA, May 7-9, 2015, Conference Track Pro-\\nceedings.\\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\\nual Learning for Image Recognition. In 2016 IEEE Confer-\\nence on Computer Vision and Pattern Recognition, CVPR\\n2016, Las Vegas, NV, USA, June 27-30, 2016, 770–778.\\nIEEE Computer Society.\\nKatz, G.; Barrett, C. W.; Dill, D. L.; Julian, K.; and Kochen-\\nderfer, M. J. 2017. Reluplex: An Efﬁcient SMT Solver for\\nVerifying Deep Neural Networks.\\nIn Majumdar, R.; and\\nKuncak, V., eds., Computer Aided Veriﬁcation - 29th Inter-\\nnational Conference, CAV 2017, Heidelberg, Germany, July\\n24-28, 2017, Proceedings, Part I, volume 10426 of Lecture\\nNotes in Computer Science, 97–117. Springer.\\nKrizhevsky, A. 2009. Learning Multiple Layers of Features\\nfrom Tiny Images. Technical report.\\nKurakin, A.; Goodfellow, I. J.; and Bengio, S. 2017. Adver-\\nsarial Machine Learning at Scale. In 5th International Con-\\nference on Learning Representations, ICLR 2017, Toulon,\\nFrance, April 24-26, 2017, Conference Track Proceedings.\\nLeCun, Y.; Bengio, Y.; and Hinton, G. E. 2015. Deep learn-\\ning. Nature, 521(7553): 436–444.\\nLecun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.\\nGradient-based learning applied to document recognition.\\nProceedings of the IEEE, 86(11): 2278–2324.\\nLee, K.; Yun, S.; Lee, K.; Lee, H.; Li, B.; and Shin, J.\\n2019. Robust Inference via Generative Classiﬁers for Han-\\ndling Noisy Labels. In Chaudhuri, K.; and Salakhutdinov,\\nR., eds., Proceedings of the 36th International Conference\\non Machine Learning, ICML 2019, 9-15 June 2019, Long\\nBeach, California, USA, volume 97 of Proceedings of Ma-\\nchine Learning Research, 3763–3772. PMLR.\\nLi, C.; Kang, Q.; Ge, G.; Song, Q.; Lu, H.; and Cheng, J.\\n2016. DeepBE: Learning Deep Binary Encoding for Multi-\\nlabel Classiﬁcation. In 2016 IEEE Conference on Computer\\nVision and Pattern Recognition Workshops, CVPR Work-\\nshops 2016, Las Vegas, NV, USA, June 26 - July 1, 2016,\\n744–751. IEEE Computer Society.\\nLi, M.; He, L.; and Lin, Z. 2020.\\nImplicit Euler Skip\\nConnections: Enhancing Adversarial Robustness via Nu-\\nmerical Stability. In Proceedings of the 37th International\\nConference on Machine Learning, ICML 2020, 13-18 July\\n2020, Virtual Event, volume 119 of Proceedings of Machine\\nLearning Research, 5874–5883. PMLR.\\nMa, S.; Liu, Y.; Tao, G.; Lee, W.; and Zhang, X. 2019. NIC:\\nDetecting Adversarial Samples with Neural Network Invari-\\nant Checking. In 26th Annual Network and Distributed Sys-\\ntem Security Symposium, NDSS 2019, San Diego, Califor-\\nnia, USA, February 24-27, 2019. The Internet Society.\\nMaaten, L. V. D.; and Geoffrey, H. 2008.\\nVisualizing\\nData using t-SNE. Journal of Machine Learning Research,\\n9(2605): 2579–2605.\\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and\\nVladu, A. 2018.\\nTowards Deep Learning Models Resis-\\ntant to Adversarial Attacks.\\nIn 6th International Confer-\\nence on Learning Representations, ICLR 2018, Vancouver,\\nBC, Canada, April 30 - May 3, 2018, Conference Track Pro-\\nceedings.\\nMeng, D.; and Chen, H. 2017. MagNet: A Two-Pronged De-\\nfense against Adversarial Examples. In Proceedings of the\\n2017 ACM SIGSAC Conference on Computer and Commu-\\nnications Security, CCS 2017, Dallas, TX, USA, October 30\\n- November 03, 2017, 135–147.\\nMoosavi-Dezfooli, S.; Fawzi, A.; and Frossard, P. 2016.\\nDeepFool: A Simple and Accurate Method to Fool Deep\\nNeural Networks. In 2016 IEEE Conference on Computer\\nVision and Pattern Recognition, CVPR 2016, Las Vegas, NV,\\nUSA, June 27-30, 2016, 2574–2582.\\nNair, V.; and Hinton, G. E. 2010. Rectiﬁed Linear Units\\nImprove Restricted Boltzmann Machines. In F¨urnkranz, J.;\\nand Joachims, T., eds., Proceedings of the 27th International\\nConference on Machine Learning (ICML-10), June 21-24,\\n2010, Haifa, Israel, 807–814. Omnipress.\\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\\nD.; Goodfellow, I. J.; and Fergus, R. 2014. Intriguing proper-\\nties of neural networks. In 2nd International Conference on\\nLearning Representations, ICLR 2014, Banff, AB, Canada,\\nApril 14-16, 2014, Conference Track Proceedings.\\nTan, M.; and Le, Q. V. 2019. EfﬁcientNet: Rethinking Model\\nScaling for Convolutional Neural Networks. In Chaudhuri,\\nK.; and Salakhutdinov, R., eds., Proceedings of the 36th In-\\nternational Conference on Machine Learning, ICML 2019,\\n9-15 June 2019, Long Beach, California, USA, volume 97\\nof Proceedings of Machine Learning Research, 6105–6114.\\nPMLR.\\nTanay, T.; and Grifﬁn, L. D. 2016.\\nA Boundary Tilting\\nPersepective on the Phenomenon of Adversarial Examples.\\nCoRR, abs/1608.07690.\\nTram`er, F.; Papernot, N.; Goodfellow, I.; Boneh, D.; and Mc-\\nDaniel, P. 2017. The Space of Transferable Adversarial Ex-\\namples. arXiv e-prints, arXiv:1704.03453.\\nTsipras, D.; Santurkar, S.; Engstrom, L.; Turner, A.; and\\nMadry, A. 2019. Robustness May Be at Odds with Accu-\\nracy. In 7th International Conference on Learning Repre-\\nsentations, ICLR 2019, New Orleans, LA, USA, May 6-9,\\n2019. OpenReview.net.\\nWang, Q.; Guo, W.; Zhang, K.; Ororbia, A. G.; and Giles,\\nC. L. 2017. Adversary Resistant Deep Neural Networks with\\nan Application to Malware Detection.\\nWang, Y.; Zou, D.; Yi, J.; Bailey, J.; Ma, X.; and Gu, Q.\\n2020.\\nImproving adversarial robustness requires revisit-\\ning misclassiﬁed examples. In International Conference on\\nLearning Representations.\\nXiao, C.; Li, B.; Zhu, J.-Y.; He, W.; Liu, M.; and Song, D.\\n2018. Generating Adversarial Examples with Adversarial\\nNetworks. arXiv e-prints, arXiv:1801.02610.\\nXie, C.; Wu, Y.; van der Maaten, L.; Yuille, A. L.; and He, K.\\n2019. Feature Denoising for Improving Adversarial Robust-\\nness. In IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-\\n20, 2019, 501–509.\\nXie, C.; and Yuille, A. L. 2020.\\nIntriguing Properties of\\nAdversarial Training at Scale.\\nIn 8th International Con-\\nference on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\\nXu, W.; Evans, D.; and Qi, Y. 2018. Feature Squeezing: De-\\ntecting Adversarial Examples in Deep Neural Networks. In\\n25th Annual Network and Distributed System Security Sym-\\nposium, NDSS 2018, San Diego, California, USA, February\\n18-21, 2018.\\nYuan, X.; He, P.; Zhu, Q.; and Li, X. 2019.\\nAdversarial\\nExamples: Attacks and Defenses for Deep Learning. IEEE\\nTrans. Neural Netw. Learning Syst., 30(9): 2805–2824.\\nZhang, H.; Yu, Y.; Jiao, J.; Xing, E. P.; Ghaoui, L. E.; and\\nJordan, M. I. 2019. Theoretically Principled Trade-off be-\\ntween Robustness and Accuracy.\\nIn Chaudhuri, K.; and\\nSalakhutdinov, R., eds., Proceedings of the 36th Interna-\\ntional Conference on Machine Learning, ICML 2019, 9-\\n15 June 2019, Long Beach, California, USA, volume 97\\nof Proceedings of Machine Learning Research, 7472–7482.\\nPMLR.\\n',\n",
       " '2106.00120v3.pdf': 'Probabilistic Deep Learning with Probabilistic Neural Networks and Deep \\nProbabilistic Models  \\n \\nDaniel T. Chang (张遵) \\n \\nIBM (Retired) dtchang43@gmail.com \\nAbstract:  Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data \\nuncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to \\nprobabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural \\nnetworks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that \\nincorporate deep neural network components which capture complex non-linear stochastic relationships between the random \\nvariables. We discuss some major examples of each approach including Bayesian neural networks and mixture density \\nnetworks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects \\nmodels (for deep probabilistic models). TensorFlow Probability is a library for probabilistic modeling and inference which \\ncan be used for both approaches of probabilistic deep learning. We include its code examples for illustration.  \\n1 Introduction \\nStandard deep learning is deterministic and does not account for uncertainty, either model uncertainty or data uncertainty \\n[1]. This is a major limitation, particularly for mission-critical and real-world applications such as biomedical applications. \\nProbabilistic deep learning removes this limitation by accounting for uncertainty, both model uncertainty and data \\nuncertainty. \\nProbabilistic deep learning is based on the use of probabilistic models and deep neural networks. We distinguish two \\napproaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models, due to their \\ndifference in focus. The former employs deep neural networks that utilize probabilistic layers [7] which can represent and \\nprocess uncertainty; the latter uses probabilistic models that incorporate deep neural network components [10] which capture \\ncomplex non-linear stochastic relationships between the random variables. The distinction, however, may not always be clear \\ncut: A probabilistic neural network may be used to realize a deep probabilistic model, and a deep probabilistic model may \\nincorporate probabilistic neural network components. \\nFor probabilistic neural networks, we discuss two major examples: Bayesian neural networks and mixture density \\nnetworks. Bayesian neural networks [8] utilize probabilistic layers that capture uncertainty over weights and activations, and \\nare trained using Bayesian inference. Mixture density networks [9] are designed to quantify neural network prediction \\nuncertainty. They consist of two components: a mixture model and a deep neural network. \\n2 \\n \\nFor deep probabilistic models, we discuss three major examples: variational autoencoders, deep Gaussian processes, and \\ndeep mixed effects models. Variational autoencoders [1] are prescribed deep generative models realized using an \\nautoencoder neural network that consists of an encoder network and a decoder network, which encodes a data sample to a \\nlatent representation and generates data samples from the latent space, respectively. Deep Gaussian processes [11] are multi-\\nlayer generalizations of Gaussian processes [2], which capture the compositional nature of (deterministic) deep learning \\nwhile mitigating some of the disadvantages through a Bayesian approach. Deep mixed effects models [12] are extensions to \\nlinear mixed effects models that use nonlinear mappings between the response variable and predictor variables, which are \\nrealized using deep neural networks. \\nTensorFlow Probability  [4-6] is a library for probabilistic modeling and inference which, with Keras / TensorFlow, can \\nbe used for building both probabilistic neural networks and deep probabilistic models. We include its code examples, where \\napplicable, for illustration. \\n2 Background Information \\nAs background information, we briefly discuss uncertainty, probabilistic models, and Bayesian inference, which are the \\nprobabilistic foundation of probabilistic deep learning. We also briefly summarize TensorFlow Probability. The discussions \\non uncertainty and probabilistic models are extracted from [1]. Please see it for references and further discussions. \\n2.1 Uncertainty \\nUncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and \\npossibly inherent non-determinism (e.g., quantum phenomena). From the perspective of deep learning, there are two types of \\nuncertainty: model uncertainty and data uncertainty. Model uncertainty accounts for uncertainty in the model structure and \\nmodel parameters. This is important to consider for mission-critical applications and small datasets. Data uncertainty \\naccounts for out-of-distribution data and noisy data. This is important to consider for real-world applications and large \\ndatasets.  \\nUncertainty, therefore, should be an inescapable aspect of deep learning. Because of this, we need to allow our learning \\nsystem to represent uncertainty and our reasoning system to consider different possibilities due to uncertainty. Further, to \\nobtain meaningful conclusions, we need to reason not just about what is possible, but also about what is probable. Probability \\n3 \\n \\ntheory provides us with a formal framework for representing uncertainty and for considering multiple possible outcomes and \\ntheir likelihood. \\n2.2 Probabilistic Models \\nThe probabilistic approach to modeling uses probability theory to represent all forms of uncertainty and for inference: \\n- Probability distributions are used to represent all uncertain elements in a model (including structural, parametric, out-\\nof-distribution and noise-related) and how they relate to the data. \\n- The basic rules of probability theory are used to infer the uncertain elements given the observed data. \\n- Learning from data occurs through the transformation of the prior distributions (defined before observing the data) into \\nposterior distributions (obtained after observing the data). The application of probability theory to learning from data is \\ncalled Bayesian learning or Bayesian inference. \\nSimple probability distributions over a single or a few random variables can be composed to form the building blocks of \\nlarger, more complex models. The compositionality of probabilistic models means that the behavior of these building blocks \\nin the context of the larger model is often much easier to understand. The dominant paradigm for representing such \\ncompositional probabilistic models is probabilistic graphical models. \\nProbabilistic Graphical Models \\nProbabilistic graphical models (PGMs) are structured probabilistic models. A PGM is a way of describing a probability \\ndistribution of random variables, using a graph to describe which variables in the probability distribution directly interact \\nwith each other, with each node representing a variable and each edge representing a direct interaction. This allows the \\nmodels to have significantly fewer parameters which can in turn be estimated reliably from less data. These smaller models \\nalso have dramatically reduced computational cost in terms of storing the model, performing inference in the model, and \\ndrawing samples from the model. \\nUsually a PGM comes with both a graphical representation of the model and a generative process to depict how the \\nrandom variables are generated. Due to its Bayesian nature, a PGM is easy to extend to incorporate other information or to \\n4 \\n \\nperform other tasks. There are essentially two types of PGM: directed PGM (also known as Bayesian network) and \\nundirected PGM (also known as Markov random field). \\nA good PGM needs to accurately capture the distribution over the observed variables x, p(x). Often the different \\nelements of x are highly dependent on each other. The approach most commonly used to model these dependencies is to \\nintroduce several latent variables z. The model can then capture dependencies between any pair of variables xi and xj \\nindirectly, via direct dependencies between xi and z, and direct dependencies between z and xj. Latent variables have \\nadvantages beyond their role in efficiently capturing p(x). The latent variables z also provides an alternative representation \\nfor x. Many approaches accomplish representation learning by learning latent variables. \\n2.3 Bayesian Inference \\nThe main idea of Bayesian inference [3] is to infer a posterior distribution over the parameters θ of a probabilistic model \\ngiven some observed data D using Bayes theorem as: \\np(θ | D) = p(D | θ)p(θ) / p(D) = p(D | θ)p(θ) / ∫ p(D | θ)p(θ)dθ,  \\nwhere p(D | θ) is the likelihood, p(D) is the marginal likelihood (or evidence), and p(θ) is the prior. Computing the posterior, \\nand moreover sampling from it, is usually intractable, especially since computing the evidence (integrals) is hard. \\nApproximate inference methods are usually used, including: \\n1. Markov Chain Monte Carlo: Approximates integrals via sampling. \\n2. Variational Inference: Approximates integrals via optimization. \\nThe Bayesian posterior can be used to model new unseen data D∗ using the posterior predictive: \\np(D* | D) = ∫ p(D* | θ)p(θ | D)dθ,  \\nwhich is also called the Bayesian model average because it averages the predictions of all plausible models weighted by their \\nposterior probability.  \\nThe choice of prior [3], p(θ), is one of the most critical parts of the Bayesian inference. It should be chosen in a way such \\nthat it accurately reflects our beliefs about the parameters θ before seeing any data. There is no universally preferred prior, \\nbut each probabilistic model / inference task is potentially endowed with its own optimal prior. \\n5 \\n \\n2.4 TensorFlow Probability \\nTensorFlow Probability (TFP) [4-6] is a library for probabilistic modeling and inference in TensorFlow. It provides \\nintegration of probabilistic models with deep neural networks, gradient-based inference via automatic differentiation, and \\nscalability to large datasets and models via hardware acceleration (e.g., GPUs) and distributed computation. \\nTFP is structured as following layers, from bottom to top: \\n1. TensorFlow: Numerical operations. \\n2. Probabilistic building blocks \\na. \\nDistributions (tfp.distributions): A large collection of probability distributions which provide fast, \\nnumerically stable methods for generating samples and computing statistics. \\nb. Bijectors (tfp.bijectors): Reversible and composable transformations of distributions, with automatic \\ncaching, preserving the ability to generate samples and compute statistics. \\n3. Probabilistic model / neural network building \\na. \\nJoint Distributions (tfp.distributions.JointDistributionSequential and others): Joint distributions over one or \\nmore possibly-interdependent distributions, designed for building small- to medium-size PGMs. \\nb. Probabilistic Layers (tfp.layers): Neural network layers with uncertainty. \\n4. Bayesian inference \\na. \\nMarkov Chain Monte Carlo (tfp.mcmc): Algorithms for approximating integrals via sampling. \\nb. Variational Inference (tfp.vi): Algorithms for approximating integrals via optimization. \\nc. \\nOptimizers (tfp.optimizer): Stochastic optimization methods. \\nd. Monte Carlo (tfp.monte_carlo): Tools for computing Monte Carlo expectations. \\nTFP can be used for building both probabilistic neural networks and deep probabilistic models, as can be seen in \\nsubsequent discussions. \\n3 Probabilistic Neural Networks \\nProbabilistic neural networks are deep neural networks that utilize probabilistic layers which can represent and process \\nuncertainty. Probabilistic layers [7] can capture uncertainty over weights (Bayesian neural networks), pre-activation units \\n(dropout), activations (“probabilistic output layers”), or the function itself (Gaussian processes). They can also be layers that \\n6 \\n \\npropagate uncertainty from input to output. Probabilistic layers are designed to be drop-in replacement of their deterministic \\ncounter parts. \\n3.1 Bayesian Neural Networks \\nBayesian neural networks (BNNs) [8] utilize probabilistic layers that capture uncertainty over weights and activations, \\nand are trained using Bayesian inference. At a high level, they can be represented as follow: \\nθ ∼ p(θ), \\ny = BNNθ(x) + ϵ, \\nwhere θ represents BNN parameters and ϵ represents random noise. \\nTo design a BNN, the first step is to choose a neural network architecture, e.g., a convolutional neural network. Then, \\none has to choose a probability model: a prior distribution over the possible model parameters p(θ) and a prior confidence in \\nthe predictive power of the model p(y | x, θ), i.e., BNNθ(x). For supervised learning, the Bayesian posterior can be written as: \\np(θ | D) = p(Dy | Dx, θ)p(θ) / ∫ p(Dy | Dx, θ)p(θ)dθ, \\nwhere D is the training set, Dx the training features, and Dy the training labels. Computing the posterior, and moreover \\nsampling from it, is usually intractable, especially since computing the evidence (denominator) is hard. In general one uses a \\nvariational inference approach, which learns a variational distribution qϕ(θ) to approximate the exact posterior. \\nGiven the Bayesian posterior, or in practice its variational approximation, the posterior predictive can be computed as: \\np(y | x, D) = ∫ p(y | x, θ)p(θ | D)dθ. \\nIn practice, the distribution p(y | x, θ) is sampled indirectly using BNNθ(x), and θ is sampled from the variational distribution \\nqϕ(θ). \\nThe following shows an example code of a convolutional BNN model in TensorFlow Probability [4]. It defines a LeNet-\\n5 model using three convolutional (with max pooling) layers and two fully connected dense layers. It uses the Flipout Monte \\nCarlo estimator for these layers. KL divergence, using Lambda function, is passed as input to the kernel_divergence_fn on \\n7 \\n \\nflipout layers. The Keras API will automatically add the KL divergence to the cross entropy loss, effectively calculating the \\n(negated) Evidence Lower Bound Loss (ELBO). \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n3.2 Mixture Density Networks \\nMixture density networks (MDNs) [9] are designed to quantify neural network prediction uncertainty. MDNs consist of \\ntwo components: a mixture model and a deep neural network. The mixture model can be written as: \\np(y | x) = ∑\\np(y; фk(x))p(фk(x); π(x))\\n\\u0bc4\\n\\u0bdeୀଵ\\n. \\nThe deep neural network can be of any valid architecture that maps x onto both the parameters {фk(x)}k=1\\nK of the K mixture \\ncomponents and the parameters π(x) of the mixing distribution. MDNs can be trained by maximizing the log-likelihood of \\nthe parameters of the deep neural network given the training set D = {xn, yn} n=1\\nN using gradient-based optimizers. \\n   \\n  kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /        \\n                            tf.cast(NUM_TRAIN_EXAMPLES, dtype=tf.float32)) \\n \\n  model = tf.keras.models.Sequential([ \\n      tfp.layers.Convolution2DFlipout( \\n          6, kernel_size=5, padding=\\'SAME\\', \\n          kernel_divergence_fn=kl_divergence_function, \\n          activation=tf.nn.relu), \\n      tf.keras.layers.MaxPooling2D( \\n          pool_size=[2, 2], strides=[2, 2], \\n          padding=\\'SAME\\'), \\n      tfp.layers.Convolution2DFlipout( \\n          16, kernel_size=5, padding=\\'SAME\\', \\n          kernel_divergence_fn=kl_divergence_function, \\n          activation=tf.nn.relu), \\n      tf.keras.layers.MaxPooling2D( \\n          pool_size=[2, 2], strides=[2, 2], \\n          padding=\\'SAME\\'), \\n      tfp.layers.Convolution2DFlipout( \\n          120, kernel_size=5, padding=\\'SAME\\', \\n          kernel_divergence_fn=kl_divergence_function, \\n          activation=tf.nn.relu), \\n      tf.keras.layers.Flatten(), \\n      tfp.layers.DenseFlipout( \\n          84, kernel_divergence_fn=kl_divergence_function, \\n          activation=tf.nn.relu), \\n      tfp.layers.DenseFlipout( \\n          NUM_CLASSES, kernel_divergence_fn=kl_divergence_function, \\n          activation=tf.nn.softmax) \\n  ]) \\n8 \\n \\nThe following shows an example code of a MDN model in TensorFlow Probability [4]. It utilizes a mixture distribution \\nlayer with independent normal (Gaussian) components. \\n \\n4 Deep Probabilistic Models \\nDeep probabilistic models [10] are probabilistic models that incorporate deep neural network components which capture \\ncomplex non-linear stochastic relationships between the random variables. \\nUnsupervised and semi-supervised learning methods are often based on generative models which are probabilistic \\nmodels that express hypotheses about the way in which data may have been generated. PGMs have emerged as a broadly \\nuseful approach to specifying generative models. Deep generative models (DGMs) [1] are PGMs that employ deep neural \\nnetworks for parameterizing the models. In particular, prescribed DGMs are those that provide an explicit parametric \\nspecification of the probability distribution of the observed variable x, specifying the likelihood  function pθ(x) with \\nparameter θ. DGMs are among the most widely used deep probabilistic models. \\n4.1 Variational Autoencoders \\n(The discussion in this subsection is extracted from [1]. Please see it for references and further discussions.) \\nThe variational autoencoder (VAE) is a prescribed DGM realized using an autoencoder neural network that consists of \\nan encoder network and a decoder network, which encodes a data sample to a latent representation and generates data \\nsamples from the latent space, respectively. The decoder network is a differentiable generator network and the encoder \\nnetwork is an auxiliary inference network. Both networks are jointly trained using variational learning. Variational learning \\nuses the variational lower bound of the marginal log-likelihood as the single objective function to optimize both the \\ngenerator network and the auxiliary inference network. \\n   \\nevent_shape = [1] \\nnum_components = 5 \\nparams_size = tfpl.MixtureNormal.params_size(num_components, event_shape) \\n \\nmodel = tfk.Sequential([ \\n  tfkl.Dense(12, activation=\\'relu\\'), \\n  tfkl.Dense(params_size, activation=None), \\n  tfpl.MixtureNormal(num_components, event_shape) \\n]) \\n9 \\n \\nThe likelihood function of a prescribed DGM can be written as: \\npθ(x) ≡ pθ(z) pθ(x | z) \\nIt is usually intractable to directly evaluate and maximize the marginal log-likelihood log(pθ(x)). Following the variational \\ninference approach, one introduces an auxiliary inference model qφ(z | x) with parameters φ, which serves as an \\napproximation to the exact posterior pθ(z | x). \\nThe variational lower bound can be rewritten as: \\n \\nL(x; θ, φ) = ∑qφ(𝐳 | 𝐱)\\n௭\\nlog(pθ(x | z)) – DKL(qφ(z | x) || pθ(z)) \\nThe first term is the expected reconstruction quality, requiring that pθ(x | z) is high for samples of z from qφ(z | x), while the \\nsecond term (the KL divergence between the approximate posterior and the prior) acts as a regularizer, ensuring we can \\ngenerate realistic data by sampling latent variables from pθ(z). \\n \\nVariational learning is to maximize the variational lower bound over the training data. It performs something like the \\nautoencoder, with qφ(z | x) as the encoder and pθ(x | z) as the decoder. As such, the VAE introduces the constraint on the \\nautoencoder that the latent variable z is distributed according to a prior p(z). The encoder qφ(z | x) approximates the posterior \\np(z | x), and the decoder pθ(x | z) parameterizes the likelihood p(x | z). The generation model is then z ~ p(z); x ~ p(x | z). \\nThe following shows an example code of a convolutional VAE in TensorFlow Probability [4]: encoder, decoder and \\nVAE. Note that KLDivergenceRegulizer is used in the encoder. \\n10 \\n \\n \\n \\n   \\ndecoder = tfk.Sequential([ \\n    tfkl.InputLayer(input_shape=[encoded_size]), \\n    tfkl.Reshape([1, 1, encoded_size]), \\n    tfkl.Conv2DTranspose(2 * base_depth, 7, strides=1, \\n                         padding=\\'valid\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=1, \\n                         padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=2, \\n                         padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2DTranspose(base_depth, 5, strides=1, \\n                         padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2DTranspose(base_depth, 5, strides=2, \\n                         padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2DTranspose(base_depth, 5, strides=1, \\n                         padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2D(filters=1, kernel_size=5, strides=1, \\n                padding=\\'same\\', activation=None), \\n    tfkl.Flatten(), \\n    tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits), \\n]) \\n   \\nencoder = tfk.Sequential([ \\n    tfkl.InputLayer(input_shape=input_shape), \\n    tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5), \\n    tfkl.Conv2D(base_depth, 5, strides=1, \\n                padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2D(base_depth, 5, strides=2, \\n                padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2D(2 * base_depth, 5, strides=1, \\n                padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2D(2 * base_depth, 5, strides=2, \\n                padding=\\'same\\', activation=tf.nn.leaky_relu), \\n    tfkl.Conv2D(4 * encoded_size, 7, strides=1, \\n                padding=\\'valid\\', activation=tf.nn.leaky_relu), \\n    tfkl.Flatten(), \\n    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size), \\n               activation=None), \\n    tfpl.MultivariateNormalTriL( \\n        encoded_size, \\n        activity_regularizer=tfpl.KLDivergenceRegularizer(prior)), \\n]) \\n11 \\n \\n \\n4.2 Deep Gaussian Processes \\nGaussian Processes \\n(The discussion in this subsection is extracted from [2]. Please see it for references and further discussions.)  \\nThe Gaussian process (GP) is a well-known non-parametric probabilistic model. A Gaussian process is a generalization \\nof the Gaussian distribution. Whereas a probability distribution describes random variables which are scalars or vectors, a \\nstochastic process governs the properties of functions. We can loosely think of a function as an infinite vector, each entry in \\nthe vector specifying the function value f(xi) at a particular input xi. A key aspect of this is that if we ask only for the \\nproperties of the function at a finite number of points, then GP inference will give us the same answer if we ignore the \\ninfinitely many other points. \\nIn the function-space view a Gaussian process defines a distribution over functions, and inference takes place directly in \\nthe space of functions. A Gaussian process is completely specified by its mean function (average of all functions) and kernel \\n(covariance) function (how much individual functions can vary around the mean function): \\nm(x) = E[f(x)], \\nk(x, x’) = E[(f(x) – m(x))(f(x’) – m(x’))] \\nAs such, we write the Gaussian process as \\nf(x) ∼ GP(m(x), k(x, x’)). \\n \\nUnder the Gaussian process, the true objective is modeled by a GP prior with a mean and a kernel function. Given a set \\nof (noisy) observations from initial evaluations, a Bayesian posterior update gives the GP posterior with an updated mean \\nand kernel function. The mean function of the GP posterior gives the best predictions at any point conditional on the \\n   \\nvae = tfk.Model(inputs=encoder.inputs, \\n                outputs=decoder(encoder.outputs[0])) \\n12 \\n \\navailable observations, and the kernel function quantifies the uncertainty in the predictions. The GP prior is a multivariate \\nGaussian distribution:  \\np(f | θ) ∼ 𝒩(µ, K(θ)). \\nSo is the GP posterior: \\np(f | D, θ) ∼ 𝒩(µ’, K’(θ)). \\nIn the above, θ is GP parameters (parameters of the GP kernel function). \\nDeep Gaussian Processes \\nGaussian processes have the following limitations [11]:   \\n\\uf0b7 \\nThe assumption of Gaussian marginals: A Gaussian process cannot model heavy-tailed, asymmetric or \\nmultimodal marginal distributions. \\n\\uf0b7 \\nThe requirement to define a prior entirely in terms of mean and covariance: Even very complicated covariance \\nfunctions cannot express prior relationships that are not captured by mean and covariance alone. \\n\\uf0b7 \\nThe predictive covariance of the exact posterior process is independent of the observed outputs: It is \\nunsatisfactory that the predictive uncertainty is decoupled from the observations. \\n\\uf0b7 \\nThe predictive mean is linear in the observations: There are many situations where this linearity might be \\ninappropriate. \\nDeep Gaussian processes (DGPs) [11] are multi-layer generalizations of GPs that promise to overcome some of the \\nabove limitations of the single layer model. The DGP presents a paradigm for building deep models from a Bayesian \\nperspective. It captures the compositional nature of (deterministic) deep learning while mitigating some of the disadvantages, \\ne.g., lack of treatment for uncertainty, through a Bayesian approach. \\nIndependent GPs f1, … , fL (at layer l = 1, …, L) can be combined through function composition to construct a DGP [11]. \\nThe composite function g(x) is given by  \\ng(x) = fL(fL−1(…f1(x))), \\n13 \\n \\nwhere the outputs of each layer are the inputs to the next higher layer. Given a likelihood p(y | g(x)), the DGP model has the \\njoint distribution, \\np(y, {fl}l=1\\nL) = ∏i=1\\nN p(yi  | fL(fL−1(…f1(xi)))) ∏l=1\\nL p(fl),  \\nwhere the first part on the right-hand side is likelihood, the second part is prior, and each layer is a GP p(fl) = GP (ml, kl). \\nThe compositional and hierarchical structure of a DGP makes it natural for a DGP to be realized using a deep neural \\nnetwork. The following shows an example code of a DGP in Edward2 [7]. \\n \\n4.3 Deep Mixed Effects Models \\nGrouped data commonly occur in social sciences and medicine. The usual approach when dealing with multiple samples \\nfrom each group is to utilize the mixed effects models [12]. The mixed effects are composed of two parts: the fixed effects and \\nthe random effects. The fixed effects are common in all samples and so their coefficients are called fixed. In contrast, the \\nrandom effects are specific to groups and their coefficients can vary depending on the groups, which are assumed to be drawn \\nfrom some unknown distributions. Using mixed effects is natural whenever data are clustered in groups / hierarchy. \\nLinear Mixed Effects Models \\nA linear mixed effects model [12] can be expressed as: \\nyi = β0 + β1x1 + … + βpxp + ui1z1 + … + uiqzq + ϵi, \\n   \\nbatch_size = 256 \\nfeatures, labels = load_spatial_data(batch_size) \\n \\nmodel = tf.keras.Sequential([ \\n    tf.keras.layers.Flatten(), \\n    layers.SparseGaussianProcess(units=256,num_inducing=512), \\n    layers.SparseGaussianProcess(units=256,num_inducing=512), \\n    layers.SparseGaussianProcess(units=10,num_inducing=512), \\n]) \\n14 \\n \\nwhere β := [β1, ... , βp]T are the fixed effects shared over the entire sample, ui := [ui1 , … ,uip ]T are the random effects of the \\nith group, and z = [z1, ... , zq] is a design vector for random effects. ui and ϵi are drawn from group-specific unknown \\ndistributions. Typically, the unknown distributions are assumed to be a zero-mean Gaussian with an unknown covariance \\nstructure: \\nui ∼ 𝒩(0, Σu) and ϵi ∼ 𝒩(0, Σϵi). \\nSince the linear mixed effects models have multiple random effects from unknown distributions, no closed form solution \\nis available. For estimation, Expectation-Maximization algorithms and Markov Chain Monte Carlo sampling are used. \\nDeep Mixed Effects Models \\nA deep mixed effects model [12] is an extension to linear mixed effects models that uses nonlinear mappings between the \\nresponse variable and predictor variables, which are realized using deep neural networks. Let yi = [y(ij)]j=1\\nni be a set of ni \\nobservations for a response variable from group i, Xi = [x(ij)1, … , x(ij)p]j=1\\nni the corresponding predictor variables for group i, \\nand Zi = [z(ij)1, … , z(ij)q]j=1\\nni the corresponding design matrix for group i. The deep mixed effects model can be written as \\nyi = Γ(Xi)β + Γ(Zi)ui, \\nwhere β are the coefficients for the fixed effects and ui are the coefficients for the random effects of group i. Γ(.) is a nonlinear \\ntransformation that can be realized by a deep neural network of any valid architecture. Recall that \\nui ∼ 𝒩(0, Σu), \\n i.e., ui is drawn from a zero-mean Gaussian with an unknown covariance structure. \\nThe deep mixed effects model thus consists of a fixed effects component, f(x(ij)) = Γ(x(ij))β, and a separate random effects \\ncomponent, f’(x(ij)) = Γ(x(ij))ui. For regression problems, the deep mixed effects model can be written as an optimization \\nproblem by training the deep neural network Γ(.) with the loss function \\n∑|| y(ij) - f(x(ij)) - f’(x(ij))||2\\nij\\n. \\n15 \\n \\n5 Summary and Conclusion \\nProbabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It \\nis based on the use of probabilistic models and deep neural networks. As background information, we discussed the \\nprobabilistic foundation of probabilistic deep learning: uncertainty, probabilistic models, and Bayesian inference. \\nWe distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic \\nmodels. For probabilistic neural networks, we discussed Bayesian neural networks and mixture density networks; for deep \\nprobabilistic models we discussed variational autoencoders, deep Gaussian processes and deep mixed effects models. Where \\napplicable, we included TensoFlow Probability code examples for illustration. \\nWith the availability of libraries for probabilistic modeling and inference, such as TensorFlow Probability, it is hopeful \\nthat probabilistic deep learning will become more widely used in the near future so that uncertainty, both model uncertainty \\nand data uncertainty, in deep learning will routinely be accounted for and quantified. \\nAcknowledgement: Thanks to my wife Hedy (期芳) for her support. \\nReferences \\n \\n[1] Daniel T. Chang, “Concept-Oriented Deep Learning: Generative Concept Representations,” arXiv preprint \\narXiv:1811.06622 (2018). \\n[2] Daniel T. Chang, “Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax,” arXiv preprint arXiv: \\n1912.05686 (2019). \\n[3] Vincent Fortuin, “Priors in Bayesian Deep Learning: A Review,” arXiv preprint arXiv: 2105.06868 (2021). \\n[4] TensorFlow Probability (https://www.tensorflow.org/probability) \\n[5] Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex \\nAlemi, Matt Hoffman, and Rif A. Saurous, “TensorFlow Distributions,“ arXiv preprint arXiv:1711.10604 (2017). \\n[6] Dan Piponi, Dave Moore, and Joshua V. Dillon, “Joint Distributions for TensorFlow Probability,” arXiv preprint arXiv: \\n2001.11819 (2020). \\n[7] Dustin Tran, Michael W. Dusenberry, Mark van der Wilk, and Danijar Hafner, “Bayesian Layers: A Module for Neural \\nNetwork Uncertainty,” arXiv preprint arXiv:1812.03973 (2019).  \\n[8] Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Bennamoun, \\n“Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users”, ACM Comput. Surv. 1, 1 (July 2020). \\n[9] Agustinus Kristiadi, Sina Daubener, and Asja Fischer, “Predictive Uncertainty Quantification with Compound Density \\nNetworks,” arXiv preprint arXiv:1902.01080 (2019). \\n[10] Andres R. Masegosa, Rafael Cabanas, Helge Langseth, Thomas D. Nielsen, and Antonio Salmeron, “Probabilistic \\nModels with Deep Neural Networks,” arXiv preprint arXiv:1908.03442 (2019). \\n[11] Hugh Salimbeni, “Deep Gaussian Processes: Advances in Models and Inference,” Ph.D. Thesis, Imperial College \\nLondon (June 7, 2020). \\n[12] Y. Xiong, H. J. Kim and V. Singh, \"Mixed Effects Neural Networks (MeNets) With Applications to Gaze Estimation,\" \\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 7743-7752 (2019). \\n',\n",
       " '2108.01468v1.pdf': 'Quantum Neural Networks: Concepts, Applications,\\nand Challenges\\n◦Yunseok Kwak, ◦Won Joon Yun, ◦Soyi Jung, and ◦Joongheon Kim\\n◦Department of Electrical and Computer Engineering, Korea University, Seoul 02841, Republic of Korea\\nE-mails: yskwak6937@gmail.com, ywjoon95@korea.ac.kr,\\njungsoyi@korea.ac.kr, joongheon@korea.ac.kr\\nAbstract—Quantum deep learning is a research ﬁeld for the\\nuse of quantum computing techniques for training deep neural\\nnetworks. The research topics and directions of deep learning and\\nquantum computing have been separated for long time, however\\nby discovering that quantum circuits can act like artiﬁcial neural\\nnetworks, quantum deep learning research is widely adopted.\\nThis paper explains the backgrounds and basic principles of\\nquantum deep learning and also introduces major achievements.\\nAfter that, this paper discusses the challenges of quantum deep\\nlearning research in multiple perspectives. Lastly, this paper\\npresents various future research directions and application ﬁelds\\nof quantum deep learning.\\nI. INTRODUCTION\\nAs quantum computing and deep learning have recently\\nbegun to draw attentions, notable research achievements have\\nbeen pouring over past decades. In the ﬁeld of deep learn-\\ning, the problems which were considered as their inherent\\nlimitations like gradient vanishing, local minimum, learning\\ninefﬁciencies in large-scale parameter training are gradually\\nbeing conquered [1]. On the one hand, innovative new deep\\nlearning algorithms such as quantum neural network (QNN),\\nconvolutional neural network (CNN), and recurrent neural\\nnetwork (RNN) are completely changing the way various\\nkinds of data are processed. Meanwhile, the ﬁeld of quantum\\ncomputing has also undergone rapid developments in recent\\nyears. Quantum computing, which has been recognized only\\nfor its potential for a long time, has opened up a new era\\nof enormous potentials with the recent advances of variational\\nquantum circuits (VQC). The surprising potentials of the varia-\\ntional quantum algorithms were made clear by solving various\\ncombinatorial optimization problems and the intrinsic energy\\nproblems of molecules, which were difﬁcult to solve using\\nconventional methods, and further extensions are considered\\nto design machine learning algorithms using quantum comput-\\ning. Among them, quantum deep learning ﬁelds are growing\\nrapidly, inheriting the achievements of existing deep learning\\nresearch. Accordingly, numerous notable achievements related\\nto quantum deep learning have been published, and active\\nfollow-up studies are being conducted at this time. In this pa-\\nper, we ﬁrst brieﬂy introduce the background knowledge, basic\\nprinciples of quantum deep learning, and look at the current\\nresearch directions. We then discuss the various directions and\\nchallenges of future research in quantum deep learning.\\nA. Quantum Computing\\nQuantum computers use qubits as the basic units of com-\\nputation, which represent a superposition state between |0⟩\\nand |1⟩[2]–[5]. A single qubit state can be represented as a\\nnormalized two-dimensional complex vector, i.e.,\\n|ψ⟩= α|0⟩+ β|1⟩, ∥α∥2 + ∥β∥2 = 1\\n(1)\\nand ∥α∥2 and ∥β∥2 are the probabilities of observing |0⟩and\\n|1⟩from the qubit, respectively. This can be also geometrically\\nrepresented using polar coordinates θ and φ,\\n|ψ⟩= cos(θ/2)|0⟩+ eiφ sin(θ/2)|1⟩,\\n(2)\\nwhere 0 ≤θ ≤π and 0 ≤φ ≤π. This representation maps a\\nsingle qubit state into the surface of 3-dimensional unit sphere,\\nwhich is called Bloch sphere. A multi qubit system can be\\nrepresented as the tensor product of n single qubits, which\\nexists as a superposition of 2n basis states from |00...00⟩\\nto |11...11⟩. Quantum entanglement appears as a correlation\\nbetween different qubits in this system. For example, in a 2-\\nqubit system\\n1\\n√\\n2|00⟩+ 1\\n√\\n2|11⟩, the observation of the ﬁrst qubit\\ndirectly determines that of the second qubit. Those systems are\\ncontrolled by quantum gates in a quantum circuit to perform\\na quantum computation on its purpose [6], [7].\\nQuantum gates are unitary operators mapping a qubit system\\ninto another one, and as classical computing, it is known that\\nevery quantum gate can be factorized into the combination\\nof several basic operators like rotation operator gates and CX\\ngate [8]. Rotation operator gates Rx(θ), Ry(θ), Rz(θ) rotates\\na qubit state in Bloch sphere around corresponding axis by θ\\nand CX gate entangles two qubits by ﬂipping a qubit state if\\nthe other is |1⟩. Those quantum gates utilizes quantum super-\\nposition and entanglement to take an advantage over classical\\ncomputing, and it is well known that quantum algorithms\\ncan obtain an exponential computational gain over existing\\nalgorithms in certain tasks such as prime factorization [9].\\nII. QUANTUM DEEP LEARNING\\nA. Variational Quantum Circuits (VQC)\\nA variational quantum circuit (VQC) is a quantum circuit\\nusing rotation operator gates with free parameters to perform\\nvarious numerical tasks, such as approximation, optimization,\\nclassiﬁcation. An algorithm using a variational quantum circuit\\nis called variational quantum algorithm (VQA), which is\\narXiv:2108.01468v1  [quant-ph]  2 Aug 2021\\na classical-quantum hybrid algorithm because its parameter\\noptimization is often performed by a classical computer.\\nSince its universal function approximating property [10], many\\nalgorithms using VQC [11] are designed to solve various\\nnumerical problems [2], [8], [12]–[14]. This ﬂow led to many\\napplications of VQA in machine learning and is also for re-\\nplacing the artiﬁcial neural network of the existing model with\\nVQC [15]–[18]. VQC is similar to artiﬁcial neural networks in\\nthat it approximates functions through parameter learning, but\\nhas differences due to the several characteristics of quantum\\ncomputing. Since all quantum gate operations are reversible\\nlinear operations, quantum circuits use entanglement layers\\ninstead of activation functions to have multilayer structures.\\nThese VQCs are called quantum neural networks, and this\\npaper will look at them through classiﬁcation according to\\ntheir structure and characteristics.\\nB. Quantum Neural Networks\\nFig. 1. Illustration of QNN with the input |ψ⟩, the parameter θ and linear\\nentanglement structure.\\nIn this section, we try to demonstrate how a basic quantum\\nneural network(QNN) works with a simple example described\\nin the Fig. 1. The way a QNN processes data is as follows.\\nFirst, the input data is encoded into the corresponding qubit\\nstate of an appropriate number of qubits. Then, the qubit state\\nis transformed through the parameterized rotation gates and\\nentangling gates for a given number of layers. The transformed\\nqubit state is then measured by obtaining expected value of a\\nhamiltonian operator, such as Pauli gates. These measurements\\nare decoded back into the form of appropriate output data.\\nThe parameters are then updated by an optimizer like Adam\\noptimizer. A neural network constructed in the form of VQC\\ncan perform various roles in various forms, which will be\\nexplored as quantum neural networks.\\nFig. 2. Illustration of QCNN with the input |ψ⟩, the parameter θ with single\\nconvolution and pooling layer.\\n1) Quantum Convolutional Neural Networks:\\nQuantum\\nconvolutional neural network (QCNN) was proposed in [16],\\nimplementing the convolution layer and pooling layer on the\\nquantum circuits. According to the previous research results in\\n[5], [19], the QCNN circuit computation proceeds as follows.\\nThe ﬁrst step is same as any other QNN models, encoding\\ninput data into a qubit state with rotation operator gates. Then\\nthe convolution layer with quasi-local unitary gates ﬁlters\\nthe input data into a feature map. The pooling layer with\\ncontrolled rotation operators then downsizes the feature map.\\nBy repeating this process sufﬁciently, the fully connected layer\\nacts on the qubit state as classical CNN models. Finally, the\\nmeasurement of the qubit state is decoded into an output data\\nwith desired sizes. The circuit parameters are updated with\\ngradient descent based optimizer after each measurements.\\nUnfortunaltely, in the current quantum computing envi-\\nronment [20], QCNN is difﬁcult to perform better than the\\nexisting classical CNN. However, it is expected that the QCNN\\nwill be able to obtain sufﬁcient computational gains over\\nthe classical ones in future quantum computing environment\\nwhere larger-size quantum calculations are possible [5], [16].\\nIII. FUTURE WORK DIRECTIONS AND CHALLENGES\\nA. Applications of Quantum Deep Learning to Reinforcement\\nLearning\\nThere are many research results applying deep learning to\\nreinforcement learning to derive optimal actions from a com-\\nplex state space [21]–[24]. However, reinforcement learning\\nresearch using quantum deep learning [18], [25], [26] is still\\nin its infancy. The current approach step is to replace the policy\\ntraining network with a quantum neural network from the\\nexisting deep neural network, but there remains the possibility\\nof many algorithms applying various ideas of classical deep\\nreinforcement learning researches. In particular, if it is proved\\nthat quantum computational gains can be obtained through\\nQNN in a situation of high computational complexity due to\\nthe complex Markov decision process environment, quantum\\nreinforcement learning will open a new horizon for reinforce-\\nment learning research.\\nB. Applications of Quantum Deep Learning to Communica-\\ntion Networks\\nThe QNN and quantum reinforcement learning algorithms\\ncan be used in various research ﬁelds, and this paper considers\\nthe applications in terms of communications and networks. In\\nterms of the acceleration of computation in fully distributed\\nplatforms, e.g., blockchain [27], [28], QNN can be used.\\nIn addition, various advanced communication technologies\\nsuch as Internet of Things (IoT) [29], [30], millimeter-wave\\nnetworks [31], [32], caching networks [33]–[36], and video\\nstreaming/scheduling [37]–[40] are good applications of QNN\\nand quantum reinforcement learning algorithms.\\nC. Challenges\\n1) Gradient Vanishing: Vanishing gradient is a crucial\\nproblem in quantum deep learning as of classical deep learn-\\ning. The problem of gradient disappearance while backprop-\\nagating many hidden layers has been considered a chronic\\nproblem in deep neural network computation. Since quantum\\nneural networks also use gradient descent method training\\ntheir parameters as classical ones, they have to solve the same\\nproblem. Classical deep learning models solve this problem by\\nutilizing an appropriate activation function, but quantum deep\\nlearning does not use an activation function, thus eventually, a\\ndifferent solution is needed. A former research [41] called this\\nquantum gradient vanishing pheonomena as barren plateaus,\\nwhile proving that when the number of qubits increases, the\\nprobability of occurring barren plateaus increases exponen-\\ntially. This can be avoided by setting good initial parameters\\nin small-scale QNN, but it is unavoidable to deal with this\\nproblem when designing large-scale QNN. This is an open\\nproblem for which a solution is not yet clear.\\n2) Near-Term Device Compatibility: Noisy intermediate\\nscale quantum (NISQ) [20], which means fewer qubits and a\\nlot of computational error of near-term quantum devices, has\\nalready become a familiar term to quantum researchers. Many\\nalgorithms designed to implement quantum computational\\ngains do not work at all in this NISQ environment, and are\\nexpected to be implemented at least several decades later. For\\nexample, a practical implementation of the Shor’s algorithm\\nrequires at least thousand of qubits even without an error\\ncorrection processes, current quantum devices have only a few\\ntens of qubits with non-negligible computational error rate of\\nseveral percent. However, due to the relatively small circuit\\ndepth and qubit requirements, VQA and QNN based on them\\nare tolerant to these environmental constraints. Nevertheless,\\nin order to increase the data processing capability of quantum\\nneural network, it is necessary to consider near-term device\\ncompatibility. For example, using many multi-qubit controlling\\ngates for quantum entanglement is theoretically thought to\\nincrease the performance of QNN, but it entails a large error\\nrate and a complicated error correction process. Therefore, it\\nis essential to design an algorithm regarding these tradeoffs in\\nquantum deep learning research.\\n3) The Quantum Advantage: The term quantum supremacy\\nmay lead to the illusion that quantum algorithms are always\\nbetter than classical algorithms performing the same function.\\nHowever, given the inherent limitations of quantum comput-\\ning, quantum computing beneﬁts can only be realized through\\nwell-thought-out algorithms under certain circumstances. In\\nfact, especially among variational quantum-based algorithms,\\nonly a few of them have proven their quantum advantage in a\\nlimited situation.\\nDue to the universal approximation property of QNN, it\\nis known that quantum deep learning can perform most of\\nthe computations performed in classical deep learning [10].\\nNevertheless, if one approaches simply based on this fact\\nwithout the consideration of quantum gain, the result may be\\nmuch inefﬁcient compared to the existing classical algorithm.\\nTherefore, in designing a new QNN-based deep learning algo-\\nrithm, it is necessary to justify it by articulating its advantages\\nover the corresponding classical models.\\nIV. CONCLUSION\\nThis paper introduces the basic concepts of quantum neural\\nnetworks and their applications scenarios in various ﬁelds. Fur-\\nthermore, this paper presents research challenges and potential\\nsolutions of quantum neural network computation.\\nACKNOWLEDGMENT\\nThis work was supported by the National Research Foun-\\ndation of Korea (2019M3E4A1080391). Joongheon Kim is a\\ncorresponding author of this paper.\\nREFERENCES\\n[1] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis, S.-L. Kim,\\nand M. Debbah, “Communication-efﬁcient and distributed learning over\\nwireless networks: Principles and applications,” Proceedings of the\\nIEEE, vol. 109, no. 5, pp. 796–819, May 2021.\\n[2] J. Choi, S. Oh, and J. Kim, “Quantum approximation for wireless\\nscheduling,” Applied Sciences, vol. 10, no. 20, 2020.\\n[3] J. Choi and J. Kim, “A tutorial on quantum approximate optimization\\nalgorithm (QAOA): Fundamentals and applications,” in Proceedings of\\nthe IEEE International Conference on Information and Communication\\nTechnology Convergence (ICTC), 2019, pp. 138–142.\\n[4] J. Choi, S. Oh, and J. Kim, “The useful quantum computing techniques\\nfor artiﬁcial intelligence engineers,” in Proceedings of the IEEE Interna-\\ntional Conference on Information Networking (ICOIN), 2020, pp. 1–3.\\n[5] S. Oh, J. Choi, and J. Kim, “A tutorial on quantum convolutional neural\\nnetworks (QCNN),” in Proceedings of the IEEE International Con-\\nference on Information and Communication Technology Convergence\\n(ICTC), 2020, pp. 236–239.\\n[6] J. Choi, S. Oh, and J. Kim, “A tutorial on quantum graph recurrent\\nneural network (QGRNN),” in Proceedings of the IEEE International\\nConference on Information Networking (ICOIN), 2021, pp. 46–49.\\n[7] S. Oh, J. Choi, J.-K. Kim, and J. Kim, “Quantum convolutional\\nneural network for resource-efﬁcient image classiﬁcation: A quantum\\nrandom access memory (QRAM) approach,” in Proceedings of the IEEE\\nInternational Conference on Information Networking (ICOIN), 2021, pp.\\n50–52.\\n[8] J. Choi, S. Oh, and J. Kim, “Energy-efﬁcient cluster head selection via\\nquantum approximate optimization,” Electronics, vol. 9, no. 10, 2020.\\n[9] P. W. Shor, “Polynomial-time algorithms for prime factorization and\\ndiscrete logarithms on a quantum computer,” SIAM review, vol. 41, no. 2,\\npp. 303–332, 1999.\\n[10] J. Biamonte, “Universal variational quantum computation,” Physical\\nReview A, vol. 103, no. 3, p. L030401, 2021.\\n[11] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii,\\nJ. R. McClean, K. Mitarai, X. Yuan, L. Cincio et al., “Variational\\nquantum algorithms,” arXiv preprint arXiv:2012.09265, 2020.\\n[12] E. Farhi, J. Goldstone, and S. Gutmann, “A quantum approximate\\noptimization algorithm,” arXiv preprint arXiv:1411.4028, 2014.\\n[13] A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink, J. M.\\nChow, and J. M. Gambetta, “Hardware-efﬁcient variational quantum\\neigensolver for small molecules and quantum magnets,” Nature, vol.\\n549, no. 7671, pp. 242–246, 2017.\\n[14] J. Kim, Y. Kwak, S. Jung, and J.-H. Kim, “Quantum scheduling\\nfor millimeter-wave observation satellite constellation,” in Proceedings\\nof the IEEE VTS Asia Paciﬁc Wireless Communications Symposium\\n(APWCS), 2021, pp. 1–1.\\n[15] M. Schuld and N. Killoran, “Quantum machine learning in feature\\nhilbert spaces,” Physical review letters, vol. 122, no. 4, p. 040504, 2019.\\n[16] I. Cong, S. Choi, and M. D. Lukin, “Quantum convolutional neural\\nnetworks,” Nature Physics, vol. 15, no. 12, pp. 1273–1278, 2019.\\n[17] J. Bausch, “Recurrent quantum neural networks,” Advances in Neural\\nInformation Processing Systems, vol. 33, 2020.\\n[18] D. Dong, C. Chen, H. Li, and T.-J. Tarn, “Quantum reinforcement\\nlearning,” IEEE Transactions on Systems, Man, and Cybernetics, Part\\nB (Cybernetics), vol. 38, no. 5, pp. 1207–1220, 2008.\\n[19] S. Garg and G. Ramakrishnan, “Advances in quantum deep learning:\\nAn overview,” arXiv preprint arXiv:2005.04316, 2020.\\n[20] J. Preskill, “Quantum computing in the nisq era and beyond,” Quantum,\\nvol. 2, p. 79, 2018.\\n[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\\ning,” arXiv preprint arXiv:1312.5602, 2013.\\n[22] M. Choi, A. No, M. Ji, and J. Kim, “Markov decision policies for dy-\\nnamic video delivery in wireless caching networks,” IEEE Transactions\\non Wireless Communications, vol. 18, no. 12, pp. 5705–5718, December\\n2019.\\n[23] M. Shin, J. Kim, and M. Levorato, “Auction-based charging scheduling\\nwith deep learning framework for multi-drone networks,” IEEE Trans-\\nactions on Vehicular Technology, vol. 68, no. 5, pp. 4235–4248, May\\n2019.\\n[24] S. Jung, W. J. Yun, M. Shin, J. Kim, and J.-H. Kim, “Orchestrated\\nscheduling and multi-agent deep reinforcement learning for cloud-\\nassisted multi-UAV charging systems,” IEEE Transactions on Vehicular\\nTechnology, vol. 70, no. 6, pp. 5362–5377, June 2021.\\n[25] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.\\nGoan, “Variational quantum circuits for deep reinforcement learning,”\\nIEEE Access, vol. 8, pp. 141 007–141 024, 2020.\\n[26] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and V. Dunjko, “Vari-\\national quantum policies for reinforcement learning,” arXiv preprint\\narXiv:2103.05577, 2021.\\n[27] M. Saad, J. Choi, D. Nyang, J. Kim, and A. Mohaisen, “Toward\\ncharacterizing blockchain-based cryptocurrencies for highly accurate\\npredictions,” IEEE Systems Journal, vol. 14, no. 1, pp. 321–332, March\\n2020.\\n[28] E. Boo, J. Kim, and J. Ko, “LiteZKP: Lightening zero-knowledge proof-\\nbased blockchains for IoT and edge platforms,” IEEE Systems Journal,\\npp. 1–12, 2021.\\n[29] N.-N. Dao, D.-N. Vu, W. Na, J. Kim, and S. Cho, “SGCO: Stabilized\\ngreen crosshaul orchestration for dense IoT ofﬂoading services,” IEEE\\nJournal on Selected Areas in Communications, vol. 36, no. 11, pp. 2538–\\n2548, November 2018.\\n[30] N.-N. Dao, T. V. Phan, U. Sa’ad, J. Kim, T. Bauschert, D.-T. Do,\\nand S. Cho, “Securing heterogeneous IoT with intelligent DDoS attack\\nbehavior learning,” IEEE Systems Journal, pp. 1–10, 2021.\\n[31] S.\\nJung,\\nJ.\\nKim,\\nM.\\nLevorato,\\nC.\\nCordeiro,\\nand\\nJ.-H.\\nKim,\\n“Infrastructure-assisted on-driving experience sharing for millimeter-\\nwave connected vehicles,” IEEE Transactions on Vehicular Technology,\\npp. 1–1, 2021.\\n[32] J. Kim and A. F. Molisch, “Fast millimeter-wave beam training with re-\\nceive beamforming,” Journal of Communications and Networks, vol. 16,\\nno. 5, pp. 512–522, October 2014.\\n[33] A. Malik, J. Kim, K. S. Kim, and W.-Y. Shin, “A personalized preference\\nlearning framework for caching in mobile networks,” IEEE Transactions\\non Mobile Computing, vol. 20, no. 6, pp. 2124–2139, June 2021.\\n[34] M. Choi, J. Kim, and J. Moon, “Dynamic power allocation and user\\nscheduling for power-efﬁcient and delay-constrained multiple access\\nnetworks,” IEEE Transactions on Wireless Communications, vol. 18,\\nno. 10, pp. 4846–4858, October 2019.\\n[35] M. Choi, A. F. Molisch, and J. Kim, “Joint distributed link scheduling\\nand power allocation for content delivery in wireless caching networks,”\\nIEEE Transactions on Wireless Communications, vol. 19, no. 12, pp.\\n7810–7824, December 2020.\\n[36] M. Choi, A. F. Molisch, D.-J. Han, D. Kim, J. Kim, and J. Moon,\\n“Probabilistic caching and dynamic delivery policies for categorized\\ncontents and consecutive user demands,” IEEE Transactions on Wireless\\nCommunications, vol. 20, no. 4, pp. 2685–2699, April 2021.\\n[37] J. Kim, G. Caire, and A. F. Molisch, “Quality-aware streaming and\\nscheduling for device-to-device video delivery,” IEEE/ACM Transactions\\non Networking, vol. 24, no. 4, pp. 2319–2331, August 2016.\\n[38] M. Choi, J. Kim, and J. Moon, “Wireless video caching and dynamic\\nstreaming under differentiated quality requirements,” IEEE Journal on\\nSelected Areas in Communications, vol. 36, no. 6, pp. 1245–1257, June\\n2018.\\n[39] J. Koo, J. Yi, J. Kim, M. A. Hoque, and S. Choi, “Seamless dynamic\\nadaptive streaming in LTE/Wi-Fi integrated network under smartphone\\nresource constraints,” IEEE Transactions on Mobile Computing, vol. 18,\\nno. 7, pp. 1647–1660, July 2019.\\n[40] J. Yi, S. Kim, J. Kim, and S. Choi, “Supremo: Cloud-assisted low-\\nlatency super-resolution in mobile devices,” IEEE Transactions on\\nMobile Computing, pp. 1–1, 2021.\\n[41] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven,\\n“Barren plateaus in quantum neural network training landscapes,” Nature\\ncommunications, vol. 9, no. 1, pp. 1–6, 2018.\\n',\n",
       " '2108.11510v1.pdf': 'Deep Reinforcement Learning in Computer Vision:\\nA Comprehensive Survey\\nNgan Le∗∗\\nVidhiwar Singh Rathour∗\\nKashu Yamazaki∗\\nKhoa Luu\\nMarios Savvides\\nJanuary 13, 2022\\nAbstract\\nDeep reinforcement learning augments the reinforcement learning framework and\\nutilizes the powerful representation of deep neural networks. Recent works have demon-\\nstrated the remarkable successes of deep reinforcement learning in various domains in-\\ncluding ﬁnance, medicine, healthcare, video games, robotics, and computer vision. In\\nthis work, we provide a detailed review of recent and state-of-the-art research advances\\nof deep reinforcement learning in computer vision. We start with comprehending the\\ntheories of deep learning, reinforcement learning, and deep reinforcement learning. We\\nthen propose a categorization of deep reinforcement learning methodologies and discuss\\ntheir advantages and limitations. In particular, we divide deep reinforcement learning\\ninto seven main categories according to their applications in computer vision, i.e. (i)\\nlandmark localization (ii) object detection; (iii) object tracking; (iv) registration on\\nboth 2D image and 3D image volumetric data (v) image segmentation; (vi) videos\\nanalysis; and (vii) other applications. Each of these categories is further analyzed with\\nreinforcement learning techniques, network design, and performance. Moreover, we\\nprovide a comprehensive analysis of the existing publicly available datasets and exam-\\nine source code availability. Finally, we present some open issues and discuss future\\nresearch directions on deep reinforcement learning in computer vision.\\n1\\nIntroduction\\nReinforcement learning (RL) is a machine learning technique for learning a sequence of ac-\\ntions in an interactive environment by trial and error that maximizes the expected reward\\n[351]. Deep Reinforcement Learning (DRL) is the combination of Reinforcement Learning\\nand Deep Learning (DL) and it has become one of the most intriguing areas of artiﬁcial intel-\\nligence today. DRL can solve a wide range of complex real-world decision-making problems\\nwith human-like intelligence that were previously intractable. DRL was selected by [316],\\n[106] as one of ten breakthrough techniques in 2013 and 2017, respectively.\\nThe past years have witnessed the rapid development of DRL thanks to its amazing\\nachievement in solving challenging decision-making problems in the real world. DRL has\\n1\\narXiv:2108.11510v1  [cs.CV]  25 Aug 2021\\nbeen successfully applied into many domains including games, robotics, autonomous driving,\\nhealthcare, natural language processing, and computer vision.\\nIn contrast to supervised\\nlearning which requires large labeled training data, DRL samples training data from an\\nenvironment. This opens up many machine learning applications where big labeled training\\ndata does not exist.\\nFar from supervised learning, DRL-based approaches focus on solving sequential decision-\\nmaking problems. They aim at deciding, based on a set of experiences collected by interacting\\nwith the environment, the sequence of actions in an uncertain environment to achieve some\\ntargets. Diﬀerent from supervised learning where the feedback is available after each system\\naction, it is simply a scalar value that may be delayed in time in the DRL framework. For\\nexample, the success or failure of the entire system is reﬂected after a sequence of actions.\\nFurthermore, the supervised learning model is updated based on the loss/error of the output\\nand there is no mechanism to get the correct value when it is wrong. This is addressed by\\npolicy gradients in DRL by assigning gradients without a diﬀerentiable loss function. This\\naims at teaching a model to try things out randomly and learn to do correct things more.\\nMany survey papers in the ﬁeld of DRL including [13] [97] [414] have been introduced\\nrecently.\\nWhile [13] covers central algorithms in DRL, [97] provides an introduction to\\nDRL models, algorithms, and techniques, where particular focus is the aspects related to\\ngeneralization and how DRL can be used for practical applications. Recently, [414] introduces\\na survey, which discusses the broad applications of RL techniques in healthcare domains\\nranging from dynamic treatment regimes in chronic diseases and critical care, an automated\\nmedical diagnosis from both unstructured and structured clinical data, to many other control\\nor scheduling domains that have inﬁltrated many aspects of a healthcare system. Diﬀerent\\nfrom the previous work, our survey focuses on how to implement DRL in various computer\\nvision applications such as landmark detection, object detection, object tracking, image\\nregistration, image segmentation, and video analysis.\\nOur goal is to provide our readers good knowledge about the principle of RL/DRL and\\nthorough coverage of the latest examples of how DRL is used for solving computer vision\\ntasks. We structure the rest of the paper as follows: we ﬁrst introduce fundamentals of\\nDeep Learning (DL) in section 2 including Multi-Layer Perceptron (MLP), Autoencoder,\\nDeep Belief Network, Convolutional Neural Networks (CNNs), Recurrent Neural Networks\\n(RNNs). Then, we present the theories of RL in section 3, which starts with the Markov\\nDecision Process (MDP) and continues with value function and Q-function. In the end of\\nsection 3, we introduce various techniques in RL under two categories of model-based and\\nmodel-free RL. Next, we introduce DRL in section 4 with main techniques in both value-\\nbased methods, policy gradient methods, and actor-critic methods under model-based and\\nmodel-free categories. The application of DRL in computer vision will then be introduced\\nin sections 5, 6, 7, 8, 9, 10, 11 corresponding respectively to DRL in landmark detection,\\nDRL in object detection, DRL in object tracking, DRL in image registration, DRL in im-\\nage segmentation, DRL in video analysis and other applications of DRL. Each application\\ncategory ﬁrst starts with a problem introduction and then state-of-the-art approaches in the\\nﬁeld are discussed and compared through a summary table. We are going to discuss some\\n2\\nfuture perspectives in section 12 including challenges of DRL in computer vision and the\\nrecent advanced techniques.\\n2\\nIntroduction to Deep Learning\\n2.1\\nMulti-Layer Perceptron (MLP)\\nDeep learning models, in simple words, are large and deep artiﬁcial neural networks. Let us\\nconsider the simplest possible neural network which is called ”neuron” as illustrated in Fig.\\n1. A computational model of a single neuron is called a perceptron which consists of one or\\nmore inputs, a processor, and a single output.\\no\\nx0\\nx1\\nx2\\n+1\\nFigure 1: An example of one neuron which takes input x = [x1, x2, x3], the intercept term\\n+1 as bias, and the output o.\\no\\nx2\\nx1\\n+1\\nx0\\n+1\\nLayer l0\\nLayer l1\\nLayer l2\\nFigure 2: An example of multi-layer perceptron network (MLP)\\nIn this example, the neuron is a computational unit that takes x = [x0, x1, x2] as input,\\nthe intercept term +1 as bias b, and the output o. The goal of this simple network is to\\nlearn a function f : RN →RM where N is the number of dimensions for input x and M is\\nthe number of dimensions for output which is computed as o = f(x, θ), where θ is a set of\\n3\\nLatent Space\\nLatent Space\\nLatent Distribution\\nInput Layer\\nHidden Layer\\nHidden Layer\\nOutput Layer\\nInput Layer\\nHidden Layer\\nOutput Layer\\n(a)\\n(b)\\n(c)\\n(d)\\nRBM 1\\nRBM 2\\nRBM 3\\nOutput\\nFigure 3: An illustration of various DL architectures. (a): Autoencoder (AE); (b): Deep\\nBelief Network; (c): Convolutional Neural Network (CNN); (d): Recurrent Neural Network\\n(RNN).\\nweights and are known as weights θ = {wi}. Mathematically, the output o of a one neuron\\nis deﬁned as:\\no = f(x, θ) = σ\\n N\\nX\\ni=1\\nwixi + b\\n!\\n= σ(WTx + b)\\n(1)\\nIn this equation, σ is the point-wise non-linear activation function. The common non-\\nlinear activation functions for hidden units are hyperbolic tangent (Tanh), sigmoid, softmax,\\nReLU, and LeakyReLU. A typical multi-layer perception (MLP) neural network is composed\\nof one input layer, one output layer, and many hidden layers. Each layer may contain many\\nunits. In this network, x is the input layer, o is the output layer. The middle layer is called\\nthe hidden layer. In Fig. 2(b), MLP contains 3 units of the input layer, 3 units of the hidden\\nlayer, and 1 unit of the output layer.\\nIn general, we consider a MLP neural network with L hidden layers of units, one layer\\nof input units and one layer of output units. The number of input units is N, output units\\nis M, and units in hidden layer lth is N l. The weight of the jth unit in layer lth and the ith\\nunit in layer (l + 1)th is denoted by wl\\nij. The activation of the ith unit in layer lth is hl\\ni.\\n2.2\\nAutoencoder\\nAutoencoder is an unsupervised algorithm used for representation learning, such as feature\\nselection or dimension reduction. A gentle introduction to Variational Autoencoder (VAE)\\nis given in [11] and VAE framework is illustrated in Fig.3(a). In general, VAE aims to learn\\na parametric latent variable model by maximizing the marginal log-likelihood of the training\\ndata.\\n2.3\\nDeep Belief Network\\nDeep Belief Network (DBN) and Deep Autoencoder are two common unsupervised ap-\\nproaches that have been used to initialize the network instead of random initialization.\\n4\\nFigure 4: Architecture of a typical convolutional network for image classiﬁcation containing\\nthree basic layers: convolution layer, pooling layer and fully connected layer\\nWhile Deep Autoencoder is based on Autoencoder, Deep Belief Networks is based on Re-\\nstricted Boltzmann Machine (RBM), which contains a layer of input data and a layer of\\nhidden units that learn to represent features that capture high-order correlations in the data\\nas illustrated in Fig.3(b).\\n2.4\\nConvolutional Neural Networks (CNN)\\nConvolutional Neural Network (CNN) [204] [203] is a special case of fully connected MLP that\\nimplements weight sharing for processing data. CNN uses the spatial correlation of the signal\\nto utilize the architecture in a more sensible way. Their architecture, somewhat inspired by\\nthe biological visual system, possesses two key properties that make them extremely useful\\nfor image applications: spatially shared weights and spatial pooling. These kinds of networks\\nlearn features that are shift-invariant, i.e., ﬁlters that are useful across the entire image (due\\nto the fact that image statistics are stationary).\\nThe pooling layers are responsible for\\nreducing the sensitivity of the output to slight input shifts and distortions, and increasing\\nthe reception ﬁeld for next layers.\\nSince 2012, one of the most notable results in Deep\\nLearning is the use of CNN to obtain a remarkable improvement in object recognition in\\nImageNet classiﬁcation challenge [72] [187].\\nA typical CNN is composed of multiple stages, as shown in Fig. 3(c). The output of each\\nstage is made of a set of 2D arrays called feature maps. Each feature map is the outcome of\\none convolutional (and an optional pooling) ﬁlter applied over the full image. A point-wise\\nnon-linear activation function is applied after each convolution. In its more general form, a\\nCNN can be written as\\nh0 =x\\nhl =pooll(σl(wlhl−1 + bl)), ∀l ∈1, 2, ...L\\no =hL\\n(2)\\n5\\nwhere wl, bl are trainable parameters as in MLPs at layer lth. x ∈Rc×h×w is vectorized from\\nan input image with c being the color channels, h the image height and w the image width.\\no ∈Rn×h′×w′ is vectorized from an array of dimension h′ × w′ of output vector (of dimension\\nn). pooll is a (optional) pooling function at layer lth.\\nCompared to traditional machine learning methods, CNN has achieved state-of-the-\\nart performance in many domains including image understanding, video analysis and au-\\ndio/speech recognition. In image understanding [404], [426], CNN outperforms human ca-\\npacities [39]. Video analysis [422], [217] is another application that turns the CNN model\\nfrom a detector [374] into a tracker [94]. As a special case of image segmentation [194], [193],\\nsaliency detection is another computer vision application that uses CNN [381], [213]. In addi-\\ntion to the previous applications, pose estimation [290], [362] is another interesting research\\nthat uses CNN to estimate human-body pose. Action recognition in both still images and\\nvideos is a special case of recognition and is a challenging problem. [110] utilizes CNN-based\\nrepresentation of contextual information in which the most representative secondary region\\nwithin a large number of object proposal regions, together with the contextual features,\\nis used to describe the primary region. CNN-based action recognition in video sequences\\nis reviewed in [420]. Text detection and recognition using CNN is the next step of optical\\ncharacter recognition (OCR) [406] and word spotting [160]. Not only in computer vision,\\nCNN has been successfully applied into other domains such as speech recognition and speech\\nsynthesis [274], [283], biometrics [242], [85], [281], [350],[304], [261], biomedical [191], [342],\\n[192], [411].\\n2.5\\nRecurrent Neural Networks (RNN)\\nRNN is an extremely powerful sequence model and was introduced in the early 1990s [172].\\nA typical RNN contains three parts, namely, sequential input data (xt), hidden state (ht)\\nand sequential output data (yt) as shown in Fig. 3(d).\\nRNN makes use of sequential information and performs the same task for every element\\nof a sequence where the output is dependent on the previous computations. The activation\\nof the hidden states at time-step t is computed as a function f of the current input symbol\\nxt and the previous hidden states ht−1. The output at time t is calculated as a function g\\nof the current hidden state ht as follows\\nht = f(Uxt + Wht−1)\\nyt = g(Vht)\\n(3)\\nwhere U is the input-to-hidden weight matrix, W is the state-to-state recurrent weight\\nmatrix, V is the hidden-to-output weight matrix. f is usually a logistic sigmoid function or\\na hyperbolic tangent function and g is deﬁned as a softmax function.\\nMost works on RNN have made use of the method of backpropagation through time\\n(BPTT) [318] to train the parameter set (U, V, W) and propagate error backward through\\ntime. In classic backpropagation, the error or loss function is deﬁned as\\nE(y’, y) =\\nX\\nt\\n||y’t −yt||2\\n(4)\\n6\\nwhere yt is the prediction and y’t is the labeled groundtruth.\\nFor a speciﬁc weight W, the update rule for gradient descent is deﬁned as Wnew =\\nW −γ ∂E\\n∂W, where γ is the learning rate. In RNN model, the gradients of the error with\\nrespect to our parameters U, V and W are learned using Stochastic Gradient Descent\\n(SGD) and chain rule of diﬀerentiation.\\nThe diﬃculty of training RNN to capture long-term dependencies has been studied in\\n[26]. To address the issue of learning long-term dependencies, Hochreiter and Schmidhuber\\n[139] proposed Long Short-Term Memory (LSTM), which can maintain a separate memory\\ncell inside it that updates and exposes its content only when deemed necessary. Recently, a\\nGated Recurrent Unit (GRU) was proposed by [51] to make each recurrent unit adaptively\\ncapture dependencies of diﬀerent time scales. Like the LSTM unit, the GRU has gating units\\nthat modulate the ﬂow of information inside the unit but without having separate memory\\ncells.\\nSeveral variants of RNN have been later introduced and successfully applied to wide\\nvariety of tasks, such as natural language processing [257], [214], speech recognition [115],\\n[54], machine translation [175], [241], question answering [138], image captioning [247], [78],\\nand many more.\\n3\\nBasics of Reinforcement Learning\\nThis section serves as a brief introduction to the theoretical models and techniques in RL. In\\norder to provide a quick overview of what constitutes the main components of RL methods,\\nsome fundamental concepts and major theoretical problems are also clariﬁed. RL is a kind\\nof machine learning method where agents learn the optimal policy by trial and error. Unlike\\nsupervised learning, the feedback is available after each system action, it is simply a scalar\\nvalue that may be delayed in time in RL framework, for example, the success or failure of the\\nentire system is reﬂected after a sequence of actions. Furthermore, the supervised learning\\nmodel is updated based on the loss/error of the output and there is no mechanism to get\\nthe correct value when it is wrong. This is addressed by policy gradients in RL by assigning\\ngradients without a diﬀerentiable loss function which aims at teaching a model to try things\\nout randomly and learn to do correct things more.\\nInspired by behavioral psychology, RL was proposed to address the sequential decision-\\nmaking problems which exist in many applications such as games, robotics, healthcare, smart\\ngrids, stock, autonomous driving, etc. Unlike supervised learning where the data is given,\\nan artiﬁcial agent collects experiences (data) by interacting with its environment in RL\\nframework. Such experience is then gathered to optimize the cumulative rewards/utilities.\\nIn this section, we focus on how the RL problem can be formalized as an agent that\\ncan make decisions in an environment to optimize some objectives presented under reward\\nfunctions. Some key aspects of RL are: (i) Address the sequential decision making; (ii) There\\nis no supervisor, only a reward presented as scalar number; and (iii) The feedback is highly\\ndelayed. Markov Decision Process (MDP) is a framework that has commonly been used to\\nsolve most RL problems with discrete actions, thus we will ﬁrst discuss MDP in this section.\\n7\\nWe then introduce value function and how to categorize RL into model-based or model-free\\nmethods. At the end of this section, we discuss some challenges in RL.\\nEnvironment\\nAction\\nReward\\nObservations\\nFigure 5: An illustration of agent-environment interaction in RL\\n3.1\\nMarkov Decision Process\\nThe standard theory of RL is deﬁned by a Markov Decision Process (MDP), which is an\\nextension of the Markov process (also known as the Markov chain). Mathematically, the\\nMarkov process is a discrete-time stochastic process whose conditional probability distribu-\\ntion of the future states only depends upon the present state and it provides a framework to\\nmodel decision-making situations. An MDP is typically deﬁned by ﬁve elements as follows:\\n• S: a set of state or observation space of an environment. s0 is starting state.\\n• A: set of actions the agent can choose.\\n• T: a transition probability function T(st+1|st, at), specifying the probability that the\\nenvironment will transition to state st+1 ∈S if the agent takes action at ∈A in state\\nst ∈S.\\n• R: a reward function where rt+1 = R(st, st+1) is a reward received for taking action at\\nat state st and transfer to the next state st+1.\\n• γ: a discount factor.\\nConsidering MDP(S, A, γ, T, R), the agent chooses an action at according to the pol-\\nicy π(at|st) at state st.\\nNotably, agent’s algorithm for choosing action a given current\\nstate s, which in general can be viewed as distribution π(a|s), is called a policy (strat-\\negy). The environment receives the action, produces a reward rt+1 and transfers to the next\\nstate st+1 according to the transition probability T(st+1|st, at). The process continues until\\nthe agent reaches a terminal state or a maximum time step. In RL framework, the tuple\\n(st, at, rt+1, st+1) is called transition. Several sequential transitions are usually referred to as\\n8\\nroll-out. Full sequence (s0, a0, r1, s1, a1, r2, ...) is called a trajectory. Theoretically, trajectory\\nis inﬁnitely long, but the episodic property holds in most practical cases. One trajectory of\\nsome ﬁnite length τ is called an episode. For given MDP and policy π, the probability of\\nobserving (s0, a0, r1, s1, a1, r2, ...) is called trajectory distribution and is denoted as:\\nTπ =\\nY\\nt\\nπ(at|st)T(st+1|st, at)\\n(5)\\nThe objective of RL is to ﬁnd the optimal policy π∗for the agent that maximizes the cumu-\\nlative reward, which is called return. For every episode, the return is deﬁned as the weighted\\nsum of immediate rewards:\\nR =\\nτ−1\\nX\\nt=0\\nγtrt+1\\n(6)\\nBecause the policy induces a trajectory distribution, the expected reward maximization can\\nbe written as:\\nETπ\\nτ−1\\nX\\nt=0\\nrt+1 →max\\nπ\\n(7)\\nThus, given MDP and policy π, the discounted expected reward is deﬁned:\\nG(π) = ETπ\\nτ−1\\nX\\nt=0\\nγtrt+1\\n(8)\\nThe goal of RL is to ﬁnd an optimal policy π∗, which maximizes the discounted expected\\nreward, i.e. G(π) →maxπ.\\n3.2\\nValue and Q- functions\\nThe value function is applied to evaluate how good it is for an agent to utilize policy π\\nto visit state s. The concept of ”good” is deﬁned in terms of expected return, i.e. future\\nrewards that can be expected to receive in the future and it depends on what actions it will\\ntake. Mathematically, the value is the expectation of return, and value approximation is\\nobtained by Bellman expectation equation as follows:\\nV π(st) = E[rt+1 + γV π(st+1)]\\n(9)\\nV π(st) is also known as state-value function, and the expectation term can be expanded as\\na product of policy, transition probability, and return as follows:\\nV π(st) =\\nX\\nat∈A\\nπ(at|st)\\nX\\nst+1∈S\\nT(st+1|st, at)[R(st, st+1) + γV π(st+1)]\\n(10)\\nThis equation is called the Bellman equation. When the agent always selects the action\\naccording to the optimal policy π∗that maximizes the value, the Bellman equation can be\\n9\\nexpressed as follows:\\nV ∗(st) = max\\nat\\nX\\nst+1∈S\\nT(st+1|st, at)[R(st, st+1) + γV ∗(st+1)]\\n∆= max\\nat Q∗(st, at)\\n(11)\\nHowever, obtaining optimal value function V ∗does not provide enough information to re-\\nconstruct some optimal policy π∗because the real-world environment is complicated. Thus,\\na quality function (Q-function) is also called the action-value function under policy π. The\\nQ-function is used to estimate how good it is for an agent to perform a particular action (at)\\nin a state (st) with a policy π and it is introduced as:\\nQπ(st, at) =\\nX\\nst+1\\nT(st+1|st, at)[R(st, st+1) + γV π(st+1)]\\n(12)\\nUnlike value function which speciﬁes the goodness of a state, a Q-function speciﬁes the\\ngoodness of action in a state.\\n3.3\\nCategory\\nIn general, RL can be divided into either model-free or model-based methods. Here, ”model”\\nis deﬁned by the two quantity: transition probability function T(st+1|st, at) and the reward\\nfunction R(st, st+1).\\n3.3.1\\nModel-based RL\\nModel-based RL is an approach that uses a learnt model, i.e.\\nT(st+1|st, at) and reward\\nfunction R(st, st+1) to predict the future action. There are four main model-based techniques\\nas follows:\\n• Value Function: The objective of value function methods is to obtain the best policy\\nby maximizing the value functions in each state. A value function of a RL problem\\ncan be deﬁned as in Eq.10 and the optimal state-value function is given in Eq.11\\nwhich are known as Bellman equations. Some common approaches in this group are\\nDiﬀerential Dynamic Programming [208], [266], Temporal Diﬀerence Learning [249],\\nPolicy Iteration [334] and Monte Carlo [137].\\n• Transition Models: Transition models decide how to map from a state s, taking\\naction a to the next state (s’) and it strongly aﬀects the performance of model-based\\nRL algorithms. Based on whether predicting the future state s’ is based on the proba-\\nbility distribution of a random variable or not, there are two main approaches in this\\ngroup: stochastic and deterministic. Some common methods for deterministic models\\nare decision trees [280] and linear regression [265]. Some common methods for stochas-\\ntic models are Gaussian processes [71], [1], [12], Expectation-Maximization [59] and\\ndynamic Bayesian networks [280].\\n10\\n• Policy Search: Policy search approach directly searches for the optimal policy by\\nmodifying its parameters, whereas the value function methods indirectly ﬁnd the ac-\\ntions that maximize the value function at each state. Some of the popular approaches\\nin this group are: gradient-based [87], [267], information theory [1], [189] and sampling\\nbased [21].\\n• Return Functions: Return functions decide how to aggregate rewards or punishments\\nover an episode. They aﬀect both the convergence and the feasibility of the model.\\nThere are two main approaches in this group: discounted returns functions [21], [75],\\n[393] and averaged returns functions [34], [3]. Between the two approaches, the former\\nis the most popular which represents the uncertainty about future rewards. While\\nsmall discount factors provide faster convergence, its solution may not be optimal.\\nIn practice, transition and reward functions are rarely known and hard to model.\\nThe\\ncomparative performance among all model-based techniques is reported in [385] with over 18\\nbenchmarking environments including noisy environments. The Fig.6 summarizes diﬀerent\\nmodel-based RL approaches.\\n3.3.2\\nModel-free methods\\nLearning through the experience gained from interactions with the environment, i.e. model-\\nfree method tries to estimate the t. discrete problems transition probability function and the\\nreward function from the experience to exploit them in acquisition of policy. Policy gradient\\nand value-based algorithms are popularly used in model-free methods.\\n• The policy gradient methods: In this approach, RL task is considered as optimiza-\\ntion with stochastic ﬁrst-order optimization. Policy gradient methods directly optimize\\nthe discounted expected reward, i.e. G(π) →maxπ to obtains the optimal policy π∗\\nwithout any additional information about MDP. To do so, approximate estimations of\\nthe gradient with respect to policy parameters are used. Take [392] as an example,\\npolicy gradient parameterizes the policy and updates parameters θ,\\nGθ(π) = ETφ\\nX\\nt=0\\nlog(πθ(at|st))γtR\\n(13)\\nwhere R is the total accumulated return and deﬁned in Eq. 6. Common used policies\\nare Gibbs policies [20], [352] and Gaussian policies [294]. Gibbs policies are used in\\ndiscrete problems whereas Gaussian policies are used in continuous problems.\\n• Value-based methods: In this approach, the optimal policy π∗is implicitly con-\\nducted by gaining an approximation of optimal Q-function Q∗(s, a). In value-based\\nmethods, agents update the value function to learn suitable policy while policy-based\\nRL agents learn the policy directly. To do that, Q-learning is a typical value-based\\nmethod. The update rule of Q-learning with learning rate λ is deﬁned as:\\nQ(st, at) = Q(st, at) + λδt\\n(14)\\n11\\nTable 1: Comparison between model-based RL and model-free RL\\nFactors\\nModel-based RL\\nModel-free RL\\nNumber of iterations between\\nagent and environment\\nSmall\\nBig\\nConvergence\\nFast\\nSlow\\nPrior knowledge of transitions\\nYes\\nNo\\nFlexibility\\nStrongly depend on\\na learnt model\\nAdjust based\\non trials and errors\\nwhere δt = R(st, st+1)+γarg maxa Q(st+1, a) −Q(st, a) is the temporal diﬀerence (TD)\\nerror.\\nTarget at self-play Chess, [394] investigates inasmuch it is possible to leverage the\\nqualitative feedback for learning an evaluation function for the game. [319] provides\\nthe comparison of learning of linear evaluation functions between using preference\\nlearning and using least-squares temporal diﬀerence learning, from samples of game\\ntrajectories. The value-based methods depend on a speciﬁc, optimal policy, thus it is\\nhard for transfer learning.\\n• Actor-critic is an improvement of policy gradient with an value-based critic Γ, thus,\\nEq.13 is rewritten as:\\nGθ(π) = ETφ\\nX\\nt=0\\nlog(πθ(at|st))γtΓt\\n(15)\\nThe critic function Γ can be deﬁned as Qπ(st, at) or Qπ(st, at) −V π\\nt\\nor R[st−1, st] +\\nV π\\nt+1 −V π\\nt\\nActor-critic methods are combinations of actor-only methods and critic-only methods. Thus,\\nactor-critic methods have been commonly used RL. Depend on reward setting, there are two\\ngroups of actor-critic methods, namely discounted return [282], [30] and average return [289],\\n[31]. The comparison between model-based and model-free methods is given in Table 1.\\n4\\nIntroduction to Deep Reinforcement Learning\\nDRL, which was proposed as a combination of RL and DL, has achieved rapid developments,\\nthanks to the rich context representation of DL. Under DRL, the aforementioned value and\\npolicy can be expressed by neural networks which allow dealing with a continuous state or\\naction that was hard for a table representation. Similar to RL, DRL can be categorized into\\nmodel-based algorithms and model-free algorithms which will be introduced in this section.\\n4.1\\nModel-Free Algorithms\\nThere are two approaches, namely, Value-based DRL methods and Policy gradient DRL\\nmethods to implement model-free algorithms.\\n12\\nModel-based RL\\nValue Functions\\nDiﬀerential Dynamic Programming [208], [266]\\nTemporal Diﬀerence Learning [249]\\nPolicy Iteration [334]\\nMonte Carlo [137]\\nTransition Models\\nDeterministic models\\nDecision trees [280]\\nLinear regression [265]\\nStochastic models\\nGaussian processes [71], [1], [12]\\nExpectation-Maximization [59]\\nDynamic Bayesian networks [280]\\nPolicy Search\\nGradient-based [87], [267]\\nInformation theory [1], [189]\\nSampling based [21]\\nReturn Functions\\nDiscounted returns functions [21], [75], [393]\\nAveraged returns functions [34], [3]\\nFigure 6: Summarization of model-based RL approaches\\n13\\n4.1.1\\nValue-based DRL methods\\nDeep Q-Learning Network (DQN): Deep Q-learning [264] (DQN) is the most famous\\nDRL model which learns policies directly from high-dimensional inputs by CNNs. In DQN,\\ninput is raw pixels and output is a quality function to estimate future rewards as given in\\nFig.7. Take regression problem as an instance. Let y denote the target of our regression\\ntask, the regression with input (s, a), target y(s, a) and the MSE loss function is as:\\nLDQN = L(y(st, at), Q∗(st, at, θt))\\n= ||y(st, at) −Q∗(st, at, θt)||2\\ny(st, at) = R(st, st+1) + γ max\\nat+1 Q∗(st1, at+1, θt)\\n(16)\\nWhere θ is vector of parameters, θ ∈R|S||R| and st+1 is a sample from T(st+1|st, at) with\\ninput of (st, at).\\nMinimizing the loss function yields a gradient descent step formula to update θ as follows:\\nθt+1 = θt −αt\\n∂LDQN\\n∂θ\\n(17)\\nFigure 7: Network structure of Deep Q-Network (DQN), where Q-values Q(s,a) are generated\\nfor all actions for a given state.\\nDouble DQN: In DQN, the values of Q∗in many domains were leading to overestimation\\nbecause of max. In Eq.16, y(s, a) = R(s, s′) + γ maxa′ Q∗(s′, a′, θ) shifts Q-value estima-\\ntion towards either to the actions with high reward or to the actions with overestimating\\napproximation error. Double DQN [370] is an improvement of DQN that combines double\\nQ-learning [130] with DQN and it aims at reducing observed overestimation with better\\n14\\nperformance. The idea of Double DQN is based on separating action selection and action\\nevaluation using its own approximation of Q∗as follows:\\nmax\\nat+1 Q∗(st+1, at+1; θ) = Q∗(st+1, arg max\\nat+1\\nQ∗(st+1, at+1; θ1); θ2)\\n(18)\\nThus\\ny = R(st, st+1) + γQ∗(st+1, arg max\\nat+1\\nQ∗(st+1, at+1; θ1); θ2)\\n(19)\\nThe easiest and most expensive implementation of double DQN is to run two independent\\nDQNs as follows:\\ny1 = R(st, st+1)+\\nγQ∗\\n1(st+1, arg max\\nat+1\\nQ∗\\n2(st+1, at+1; θ2); θ1)\\ny2 = R(st, st+1)+\\nγQ∗\\n2(st+1, arg max\\nat+1\\nQ∗\\n1(st+1, at+1; θ1); θ2)\\n(20)\\nDueling DQN: In DQN, when the agent visits an unfavorable state, instead of lowering its\\nvalue V ∗, it remembers only low pay-oﬀby updating Q∗. In order to address this limitation,\\nDueling DQN [390] incorporates approximation of V ∗explicitly in a computational graph\\nby introducing an advantage function as follows:\\nAπ(st, at) = Qπ(st, at) −V π(st)\\n(21)\\nTherefore, we can reformulate Q-value: Q∗(s, a) = A∗(s, a) + V ∗(s). This implies that after\\nDL the feature map is decomposed into two parts corresponding to V ∗(v) and A∗(s, a) as\\nillustrated in Fig.8.\\nThis can be implemented by splitting the fully connected layers in\\nthe DQN architecture to compute the advantage and state value functions separately, then\\ncombining them back into a single Q-function. An interesting result has shown that Dueling\\nDQN obtains better performance if it is formulated as:\\nQ∗(st, at) = V ∗(st) + A∗(st, at) −max\\nat+1 A∗(st, at+1)\\n(22)\\nIn practical implementation, averaging instead of maximum is used, i.e.\\nQ∗(st, at) = V ∗(st) + A∗(st, at) −meanat+1A∗(st, at+1)\\nFurthermore, to address the limitation of memory and imperfect information at each decision\\npoint, Deep Recurrent Q-Network (DRQN) [131] employed RNNs into DQN by replacing\\nthe ﬁrst fully-connected layer with an RNN. Multi-step DQN [68] is one of the most popular\\nimprovements of DQN by substituting one-step approximation with N-steps.\\n15\\nFigure 8: Network structure of Dueling DQN, where value function V (s) and advantage\\nfunction A(s, a) are combined to predict Q-values Q(s, a) for all actions for a given state.\\n4.1.2\\nPolicy gradient DRL methods\\nPolicy Gradient Theorem: Diﬀerent from value-based DRL methods, policy gradient\\nDRL optimizes the policy directly by optimizing the following objective function which is\\ndeﬁned as a function of θ.\\nG(θ) = ET ∼πθ\\nX\\nt=1\\nγt−1R(st−1, st) →max\\nθ\\n(23)\\nFor any MDP and diﬀerentiable policy πθ, the gradient of objective Eq.23 is deﬁned by policy\\ngradient theorem [353] as follows:\\n▽θ G(θ) = ET ∼πθ\\nX\\nt=0\\nγtQπ(st, at) ▽θ logπθ(at|st)\\n(24)\\nREINFORCE: REINFORCE was introduced by [392] to approximately calculate the gra-\\ndient in Eq.24 by using Monte-Carlo estimation. In REINFORCE approximate estimator,\\nEq.24 is reformulated as:\\n▽θ G(θ) ≈\\nN\\nX\\nT\\nX\\nt=0\\nγt ▽θ logπθ(at|st)(\\nX\\nt′=t\\nγt′−tR(st′, st′+1))\\n(25)\\nwhere T is trajectory distribution and deﬁned in Eq.5. Theoretically, REINFORCE can be\\nstraightforwardly applied into any parametric πtheta(a|s). However, it is impractical to use\\nbecause of its time-consuming nature for convergence and local optimums problem. Based\\non the observation that the convergence rate of stochastic gradient descent directly depends\\n16\\non the variance of gradient estimation, the variance reduction technique was proposed to\\naddress naive REINFORCE’s limitations by adding a term that reduces the variance without\\naﬀecting the expectation.\\n4.1.3\\nActor-Critic DRL algorithm\\nBoth value-based and policy gradient algorithms have their own pros and cons, i.e. policy\\ngradient methods are better for continuous and stochastic environments, and have a faster\\nconvergence whereas, value-based methods are more sample eﬃcient and steady. Lately,\\nactor-critic [182] [262] was born to take advantage from both value-based and policy gradient\\nwhile limiting their drawbacks. Actor-critic architecture computes the policy gradient using\\na value-based critic function to estimate expected future reward. The principal idea of actor-\\ncritic is to divide the model into two parts: (i) computing an action based on a state and (ii)\\nproducing the Q values of the action. As given in Fig.9, the actor takes as input the state\\nst and outputs the best action at. It essentially controls how the agent behaves by learning\\nthe optimal policy (policy-based). The critic, on the other hand, evaluates the action by\\ncomputing the value function (value-based). The most basic actor-critic method (beyond the\\ntabular case) is naive policy gradients (REINFORCE). The relationship between actor-critic\\nis similar to kid-mom. The kid (actor) explores the environment around him/her with new\\nactions i.e. tough ﬁre, hit a wall, climb a tree, etc while the mom (critic) watches the kid\\nand criticizes/compliments him/her. The kid then adjusts his/her behavior based on what\\nhis/her mom told. When the kids get older, he/she can realize which action is bad/good.\\nFigure 9: Flowchart showing the structure of actor critic algorithm.\\nAdvantage Actor-Critic (A2C) Advantage Actor-Critic (A2C) [263] consist of two neural\\nnetworks i.e.\\nactor network πθ(at|st) representing for policy and critic network V π\\nω with\\nparameters ω approximately estimating actor’s performance.\\nIn order to determine how\\nmuch better, it is to take a speciﬁc action compared to the average, an advantage value is\\n17\\nFigure 10: An illustration of Actor-Critic algorithm in two cases: sharing parameters (a)\\nand not sharing parameters (b).\\ndeﬁned as:\\nAπ(st, at) = Qπ(st, at) −V π(st)\\n(26)\\nInstead of constructing two neural networks for both the Q value and the V value, using the\\nBellman optimization equation, we can rewrite the advantage function as:\\nAπ(st, at) = R(st, st+1) + γV π\\nω (st+1) −V π\\nω (st)\\n(27)\\nFor given policy π, its value function can be obtained using point iteration for solving:\\nV π(st) = Eat∼π(at|st)Est+1∼T(st+1|at,st)(R(st, st+1) + γV π(st+1))\\n(28)\\nSimilar to DQN, on each update a target is computed using current approximation:\\ny = R(st, st+1) + γV π\\nω (st+1)\\n(29)\\nAt time step t, the A2C algorithm can be implemented as following steps:\\n• Step 1: Compute advantage function using Eq.27.\\n• Step 2: Compute target using Eq.29.\\n• Step 3: Compute critic loss with MSE loss: L =\\n1\\nB\\nP\\nT ||y −V π(st))||2, where B is\\nbatch size and V π(st) is deﬁned in Eq.28.\\n18\\n• Step 4: Compute critic gradient: ▽critic = ∂L\\n∂ω.\\n• Step 5: Compute actor gradient: ▽actor = 1\\nB\\nP\\nT ▽θlogπ(at|st)Aπ(st, at)\\nAsynchronous Advantage Actor Critic (A3C) Besides A2C, there is another strategy\\nto implement an Actor-Critic agent.\\nAsynchronous Advantage Actor-Critic (A3C) [263]\\napproach does not use experience replay because this requires a lot of memory. Instead, A3C\\nasynchronously executes diﬀerent agents in parallel on multiple instances of the environment.\\nEach worker (copy of the network) will update the global network asynchronously. Because\\nof the asynchronous nature of A3C, some workers (copy of the agents) will work with older\\nvalues of the parameters. Thus the aggregating update will not be optimal. On the other\\nhand, A2C synchronously updates the global network. A2C waits until all workers ﬁnished\\ntheir training and calculated their gradients to average them, to update the global network.\\nIn order to update the entire network, A2C waits for each actor to ﬁnish their segment of\\nexperience before updating the global parameters. As a consequence, the training will be\\nmore cohesive and faster. Diﬀerent from A3C, each worker in A2C has the same set of\\nweights since and A2C updates all their workers at the same time. In short, A2C is an\\nalternative to the synchronous version of the A3C. In A2C, it waits for each actor to ﬁnish\\nits segment of experience before updating, averaging over all of the actors. In a practical\\nexperiment, this implementation is more eﬀectively uses GPUs due to larger batch sizes. The\\nstructure of an actor-critic algorithm can be divided into two types depending on parameter\\nsharing as illustrated in Fig.10.\\nIn order to overcome the limitation of speed, GA3C [16] was proposed and it achieved\\na signiﬁcant speedup compared to the original CPU implementation. To more eﬀectively\\ntrain A3C, [141] proposed FFE which forces random exploration at the right time during a\\ntraining episode, that can lead to improved training performance.\\n4.2\\nModel-Based Algorithms\\nWe have discussed so far model-free methods including the value-based approach and pol-\\nicy gradient approach. In this section, we focus on the model-based approach, that deals\\nwith the dynamics of the environment by learning a transition model that allows for sim-\\nulation of the environment without interacting with the environment directly. In contrast\\nto model-free approaches, model-based approaches are learned from experience by a func-\\ntion approximation. Theoretically, no speciﬁc prior knowledge is required in model-based\\nRL/DRL but incorporating prior knowledge can help faster convergence and better-trained\\nmodel, speed up training time as well as the number of training samples. While using raw\\ndata with pixel, it is diﬃcult for model-based RL to work on high dimensional and dynamic\\nenvironments. This is addressed in DRL by embedding the high-dimensional observations\\ninto a lower-dimensional space using autoencoders [95]. Many DRL approaches have been\\nbased on scaling up prior work in RL to high-dimensional problems. A good overview of\\nmodel-based RL for high-dimensional problems can be found in [297] which partition model-\\nbased DRL into three categories: explicit planning on given transitions, explicit planning on\\n19\\nlearned transitions, and end-to-end learning of both planning and transitions. In general,\\nDRL targets training DNNs to approximate the optimal policy π∗together with optimal\\nvalue functions V ∗and Q∗. In the following, we will cover the most common model-based\\nDRL approaches including value function and policy search methods.\\n4.2.1\\nValue function\\nWe start this category with DQN [264] which has been successfully applied to classic Atari\\nand illustrated in Fig.7. DQN uses CNNs to deal with high dimensional state space like\\npixels, to approximate the Q-value function.\\nMonte Carlo tree search (MCTS) MCTS [62] is one of the most popular methods to\\nlook-ahead search and it is combined with a DNN-based transition model to build a model-\\nbased DRL in [9]. In this work, the learned transition model predicts the next frame and\\nthe rewards one step ahead using the input of the last four frames of the agent’s ﬁrst-person-\\nview image and the current action. This model is then used by the Monte Carlo tree search\\nalgorithm to plan the best sequence of actions for the agent to perform.\\nValue-Targeted Regression (UCRL-VTR) Alex, et al. proposed model-based DRL for\\nregret minimization [167]. In their work, a set of models, that are ‘consistent’ with the data\\ncollected, is constructed at each episode. The consistency is deﬁned as the total squared\\nerror, whereas the value function is determined by solving the optimistic planning problem\\nwith the constructed set of models\\n4.2.2\\nPolicy search\\nPolicy search methods aim to directly ﬁnd policies by means of gradient-free or gradient-\\nbased methods.\\nModel-Ensemble Trust-Region Policy Optimization (ME-TRPO) ME-TRPO [190]\\nis mainly based on Trust Region Policy Optimization (TRPO) [327] which imposes a trust\\nregion constraint on the policy to further stabilize learning.\\nModel-Based Meta-Policy-Optimization (MB-MPO) MB-MPO [58] addresses the\\nperformance limitation of model-based DRL compared against model-free DRL when learn-\\ning dynamics models. MB-MPO learns an ensemble of dynamics models, a policy that can\\nquickly adapt to any model in the ensemble with one policy gradient step. As a result, the\\nlearned policy exhibits less model bias without the need to behave conservatively.\\nA summary of both model-based and model-free DRL algorithms is given in Table 2.\\nIn this Table, we also categorized DRL techniques into either on-policy or oﬀ-policy. In\\non-policy RL, it allows the use of older samples (collected using the older policies) in the\\ncalculation. The policy πk is updated with data collected by πk itself. In oﬀ-policy RL, the\\ndata is assumed to be composed of diﬀerent policies π0, π0, ..., πk. Each policy has its own\\ndata collection, then the data collected from π0, π1, ..., πk is used to train πk+1.\\n20\\nTable 2: Summary of model-based and model-free DRL algorithms consisting of value-based\\nand policy gradient methods.\\nDRL Algorithms\\nDescription\\nCategory\\nDQN [264]\\nDeep Q Network\\nValue-based\\nOﬀ-policy\\nDouble DQN [370]\\nDouble Deep Q Network\\nValue-based\\nOﬀ-policy\\nDueling DQN [390]\\nDueling Deep Q Network\\nValue-based\\nOﬀ-policy\\nMCTS [9]\\nMonte Carlo tree search\\nValue-based\\nOn-policy\\nUCRL-VTR[167]\\noptimistic planning problem\\nValue-based\\nOﬀ-policy\\nDDPG [223]\\nDQN with Deterministic Policy Gradient\\nPolicy gradient\\nOﬀ-policy\\nTRPO [327]\\nTrust Region Policy Optimization\\nPolicy gradient\\nOn-policy\\nPPO [328]\\nProximal Policy Optimization\\nPolicy gradient\\nOn-policy\\nME-TRPO [190]\\nModel-Ensemble Trust-Region Policy Optimization\\nPolicy gradient\\nOn-policy\\nMB-MPO [58]\\nModel-Based Meta- Policy-Optimization\\nPolicy gradient\\nOn-policy\\nA3C [263]\\nAsynchronous Advantage Actor Critic\\nActor Critic\\nOn-Policy\\nA2C [263]\\nAdvantage Actor Critic\\nActor Critic\\nOn-Policy\\n21\\n4.3\\nGood practices\\nInspired by Deep Q-learning [264], we discuss some useful techniques that are used during\\ntraining an agent in DRL framework in practices.\\nExperience replay Experience replay [417] is a useful part of oﬀ-policy learning and is\\noften used while training an agent in RL framework. By getting rid of as much information\\nas possible from past experiences, it removes the correlations in training data and reduces\\nthe oscillation of the learning procedure. As a result, it enables agents to remember and\\nre-use past experiences sometimes in many weights updates which increases data eﬃciency.\\nMinibatch learning Minibatch learning is a common technique that is used together with\\nexperience replay. Minibatch allows learning more than one training sample at each step,\\nthus, it makes the learning process robust to outliers and noise.\\nTarget Q-network freezing As described in [264], two networks are used for the training\\nprocess.\\nIn target Q-network freezing: one network interacts with the environment and\\nanother network plays the role of a target network. The ﬁrst network is used to generate\\ntarget Q-values that are used to calculate losses. The weights of the second network i.e.\\ntarget network are ﬁxed and slowly updated to the ﬁrst network [224].\\nReward clipping A reward is the scalar number provided by the environment and it aims\\nat optimizing the network. To keep the rewards in a reasonable scale and to ensure proper\\nlearning, they are clipped to a speciﬁc range (-1 ,1). Here 1 refers to as positive reinforcement\\nor reward and -1 is referred to as negative reinforcement or punishment.\\nModel-based v.s.\\nmodel-free approach Whether the model-free or model-based ap-\\nproaches is chosen mainly depends on the model architecture i.e. policy and value function.\\n5\\nDRL in Landmark Detection\\nAutonomous landmark detection has gained more and more attention in the past few years.\\nOne of the main reasons for this increased inclination is the rise of automation for evaluating\\ndata. The motivation behind using an algorithm for landmarking instead of a person is that\\nmanual annotation is a time-consuming tedious task and is prone to errors. Many eﬀorts\\nhave been made for the automation of this task. Most of the works that were published for\\nthis task using a machine learning algorithm to solve the problem. [64] proposed a regression\\nforest-based method for detecting landmark in a full-body CT scan. Although the method\\nwas fast it was less accurate when dealing with large organs. [101] extended the work of [64]\\nby adding statistical shape priors that were derived from segmentation masks with cascade\\nregression.\\nIn order to address the limitations of previous works on anatomy detection, [105] re-\\nformulated the detection problem as a behavior learning task for an artiﬁcial agent using\\nMDP. By using the capabilities of DRL and scale-space theory [226], the optimal search\\nstrategies for ﬁnding anatomical structures are learned based on the image information at\\nmultiple scales. In their approach, the search starts at the coarsest scale level for capturing\\nglobal context and continues to ﬁner scales for capturing more local information. In their\\n22\\nRL conﬁguration, the state of the agent at time t, st = I(⃗pt) is deﬁned as an axis-aligned\\nbox of image intensities extracted from the image I and centered at the voxel-position ⃗pt\\nin image space. An action at allows the agent to move from any voxel position ⃗pt to an\\nadjacent voxel position ⃗pt+1. The reward function represents distance-based feedback, which\\nis positive if the agent gets closer to the target structure and negative otherwise. In this\\nwork, a CNN is used to extract deep semantic features. The search starts with the coarsest\\nscale level M −1, the algorithm tries to maximize the reward which is the change in distance\\nbetween ground truth and predicted landmark location before and after the action of moving\\nthe scale window across the image. Upon convergence, the scale level is changed to M −2\\nand the search continued from the convergence point at scale level M −1. The process is\\nrepeated on the following scales until convergence on the ﬁnest scale. The authors performed\\nexperiments on 3D CT scans and obtained an average accuracy increase of 20-30% and lower\\ndistance error than the other techniques such as SADNN [104] and 3D-DL [427]\\nFocus on anatomical landmark localization in 3D fetal US images, [10] proposed and\\ndemonstrated use cases of several diﬀerent Deep Q-Network RL models to train agents that\\ncan precisely localize target landmarks in medical scans. In their work, they formulate the\\nlandmark detection problem as an MDP of a goal-oriented agent, where an artiﬁcial agent\\nis learned to make a sequence of decisions towards the target point of interest. At each time\\nstep, the agent should decide which direction it has to move to ﬁnd the target landmark.\\nThese sequential actions form a learned policy forming a path between the starting point\\nand the target landmark. This sequential decision-making process is approximated under\\nRL. In this RL conﬁguration, the environment is deﬁned as a 3D input image, action A is a\\nset of six actions ax+, ax−, ay+, ay−, az+, az−corresponding to three directions, the state\\ns is deﬁned as a 3D region of interest (ROI) centered around the target landmark and the\\nreward is chosen as the diﬀerence between the two Euclidean distances: the previous step\\nand current step. This reward signiﬁes whether the agent is moving closer to or further away\\nfrom the desired target location. In this work, they also proposed a novel ﬁxed- and multi-\\nscale optimal path search strategy with hierarchical action steps for agent-based landmark\\nlocalization frameworks.\\nWhereas pure policy or value-based methods have been widely used to solve RL-based\\nlocalization problems, [7] adopts an actor-critic [262] based direct policy search method\\nframed in a temporal diﬀerence learning approach. In their work, the state is deﬁned as\\na function of the agent-position which allows the agent at any position to observe an m ×\\nm × 3 block of surrounding voxels.\\nSimilar to other previous work, the action space is\\nax+, ax−, ay+, ay−, az+, az−.\\nThe reward is chosen as a simple binary reward function,\\nwhere a positive reward is given if an action leads the agent closer to the target landmark,\\nand a negative reward is given otherwise. Far apart from the previous work, their approach\\nproposes a non-linear policy function approximator represented by an MLP whereas the value\\nfunction approximator is presented by another MLP stacked on top of the same CNN from\\nthe policy net. Both policy (actor) and value (critic) networks are updated by actor-critic\\nlearning. To improve the learning, they introduce a partial policy-based RL to enable solving\\nthe large problem of localization by learning the optimal policy on smaller partial domains.\\n23\\nThe objective of the partial policy is to obtain multiple simple policies on the projections\\nof the actual action space, where the projected policies can reconstruct the policy on the\\noriginal action space.\\nBased on the hypothesis that the position of all anatomical landmarks is interdependent\\nand non-random within the human anatomy and this is necessary as the localization of dif-\\nferent landmarks requires learning partly heterogeneous policies, [377] concluded that one\\nlandmark can help to deduce the location of others. For collective gain, the agents share\\ntheir accumulated knowledge during training. In their approach, the state is deﬁned as RoI\\ncentered around the location of the agent. The reward function is deﬁned as the relative\\nimprovement in Euclidean distance between their location at time t and the target land-\\nmark location. Each agent is considered as Partially Observable Markov Decision Process\\n(POMDP) [107] and calculates its individual reward as their policies are disjoint. In or-\\nder to reduce the computational load in locating multiple landmarks and increase accuracy\\nthrough anatomical interdependence, they propose a collaborative multi-agent landmark de-\\ntection framework (Collab-DQN) where DQN is built upon a CNN. The backbone CNN is\\nshared across all agents while the policy-making fully connected layers are separate for each\\nagent.\\nTable 3: Comparing various DRL-based landmark detection meth-\\nods. The ﬁrst group on Single Landmark Detection (SLD) and the\\nsecond group for Multiple Landmark Detection (MLD)\\nApproaches Year\\nTraining\\nTech-\\nnique\\nActions\\nRemarks\\nPerformance\\nDatasets\\nand\\nsource code\\nSLD [105]\\n2017\\nDQN\\n6 action:\\n2 per axis\\nState:\\nan\\naxis-\\naligned\\nbox\\ncen-\\ntered at the voxel-\\nposition.\\nAction:\\nmove\\nfrom\\n⃗pt\\nto\\n⃗pt+1.\\nReward:\\ndistance-based\\nfeedback\\nAverage accuracy in-\\ncrease 20-30%. Lower\\ndistance\\nerror\\nthan\\nother techniques such\\nas SADNN [104] and\\n3D-DL [427]\\n3D CT Scan\\nSLD [10]\\n2019\\nDQN,\\nDDQN,\\nDuel\\nDQN\\nand\\nDuel\\nDDQN\\n6 action:\\n2 per axis\\nEnvironment:\\n3D\\ninput image. State:\\n3D\\nRoI\\ncentered\\naround\\nthe\\ntarget\\nlandmark.\\nRe-\\nward:\\nEuclidean\\ndistance\\nbetween\\npredicted points and\\ngroundtruth points.\\nDuel DQN performs\\nthe\\nbest\\non\\nRight\\nCerebellum\\n(FS),\\nLeft Cerebellum (FS,\\nMS) Duel DDQN is\\nthe\\nbest\\non\\nRight\\nCerebellum\\n(MS)\\nDQN\\nperforms\\nthe\\nbest on Cavum Sep-\\ntum\\nPellucidum(FS,\\nMS)\\nFetal head, ul-\\ntrasound\\nscans\\n[219].\\nAvailable code\\n24\\nSLD [7]\\n2019\\nActor-\\nCritic\\n-based\\nPartial\\n-Policy\\nRL\\n6 action:\\n2 per axis\\nState: a function of\\nthe\\nagent-position.\\nReward:\\nbinary\\nreward\\nfunction.\\npolicy\\nfunction:\\nMLP.\\nvalue\\nfunc-\\ntion: MLP\\nFaster\\nand\\nbetter\\nconvergence,\\nout-\\nperforms than other\\nconventional\\nactor-\\ncritic and Q-learning\\nCT\\nvolumes:\\nAortic\\nvalve.\\nCT\\nvolumes:\\nLAA\\nseed-\\npoint.\\nMR\\nimages:\\nVer-\\ntebra\\ncenters\\n[42].\\nMLD\\n[377]\\n2019\\nCollab\\nDQN\\n6 action:\\n2 per axis\\nState:\\nRoI centred\\naround\\nthe\\nagent.\\nReward:\\nrelative\\nimprovement\\nin\\nEuclidean distance.\\nEach\\nAgent\\nis\\na\\nPOMDP\\nhas\\nits\\nown reward. Collab-\\nDQN:\\nreduce\\nthe\\ncomputational load\\nColab DQN got bet-\\nter results than su-\\npervised\\nCNN\\nand\\nDQN\\nBrain\\nMRI\\nlandmark [158],\\nCardiac\\nMRI\\nlandmark\\n[70],\\nFetal\\nbrain\\nlandmark [10].\\nAvailable code\\nMLD\\n[161]\\n2020\\nDQN\\n6 action 2\\nper axis\\nState:\\n3D\\nimage\\npatch. Reward: Eu-\\nclidean distance and\\n∈[−1, 1]. Backbone\\nCNN is share among\\nagents\\nEach\\nagent\\nhas it own Fully con-\\nnected layer\\nDetection\\nerror\\nin-\\ncreased as the degree\\nof\\nmissing\\ninforma-\\ntion increased Perfor-\\nmance is aﬀected by\\nthe choice of land-\\nmarks\\n3D\\nHead\\nMR\\nimages\\nDiﬀerent from the previous works on RL-based landmark detection, which detect a single\\nlandmark,[161] proposed a multiple landmark detection approach to better time-eﬃcient and\\nmore robust to missing data. In their approach, each landmark is guided by one agent. The\\nMDP is models as follows: The state is deﬁned as a 3D image patch. The reward, clipped in\\n[-1, +1], is deﬁned as the diﬀerence in the Euclidean distance between the landmark predicted\\nin the previous time step and the target, and in the landmark predicted in the current time\\nstep and the target. The action space is deﬁned as in other previous works i.e. there are 6\\nactions ax+, ax−, ay+, ay−, az+, az−in the action space. To enable the agents to share the\\ninformation learned by detecting one landmark for use in detecting other landmarks, hard\\nparameter sharing from multi-task learning is used. In this work, the backbone network is\\nshared among agents and each agent has its own fully connected layer.\\nTable 3 summarizes and compares all approaches for DRL in landmark detection, and\\na basic implementation of landmark detection using DRL has been shown in Fig. 11. The\\nﬁgure illustrates a general implementation of landmark detection with the help of DRL, where\\nthe state is the Region of interest (ROI) around the current landmark location cropped from\\nthe image, The actions performed by the DRL agent are responsible for shifting the ROI\\nacross the image forming a new state and the reward corresponds to the improvement in\\n25\\neuclidean distance between ground truth and predicted landmark location with iterations as\\nused by [105],[7],[10],[377],[161].\\nFigure 11: DRL implementation for landmark detection, The red point corresponds to the\\ncurrent landmark location and Red box is the Region of Interest (ROI) centered around the\\nlandmark, the actions of DRL agent shift the ROI across the image to maximize the re-\\nward corresponding to the improvement in distance between the ground truth and predicted\\nlandmark location.\\n6\\nDRL in Object Detection\\nObject detection is a task that requires the algorithm to ﬁnd bounding boxes for all objects\\nin a given image. Many attempts have been made towards object detection. A method for\\nbounding box prediction for object detection was proposed by [109], in which the task was\\nperformed by extracting region proposals from an image and then feeding each of them to\\na CNN to classify each region. An improvement to this technique was proposed by [108],\\nwhere they used the feature from the CNN to propose region proposals instead of the image\\nitself, this resulted in fast detection. Further improvement was proposed by [309], where the\\nauthors proposed using a region proposal network (RPN) to identify the region of interest,\\nresulting in much faster detection. Other attempts including focal loss [225] and Fast YOLO\\n[332] have been proposed to address the imbalanced data problem in object detection with\\nfocal loss [225], and perform object detection in video on embedded devices in a real-time\\nmanner [332].\\nConsidering MDP as the framework for solving the problem, [43] used DRL for active\\nobject localization. The authors considered 8 diﬀerent actions (up, down, left, right, bigger,\\nsmaller, fatter, taller) to improve the ﬁt of the bounding box around the object and additional\\naction to trigger the goal state. They used a tuple of feature vector and history of actions\\nfor state and change in IOU across actions as a reward.\\n26\\nAn improvement to [43] was proposed by [25], where the authors used a hierarchical\\napproach for object detection by treating the problem of object detection as an MDP. In\\ntheir method, the agent was responsible to ﬁnd a region of interest in the image and then\\nreducing the region of interest to ﬁnd smaller regions from the previously selected region and\\nhence forming a hierarchy. For the reward function, they used the change in Intersection over\\nunion (IOU) across the actions and used DQN as the agent. As described in their paper, two\\nnetworks namely, Image-zooms and Pool45-crops with VGG-16 [340] backbone were used to\\nextract the feature information that formed the state for DQN along with a memory vector\\nof the last four actions.\\nUsing a sequential search strategy, [251] proposed a method for object detection using\\nDRL. The authors trained the model with a set of image regions where at each time step\\nthe agent returned ﬁxate actions that speciﬁed a location in image for actor to explore next\\nand the terminal state was speciﬁed by done action. The state consisted of a tuple three\\nelements: the observed region history Ht, selected evidence region history Et and ﬁxate\\nhistory Ft. The fixate action was also a tuple of three elements: fixate action, index of\\nevidence region et and image coordinate of next ﬁxate zt. The done action consisted of: done\\naction, index of region representing the detected output bt and the detection conﬁdence ct.\\nThe authors deﬁned the reward function that was sensitive to the detection location, the\\nconﬁdence at the ﬁnal state and incurs a penalty for each region evaluation.\\nTo map the inter-dependencies among the diﬀerent objects, [170] proposed a tree-structured\\nRL agent (Tree-RL) for object localization by considering the problem as an MDP. The au-\\nthors in their implementation considered actions of two types: translation and scaling, where\\nthe scaling consisted of ﬁve actions whereas translation consisted of eight actions. In the\\nspeciﬁed work, the authors used the state as a concatenation of the feature vector of the\\ncurrent window, feature vector of the whole image, and history of taken actions. The feature\\nvector were extracted from an ImageNet [72] [320] trained VGG-16 [340] model and for re-\\nward the change in IOU across an action was used. Tree-RL utilized a top-down tress search\\nstarting from the whole image where each window recursively takes the best action from\\neach action group which further gives two new windows. This process is repeated recursively\\nto ﬁnd the object.\\nThe task of breast lesion detection is a challenging yet very important task in the medical\\nimaging ﬁeld. A DRL method for active lesion detection in the breast was proposed by [246],\\nwhere the authors formulated the problem as an MDP. In their formulation, a total of nine\\nactions consisting of 6 translation actions, 2 scaling actions, and 1 trigger action were used.\\nIn the speciﬁed work, the change in dice coeﬃcient across an action was used as the reward for\\nscaling and translation actions, and for trigger action, the reward was +η for dice coeﬃcient\\ngreater than rw and −η otherwise, where η and rw were the hyperparameters chosen by the\\nauthors. For network structure, ResNet [133] was used as the backbone and DQN as the\\nagent.\\nDiﬀerent from the previous methods, [386] proposed a method for multitask learning using\\nDRL for object localization. The authors considered the problem as an MDP where the agent\\nwas responsible to perform a series of transformations on the bounding box using a series\\n27\\nof actions. Utilizing an RL framework the states consisted of feature vector and historical\\nactions concatenated together, and a total of 8 actions for Bounding box transformation\\n(left, right, up, down, bigger, smaller, fatter, and taller) were used. For reward the authors\\nused the change in IOU between actions, the reward being 0 for an increase in IOU and -1\\notherwise. For terminal action, however, the reward was 8 for IOU greater than 0.5 and -8\\notherwise. The authors in the paper used DQN with multitask learning for localization and\\ndivided terminal action and 8 transformation actions into two networks and trained them\\ntogether.\\nAn improvement for the Region proposal networks that greedily select the ROIs was\\nproposed by [295], where they used RL for the task.\\nThe authors in this paper used a\\ntwo-stage detector similar to Fast and Faster R-CNN But used RL for the decision-making\\nProcess. For the reward, they used the normalized change in Intersection over Union (IOU).\\nInstead of learning a policy from a large set of data, [15] proposed a method for bounding\\nbox reﬁnement (BAR) using RL. In the paper, once the authors have an inaccurate bounding\\nbox that is predicted by some algorithm they use the BAR algorithm to predict a series of\\nactions for reﬁnement of a bounding box. They considered a total of 8 actions (up, down,\\nleft, right, wider, taller, fatter, thinner) for bounding box transformation and considered\\nthe problem as a sequential decision-making problem (SDMP). They proposed an oﬄine\\nmethod called BAR-DRL and an online method called BAR-CB where training is done on\\nevery image. In BAR-DRL the authors trained a DQN over the states which consisted of\\nfeatures extracted from ResNet50 [133] [354] pretrained on ImageNet [72] [320] and a history\\nvector of 10 actions. The Reward for BAR-DRL was 1 if the IOU increase after action and -3\\notherwise. For BAR-CB they adapted the LinUCB [216] algorithm for an episodic scenario\\nand considered The Histogram of Oriented Gradients (HOG) for the state to capture the\\noutline and edges of the object of interest. The actions in the online method (BAR-CB) were\\nthe same as the oﬄine method and the reward was 1 for increasing IOU and 0 otherwise.\\nFor both the implementations, the authors considered β as terminal IOU.\\nAn improvement to sequential search strategy by [251] was proposed by [367], where\\nthey used a framework consisting of two modules, Coarse and ﬁne level search. According to\\nthe authors, this method is eﬃcient for object detection in large images (dimensions larger\\nthan 3000 pixels). The authors ﬁrst performed a course level search on a large image to\\nﬁnd a set of patches that are used by ﬁne level search to ﬁnd sub-patches. Both ﬁne and\\ncoarse levels were conducted using a two-step episodic MDP, where The policy network\\nwas responsible for returning the probability distribution of all actions. In the paper, the\\nauthors considered the actions to be the binary action array (0,1) where 1 means that the\\nagent would consider acquiring sub-patches for that particular patch. The authors in their\\nimplementation considered a number of patches and sub-patches as 16 and 4 respectively\\nand used the linear combination of Racc (detection recall) and Rcost which combines image\\nacquisition cost and run-time performance reward.\\nTable 4: Comparing various DRL-based object detection methods\\n28\\nApproaches\\nYear Training\\nTech-\\nnique\\nActions\\nRemarks\\nBackbone Performance\\nDatasets and\\nsource code\\nActive\\nObject\\nLocal-\\nization\\n[43]\\n2015 DQN\\n8\\nac-\\ntions: up,\\ndown,\\nleft,\\nright,\\nbigger,\\nsmaller,\\nfatter,\\ntaller\\nStates:\\nfeature\\nvector\\nof\\nob-\\nserved\\nregion\\nand\\naction\\nhis-\\ntory.\\nReward:\\nChange in IOU.\\n5 layer\\npre-\\ntrained\\nCNN\\nHigher\\nmAP\\nas compared to\\nmethods that did\\nnot\\nuse\\nregion\\nproposals\\nlike\\nMultiBox\\n[89],\\nRegionLets [433],\\nDetNet\\n[356],\\nand second best\\nmAP\\nas\\ncom-\\npared to R-CNN\\n[109]\\nPascal VOC-\\n2007\\n[90],\\n2012 [91] Im-\\nage Dataset.\\nHierarchical\\nObject\\nDetection\\n[25]\\n2016 DQN\\n5 actions:\\n1\\naction\\nper image\\nquarter\\nand\\n1\\nat\\nthe\\ncenter\\nStates:\\ncurrent\\nregion and mem-\\nory vector using\\nImage-zooms\\nand\\nPool45-\\ncrops.\\nReward:\\nchange in IOU.\\nVGG-\\n16\\n[340]\\nObjects detected\\nwith\\nvery\\nfew\\nregion proposals\\nper image\\nPascal VOC-\\n2007\\nImage\\nDataset [90].\\nAvailable Code\\nVisual Ob-\\nject Detec-\\ntion [251]\\n2016 Policy\\nsam-\\npling\\nand\\nstate\\ntran-\\nsition\\nalgo-\\nrithm\\n2 actions:\\nﬁxate\\nand done,\\nwhere\\neach is a\\ntuple\\nof\\nthree.\\nStates: Observed\\nregion\\nhistory,\\nevidence\\nregion\\nhistory\\nand\\nﬁxate\\nhistory.\\nReward:\\nsensi-\\ntive to detection\\nlocation\\nDeep\\nNN\\n[187]\\nComparable\\nmAP and lower\\nrun time as com-\\npared\\nto\\nother\\nmethods\\nsuch\\nas to exhaustive\\nsliding\\nwindow\\nsearch(SW),\\nex-\\nhaustive\\nsearch\\nover the CPMC\\nand region pro-\\nposal\\nset(RP)\\n[112] [366]\\nPascal\\nVOC\\n2012\\nObject\\ndetection\\nchallenge [91].\\nTree-\\nStructured\\nSequential\\nObject Lo-\\ncalization\\n(Tree-RL)\\n[170]\\n2016 DQN\\n13\\nac-\\ntions:\\n8\\ntransla-\\ntion,\\n5\\nscaling.\\nStates:\\nFeature\\nvector\\nof\\ncur-\\nrent region, and\\nwhole\\nimage.\\nReward:\\nchange\\nin IOU.\\nCNN\\ntrained\\non Im-\\nageNet\\n[72]\\n[320]\\nTree-RL\\nwith\\nfaster\\nR-CNN\\noutperformed\\nRPN\\nwith\\nfast\\nR-CNN\\n[108]\\nin terms of AP\\nand\\ncomparable\\nresults to Faster\\nR-CNN [309]\\nPascal\\nVOC\\n2007 [90] and\\n2012 [91].\\n29\\nActive\\nBreast\\nLesion\\nDetection\\n[246]\\n2017 DQN\\n9 actions:\\n6\\ntrans-\\nlation,\\n2\\nscaling, 1\\ntrigger\\nStates:\\nfeature\\nvector of current\\nregion,\\nReward:\\nimprovement\\nin\\nlocalization.\\nResNet\\n[133]\\nComparable true\\npositive\\nand\\nfalse\\npositive\\nproportions\\nas\\ncompared\\nto\\nSL\\n[253]\\nand\\nMs-C [116], but\\nwith lesser mean\\ninference time.\\nDCE-MRI\\nand\\nT1-\\nweighted\\nanatomical\\ndataset [253]\\nMultitask\\nobject\\nlo-\\ncalization\\n[386]\\n2018 DQN\\n8 actions:\\nleft,\\nright, up,\\ndown,\\nbigger,\\nsmaller,\\nfatter\\nand taller\\nStates:\\nfeature\\nvector, historical\\nactions. Reward:\\nchange in IOU.\\ndiﬀerent network\\nfor\\ntransforma-\\ntion actions and\\nterminal actions.\\nPretrained\\nVGG-\\n16\\n[340]\\nwith\\nIma-\\ngeNet\\n[72]\\n[320]\\nBetter mAP as\\ncompared\\nto\\nMultiBox\\n[89],\\nCaicedo\\net\\nal.\\n[43] and second\\nbest to R-CNN\\n[109].\\nPascal VOC-\\n2007\\nImage\\nDataset [90].\\nBounding-\\nBox\\nAu-\\ntomated\\nReﬁne-\\nment\\n[15]\\n2020 DQN\\n8\\nac-\\ntions: up,\\ndown,\\nleft,\\nright,\\nbigger,\\nsmaller,\\nfatter,\\ntaller\\nOﬄine\\nand\\nonline implemen-\\ntation\\nStates:\\nfeature\\nvector\\nfor oﬄine (BAR-\\nDRL), HOG for\\nonline\\n(BAR-\\nCB).\\nReward:\\nchange in IOU\\nResNet50\\n[133]\\nBetter ﬁnal IOU\\nfor boxes gener-\\nated\\nby\\nmeth-\\nods such as Reti-\\nnaNet [225].\\nPascal VOC-\\n2007\\n[90],\\n2012 [91] Im-\\nage Dataset.\\nEﬃcient\\nObject\\nDetection\\nin\\nLarge\\nImages\\n[367]\\n2020 DQN\\nbinary\\naction\\narray:\\nwhere\\n1\\nmeans\\nthat\\nthe\\nagent\\nwould\\nconsider\\nacquiring\\nsub-\\npatches\\nfor\\nthat\\npar-\\nticular\\npatch\\nCourse\\nCPNet\\nand ﬁne FPNet\\nlevel\\nsearch.\\nStates:\\nselected\\nregion.\\nReward:\\ndetection\\nrecall\\nimage\\nacquisi-\\ntion cost. Policy:\\nREINFORCE\\n[351]\\nResNet32\\n[133]\\nfor\\npolicy\\nnet-\\nwork.\\nand\\nYOLOv3\\n[306]\\nwith\\nDarkNet-\\n53\\nfor\\nObject\\ndetec-\\ntor\\nHigher mAP and\\nlower\\nrun\\ntime\\nas compared to\\nother\\nmethods\\nsuch as [99].\\nCaltech\\nPedestrian\\ndataset\\n(CPD) [77]\\nAvailable Code\\n30\\nOrgan Lo-\\ncalization\\nin\\nCT\\n[275]\\n2020 DQN\\n11\\nac-\\ntions:\\n6\\ntransla-\\ntion,\\n2\\nscaling, 3\\ndeforma-\\ntion\\nStates: region in-\\nside the Bound-\\ning box. Reward:\\nchange in IOU.\\nArchitecture\\nsimilar\\nto [10]\\nLower\\ndistance\\nerror\\nfor\\norgan\\nlocalization\\nand\\nrun time as com-\\npared\\nto\\nother\\nmethods such as\\n3D-RCNN\\n[409]\\nand CNNs [152]\\nCT\\nscans\\nfrom\\nthe\\nVISCERAL\\ndataset [171]\\nMonocular\\n3D Object\\nDetection\\n[231]\\n2020 DQN\\n[264]\\n15\\nac-\\ntions,\\neach\\nmodiﬁes\\nthe\\n3D\\nbounding\\nbox in a\\nspeciﬁc\\nparame-\\nter\\nState:\\n3D\\nbounding\\nbox\\nparameters,\\n2D\\nimage\\nof\\nob-\\nject\\ncropped\\nby\\n2D\\nits\\nde-\\ntected bounding\\nbox.\\nReward:\\naccuracy\\nim-\\nprovement\\nafter\\napplying\\nan\\naction.\\nResNet-\\n101\\n[133]\\nHigher\\naverage\\nprecision\\n(AP)\\ncompared\\nto\\n[268], [302], [210]\\nand [35]\\nKITTI [102]\\nLocalization of organs in CT scans is an important pre-processing requirement for taking\\nthe images of an organ, planning radiotherapy, etc. A DRL method for organ localization was\\nproposed by [275], where the problem was formulated as an MDP. In the implementation,\\nthe agent was responsible for predicting a 3D bounding box around the organ. The authors\\nused the last 4 states as input to the agent to stabilize the search and the action space\\nconsists of Eleven actions, 6 for the position of the bounding box, 2 for zoom in and zoom\\nout the action, and last 3 for height, width, and depth. For Reward, they used the change\\nthe in Intersection over union (IOU) across an action.\\nMonocular 3D object detection is a problem where 3D bounding boxes of objects are\\nrequired to be detected from a single 2D image. Even the sampling-based method is the\\nSOTA approach, it has a huge ﬂaw, in which most of the samples it generates do not\\noverlap with the groundtruth. To leverage that method, [231] introduced Reinforced Axial\\nReﬁnement Network (RARN) for monocular 3D object detection by utilizing an RL model to\\niteratively reﬁning the sampled bounding box to be more overlapped with the groundtruth\\nbounding box. Given a state having the coordinates of the 3D bounding box and image\\npatch of the image, the model predicts an action out of a set of 15 actions to reﬁne one of\\nthe bounding box coordinates in a direction at every timestep, the model is trained by DQN\\nmethod with the immediate reward is the improvement in detection accuracy between every\\npair of timesteps. The whole pipeline, namely RAR-Net, was evaluated on the real-world\\nKITTI dataset [102] and achieved state-of-the-art performance.\\nAll these methods have been summarised and compared in Table 4, and a basic imple-\\nmentation of object detection using DRL has been shown in Fig. 12. The ﬁgure illustrates\\na general implementation of object detection using DRL, where the state is an image seg-\\nment cropped using a bounding box produced by some other algorithm or previous iteration\\nof DRL, actions predicted by the DRL agent predict a series of bounding box transforma-\\n31\\ntion to ﬁt the object better, hence forming a new state and Reward is the improvement in\\nIntersection over union (IOU) with iterations as used by [43],[25],[15],[386],[170],[275].\\nFigure 12: DRL implementation for object detection.\\nThe red box corresponds to the\\ninitial bounding box which for t=0 is predicted by some other algorithm or the transformed\\nbounding box by previous iterations of DRL using the actions to maximize the improvement\\nin IOU.\\n7\\nDRL in Object Tracking\\nReal-time object tracking has a large number of applications in the ﬁeld of autonomous\\ndriving, robotics, security, and even in sports where the umpire needs accurate estimation of\\nball movement to make decisions. Object tracking can be divided into two main categories:\\nSingle object tracking (SOT) and Multiple object tracking (MOT).\\nMany attempts have been made for both SOT and MOT. SOT can be divided into two\\ntypes, active and passive. In passive tracking it is assumed that the object that is being\\ntracked is always in the camera frame, hence camera movement is not required. In active\\ntracking, however, the decision to move the camera frame is required so that the object is\\nalways in the frame. Passive tracking has been performed by [397], [146], where [146] per-\\nformed tracking for both single and multiple objects. The authors of these papers proposed\\nvarious solutions to overcome common problems such as a change in lighting and occlusion.\\nActive tracking is a little bit harder as compared to a passive one because additional deci-\\nsions are required for camera movement. Some eﬀorts towards active tracking include [74]\\n[270] [178]. These solutions treat object detection and object tracking as two separate tasks\\nand tend to fail when there is background noise.\\nAn end-to-end active object tracker using DRL was proposed by [240], where the authors\\nused CNNs along with an LSTM [139] in their implementation. They used the actor-critic\\nalgorithm [262] to calculate the probability distribution of diﬀerent actions and the value\\n32\\nof state and used the object orientation and distance from the camera to calculate rewards.\\nFor experiments, the authors used VizDoom and Unreal Engine as the environment.\\nAnother end-to-end method for SOT using sequential search strategy and DRL was\\nproposed by [418].\\nThe method included using an RNN along with REINFORCE [392]\\nalgorithm to train the network. The authors used a function f(W0) that takes in St and\\nframe as input, where St is the object location for the ﬁrst frame and is zero elsewhere. The\\noutput is fed to an LSTM module [139] with past hidden state ht. The authors calculated the\\nreward function by using insertion over union (IoU) and the diﬀerence between the average\\nand max.\\nA deformable face tracking method that could predict bounding box along with facial\\nlandmarks in real-time was proposed by [118].\\nThe dual-agent DRL method (DADRL)\\nmentioned in the paper consisted of two agents: a tracking and an alignment agent. The\\nproblem of object tracking was formulated as an MDP where state consisted of image regions\\nextracted by the bounding box and a total of 8 actions (left, right, up, down, scale-up, scale\\ndown, stop and continue) were used, where ﬁrst six consists of movement actions used by\\ntracking agent and last two for alignment agent.\\nThe tracking agent is responsible for\\nchanging the current observable region and the alignment agent determines whether the\\niteration should be terminated.\\nFor the tracking agent, the reward corresponded to the\\nmisalignment descent and for the alignment agent the reward was +η for misalignment\\nless than the threshold and −η otherwise. The DADRL implementation also consisted of\\ncommunicated message channels beside the tracking agent and the alignment agent. The\\ntracking agent consisted of a VGG-M [340] backbone followed by a one-layer Q-Network\\nand the alignment agent was designed as a combination of a stacked hourglass network\\nwith a conﬁdence network. The two communicated message channels were encoded by a\\ndeconvolution layer and an LSTM unit [139] respectively.\\nVisual object tracking when dealing with deformations and abrupt changes can be a\\nchallenging task. A DRL method for object tracking with iterative shift was proposed by\\n[308]. The approach (DRL-IS) consisted of three networks: The actor network, the prediction\\nnetwork, and the critic network, where all three networks shared the same CNN and a fully\\nconnected layer. Given the initial frame and bounding box, the cropped frame is fed to the\\nCNNs to extract the features to be used as a state by the networks. The actions included\\ncontinue, stop and update, stop and ignore, and restart. For continue, the bounding boxes\\nare adjusted according to the output of the prediction network, for stop and update the\\niteration is stopped and the appearance feature of the target is updated according to the\\nprediction network, for stop and ignore the updating of target appearance feature is ignored\\nand restart means that the target is lost and the algorithm needs to start from the initial\\nbounding box. The authors of the paper used reward as 1 for change in IoU greater than\\nthe threshold, 0 for change in IOU between + and - threshold, and -1 otherwise.\\nConsidering the performance of actor-critic framework for various applications, [45] pro-\\nposed an actor-critic [262] framework for real-time object tracking. The authors of the paper\\nused a pre-processing function to obtain an image patch using the bounding box that is fed\\ninto the network to ﬁnd the bounding box location in subsequent frames. For actions the\\n33\\nauthors used △x for relative horizontal translation, △y for relative vertical translation, and\\n△s for relative scale change, and for a reward they used 1 for IoU greater than a threshold\\nand -1 otherwise. They proposed oﬄine training and online tracking, where for oﬄine train-\\ning a pre-trained VGG-M [340] was used as a backbone, and the actor-critic network was\\ntrained using the DDPG approach [224].\\nAn improvement to [45] for SOT was proposed by [84], where a visual tracker was for-\\nmulated using DRL and an expert demonstrator. The authors treated the problem as an\\nMDP, where the state consists of two consecutive frames that have been cropped using the\\nbounding box corresponding to the former frame and used a scaling factor to control the\\noﬀset while cropping. The actions consisted of four elements: △x for relative horizontal\\ntranslation, △y for relative vertical translation, △w for width scaling, and △h for height\\nscaling, and the reward was calculated by considering whether the IoU is greater than a\\nthreshold or not. For the agent architecture the authors used a ResNet-18 [133] as backbone\\nfollowed by an LSTM unit [391][139] to encode past information, and performed training\\nbased on the on-policy A3C framework [262].\\nIn MOT the algorithm is responsible to track trajectories of multiple objects in the given\\nvideo. Many attempts have been made with MOT including [53], [55] and [143]. However,\\nMOT is a challenging task because of environmental constraints such as crowding or object\\noverlapping. MOT can be divided into two main techniques: Oﬄine [53] and Online [55]\\n[143]. In oﬄine batch, tracking is done using a small batch to obtain tracklets and later\\nall these are connected to obtain a complete trajectory. The online method includes using\\npresent and past frames to calculate the trajectory. Some common methods include Kalman\\nﬁltering [177], Particle Filtering [284] or Markov decision [401]. These techniques however\\nare prone to errors due to environmental constraints.\\nTo overcome the constraints of MOT by previous methods, [401] proposed a method for\\nMOT where the problem was approached as an MDP. The authors tracked each object in\\nthe frame through the Markov decision process, where each object has four states consisting:\\nActive, Tracked, Lost, and Inactive.\\nObject detection is the active state and when the\\nobject is in the lost state for a suﬃcient amount of time it is considered Inactive, which is\\nthe terminal state. The reward function in the implementation was learned through data by\\ninverse RL problem [279].\\nPrevious approaches for MOT follow a tracking by detection technique that is prone\\nto errors.\\nAn improvement was proposed by [307], where detection and tracking of the\\nobjects were carried out simultaneously. The authors used a collaborative Q-Network to\\ntrack trajectories of multiple objects, given the initial position of an object the algorithm\\ntracked the trajectory of that object in all subsequent frames. For actions the authors used\\n△x for relative horizontal translation, △y for relative vertical translation, △w for width\\nscaling, and △h for height scaling, and the reward consisted of values 1,0,-1 based on the\\nIoU.\\nAnother method for MOT was proposed by [168], where the authors used LSTM [139]\\nand DRL to approach the problem of multi-object tracking. The method described in the\\npaper used three basic components: a YOLO V2 [260] object detector, many single object\\n34\\ntrackers, and a data association module. Firstly the YOLO V2 object detector is used to\\nﬁnd objects in a frame, then each detected object goes through the agent which consists of\\nCNN followed by an LSTM to encode past information for the object. The state consisted\\nof the image patch and history of past 10 actions, where six actions (right, left, up, down,\\nscale-up, scale down) were used for bounding box movement across the frame with a stop\\naction for the terminal state. To provide reinforcement to the agent the reward was 1 if the\\nIOU is greater than a threshold and 0 otherwise. In their experiments, the authors used\\nVGG-16 [340] for CNN backbone and performed experiments on MOT benchmark [201] for\\npeople tracking.\\nTable 5: Comparing various DRL-based object tracking methods.\\nThe First group for Single object tracking (SOT) and the second\\ngroup for multi-object tracking (MOT)\\nApproaches\\nYear\\nTraining\\nTech-\\nnique\\nActions\\nRemarks\\nBackbone\\nPerformance\\nDatasets and\\nSource code\\nEnd\\nto\\nend\\nac-\\ntive\\nobject\\ntracking\\n[240]\\n2017\\nActor-\\nCritic\\n(a3c)\\n[262]\\n6 actions:\\nturn\\nleft, turn\\nright,\\nturn\\nleft\\nand move\\nforward,\\nturn right\\nand move\\nforward,\\nmove\\nforward,\\nno-op\\nEnvironment:\\nvirtual environ-\\nment.\\nReward:\\ncalculated\\nusing\\nobject\\norientation\\nand\\nposition.\\nTracking Using\\nLSTM [139]\\nConvNet-\\nLSTM\\nHigher\\nac-\\ncumulated\\nreward\\nand\\nepisode\\nlength\\nas\\ncompared\\nto\\nmethods\\nlike\\nMIL\\n[17],\\nMeanshift\\n[60],\\nKCF [134].\\nViZDoom\\n[176],\\nUnreal\\nEngine\\nDRL for ob-\\nject\\ntrack-\\ning [418]\\n2017\\nDRLT\\nNone\\nState:\\nfeature\\nvector, Reward:\\nchange in IOU\\nuse\\nof\\nLSTM\\n[139] and REIN-\\nFORCE [392]\\nYOLO\\nnetwork\\n[305]\\nHigher\\narea\\nunder\\ncurve\\n(success rate Vs\\noverlap\\nthresh-\\nold),\\nprecision\\nand speed (fps)\\nas compared to\\nSTUCK\\n[126]\\nand DLT [384].\\nObject track-\\ning\\nbench-\\nmark [397].\\nAvailable Code\\n35\\nDual-agent\\ndeformable\\nface tracker\\n[118]\\n2018\\nDQN\\n8 actions:\\nleft,\\nright, up,\\ndown,\\nscale\\nup, scale\\ndown,\\nstop and\\ncontinue.\\nStates:\\nimage\\nregion\\nusing\\nBounding\\nbox.\\nReward:\\ndis-\\ntance\\nerror.\\nFacial landmark\\ndetection\\nand\\ntracking\\nusing\\nLSTM [139]\\nVGG-M\\n[340]\\nLower\\nnormal-\\nized\\npoint\\nto\\npoint error for\\nlandmarks\\nand\\nhigher\\nsuccess\\nrate\\nfor\\nfacial\\ntracking\\nas\\ncompared\\nto\\nICCR\\n[187],\\nMDM\\n[336],\\nXiao et al [32],\\netc.\\nLarge-scale\\nface\\ntracking\\ndataset,\\nthe\\n300-VW\\ntest\\nset [336]\\nTracking\\nwith\\niter-\\native\\nshift\\n[308]\\n2018\\nActor-\\ncritic\\n[262]\\n4 actions:\\ncontinue,\\nstop and\\nupdate,\\nstop\\nand\\nig-\\nnore and\\nrestart\\nStates:\\nim-\\nage\\nregion\\nusing bounding\\nbox.\\nReward:\\nchange in IOU.\\nThree networks:\\nactor,\\ncritic\\nand\\nprediction\\nnetwork\\n3\\nLayer\\nCNN and\\nFC layer\\nHigher\\narea\\nunder\\ncurve\\nfor success rate\\nVs\\noverlap\\nthreshold\\nand\\nprecision\\nVs\\nlocation\\nerror\\nthreshold\\nas\\ncompared\\nto\\nCREST\\n[345],\\nADNet\\n[416],\\nMDNet\\n[273],\\nHCFT\\n[243],\\nSINT\\n[358],\\nDeepSRDCF\\n[67], and HDT\\n[301]\\nOTB-2015\\n[398], Temple-\\nColor\\n[220],\\nand\\nVOT-\\n2016 Dataset\\n[186]\\nTracking\\nwith actor-\\ncritic [45]\\n2018\\nActor-\\ncritic\\n[262]\\n3 actions:\\n△x,\\n△y\\nand △s\\nStates:\\nimage\\nregion\\nusing\\nbounding\\nbox.\\nReward:\\nIOU\\ngreater\\nthen\\nthreshold.\\nOf-\\nﬂine\\ntraining,\\nonline tracking\\nVGG-M\\n[340]\\nHigher\\naverage\\nprecision\\nscore\\nthen PTAV [93],\\nCFNet\\n[368],\\nACFN\\n[52],\\nSiameFC\\n[29],\\nECO-HC\\n[67],\\netc.\\nOTB-2013\\n[397],\\nOTB-\\n2015\\n[398]\\nand\\nVOT-\\n2016\\ndataset\\n[186]\\nAvailable Code\\n36\\nVisual\\ntracking\\nand expert\\ndemon-\\nstrator\\n[84]\\n2019\\nActor-\\ncritic\\n(a3c)\\n[262]\\n4 actions:\\n△x,\\n△y,△w\\nand △h\\nStates:\\nimage\\nregion\\nusing\\nbounding\\nbox.\\nReward: change\\nin\\nIOU.\\nSOT\\nusing\\nLSTM\\n[391][139]\\nResNet-\\n18 [133]\\nComparable\\nsuccess\\nand\\nprecision scores\\nas compared to\\nLADCF\\n[408],\\nSiamRPN [209]\\nand ECO [66]\\nGOT-10k\\n[148], LaSOT\\n[92], UAV123\\n[269],\\nOTB-\\n100\\n[397],\\nVOT-2018\\n[185]\\nand\\nVOT-2019.\\nObject\\ntracking\\nby decision\\nmaking\\n[401]\\n2015\\nTLD\\nTracker\\n[174]\\n7 actions:\\ncorre-\\nsponding\\nto\\nmov-\\ning\\nthe\\nobject\\nbetween\\nstates\\nsuch\\nas\\nActive,\\ntracked,\\nlost\\nand\\nInactive\\nStates: 4 states:\\nActive, tracked,\\nlost\\nand\\nInac-\\ntive.\\nReward:\\ninverse\\nRL\\nproblem [279]\\nNone\\nComparable\\nmultiple object\\ntracking\\naccu-\\nracy\\n(MOTA)\\nand\\nmultiple\\nobject\\ntrack-\\ning\\nprecision\\n(MOTP)\\n[28]\\nas compared to\\nDPNMS\\n[296],\\nTCODAL\\n[18],\\nSegTrack [259],\\nMotiCon\\n[200],\\netc\\nM0T15\\ndataset [201]\\nAvailable Code\\nCollaborative\\nmulti\\nob-\\nject tracker\\n[307]\\n2018\\nDQN\\n4 actions:\\n△x, △y,\\n△w\\nand\\n△h\\nStates:\\nimage\\nregion\\nusing\\nbounding\\nbox.\\nReward:\\nIOU\\ngreater\\nthen\\nthreshold.\\n2\\nnetworks:\\npre-\\ndiction\\nand\\ndecision\\nnet-\\nwork\\n3\\nLayer\\nCNN and\\nFC Layer\\nComparable\\nmultiple object\\ntracking\\naccu-\\nracy\\n(MOTA)\\nand\\nmultiple\\nobject\\ntrack-\\ning\\nprecision\\n(MOTP)\\n[28]\\nas\\ncompared\\nto SCEA [143],\\nMDP\\n[401],\\nCDADDALpb\\n[19],\\nAMIR15\\n[321]\\nMOT15 [201]\\nand\\nMOT16\\n[258] datasets\\n37\\nMulti\\nobject\\ntracking in\\nvideo [168]\\n2018\\nDQN\\n6 actions:\\nright,\\nleft,\\nup,\\ndown,\\nscale\\nup, scale\\ndown\\nStates:\\nimage\\nregion\\nusing\\nbounding\\nbox.\\nReward:\\nIOU\\ngreater\\nthen\\nthreshold.\\nDe-\\ntection\\nusing\\nYOLO-V2 [260]\\nfor detector and\\nLSTM [139] .\\nVGG-16\\n[340]\\nComparable\\nif\\nnot\\nbetter\\nmultiple object\\ntracking\\naccu-\\nracy\\n(MOTA)\\nand\\nmultiple\\nobject\\ntrack-\\ning\\nprecision\\n(MOTP)\\n[28]\\nas\\ncompared\\nto RNN-LSTM\\n[201], LP-SSVM\\n[401], MDPSub-\\nCNN [199], and\\nSiameseCNN\\n[123]\\nMOT15\\nDataset [201]\\nMulti agent\\nmulti\\nob-\\nject tracker\\n[169]\\n2019\\nDQN\\n9 actions:\\nmove\\nright,\\nmove left,\\nmove\\nup, move\\ndown,\\nscale\\nup, scale\\ndown,\\nfatter,\\ntaller and\\nstop\\nStates:\\nimage\\nregion\\nusing\\nbounding\\nbox.\\nReward:\\nIOU\\ngreater\\nthen\\nthreshold.\\nYOLO-V3 [306]\\nfor\\ndetection\\nand\\nLSTM\\n[139].\\nVGG-16\\n[340]\\nHigher\\nrun-\\nning time, and\\ncomparable\\nif\\nnot\\nbetter\\nmultiple object\\ntracking\\naccu-\\nracy\\n(MOTA)\\nand\\nmultiple\\nobject\\ntrack-\\ning\\nprecision\\n(MOTP)\\n[28]\\nas\\ncompared\\nto RNN-LSTM\\n[201], LP-SSVM\\n[401], MDPSub-\\nCNN [199], and\\nSiameseCNN\\n[123]\\nMOT15 chal-\\nlenge\\nbench-\\nmark [201].\\nTo address the problems in existing tracking methods such as varying numbers of tar-\\ngets, non-real-time tracking, etc, [169] proposed a multi-object tracking algorithm based on\\na multi-agent DRL tracker (MADRL). In their object tracking pipeline the authors used\\nYOLO-V3 [306] as object detector, where multiple detections produced by YOLO-V3 were\\nﬁltered using the IOU and the selected results were used as multiple agents in multiple agent\\ndetector. The input agents were fed into a pre-trained VGG-16 [340] followed by an LSTM\\nunit [139] that could share information across agents and return the actions encoded in a\\n38\\n9-dimensional vector( move right, move left, move up, move down, scale-up, scale down, as-\\npect ratio change fatter, aspect ratio change taller and stop), also a reward function similar\\nto [168] was used.\\nVarious works in the ﬁeld of object tracking have been summarized in Table 5, and a\\nbasic implementation of object tracking using DRL has been shown in Fig. 13. The ﬁgure\\nillustrates a general implementation of object tracking in videos using DRL, where the state\\nconsists of two consecutive frames (Ft, Ft+1) with a bounding box for the ﬁrst frame produced\\nby another algorithm for the ﬁrst iteration or by the previous iterations of DRL agent. The\\nactions corresponds to the moving the bounding on the image to ﬁt the object in frame Ft+1,\\nhence forming a new state with frame Ft+1 and frame Ft+2 along with the bounding box\\nfor frame Ft+1 predicted by previous iteration and reward corresponds to whether IOU is\\ngreater then a given threshold as used by [118],[308],[45], [84],[307],[168],[169].\\nFigure 13: DRL implementation for object tracking. Here the state consists of two consecu-\\ntive frames with bounding box locations for the ﬁrst frame that is predicted by some object\\ndetection algorithm or by the previous iteration of DRL, the actions move the bounding box\\npresent in the ﬁrst frame to ﬁt the object in the second frame to maximize the reward which\\nis the whether the IOU is greater than a given threshold or not.\\n8\\nDRL in Image Registration\\nImage registration is a very useful step that is performed on 3D medical images for the\\nalignment of two or more images. The goal of 3D medical image registration is to ﬁnd a\\ncorrelation between two images from either diﬀerent patients or the same patients at diﬀerent\\ntimes, where the images can be Computed Tomography (CT), Magnetic Resonance Imaging\\n(MRI), or Positron Emission Tomography (PET). In the process, the images are brought to\\nthe same coordinate system and aligned with each other. The reason for image registration\\nbeing a challenging task is the fact that the two images used may have a diﬀerent coordinate\\nsystem, scale, or resolution.\\nMany attempts have been made toward automated image registration. A multi-resolution\\nstrategy with local optimizers to perform 2D or 3D image registration was performed by\\n39\\n[359]. However, multi-resolution tends to fail with diﬀerent ﬁeld of views. Heuristic semi-\\nglobal optimization schemes were proposed to solve this problem and used by [252] through\\nsimulated annealing and through genetic algorithm [317], However, their cost of computation\\nwas very high. A CNN-based approach to this problem was suggested by [256], and [79]\\nproposed an optical ﬂow method between 2D RGB images. A descriptor learned through a\\nCNN was proposed by [395], where the authors encoded the posture and identity of a 3D\\nobject using the 2D image. Although all of these formulations produce satisfactory results\\nyet, the methods could not be applied directly to 3D medical images.\\nTo overcome the problems faced by previous methods, [238] proposed a method for im-\\nproving probabilistic image registration via RL and uncertainty evaluation. The method in-\\nvolved predicting a regression function that predicts registration error from a set of features\\nby using regression random forests (RRF) [37] method for training. The authors performed\\nexperiments on 3D MRI images and obtained an accuracy improvement of up to 25%.\\nPrevious image registration methods are often customized to a speciﬁc problem and are\\nsensitive to image quality and artifacts. To overcome these problems, [221] proposed a robust\\nmethod using DRL. The authors considered the problem as an MDP where the goal is to ﬁnd\\na set of transformations to be performed on the ﬂoating image to register it on the reference\\nimage. They used the gamma value for future reward decay and used the change in L2\\nNorm between the predicted transformation and ground truth transformation to calculate\\nthe reward. The authors also used a hierarchical approach to solve the problem with varying\\nFOVs and resolutions.\\nTable 6: Comparing various DRL-based image registration meth-\\nods.\\nApproaches\\nYear\\nTraining\\nTech-\\nnique\\nActions\\nRemarks\\nBackbone\\nPerformance\\nDatasets\\nImage\\nreg-\\nistration\\nusing\\nun-\\ncertainity\\nevaluation\\n[238]\\n2013\\nDQN\\nNot spec-\\niﬁed\\nProbabilistic\\nmodel\\nusing\\nregression\\nran-\\ndom\\nforests\\n(RRF) [37]\\nNot spec-\\niﬁed\\nHigher\\nﬁnal\\nDice\\nscore\\n(DSC) as com-\\npared to other\\nmethods\\nlike\\nrandom\\nseed\\nselection\\nand\\ngrid-based seed\\nselection\\n3D\\nMRI\\nimages\\nfrom\\nLONI\\nProb-\\nabilistic\\nBrain\\nAtlas\\n(LPBA40)\\nDataset\\n40\\nRobust\\nImage\\nreg-\\nistration\\n[221]\\n2017\\nDQN\\n12\\nac-\\ntions:\\ncorre-\\nsponding\\nto\\ndif-\\nferent\\ntransfor-\\nmations\\nStates:\\ncurrent\\ntransforma-\\ntion.\\nReward:\\ndistance error.\\n5\\nConv3D\\nlayers\\nand 3 FC\\nlayers\\nBetter\\nsuccess\\nrate then ITK\\n[153],\\nQuasi-\\nglobal\\n[255]\\nand\\nSemantic\\nregistration[277]\\nAbdominal\\nspine\\nCT\\nand\\nCBCT\\ndataset, Car-\\ndiac CT and\\nCBCT\\nMultimodal\\nimage\\nreg-\\nistration\\n[244]\\n2017\\nDuel-\\nDQN\\nDouble-\\nDQN\\nActions\\nupdate\\nthe trans-\\nforma-\\ntions\\non\\nﬂoating\\nimage\\nStates: cropped\\n3D\\nimage.\\nDuel-DQN\\nfor\\nvalue\\nes-\\ntimation\\nand\\nDouble\\nDQN\\nfor\\nupdating\\nweights.\\nBatch\\nnormal-\\nization\\nfollowed\\nby\\n5\\nConv3D\\nand\\n3\\nMaxpool\\nlayers\\nLower\\nEu-\\nclidean\\ndis-\\ntance\\nerror\\nas\\ncompared\\nto\\nmethods\\nlike\\nHausdorﬀ, ICP,\\nDQN\\n[264],\\nDueling\\n[390],\\netc.\\nThorax\\nand\\nAbdomen\\n(ABD)\\ndataset\\nRobust\\nnon-rigid\\nagent-based\\nregistration\\n[184]\\n2017\\nDQN\\n2n\\nac-\\ntions\\nfor\\nn dimen-\\nsional\\nθ\\nvector\\nStates:\\nﬁxed\\nand\\nmoving\\nimage. Reward:\\nchange in trans-\\nformation error.\\nWith\\nStatisti-\\ncal deformation\\nmodel and fuzzy\\naction control.\\nMulti\\nlayer\\nCNN,\\npooling\\nand\\nFC\\nlayers.\\nHigher\\nMean\\nDice score and\\nlower Hausdorﬀ\\ndistance\\nas\\ncompared\\nto\\nmethods\\nlike\\nLCC-Demons\\n[237]\\nand\\nElastix [180].\\nMICCAI\\nchallenge\\nPROMISE12\\n[227]\\nRobust\\nMultimodal\\nregistration\\n[349]\\n2018\\nActor-\\nCritic\\n(a3c)\\n[262]\\n8 actions:\\nfor\\ndif-\\nferent\\ntransfor-\\nmations\\nStates:\\nﬁxed\\nand\\nmoving\\nimage. Reward:\\nDistance\\nerror.\\nMonte-carlo\\nmethod\\nwith\\nLSTM [139].\\nMulti\\nlayer\\nCNN and\\nFC layer\\nComparable\\nif\\nnot\\nlower\\ntarget\\nregistra-\\ntion\\nerror\\n[96]\\nas\\ncompared\\nto\\nmethods\\nlike\\nSIFT\\n[239],\\nElastix\\n[180],\\nPure\\nSL, RL-matrix,\\nRL-LME, etc.\\nCT and MR\\nimages\\nA multi-modal method for image registration was proposed by [244], where the authors\\nused DRL for alignment of depth data with medical images. In the speciﬁed work Duel\\nDQN was used as the agent for estimating the state value and the advantage function, and\\nthe cropped 3D image tensor of both data modalities was considered as the state.\\nThe\\n41\\nalgorithm’s goal was to estimate a transformation function that could align moving images\\nto a ﬁxed image by maximizing a similarity function between the ﬁxed and moving image.\\nA large number of convolution and pooling layer were used to extract high-level contextual\\ninformation, batch normalization and concatenation of feature vector from last convolution\\nlayer with action history vector was used to solve the problem of oscillation and closed loops,\\nand Double DQN architecture for updating the network weights was used.\\nPrevious methods for image registration fail to cope with large deformations and variabil-\\nity in appearance. To overcome these issues [184] proposed a robust non-rigid agent-based\\nmethod for image registration. The method involves ﬁnding a spatial transformation Tθ that\\ncan map the ﬁxed image with the ﬂoating image using actions at each time step, that is\\nresponsible for optimizing θ. If the θ is a d dimensional vector then there will be 2d possi-\\nble actions. In this work, a DQN was used as an agent for value estimation, along with a\\nreward that corresponded to the change in θ distance between ground truth and predicted\\ntransformations across an action.\\nAn improvement to the previous methods was proposed by [349], where the authors used\\na recurrent network with RL to solve the problem. Similar to [221], they considered the\\ntwo images as a reference/ﬁxed and ﬂoating/moving, and the algorithm was responsible\\nfor predicting transformation on the moving image to register it on a ﬁxed image. In the\\nspeciﬁed work an LSTM [139] was used to encode past hidden states, Actor-critic [262] for\\npolicy estimation, and a reward function corresponding to distance between ground truth\\nand transformed predicted landmarks were used.\\nVarious methods in the ﬁeld of Image registration have been summarized and compared\\nin Table 6, and a basic implementation of image registration using DRL has been shown in\\nFig. 14. The ﬁgure illustrates a general implementation of image registration using DRL\\nwhere the state consists of a ﬁxed and ﬂoating image. The DRL agent predicts actions in\\nform of a set of transformations on a ﬂoating image to register it onto the ﬁxed image hence\\nforming a new state and accepts reward in form of improvement in distance error between\\nground truth and predicted transformations with iterations as described by [349],[184],[221].\\n9\\nDRL in Image Segmentation\\nImage segmentation is one of the most extensively performed tasks in computer vision,\\nwhere the algorithm is responsible for labeling each pixel position as foreground or back-\\nground corresponding to the object being segmented in the image. Image segmentation has\\na wide variety of applications in medical, robotics, weather, etc. One of the earlier attempts\\nwith image segmentation includes [125]. With the improvement in detection techniques and\\nintroduction of CNN, new methods are introduced every year for image segmentation. Mask\\nR-CNN [132] extended the work by Faster R-CNN [309] by adding a segmentation layer\\nafter the Bounding box has been predicted. Some earlier works include [109], [127], [128]\\netc. Most of these works give promising results in image segmentation. However, due to\\nthe supervised nature of CNN and R-CNN, these algorithms need a large amount of data.\\nIn ﬁelds like medical, the data is sometimes not readily available hence we needed a way to\\n42\\nFigure 14: DRL implementation for image registration.\\nThe state consists of ﬁxed and\\nﬂoating image and the actions in form of transformations are performed on the ﬂoating\\nimage so as to maximize reward by minimizing distance between the ground truth and\\npredicted transformations.\\ntrain algorithms to perform a given task when there are data constraints. Luckily RL tends\\nto shine when the data is not available in a large quantity.\\nOne of the ﬁrst methods for Image segmentation through RL was proposed by [324],\\nwhere the authors proposed an RL framework for medical image segmentation. In their\\nwork, they used a Q-Matrix, where the actions were responsible for adjusting the threshold\\nvalues to predict the mask and the reward was the normalized change in quality measure\\nbetween action steps. [325] also used a similar technique of Tabular method.\\nTo overcome the constraints of the previous method for segmentation, [310] proposed a\\nmethod for indoor semantic segmentation through RL. In their paper, the authors proposed\\na sequential strategy using RL to combine binary object masks of diﬀerent objects into a\\nsingle multi-object segmentation mask. They formulated the binary mask in a Conditional\\nRandom Field Framework (CRF), and used a logistic regression version of AdaBoost [140] for\\nclassiﬁcation. The authors considered the problem of adding multiple binary segmentation\\ninto one as an MDP, where the state consisted of a list of probability distributions of diﬀer-\\nent objects in an image, and the actions correspond to the selection of object/background\\nsegmentation for a particular object in the sequential semantic segmentation. In the RL\\nframework, the reward was considered in terms of pixel-wise frequency weighted Jaccard\\nIndex computed over the set of actions taken at any stage of an episode.\\nInteractive segmentation is the task of producing an interactive mask for objects in an\\nimage. Most of the previous works in this ﬁeld greatly depend on the distribution of inputs\\nwhich is user-dependent and hence produce inadequate results. An improvement was pro-\\nposed by [343], where the authors proposed SeedNet, an automatic seed generation method\\n43\\nfor robust interactive segmentation through RL. With the image and initial seed points, the\\nalgorithm is capable of generating additional seed points and image segmentation results.\\nThe implementation included Random Walk (RW) [114] as the segmentation algorithm and\\nDQN for value estimation by considering the problem as an MDP. They used the current\\nbinary segmentation mask and image features as the state, the actions corresponded to se-\\nlecting seed points in a sparse matrix of size 20×20(800 diﬀerent actions were possible), and\\nthe reward consisted of the change in IOU across an action. In addition, the authors used\\nan exponential IOU model to capture changes in IOU values more accurately.\\nMost of the previous work for image segmentation fail to produce satisfactory results\\nwhen it comes to 3D medical data. An attempt on 3D medical image segmentation was done\\nby [222], where the authors proposed an iteratively-reﬁned interactive multi-agent method\\nfor 3D medical image segmentation. They proposed a method to reﬁne an initial course\\nsegmentation produced by some segmentation methods using RL, where the state consisted of\\nthe image, previous segmentation probability, and user hint map. The actions corresponded\\nto adjusting the segmentation probability for reﬁnement of segmentation, and a relative\\ncross-entropy gain-based reward to update the model in a constrained direction was used.\\nIn simple words, it is the relative improvement of previous segmentation to the current one.\\nThe authors utilized an asynchronous advantage actor-critic algorithm for determining the\\npolicy and value of the state.\\nTable 7: Comparing various DRL-based image segmentation meth-\\nods\\nApproaches Year\\nTraining\\nTechnique\\nActions\\nRemarks\\nBackbone Performance\\nDatasets\\nSemantic\\nSegmen-\\ntation for\\nindoor\\nscenes[310]\\n2016\\nDQN\\n2\\nac-\\ntions per\\nobject:\\nobject,\\nback-\\nground\\nStates:\\ncurrent\\nprobability\\ndistribution.\\nReward:\\npixel-\\nwise\\nfrequency\\nweighted\\nJac-\\ncard\\nindex.\\nConditional\\nRandom\\nField\\nFramework\\n(CRF) and lo-\\ngistic regression\\nversion of Ad-\\naBoost [140] for\\nclassiﬁcation.\\nNot\\nSpeci-\\nﬁed\\nPixel-wise\\npercentage\\njaccard\\nindex\\ncomparable\\nto\\nGupta-L\\n[121]\\nand\\nGupta-P\\n[120].\\nNYUD\\nV2\\ndataset [338]\\n44\\nSeedNet\\n[343]\\n2018\\nDQN,\\nDouble-\\nDQN,\\nDuel-DQN\\n800\\nac-\\ntions:\\n2\\nper pixel\\nStates:\\nimage\\nfeatures\\nand\\nsegmentation\\nmask.\\nReward:\\nchange in IOU.\\nRandom\\nWalk\\n(RW) [114] for\\nsegmentation\\nalgorithm.\\nMulti\\nlayer\\nCNN\\nBetter\\nIOU\\nthen\\nmethods\\nlike FCN [236]\\nand iFCN [407].\\nMSRA10K\\nsaliency\\ndataset [49]\\nIteratively\\nreﬁned\\nmulti\\nagent\\nsegmen-\\ntation\\n[222]\\n2020\\nActor-critic\\n(a3c) [262]\\n1\\naction\\nper voxel\\nfor\\nad-\\njusting\\nsegmen-\\ntation\\nprobabil-\\nity\\nStates:\\n3D\\nimage\\nsegmen-\\ntation\\nproba-\\nbility and hint\\nmap.\\nReward:\\ncross\\nentropy\\ngain\\nbased\\nframework.\\nR-net\\n[378]\\nBetter\\nperfor-\\nmance\\nthen\\nmethods\\nlike\\nMinCut\\n[183],\\nDeepIGeoS\\n(R-Net)\\n[378]\\nand\\nInterCNN\\n[36].\\nBraTS\\n2015[254],\\nMM-WHS\\n[432]\\nand\\nNCI-ISBI\\n2013\\nChal-\\nlenge [33]\\nMulti-\\nstep\\nmedical\\nimage\\nsegmen-\\ntation\\n[360]\\n2020\\nActor-critic\\n(a3c) [262]\\nActions\\ncontrol\\nthe posi-\\ntion\\nand\\nshape\\nof\\nbrush\\nstroke to\\nmodify\\nsegmen-\\ntation\\nStates:\\nimage,\\nsegmentation\\nmask and time\\nstep.\\nReward:\\nchange\\nin\\ndis-\\ntance\\nerror.\\nPolicy:\\nDPG\\n[339].\\nResNet18\\n[133]\\nHigher\\nMean\\nDice score and\\nlower Hausdorﬀ\\ndistance\\nthen\\nmethods\\nlike\\nGrab-Cut [315],\\nPSPNet\\n[425],\\nFCN\\n[236],\\nU-Net\\n[313],\\netc.\\nProstate MR\\nimage dataset\\n(PROMISE12,\\nISBI2013)\\nand\\nretinal\\nfundus\\nim-\\nage\\ndataset\\n(REFUGE\\nchallenge\\ndataset [285])\\nAnomaly\\nDetection\\nin Images\\n[56]\\n2020\\nREINFORCE\\n[392]\\n9 actions,\\n8 for di-\\nrections\\nto\\nshift\\ncenter\\nof\\nthe\\nextracted\\npatch to,\\nthe\\nlast\\naction\\nis\\nto switch\\nto a ran-\\ndom new\\nimage\\nEnvironment:\\ninput\\nimage\\nto\\nthe\\nmodel.\\nState:\\nob-\\nserved\\npatch\\nfrom the image\\ncentered\\nby\\npredicted center\\nof interest.\\nNone\\nSuperior perfor-\\nmance\\nin\\n[27]\\nand\\n[337]\\non\\nall metrics e.g.\\nprecision, recall\\nand\\nF1\\nwhen\\ncompared\\nwith\\nU-Net\\n[313]\\nand\\nbaseline\\nunsupervised\\nmethod in [27]\\nbut\\nonly\\nwins\\non recall in [44]\\nMVTec\\nAD\\n[27],\\nNan-\\noTWICE\\n[44],\\nCrack-\\nForest [337]\\n45\\nFurther improvement in the results of medical image segmentation was proposed by\\n[360]. The authors proposed a method for multi-step medical image segmentation using RL,\\nwhere they used a deep deterministic policy gradient method (DDPG) based on actor-critic\\nframework [262] and similar to Deterministic policy gradient (DPG) [339]. The authors used\\nResNet18 [133] as backbone for actor and critic network along with batch normalisation\\n[157] and weight normalization with Translated ReLU [400]. In their MDP formulation,\\nthe state consisted of the image along with the current segmentation mask and step-index,\\nand the reward corresponded to the change in mean squared error between the predicted\\nsegmentation and ground truth across an action. According to the paper the action was\\ndeﬁned to control the position and shape of brush stroke used to modify the segmentation.\\nAn example in image segmentation outside the medical ﬁeld is [56] proposing to tackle the\\nproblem of anomalies detection and segmentation in images (i.e. damaged pins of an IC chip,\\nsmall tears in woven fabric). [56] utilizes an additional module to attend only on a speciﬁc\\npatch of the image centered by a predicted center instead of the whole image, this module\\nhelps a lot in reducing the imbalance between normal regions and abnormal locations. Given\\nan image, this module, namely Neural Batch Sampling (NBS), starts from a random initiated\\ncenter and recurrently moves that center by eight directions to the abnormal location in the\\nimage if it exists, and it has an additional action to stop moving the center when it has already\\nconverged to the anomaly location or there is not any anomaly can be observed. The NBS\\nmodule is trained by REINFORCE algorithm [392] and the whole model is evaluated on\\nmultiple datasets e.g. MVTec AD [27], NanoTWICE [44], CrackForest [337].\\nVarious works in the ﬁelds of Image segmentation have been summarised and compared\\nin Table 7, and a basic implementation of image segmentation using DRL has been shown\\nin Fig. 15. The ﬁgure shows a general implementation of image segmentation using DRL.\\nThe states consist of the image along with user hint (landmarks or segmentation mask by\\nthe other algorithm) for the ﬁrst iteration or segmentation mask by the previous iteration.\\nThe actions are responsible for labeling each pixel as foreground and background and reward\\ncorresponds to an improvement in IOU with iterations as used by [343],[222].\\n10\\nDRL in Video Analysis\\nObject segmentation in videos is a very useful yet challenging task in computer vision ﬁeld.\\nVideo object segmentation task focuses on labelling each pixel for each frame as foreground\\nor background. Previous works in the ﬁeld of video object segmentation can be divided\\ninto three main methods.\\nunsupervised [288][402], weakly supervised [48][163] [419] and\\nsemi-supervised [41] [164][292].\\nA DRL-based framework for video object segmentation was proposed by [323], where\\nthe authors divided the image into a group of sub-images and then used the algorithm on\\neach of the sub-image. They proposed a group of actions that can perform to change the\\nlocal values inside each sub-image and the agent received reward based on the change in the\\nquality of segmented object inside each sub-image across an action. In the proposed method\\ndeep belief network (DBN) [47] was used for approximating the Q-values.\\n46\\nFigure 15: DRL implementation for Image segmentation. The state consists of the image to\\nbe segmented along with a user hint for t=0 or the segmentation mask by the previous itera-\\ntions. The DRL agent performs actions by labeling each pixel as foreground and background\\nto maximize the improvement in IOU over the iterations.\\nSurgical gesture recognition is a very important yet challenging task in the computer\\nvision ﬁeld. It is useful in assessing surgical skills and for eﬃcient training of surgeons. A\\nDRL method for surgical gesture classiﬁcation and segmentation was proposed by [228]. The\\nproposed method could work on features extracted by video frames or kinematic data frames\\ncollected by some means along with the ground truth labels. The problem of classiﬁcation\\nand segmentation was considered as an MDP, where the state was a concatenation of TCN\\n[195][199] features of the current frame, 2 future frames a speciﬁed number of frames later,\\ntransition probability of each gesture computed from a statistical language model [311] and\\na one-hot encoded vector for gesture classes. The actions could be divided into two sub-\\nactions, One to decide optimal step size and one for choosing gesture class, and the reward\\nwas adopted in a way that encouraging the agent to adopt a larger step and also penalizes the\\nagent for errors caused by the action. The authors used Trust Region Policy Optimization\\n(TRPO) [326] for training the policy and a spacial CNN [196] to extract features.\\nEarlier approaches for video object segmentation required a large number of actions to\\ncomplete the task.\\nAn Improvement was proposed by [124], where authors used an RL\\nmethod for object segmentation in videos. They proposed a reinforcement cutting-agent\\nlearning framework, where the cutting-agent consists of a cutting-policy network (CPN)\\nand a cutting-execution network (CEN). The CPN learns to predict the object-context box\\npair, while CEN learns to predict the mask based on the inferred object-context box pair.\\nThe authors used MDP to solve the problem in a semi-supervised fashion. For the state of\\nCPN the authors used the input frame information, the action history, and the segmentation\\nmask provided in the ﬁrst frame. The output boxes by CPN were input for the CEN. The\\nactions for CPN network included 4 translation actions (Up, Down, Left, Right), 4 scaling\\n47\\naction (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom), and 1 terminal\\naction (Stop), and the reward corresponded to the change in IOU across an action. For the\\nnetwork architecture, a Fully-Convolutional DenseNet56 [166] was used as a backbone along\\nwith DQN as the agent for CPN and down-sampling followed by up-sampling architecture\\nfor CEN.\\nUnsupervised video object segmentation is an intuitive task in the computer vision ﬁeld.\\nA DRL method for this task was proposed by [111], where the authors proposed a motion-\\noriented unsupervised method for image segmentation in videos (MOREL). They proposed\\na two-step process to achieve the task in which ﬁrst a representation of input is learned to\\nunderstand all moving objects through unsupervised video object segmentation, Then the\\nweights are transferred to the RL framework to jointly train segmentation network along\\nwith policy and value function. The ﬁrst part of the method takes two consecutive frames\\nas input and predicts a number of segmentation masks, corresponding object translations,\\nand camera translations. They used a modiﬁed version of actor-critic [262][329][371] for the\\nnetwork of ﬁrst step. Following the unsupervised fashion, the authors used the approach\\nsimilar to [375] and trained the network to interpolate between consecutive frames and used\\nthe masks and translations to estimate the optical ﬂow using the method that was proposed\\nin Spatial Transformer Networks [159]. They also used structural dissimilarity (DSSIM)\\n[388] to calculate reconstruction loss and actor-critic [262] algorithm to learn policy in the\\nsecond step.\\nA DRL method for dynamic semantic face video segmentation was proposed by [387],\\nwhere Deep Feature Flow [431] was utilized as the feature propagation framework and RL\\nwas used for an eﬃcient and eﬀective scheduling policy.\\nThe method involved dividing\\nframes into key (Ik) and non-key (Ii), and using the last key frame features for performing\\nsegmentation of non-key frame. The actions made by the policy network corresponded to\\ncategorizing a frame as Ik or Ii and the state consisted of deviation information and expert\\ninformation, where the deviation information described the diﬀerence between current Ii and\\nlast Ik and expert information encapsulated the key decision history. The authors utilized\\nFlowNet2-s model [156] as an optical ﬂow estimation function, and divided the network into\\nfeature extraction module and task-speciﬁc module. After policy network which consisted\\nof one convolution layer, 4 fully connected layers and 2 concatenated channels consisting of\\nKAR (Key all ratio: Ratio between key frame and every other frame in decision history) and\\nLKD (Last key distance: Distance between current and last key frame) predicted the action,\\nIf the current frame is categorized as key frame the feature extraction module produced\\nthe frame features and task-speciﬁc module predicted the segmentation, However if the\\nframe is categorized as a non-key frame the features from the last key frame along with\\nthe optical ﬂow was used by the task-speciﬁc module to predict the segmentation.\\nThe\\nauthors proposed two types of reward functions, The ﬁrst reward function was calculated by\\nconsidering the diﬀerence between the IOU for key and non-key actions. The second reward\\nfunction was proposed for a situation when ground truth was not available and was calculated\\nby considering the accuracy score between segmentation for key and non-key actions.\\n48\\nTable 8: Comparing various methods associated with video. First\\ngroup for video object segmentation, second group for action recog-\\nnition and third group for video summarisation\\nApproaches Year\\nTraining\\nTechnique\\nActions\\nRemarks\\nBackbone\\nPerformance\\nDatasets and\\nSource code\\nObject\\nsegmen-\\ntation in\\nvideos[323]\\n2016\\nDeep\\nBelief\\nNetwork\\n[47]\\nActions\\nchanged\\nlocal\\nvalues\\nin\\nsub-\\nimages\\nStates:\\nsub-\\nimages.\\nRe-\\nward: quality of\\nsegmentation.\\nNot spec-\\niﬁed\\nNot speciﬁed\\nNot speciﬁed\\nSurgical\\ngesture\\nsegmen-\\ntation\\nand clas-\\nsiﬁcation\\n[228]\\n2018\\nTrust\\nRegion\\nPolicy Op-\\ntimization\\n(TRPO)\\n[326]\\n2\\ntypes:\\noptimal\\nstep\\nsize\\nand\\ngesture\\nclass\\nStates:\\nTCN\\n[[195], [199]] and\\nfuture\\nframes.\\nReward:\\nen-\\ncourage\\nlarger\\nsteps and min-\\nimize\\naction\\nerrors.\\nStatis-\\ntical\\nlanguage\\nmodel\\n[311]\\nfor\\ngesture\\nprobability.\\nSpacial\\nCNN\\n[196]\\nComparable\\naccuracy,\\nand\\nhigher edit and\\nF1\\nscores\\nas\\ncompared\\nto\\nmethods\\nlike\\nSD-SDL\\n[331],\\nBidir\\nLSTM\\n[76],\\nLC-SC-\\nCRF\\n[197],\\nSeg-ST-CNN\\n[196],\\nTCN\\n[198], etc\\nJIGSAWS\\n[[6],\\n[100]]\\nbenchmark\\ndataset\\nAvailable Code\\n49\\nCutting\\nagent\\nfor video\\nobject\\nsegmen-\\ntation\\n[124]\\n2018\\nDQN\\n8 actions:\\n4\\ntrans-\\nlation\\nactions\\n(Up,\\nDown,\\nLeft,\\nRight),\\n4 scaling\\naction\\n(Hori-\\nzontal\\nshrink,\\nVertical\\nshrink,\\nHorizon-\\ntal zoom,\\nVertical\\nzoom)\\nand\\n1\\nterminal\\naction\\n(Stop)\\nStates:\\ninput\\nframe,\\naction\\nhistory\\nand\\nsegmentation\\nmask.\\nReward:\\nchange in IOU.\\ncutting-policy\\nnetwork for box-\\ncontext\\npair\\nand\\ncutting-\\nexecution\\nnet-\\nwork for mask\\ngeneration\\nDenseNet\\n[166]\\nHigher\\nmean\\nregion\\nsimi-\\nlarity,\\ncounter\\naccuracy\\nand\\ntemporal\\nsta-\\nbility\\n[293]\\nas\\ncompared\\nto\\nmethods\\nlike\\nMSK\\n[292],\\nARP\\n[173],\\nCTN\\n[165],\\nVPN [164], etc.\\nDAVIS\\ndataset\\n[293] and the\\nYouTube Ob-\\njects\\ndataset\\n[162], [300]\\nUnsupervised\\nvideo ob-\\nject\\nsegmen-\\ntation\\n(MOREL)\\n[111]\\n2018\\nActor-\\ncritic\\n(a2c) [262]\\nNot spec-\\niﬁed\\nStates:\\nconsec-\\nutive\\nframes.\\nTwo\\nstep\\nprocess\\nwith\\noptical\\nﬂow\\nusing\\nSpatial\\nTransformer\\nNetworks\\n[159]\\nand\\nrecon-\\nstruction\\nloss\\nusing structural\\ndissimilarity\\n[388].\\nMulti-\\nlayer\\nCNN\\nHigher\\ntotal\\nepisodic reward\\nas compared to\\nmethods\\nthat\\nused\\nactor-\\ncritic\\nwithout\\nMOREL\\n59\\nAtari\\ngames.\\nAvailable Code\\n50\\nFace\\nvideo\\nsegmen-\\ntation\\n[387]\\n2020\\nNot speci-\\nﬁed\\n2 actions:\\ncategoris-\\ning\\na\\nframe\\nas\\na key or\\na non-key\\nStates:\\ndevia-\\ntion information\\nwhich described\\nthe\\ndiﬀerence\\nbetween\\ncur-\\nrent\\nnon-key\\nand\\nlast\\nkey\\ndecision,\\nand\\nexpert\\ninfor-\\nmation\\nwhich\\nencapsulated\\nthe key decision\\nhistory.\\nRe-\\nward:\\nimprove-\\nment\\nin\\nmean\\nIOU/accuracy\\nscore\\nbetween\\nsegmentation of\\nkey and non-key\\nframes\\nMulti-\\nlayer\\nCNN\\nHigher\\nmean\\nIOU then other\\nmethods\\nlike\\nDVSNet\\n[410],\\nDFF [431].\\n300VW\\ndataset\\n[336]\\nand Cityscape\\ndataset [61]\\nMulti-\\nagent\\nVideo\\nObject\\nSegmen-\\ntation\\n[373]\\n2020\\nDQN\\nActions\\nof\\n2\\ntypes:\\nmove-\\nment\\nactions\\n(up,\\ndown,\\nleft\\nand\\nright)\\nand\\nset\\naction\\n(action\\nto\\nplace\\nlocation\\nprior\\nat\\na random\\nlocation\\non\\nthe\\npatch)\\nStates:\\ninput\\nframe,\\noptical\\nﬂow [156] from\\nprevious\\nframe\\nand action his-\\ntory.\\nReward:\\nclicks generated\\nby\\ngamiﬁca-\\ntion.\\nDown-\\nsampling\\nand\\nup-sampling\\nsimilar\\nto\\nU-Net [313]\\nDenseNet\\n[147]\\nHigher\\nmean\\nregion\\nsimilar-\\nity and contour\\naccuracy\\n[293]\\nas compared to\\nsemi-supervised\\nmethods such as\\nSeamSeg\\n[14],\\nBSVS\\n[248],\\nVSOF\\n[363],\\nOSVOS\\n[41]\\nand\\nweakly-\\nsupervised\\nmethods\\nsuch\\nas GVOS [346],\\nSpftn [419]\\nDAVIS-17\\ndataset [293]\\n51\\nSkeleton-\\nbased\\nAction\\nRecog-\\nnition\\n[357]\\n2018\\nDQN\\n3 actions:\\nshifting\\nto\\nleft,\\nstaying\\nthe same\\nand shift-\\ning\\nto\\nright\\nStates:\\nGlobal\\nvideo\\ninfor-\\nmation\\nand\\nselected frames.\\nReward: change\\nin\\ncategorical\\nprobability.\\n2\\nstep\\nnetwork\\n(FDNet)\\nto\\nﬁlter\\nframes\\nand GCNN for\\naction labels\\nMulti-\\nlayer\\nCNN\\nHigher\\ncross\\nsubject\\nand\\ncross\\nview\\nmetrics\\nfor\\nNTU+RGBD\\ndataset\\n[333],\\nand\\nhigher\\naccuracy\\nfor\\nSYSU-3D [145]\\nand UT-Kinect\\nDataset\\n[399]\\nwhen\\ncom-\\npared\\nwith\\nother\\nmethods\\nlike\\nDynamic\\nSkeletons [145],\\nHBRNN-L [81],\\nPart-aware\\nLSTM\\n[333],\\nLieNet-3Blocks\\n[151],\\nTwo-\\nStream\\nCNN\\n[211], etc.\\nNTU+RGBD\\n[333],\\nSYSU-\\n3D [145] and\\nUT-Kinect\\nDataset [399]\\nVideo\\nsummari-\\nsation\\n[429]\\n2018\\nDQN\\n2 actions:\\nselecting\\nand\\nre-\\njecting\\nthe frame\\ntates:\\nbidirec-\\ntional\\nLSTM\\n[150]\\nproduced\\nstates by input\\nframe\\nfea-\\ntures.\\nReward:\\nDiversity-\\nRepresentativeness\\nReward\\nFunc-\\ntion.\\nGoogLeNet\\n[355]\\nHigher\\nF-\\nscore\\n[421]\\nas\\ncompared\\nto\\nmethods\\nlike\\nUniform\\nsampling,\\nK-medoids,\\nDictionary\\nse-\\nlection\\n[88],\\nVideo-MMR\\n[218],\\nVsumm\\n[69], etc.\\nTVSum [344]\\nand\\nSumMe\\n[122].\\nAvailable Code\\n52\\nVideo\\nsumma-\\nrization\\n[430]\\n2018\\nDuel DQN\\nDouble\\nDQN\\n2 actions:\\nselecting\\nand\\nre-\\njecting\\nthe frame\\nStates:\\nse-\\nquence\\nof\\nframes Reward:\\nDiversity-\\nRepresentativeness\\nReward\\nFunc-\\ntion\\n2\\nstage\\nimplementa-\\ntion:\\nclassi-\\nﬁcation\\nand\\nsummarisation\\nnetwork\\nusing\\nbidirectional\\nGRU\\nnetwork\\nand LSTM [150]\\nGoogLeNet\\n[355]\\nHigher\\nF-\\nscore\\n[421]\\nas\\ncompared\\nto\\nmethods\\nlike\\nDictionary\\nse-\\nlection\\n[88],\\nGAN\\n[245],\\nDR-DSN\\n[429],\\nBackprop-Grad\\n[287],\\netc\\nin\\nmost cases.\\nTVSum [344]\\nand\\nCoSum\\n[57] datasets.\\nAvailable Code\\nVideo\\nsumma-\\nrization\\nin\\nUl-\\ntrasound\\n[233]\\n2020\\nNot speci-\\nﬁed\\n2 actions:\\nselecting\\nand\\nre-\\njecting\\nthe frame\\nStates:\\nframe\\nlatent\\nscores\\nReward:\\nRdet,\\nRrep\\nand Rdiv\\nbidirectional\\nLSTM [150] and\\nKernel temporal\\nsegmentation\\n[298]\\nNot spec-\\niﬁed\\nHigher\\nF1-\\nscores\\nin\\nsu-\\npervised\\nand\\nunsupervised\\nfashion\\nas\\ncompared\\nto\\nmethods\\nlike\\nFCSN [312] and\\nDR-DSN [429].\\nFetal\\nUl-\\ntrasound\\n[179]\\nVideo object segmentation using human-provided location priors have been capable of\\nproducing promising results. An RL method for this task was proposed by [373], in which\\nthe authors proposed MASK-RL, a multiagent RL framework for object segmentation in\\nvideos. They proposed a weakly supervised method where the location priors were provided\\nby the user in form of clicks using gamiﬁcation (Web game to collect location priors by\\ndiﬀerent users) to support the segmentation and used a Gaussian ﬁlter to emphasize the\\nareas. The segmentation network is fed a 12 channel input tensor that contained a sequence\\nof video frames and their corresponding location priors (3 × 3 color channels + three gray-\\nscale images). The authors used a fully convoluted DenseNet [147] with down-sampling and\\nup-sampling similar to U-Net [313] and an LSTM [139] for the segmentation network. For\\nthe RL method, the actor takes a series of steps over a frame divided into a grid of equal\\nsize patches and makes the decision whether there is an object in the patch or not. In their\\nMDP formulation the states consisted of the input frame, optical ﬂow (computed by [156])\\nfrom the previous frame, patch from the previous iteration, and the episode location history,\\nthe actions consisted of movement actions (up, down, left and right) and set action (action\\nto place location prior at a random location on the patch), and two types of rewards one for\\nset actions and one for movement actions were used. The reward was calculated using the\\n53\\nclicks generated by the game player.\\nAction recognition is an important task in the computer vision ﬁeld which focuses on\\ncategorizing the action that is being performed in the video frame. To address the problem\\na deep progressive RL (DPRL) method for action recognition in skeleton-based videos was\\nproposed by [357]. The authors proposed a method that distills the most informative frames\\nand discards ambiguous frames by considering the quality of the frame and the relationship\\nof the frame with the complete video along with a graph-based structure to map the human\\nbody in form of joints and vertices. DPRL was utilized to ﬁlter out informative frames in a\\nvideo and graph-based CNNs were used to learn the spatial dependency between the joints.\\nThe approach consisted of two sub-networks, a frame distillation network (FDNet) to ﬁlter a\\nﬁxed number of frames from input sequence using DPRL and GCNN to recognize the action\\nlabels using output in form of a graphical structure by the FDNet. The authors modeled the\\nproblem as an MDP where the state consisted of the concatenation of two tensors F and M,\\nwhere F consisted of global information about the video and M consisted of the frames that\\nwere ﬁltered, The actions which correspond to the output of FDNet were divided into three\\ntypes: shifting to left, staying the same and shifting to the right, and the reward function\\ncorresponded to the change in probability of categorizing the video equal to the ground truth\\nclipped it between [-1 and 1] and is provided by GCNN to FDNet.\\nVideo summarization is a useful yet diﬃcult task in the computer vision ﬁeld that involves\\npredicting the object or the task that is being performed in a video. A DRL method for\\nunsupervised video summarisation was proposed by [429], in which the authors proposed a\\nDiversity-Representativeness reward system and a deep summarisation network (DSN) which\\nwas capable of predicting a probability for each video frame that speciﬁed the likeliness of\\nselecting the frame and then take actions to form video summaries. They used an encode-\\ndecoder framework for the DSN where GoogLeNet [355] pre-trained on ImageNet [320] [72]\\nwas used as an encoder and a bidirectional RNNs (BiRNNs) topped with a fully connected\\n(FC) layer was used as a decoder. The authors modeled the problem as an MDP where\\nthe action corresponded to the task of selecting or rejecting a frame.\\nThey proposed a\\nnovel Diversity-Representativeness Reward Function in their implementation, where diversity\\nreward corresponded to the degree of dissimilarity among the selected frames in feature space,\\nand representativeness reward measured how well the generated summary can represent the\\noriginal video.\\nFor the RNN unit they used an LSTM [139] to capture long-term video\\ndependencies and used REINFORCE algorithm for training the policy function.\\nAn improvement to [429] was proposed by [430], where the summarisation network was\\nimplemented using Deep Q-learning (DQSN), and a trained classiﬁcation network was used\\nto provide a reward for training the DQSN. The approach included using (Bi-GRU) bidirec-\\ntional recurrent networks with a gated recurrent unit (GRU) [50] for both classiﬁcation and\\nsummarisation network. The authors ﬁrst trained the classiﬁcation network using a super-\\nvised classiﬁcation loss and then used the classiﬁcation network with ﬁxed weights for the\\nclassiﬁcation of summaries generated by the summarisation network. The summarisation\\nnetwork included an MDP-based framework in which states consisted of a sequence of video\\nframes and actions reﬂected the task of either keeping the frame or discarding it. They used\\n54\\nFigure 16: DRL implementation for video summarization. For state a sequence of consecutive\\nframes are used and the DRL agent decided whether to include the frame in the summary\\nset that is used to predict video summary.\\na structure similar to Duel-DQN where value function and advantage function are trained\\ntogether. In their implementation, the authors considered 3 diﬀerent rewards: Global Recog-\\nnisability reward using the classiﬁcation network with +1 as reward and -5 as punishment,\\nLocal Relative Importance Reward for rewarding the action of accepting or rejecting a frame\\nby summarisation network, and an Unsupervised Reward that is computed globally using\\nthe unsupervised diversity-representativeness (DR) reward proposed in [429]. The authors\\ntrained both the networks using the features generated by GoogLeNet [355] pre-trained on\\nImageNet [72].\\nA method for video summarization in Ultrasound using DRL was proposed by [233], in\\nwhich the authors proposed a deep summarisation network in an encoder-decoder fashion\\nand used a bidirectional LSTM (Bi-LSTM) [150] for sequential modeling. In their implemen-\\ntation, the encoder-decoder convolution network extracted features from video frames and\\nfed them into the Bi-LSTM. The RL network accepted states in form of latent scores from\\nBi-LSTM and produced actions, where the actions consist of the task of including or discard-\\ning the video frame inside the summary set that is used to produce video summaries. The\\nauthors used three diﬀerent rewards Rdet, Rrep and Rdiv where Rdet evaluated the likelihood\\nof a frame being a standard diagnostic plane, Rrep deﬁned the representativeness reward and\\nRdiv was the diversity reward that evaluated the quality of the selected summary. They used\\nKernel temporal segmentation (KTS) [298] for video summary generalization.\\nVarious works associated with video analysis have been summarised and compared in\\nTable 8 and a basic implementation of video summarization using DRL has been shown in\\nFig. 16, where the states consist of a sequence of video frames. The DRL agent performs\\n55\\nactions to include or discard a frame from the summary set that is later used by the summa-\\nrization network to predict video summary. Each research paper propose their own reward\\nfunction for this application, for example [429] and [430] used diversity representativeness\\nreward function and [233] used a combination of various reward functions.\\n11\\nOthers Applications\\nObject manipulation refers to the task of handling and manipulating an object using a robot.\\nA method for deformable object manipulation using RL was proposed by [250], where the\\nauthors used a modiﬁed version of Deep Deterministic Policy Gradients (DDPG) [224]. They\\nused the simulator Pybullet [63] for the environment where the observation consisted of a\\n84 × 84 × 3 image, the state consists of joint angles and gripper positions and action of four\\ndimensions: ﬁrst three for velocity and lasts for gripper velocity was used. The authors used\\nsparse reward for the task that returns the reward at the completion of the task. They used\\nthe algorithm to perform tasks such as folding and hanging cloth and got a success rate of\\nup to 90%.\\nVisual perception-based control refers to the task of controlling robotic systems using a\\nvisual input. A virtual to real method for control using semantic segmentation was proposed\\nby [142], in which the authors combined various modules such as, Perception module, con-\\ntrol policy module, and a visual guidance module to perform the task. For the perception\\nmodule, the authors directly used models such as DeepLab [46] and ICNet [424], pre-trained\\non ADE20K [428] and Cityscape [61], and used the output of these model as the state for the\\ncontrol policy module. They implemented the control policy module using the actor-critic\\n[262] framework, where the action consisted of forward, turn right, and turn left. In their im-\\nplementation, a reward of 0.001 is given at each time step. They used the Unity3D engine for\\nthe environment and got higher success and lower collision rate than other implementations\\nsuch as ResNet-A3C and Depth-A3C.\\nAutomatic tracing of structures such as axons and blood vessels is an important yet chal-\\nlenging task in the ﬁeld of biomedical imaging. A DRL method for sub-pixel neural tracking\\nwas proposed by [65], where the authors used 2D grey-scale images as the environment. They\\nconsidered a full resolution 11px × 11px window and a 21px × 21px window down-scaled to\\n11px × 11px as state and the actions were responsible for moving the position of agent in 2D\\nspace using continuous control for sub-pixel tracking because axons can be smaller then a\\npixel. The authors used a reward that was calculated using the average integral of intensity\\nbetween the agent’s current and next location, and the agent was penalized if it does not\\nmove or changes directions more than once. They used an Actor-critic [262] framework to\\nestimate value and policy functions.\\nAn RL method for automatic diagnosis of acute appendicitis in abdominal CT images\\nwas proposed by [8], in which the authors used RL to ﬁnd the location of the appendix and\\nthen used a CNN classiﬁer to ﬁnd the likelihood of Acute Appendicitis, ﬁnally they deﬁned\\na region of low-entropy (RLE) using the spatial representation of output scores to obtain\\noptimal diagnosis scores. The authors considered the problem of appendix localization as\\n56\\nan MDP, where the state consisted of a 50 × 50 × 50 volume around the predicted appendix\\nlocation, 6 actions (2 per axis) were used and the reward consisted of the change in distance\\nbetween the predicted appendix location and actual appendix location across an action.\\nThey utilized an Actor-critic [262] framework to estimate policy and value functions.\\nTable 9: Comparing various other methods besides landmark de-\\ntection, object detection, object tracking, image registration, image\\nsegmentation, video analysis, that is associated with DRL\\nApproaches\\nYear\\nTraining\\nTechnique\\nActions\\nRemarks\\nBackbone\\nPerformance\\nDatasets\\nSource code\\nObject\\nmanip-\\nulation\\n[250]\\n2018\\nRainbow\\nDDPG\\n4\\nactions:\\n3\\nfor\\nve-\\nlocity 1 for\\ngripper ve-\\nlocity\\nState: joint an-\\ngle and gripper\\nposition.\\nRe-\\nward:\\nat\\nthe\\nend of task.\\nMulti\\nlayer CNN\\nSuccess rate up\\nto 90%\\nPybullet\\n[63].\\nCode\\nVisual\\nbased\\ncontrol\\n[142]\\n2018\\nActor-\\ncritic\\n(a3c) [262]\\n3\\nactions:\\nforward,\\nturn right\\nand\\nturn\\nleft\\nState:\\noutput\\nby\\nbackbones.\\nReward:\\n0.001\\nat\\neach\\ntime-\\nstep.\\nDeepLab\\n[46]\\nand\\nICNet\\n[424]\\nHigher\\nsuccess\\nand lower col-\\nlision rate then\\nResNet-A3C\\nand Depth-A3C\\nUnity3D\\nengine\\nAutomatic\\ntracing\\n[65]\\n2019\\nActor-\\ncritic\\n[262]\\n4 actions\\nState:\\n11px ×\\n11px\\nwindow.\\nReward:\\nav-\\nerage\\nintegral\\nof\\nintensity\\nbetween\\nthe\\nagent’s\\ncur-\\nrent\\nand\\nnext\\nlocation.\\nMulti\\nlayer CNN\\nComparable\\nconvergence\\n%\\nand\\naverage\\nerror\\nas\\ncom-\\npared to other\\nmethods\\nlike\\nVaa3D software\\n[291] and APP2\\nneuron\\ntracer\\n[403]\\nSynthetic\\nand\\nmi-\\ncroscopy\\ndataset\\n[24]\\nAutomatic\\ndiagnosis\\n(RLE) [8]\\n2019\\nActor-\\ncritic\\n[262]\\n6\\nactions:\\n2 per axis\\nState: 50 × 50 ×\\n50 volume. Re-\\nward: change in\\ndistance error.\\nFully con-\\nnected\\nCNN\\nHigher\\nsen-\\nsitivity\\nand\\nspeciﬁcity\\nas\\ncompared\\nto\\nonly CNN clas-\\nsiﬁer and CNN\\nclassiﬁer\\nwith\\nRL\\nwithout\\nRLE.\\nAbdominal\\nCT Scans\\n57\\nLearning\\nto\\npaint\\n[149]\\n2019\\nActor-\\ncritic with\\nDDPG\\nActions\\ncontrol the\\nstoke\\npa-\\nrameter:\\nlocation,\\nshape,\\ncolor\\nand\\ntrans-\\nparency\\nState:\\nRefer-\\nence\\nimage,\\nDrawing\\ncan-\\nvas\\nand\\ntime\\nstep.\\nReward:\\nchange\\nin\\ndis-\\ncriminator score\\n(calculated\\nby\\nWGAN-GP\\n[117] across an\\naction.\\nGANs\\n[113] to improve\\nimage quality\\nResNet18\\n[133]\\nAble\\nto\\nrepli-\\ncate the original\\nimages\\nto\\na\\nlarge\\nextent,\\nand\\nbetter\\nresemblance\\nto\\nthe\\norigi-\\nnal\\nimage\\nas\\ncompared\\nto\\nSPIRAL\\n[98]\\nwith same num-\\nber\\nof\\nbrush\\nstrokes.\\nMNIST\\n[202],\\nSVHN\\n[276],\\nCelebA\\n[235]\\nand\\nImageNet\\n[320].\\nCode\\nGuiding\\nmedical\\nrobots\\n[129]\\n2020\\nDouble-\\nDQN,\\nDuel-DQN\\n5\\nactions:\\nup, down,\\nleft,\\nright\\nand stop\\nState:\\nprobe\\nposition.\\nRe-\\nward:\\nMove\\ncloser:\\n0.05,\\nMove\\naway:\\n-0.1,\\nCorrect\\nstop:\\n1.0,\\nIn-\\ncorrect\\nstop:\\n-0.25.\\nResNet18\\n[133]\\nHigher % of pol-\\nicy\\ncorrectness\\nand reachability\\nas compared to\\nCNN Classiﬁer,\\nwhere MS-DQN\\nshowed the best\\nresults\\nUltrasound\\nImages\\nDataset.\\nCode\\nCrowd\\ncounting\\n[230]\\n2020\\nDQN\\n9\\nactions:\\n-10,\\n-5,\\n-2, -1, +1,\\n+2,\\n+5,\\n+10\\nand\\nend\\nState:\\nweight\\nvector Wt and\\nimage\\nfeature\\nvector\\nFVI.\\nReward:\\nInter-\\nmediate reward\\nand\\nending\\nreward\\nVGG16\\n[340]\\nLower/comparable\\nmean\\nsquared\\nerror\\n(MSE)\\nand\\nmean\\nab-\\nsolute\\nerror\\n(MAE) as com-\\npared to other\\nmethods\\nlike\\nDRSAN\\n[232],\\nPGCNet\\n[412],\\nMBTTBF [341],\\nS-DCNet [405],\\nCAN [234], etc.\\nThe\\nShang-\\nhaiTech\\n(SHT)\\nDataset\\n[423], The\\nUCFCC50\\nDataset\\n[154]\\nand\\nThe UCF-\\nQNRF\\nDataset\\n[155].\\nCode\\n58\\nAutomated\\nExposure\\nbracketing\\n[389]\\n2020\\nNot Speci-\\nﬁed\\nselecting\\noptimal\\nbracket-\\ning\\nfrom\\ncandidates\\nState: quality of\\ngenerated HDR\\nimage. Reward:\\nimprovement in\\npeak signal to\\nnoise ratio\\nAlexNet\\n[188]\\nHigher\\npeak\\nsignal to noise\\nratio\\nas\\ncom-\\npared to other\\nmethods\\nlike\\nBarakat\\n[22],\\nPourreza-\\nShahri\\n[299],\\nBeek [369], etc.\\nProposed\\nbench-\\nmark\\ndataset.\\nCode/data\\nUrban Au-\\ntonomous\\ndriving\\n[361]\\n2020\\nRainbow-\\nIQN\\n36 or 108\\nactions:\\n(9 × 4) or\\n(27 × 4),\\n9/27 steer-\\ning and 4\\nthrottle\\nState:\\nenviron-\\nment\\nvariables\\nlike traﬃc light,\\npedestrians,\\nposition\\nwith\\nrespect\\nto\\ncenter\\nlane.\\nReward: gener-\\nated by CARLA\\nwaypoint API\\nResnet18\\n[133]\\nWon\\nthe\\n2019\\ncamera\\nonly\\nCARLA\\nchal-\\nlenge [314].\\nCARLA\\nurban\\ndriving\\nsimulator\\n[314]\\nCode\\nMitigating\\nbias\\nin\\nFacial\\nRecog-\\nnition\\n[382]\\n2020\\nDQN\\n3\\nac-\\ntions:(Margin\\nadjust-\\nment)\\nstaying\\nthe\\nsame,\\nshifting to\\na\\nlarger\\nvalue\\nand\\nshifting to\\na\\nsmaller\\nvalue\\nState:\\nthe race\\ngroup,\\ncurrent\\nadaptive\\nmar-\\ngin\\nand\\nbias\\nbetween\\nthe\\nrace group and\\nCaucasians.\\nReward: change\\nin the sum of\\ninter-class\\nand\\nintra-class bias\\nMulti-\\nlayer CNN\\nProposed\\nal-\\ngorithm\\nhad\\nhigher\\nveriﬁca-\\ntion\\naccuracy\\nas compared to\\nother\\nmethods\\nsuch\\nas\\nCos-\\nFace [379] and\\nArcFace [73].\\nRFW\\n[383]\\nand\\nproposed\\nnovel\\ndatasets:\\nBUPT-\\nGlobalface\\nand\\nBUPT-\\nBalancedface\\nData\\nAttention\\nmecha-\\nnism\\nto\\nimprove\\nCNN per-\\nformance\\n[212]\\n2020\\nDQN [264]\\nActions\\nare\\nweights\\nfor\\nevery\\nlocation or\\nchannel in\\nthe feature\\nmap.\\nState:\\nFeature\\nmap\\nat\\neach\\nintermediate\\nlayer of model.\\nReward:\\npre-\\ndicted\\nby\\na\\nLSTM model.\\nResNet-\\n101 [133]\\nImproves\\nthe\\nperformances\\nof\\n[144],\\n[205]\\nand [396], which\\nattend on fea-\\nture\\nchannel,\\nspatial-channel\\nand\\nstyle,\\nrespectively\\nImageNet\\n[72]\\n59\\nFigure 17: A general DRL implementation for agent movement with visual inputs. The state\\nis provided by the environment based on which the agent performs movement actions to get\\na new state and a reward from the environment.\\nPainting using an algorithm is a fantastic yet challenging task in the computer vision\\nﬁeld. An automated painting method was proposed by [149], where the authors introduced\\na model-based DRL technique for this task. The speciﬁed work involved using a neural\\nrenderer in DRL, where the agent was responsible for making a decision about the position\\nand color of each stroke, and making long-term decisions to organize those strokes into a\\nvisual masterpiece. In this work, GANs [113] were employed to improve image quality at\\npixel-level and DDPG [224] was utilized for determining the policy. The authors formulated\\nthe problem as an MDP, where the state consisted of three parts: the target image I,\\nthe canvas on which actions (paint strokes) are performed Ct, and the time step.\\nThe\\nactions corresponding to a set of parameters that controlled the position, shape, color, and\\ntransparency of strokes, and for reward the WGAN with gradient penalty (WGAN-GP) [117]\\nwas used to calculate the discriminator score between the target image I and the canvas Ct,\\nand the change in discriminator score across an action (time-step) was used as the reward.\\nThe agent that predicted the stroke parameters was trained in actor-critic [262] fashion with\\nbackbone similar to Resnet18 [133], and the stroke parameters by the actor were used by\\nthe neural renderer network to predict paint strokes. The network structure of the neural\\nrenderer and discriminator consisted of multiple convolutions and fully connected blocks.\\nA method for guiding medical robots using Ultrasound images with the help of DRL\\nwas proposed by [129]. The authors treated the problem as an MDP where the agent takes\\nthe Ultrasound images as input and estimates the state hence the problem became Partially\\nobservable MDP (POMDP). They used Double-DQN and Duel-DQN for estimating Q-Values\\nand ResNet18 [133] backbone for extracting feature to be used by the algorithm along with\\nPrioritized Replay Memory. In their implementation the action space consisted of 8 actions\\n(up, down, left, right, and stop), probe position as compared to the sacrum was used as the\\nstate and the reward was calculated by considering the agent position as compared to the\\ntarget (Move closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect stop: -0.25). In\\n60\\ntheir implementation, the authors proposed various architectures such as V-DQN, M-DQN,\\nand MS-DQN for the task and performed experimentation on Ultrasound images.\\nCrowd counting is considered a tricky task in computer vision and is even trickier for\\nhumans. A DRL method for crowd counting was proposed by [230], where the authors used\\nsequential decision making to approach the task through RL. In the speciﬁed work, the\\nauthors proposed a DQN agent (LibraNet) based on the motivation of a weighing scale. In\\ntheir implementation crowd counting was modeled using a weighing scale where the agent was\\nresponsible for adding weights on one side of the scale sequentially to balance the crowded\\nimage on the other side. The problem of adding weights on one side of the pan for balancing\\nwas formulated as an MDP, where state consisted weight vector Wt and image feature vector\\nFVI, and the actions space was deﬁned similar to scale weighing and money system [372]\\ncontaining values (−10, −5, −2, −1, +1, +2, +5, +10, end).\\nFor reinforcing the agent two\\ndiﬀerent rewards: ending reward and intermediate reward were utilized, where ending reward\\n(following [43]) was calculated by comparing the absolute value error between the ground-\\ntruth count and the accumulated value with the error tolerance, and three counting speciﬁc\\nrewards: force ending reward, guiding reward and squeezing reward were calculated for the\\nintermediate rewards.\\nExposure bracketing is a method used in digital photography, where one scene is captured\\nusing multiple exposures for getting a high dynamic range (HDR) image. An RL method\\nfor automated bracketing selection was proposed by [389]. For ﬂexible automated bracketing\\nselection, an exposure bracketing selection network (EBSNet) was proposed for selecting\\noptimal exposure bracketing and a multi-exposure fusion network (MEFNet) for generating\\nan HDR image from selected exposure bracketing which consisted of 3 images. Since there is\\nno ground truth for the exposure bracketing selection procedure, an RL scheme was utilized\\nto train the agent (EBSNet). The authors also introduced a novel dataset consisting of a\\nsingle auto-exposure image that was used as input to the EBSNet, 10 images with varying\\nexposures from which EBSNet generated probability distribution for 120 possible candidate\\nexposure bracketing (C3\\n10) and a reference HDR image. The reward for EBSNet was deﬁned\\nas the diﬀerence between peak signal-to-noise ratio between generated and reference HDR\\nfor the current and previous iteration, and the MEFNet was trained by minimizing the\\nCharbonnier loss [23]. For performing the action of bracketing selection ESBNet consisted\\nof a semantic branch using AlexNet [188] for feature extraction, an illumination branch\\nto understand the global and local illuminations by calculating a histogram of input and\\nfeeding it to CNN layers, and a policy module to generate a probability distribution for the\\ncandidate exposure bracketing from semantic and illumination branches. The neural network\\nfor MEFNet was derived from HDRNet [103].\\nAutonomous driving in an urban environment is a challenging task, because of a large\\nnumber of environmental variables and constraints. A DRL approach to this problem was\\nproposed by [361]. In their implementation, the authors proposed an end-to-end model-free\\nRL method, where they introduced a novel technique called Implicit Aﬀordances. For the\\nenvironment, the CARLA Simulator [80] was utilized, which provided the observations and\\nthe training reward was obtained by using the CARLA waypoint API. In the novel implicit\\n61\\naﬀordances technique the training was broken into two phases, The ﬁrst phase included\\nusing a Resnet18 [133] encoder to predict the state of various environment variables such\\nas traﬃc light, pedestrians, position with respect to the center lane, etc., and the output\\nfeatures were used as a state for the RL agent, For which a modiﬁed version of Rainbow-IQN\\nApe-X [136] was used. CARLA simulator accepts actions in form of continuous steering and\\nthrottle values, so to make it work with Rainbow-IQN which supports discrete actions, the\\nauthors sampled steering values into 9 or 27 discrete values and throttle into 4 discrete values\\n(including braking), making a total of 36(9 × 4) or 108(27 × 4) actions.\\nRacial discrimination has been one of the hottest topics of the 21st century. To mitigate\\nracial discrimination in facial recognition, [382] proposed a facial recognition method using\\nskewness-aware RL. According to the authors, the reason for racial bias in facial recognition\\nalgorithms can be either due to the data or due to the algorithm, so the authors provided\\ntwo ethnicity-aware datasets, BUPT-Globalface and BUPT-Balancedface along with an RL\\nbased race balanced network (RL-RBN). In their implementation, the authors formulated\\nan MDP for adaptive margin policy learning where the state consisted of three parts: the\\nrace group (0: Indian, 1: Asian, 2: African), current adaptive margin, and bias or the\\nskewness between the race group and Caucasians. A DQN was used as a policy network\\nthat performed three actions (staying the same, shifting to a larger value, and shifting to a\\nsmaller value) to change the adaptive margin, and accepted reward in form of change in the\\nsum of inter-class and intra-class bias.\\nAttention mechanisms are currently gaining popularity because of their powerful ability\\nin eliminating uninformative parts of the input to leverage the other parts having a more\\nuseful information. Recently, attention mechanism has been integrated into typical CNN\\nmodels at every individual layer to strengthen the intermediate outputs of each layer, in\\nturn improving the ﬁnal predictions for recognition in images. This model is usually trained\\nwith a weakly supervised method, however, this optimization method may lead to sub-\\noptimal weights in the attention module. Hence, [212] proposed to train attention module\\nby deep Q-learning with an LSTM model is trained to predict the reward, the whole process\\nis called Deep REinforced Attention Learning (DREAL).\\nVarious works speciﬁed here have been summarised and compared in Table 9 and general\\nimplementation of a DRL method to control an agents movement in an environment has\\nbeen shown in ﬁg 17 where state consists of an image frame provided by the environment,\\nthe DRL agent predicts actions to move the agent in the environment providing next state\\nand the reward is provided by the environment, for example, [142].\\n12\\nFuture Perspectives\\n12.1\\nChallenge Discussion\\nDRL is a powerful framework, which has been successfully applied to various computer\\nvision applications including landmark detection, object detection, object tracking, image\\nregistration, image segmentation, video analysis, and other computer vision applications.\\n62\\nDRL has also demonstrated to be an eﬀective alternative for solving diﬃcult optimization\\nproblems, including tuning parameters, selecting augmentation strategies, and neural archi-\\ntecture search (NAS). However, most approaches, that we have reviewed, assume a stationary\\nenvironment, from which observations are made. Take landmark detection as an instance,\\nthe environment takes into account the image itself, and each state is deﬁned as an image\\npatch consisting of the landmark location. In such a case, the environment is known while\\nthe RL/DRL framework naturally accommodates a dynamic environment, that is the en-\\nvironment itself evolves with the state and action. Realizing the full potential of DRL for\\ncomputer vision requires solving several challenges. In this section, we would like to discuss\\nthe challenges of DRL in computer vision for real-world systems.\\n• Reward function: In most real-world applications, it is hard to deﬁne a speciﬁed\\nreward function because it requires the knowledge from diﬀerent domains that may\\nnot always be available. Thus, the intermediate rewards at each time step are not\\nalways easily computed. Furthermore, a reward function with too long delay will make\\ntraining diﬃcult. In contrast, assigning a reward for each action requires careful and\\nmanual human design.\\n• Continuous state and action space: Training an RL system on a continuous state\\nand action space is challenging because most RL algorithms, i.e. Q learning, can only\\ndeal with discrete states and discrete action space. To address this limitation, most\\nexisting works discretize the continuous state and action space.\\n• High-dimensional state and action space: Training Q-function on a high-dimensional\\naction space is challenging. For this reason, existing works use low-dimensional param-\\neterization, whose dimensions are typically less than 10 with an exception [184] that\\nuses 15-D and 25-D to model 2D and 3D registration, respectively.\\n• Environment is complicated: Almost all real-world systems, where we would want\\nto deploy DRL/RL, are partially observable and non-stationary. Currently, the ap-\\nproaches we have reviewed assume a stationary environment, from which observations\\nare made. However, the DRL/RL framework naturally accommodates dynamic envi-\\nronment, that is the environment itself evolves with the state and action. Furthermore,\\nthose systems are often stochastic and noisy (action delay, sensor and action noise) as\\ncompared to most simulated environments.\\n• Training data requirement: RL/DRL requires a large amount of training data or\\nexpert demonstrations. Large-scale datasets with annotations are expensive and hard\\nto come by.\\nMore details of challenges that embody diﬃculties to deploy RL/DRL in the real world\\nare discussed in [82]. In this work, they designed a set of experiments and analyzed their\\neﬀects on common RL agents. Open-sourcing an environmental suite, realworldrl-suite [83]\\nis provided in this work as well.\\n63\\n12.2\\nDRL Recent Advances\\nSome advanced DRL approaches such as Inverse DRL, Multi-agent DRL, Meta DRL, and\\nimitation learning are worth the attention and may promote new insights for many machine\\nlearning and computer vision tasks.\\n• Inverse DRL: DRL has been successfully applied into domains where the reward\\nfunction is clearly deﬁned. However, this is limited in real-world applications because\\nit requires knowledge from diﬀerent domains that may not always be available. Inverse\\nDRL is one of the special cases of imitation learning.\\nAn example is autonomous\\ndriving, the reward function should be based on all factors such as driver’s behavior,\\ngas consumption, time, speed, safety, driving quality, etc. In real-world scenario, it is\\nexhausting and hard to control all these factors. Diﬀerent from DRL, inverse DRL [278],\\n[4], [413], [86] a speciﬁc form of imitation learning [286], infers the reward function of an\\nagent, given its policy or observed behavior, thereby avoiding a manual speciﬁcation of\\nits reward function. In the same problem of autonomous driving, inverse RL ﬁrst uses a\\ndataset collected from the human-generated driving and then approximates the reward\\nfunction. Inverse RL has been successfully applied to many domains [4]. Recently, to\\nanalyze complex human movement and control high-dimensional robot systems, [215]\\nproposed an online inverse RL algorithm. [2] combined both RL and Inverse RL to\\naddress planning problems in autonomous driving.\\n• Multi-Agent DRL: Most of the successful DRL applications such as game[38], [376],\\nrobotics[181], and autonomous driving [335], stock trading [206], social science [207],\\netc., involve multiple players that requires a model with multi-agent. Take autonomous\\ndriving as an instance, multi-agent DRL addresses the sequential decision-making prob-\\nlem which involves many autonomous agents, each of which aims to optimize its own\\nutility return by interacting with the environment and other agents [40]. Learning\\nin a multi-agent scenario is more diﬃcult than a single-agent scenario because non-\\nstationarity [135], multi-dimensionality [40], credit assignment [5], etc., depend on the\\nmulti-agent DRL approach of either fully cooperative or fully competitive. The agents\\ncan either collaborate to optimize a long-term utility or compete so that the utility\\nis summed to zero. Recent work on Multi-Agent RL pays attention to learning new\\ncriteria or new setup [348].\\n• Meta DRL: As aforementioned, DRL algorithms consume large amounts of experience\\nin order to learn an individual task and are unable to generalize the learned policy to\\nnewer problems.\\nTo alleviate the data challenge, Meta-RL algorithms [330], [380]\\nare studied to enable agents to learn new skills from small amounts of experience.\\nRecently, there is a research interest in meta RL [271], [119], [322], [303], [229], each\\nusing a diﬀerent approach. For benchmarking and evaluation of meta RL algorithms,\\n[415] presented Meta-world, which is an open-source simulator consisting of 50 distinct\\nrobotic manipulation tasks.\\n64\\n• Imitation Learning: Imitation learning is close to learning from demonstrations\\nwhich aims at training a policy to mimic an expert’s behavior given the samples col-\\nlected from that expert.\\nImitation learning is also considered as an alternative to\\nRL/DRL to solve sequential decision-making problems. Besides inverse DRL, an im-\\nitation learning approach as aforementioned, behavior cloning is another imitation\\nlearning approach to train policy under supervised learning manner.\\nBradly et al.\\n[347] presented a method for unsupervised third-person imitation learning to observe\\nhow other humans perform and infer the task. Building on top of Deep Deterministic\\nPolicy Gradients and Hindsight Experience Replay, Nair et al. [272] proposed behavior\\ncloning Loss to increase imitating the demonstrations. Besides Q-learning, Generative\\nAdversarial Imitation Learning [364] proposes P-GAIL that integrates imitation learn-\\ning into the policy gradient framework. P-GAIL considers both smoothness and causal\\nentropy in policy update by utilizing Deep P-Network [365].\\nConclusion\\nDeep Reinforcement Learning (DRL) is nowadays the most popular technique for an artiﬁ-\\ncial agent to learn closely optimal strategies by experiences. This paper aims to provide a\\nstate-of-the-art comprehensive survey of DRL applications to a variety of decision-making\\nproblems in the area of computer vision.\\nIn this work, we ﬁrstly provided a structured\\nsummarization of the theoretical foundations in Deep Learning (DL) including AutoEncoder\\n(AE), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recur-\\nrent Neural Network (RNN). We then continued to introduce key techniques in RL research\\nincluding model-based methods (value functions, transaction models, policy search, return\\nfunctions) and model-free methods (value-based, policy-based, and actor-critic). Main tech-\\nniques in DRL were thirdly presented under two categories of model-based and model-free\\napproaches. We fourthly surveyed the broad-ranging applications of DRL methods in solv-\\ning problems aﬀecting areas of computer vision, from landmark detection, object detection,\\nobject tracking, image registration, image segmentation, video analysis, and many other ap-\\nplications in the computer vision area. We ﬁnally discussed several challenges ahead of us in\\norder to realize the full potential of DRL for computer vision. Some latest advanced DRL\\ntechniques were included in the last discussion.\\n65\\nReferences\\n[1] Model-based contextual policy search for data-eﬃcient generalization of robot skills.\\nArtiﬁcial Intelligence, 247:415 – 439, 2017.\\n[2] Advanced planning for autonomous vehicles using reinforcement learning and deep\\ninverse reinforcement learning. Robotics and Autonomous Systems, 114:1 – 18, 2019.\\n[3] Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobat-\\nics through apprenticeship learning. The International Journal of Robotics Research,\\n29(13):1608–1639, 2010.\\n[4] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement\\nlearning.\\nIn Proceedings of the Twenty-First International Conference on Machine\\nLearning, pages 1–8. Association for Computing Machinery, 2004.\\n[5] Adrian K. Agogino and Kagan Tumer. Unifying temporal and structural credit as-\\nsignment problems. In Proceedings of the Third International Joint Conference on\\nAutonomous Agents and Multiagent Systems - Volume 2, page 980–987. IEEE Com-\\nputer Society, 2004.\\n[6] Narges Ahmidi, Lingling Tao, Shahin Sefati, Yixin Gao, Colin Lea, Benjamin Be-\\njar Haro, Luca Zappella, Sanjeev Khudanpur, Ren´e Vidal, and Gregory D Hager.\\nA dataset and benchmarks for segmentation and recognition of gestures in robotic\\nsurgery. IEEE Transactions on Biomedical Engineering, 64(9):2025–2041, 2017.\\n[7] Walid Abdullah Al and Il Dong Yun. Partial policy-based reinforcement learning for\\nanatomical landmark localization in 3d medical images. IEEE transactions on medical\\nimaging, 2019.\\n[8] Walid Abdullah Al, Il Dong Yun, and Kyong Joon Lee.\\nReinforcement learning-\\nbased automatic diagnosis of acute appendicitis in abdominal ct.\\narXiv preprint\\narXiv:1909.00617, 2019.\\n[9] Stephan Alaniz. Deep reinforcement learning with model learning and monte carlo tree\\nsearch in minecraft. In Conference on Reinforcement Learning and Decision Making,\\n2018.\\n[10] Amir Alansary, Ozan Oktay, Yuanwei Li, Loic Le Folgoc, Benjamin Hou, Ghislain\\nVaillant, Konstantinos Kamnitsas, Athanasios Vlontzos, Ben Glocker, Bernhard Kainz,\\net al. Evaluating reinforcement learning agents for anatomical landmark detection.\\nMedical image analysis, 53:156–164, 2019.\\n[11] Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using\\nreconstruction probability. Special Lecture on IE, 2(1):1–18, 2015.\\n66\\n[12] O. Andersson, F. Heintz, and P. Doherty.\\nModel-based reinforcement learning in\\ncontinuous environments using real-time constrained optimization. In AAAI, 2015.\\n[13] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony\\nBharath.\\nA\\nbrief\\nsurvey\\nof\\ndeep\\nreinforcement\\nlearning.\\narXiv preprint\\narXiv:1708.05866, 2017.\\n[14] S Avinash Ramakanth and R Venkatesh Babu. Seamseg: Video object segmentation\\nusing patch seams. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 376–383, 2014.\\n[15] Morgane Ayle, Jimmy Tekli, Julia El-Zini, Boulos El-Asmar, and Mariette Awad.\\nBar-a reinforcement learning agent for bounding-box automated reﬁnement.\\n[16] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz.\\nGA3C: gpu-based A3C for deep reinforcement learning. CoRR, abs/1611.06256, 2016.\\n[17] Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Visual tracking with online\\nmultiple instance learning. In 2009 IEEE conference on computer vision and pattern\\nrecognition, pages 983–990. IEEE, 2009.\\n[18] Seung-Hwan Bae and Kuk-Jin Yoon. Robust online multi-object tracking based on\\ntracklet conﬁdence and online discriminative appearance learning. In Proceedings of\\nthe IEEE conference on computer vision and pattern recognition, pages 1218–1225,\\n2014.\\n[19] Seung-Hwan Bae and Kuk-Jin Yoon. Conﬁdence-based data association and discrimi-\\nnative deep appearance learning for robust online multi-object tracking. IEEE trans-\\nactions on pattern analysis and machine intelligence, 40(3):595–610, 2017.\\n[20] J. Bagnell. Learning decision: Robustness, uncertainty, and approximation. 04 2012.\\n[21] J. A. Bagnell and J. G. Schneider. Autonomous helicopter control using reinforce-\\nment learning policy search methods. In Proceedings 2001 ICRA. IEEE International\\nConference on Robotics and Automation (Cat. No.01CH37164), volume 2, pages 1615–\\n1620, 2001.\\n[22] Neil Barakat, A Nicholas Hone, and Thomas E Darcie.\\nMinimal-bracketing sets\\nfor high-dynamic-range image capture.\\nIEEE Transactions on Image Processing,\\n17(10):1864–1875, 2008.\\n[23] Jonathan T Barron. A general and adaptive robust loss function. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 4331–4339,\\n2019.\\n67\\n[24] Cher Bass, Pyry Helkkula, Vincenzo De Paola, Claudia Clopath, and Anil An-\\nthony Bharath. Detection of axonal synapses in 3d two-photon images. PloS one,\\n12(9):e0183309, 2017.\\n[25] Miriam Bellver, Xavier Gir´o-i Nieto, Ferran Marqu´es, and Jordi Torres. Hierarchical\\nobject detection with deep reinforcement learning. arXiv preprint arXiv:1611.03718,\\n2016.\\n[26] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies\\nwith gradient descent is diﬃcult. IEEE Trans. Neural Networks, 5(2):157–166, 1994.\\n[27] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad — a comprehensive\\nreal-world dataset for unsupervised anomaly detection. In 2019 IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR), pages 9584–9592, 2019.\\n[28] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking perfor-\\nmance: the clear mot metrics. EURASIP Journal on Image and Video Processing,\\n2008:1–10, 2008.\\n[29] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS\\nTorr. Fully-convolutional siamese networks for object tracking. In European conference\\non computer vision, pages 850–865. Springer, 2016.\\n[30] Shalabh Bhatnagar. An actor–critic algorithm with function approximation for dis-\\ncounted cost constrained markov decision processes.\\nSystems & Control Letters,\\n59(12):760–766, 2010.\\n[31] Shalabh Bhatnagar, Richard S. Sutton, Mohammad Ghavamzadeh, and Mark Lee.\\nNatural actorˆa-critic algorithms. Automatica, 45(11):2471 – 2482, 2009.\\n[32] Michael J Black and Yaser Yacoob.\\nTracking and recognizing rigid and non-rigid\\nfacial motions using local parametric models of image motion. In Proceedings of IEEE\\ninternational conference on computer vision, pages 374–381. IEEE, 1995.\\n[33] N Bloch, A Madabhushi, H Huisman, J Freymann, J Kirby, M Grauer, A Enquobahrie,\\nC Jaﬀe, L Clarke, and K Farahani. Nci-isbi 2013 challenge: automated segmentation\\nof prostate structures. The Cancer Imaging Archive, 370, 2015.\\n[34] J. Boedecker, J. T. Springenberg, J. W¨ulﬁng, and M. Riedmiller. Approximate real-\\ntime optimal control based on sparse gaussian process models. In 2014 IEEE Sym-\\nposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),\\npages 1–8, 2014.\\n[35] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal network for\\nobject detection. In Proceedings of the IEEE International Conference on Computer\\nVision, Seoul, South Korea, 2019.\\n68\\n[36] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative interaction training\\nfor segmentation editing networks. In International Workshop on Machine Learning\\nin Medical Imaging, pages 363–370. Springer, 2018.\\n[37] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\\n[38] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science,\\n365(6456):885–890, 2019.\\n[39] Antoine Buetti-Dinh, Vanni Galli, S˜A¶ren Bellenberg, Olga Ilie, Malte Herold, Stephan\\nChristel, Mariia Boretska, Igor V. Pivkin, Paul Wilmes, Wolfgang Sand, Mario Vera,\\nand Mark Dopson. Deep neural networks outperform human expert’s capacity in char-\\nacterizing bioleaching bacterial bioﬁlm composition. Biotechnology Reports, 22:e00321,\\n2019.\\n[40] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent\\nreinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part\\nC (Applications and Reviews), 38(2):156–172, 2008.\\n[41] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix´e, Daniel Cre-\\nmers, and Luc Van Gool. One-shot video object segmentation. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pages 221–230, 2017.\\n[42] Yunliang Cai, Said Osman, Manas Sharma, Mark Landis, and Shuo Li. Multi-modality\\nvertebra recognition in arbitrary views using 3d deformable hierarchical model. IEEE\\ntransactions on medical imaging, 34(8):1676–1693, 2015.\\n[43] Juan C Caicedo and Svetlana Lazebnik. Active object localization with deep rein-\\nforcement learning. In Proceedings of the IEEE international conference on computer\\nvision, pages 2488–2496, 2015.\\n[44] D. Carrera, F. Manganini, G. Boracchi, and E. Lanzarone. Defect detection in sem im-\\nages of nanoﬁbrous materials. IEEE Transactions on Industrial Informatics, 13(2):551–\\n561, 2017.\\n[45] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and Huchuan Lu. Real-time’actor-\\ncritic’tracking.\\nIn Proceedings of the European Conference on Computer Vision\\n(ECCV), pages 318–334, 2018.\\n[46] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L\\nYuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous\\nconvolution, and fully connected crfs. IEEE transactions on pattern analysis and ma-\\nchine intelligence, 40(4):834–848, 2017.\\n[47] Yushi Chen, Xing Zhao, and Xiuping Jia. Spectral–spatial classiﬁcation of hyperspec-\\ntral data based on deep belief network. IEEE Journal of Selected Topics in Applied\\nEarth Observations and Remote Sensing, 8(6):2381–2392, 2015.\\n69\\n[48] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-Hsuan Yang.\\nSegﬂow:\\nJoint learning for video object segmentation and optical ﬂow. In Proceedings of the\\nIEEE international conference on computer vision, pages 686–695, 2017.\\n[49] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu.\\nGlobal contrast based salient region detection. IEEE transactions on pattern analysis\\nand machine intelligence, 37(3):569–582, 2014.\\n[50] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau,\\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representa-\\ntions using rnn encoder-decoder for statistical machine translation.\\narXiv preprint\\narXiv:1406.1078, 2014.\\n[51] Kyunghyun Cho, Bart van Merrienboer, C¸aglar G¨ul¸cehre, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-\\ndecoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n[52] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris, and\\nJin Young Choi. Attentional correlation ﬁlter network for adaptive visual tracking. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\\n4807–4816, 2017.\\n[53] Wongun Choi. Near-online multi-target tracking with aggregated local ﬂow descriptor.\\nIn Proceedings of the IEEE international conference on computer vision, pages 3029–\\n3037, 2015.\\n[54] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, and Yoshua\\nBengio. Attention-based models for speech recognition. CoRR, abs/1506.07503, 2015.\\n[55] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai Yu. On-\\nline multi-object tracking using cnn-based single object tracker with spatial-temporal\\nattention mechanism. In Proceedings of the IEEE International Conference on Com-\\nputer Vision, pages 4836–4845, 2017.\\n[56] Wen-Hsuan Chu and Kris M. Kitani. Neural batch sampling with reinforcement learn-\\ning for semi-supervised anomaly detection.\\nIn European Conference on Computer\\nVision, pages 751–766, 2020.\\n[57] Wen-Sheng Chu, Yale Song, and Alejandro Jaimes. Video co-summarization: Video\\nsummarization by visual co-occurrence. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, pages 3584–3592, 2015.\\n[58] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour,\\nand Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization.\\nCoRR, abs/1809.05214, 2018.\\n70\\n[59] Adam Coates, Pieter Abbeel, and Andrew Y. Ng. Apprenticeship learning for heli-\\ncopter control. Commun. ACM, 52(7):97–105, July 2009.\\n[60] Dorin Comaniciu, Visvanathan Ramesh, and Peter Meer. Real-time tracking of non-\\nrigid objects using mean shift. In Proceedings IEEE Conference on Computer Vision\\nand Pattern Recognition. CVPR 2000 (Cat. No. PR00662), volume 2, pages 142–149.\\nIEEE, 2000.\\n[61] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,\\nRodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset\\nfor semantic urban scene understanding. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 3213–3223, 2016.\\n[62] R´emi Coulom. Eﬃcient selectivity and backup operators in monte-carlo tree search.\\nIn Proceedings of the 5th International Conference on Computers and Games, page\\n72–83, 2006.\\n[63] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for\\ngames, robotics and machine learning. 2016.\\n[64] Antonio Criminisi, Jamie Shotton, Duncan Robertson, and Ender Konukoglu. Regres-\\nsion forests for eﬃcient anatomy detection and localization in ct studies. In Inter-\\nnational MICCAI Workshop on Medical Computer Vision, pages 106–117. Springer,\\n2010.\\n[65] Tianhong Dai, Magda Dubois, Kai Arulkumaran, Jonathan Campbell, Cher Bass, Ben-\\njamin Billot, Fatmatulzehra Uslu, Vincenzo De Paola, Claudia Clopath, and Anil An-\\nthony Bharath. Deep reinforcement learning for subpixel neural tracking. In Interna-\\ntional Conference on Medical Imaging with Deep Learning, pages 130–150, 2019.\\n[66] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Eco:\\nEﬃcient convolution operators for tracking. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 6638–6646, 2017.\\n[67] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and Michael Felsberg. Learn-\\ning spatially regularized correlation ﬁlters for visual tracking. In Proceedings of the\\nIEEE international conference on computer vision, pages 4310–4318, 2015.\\n[68] Kristopher De Asis, J Fernando Hernandez-Garcia, G Zacharias Holland, and\\nRichard S Sutton. Multi-step reinforcement learning: A unifying algorithm. In Thirty-\\nSecond AAAI Conference on Artiﬁcial Intelligence, 2018.\\n[69] Sandra Eliza Fontes De Avila, Ana Paula Brand˜ao Lopes, Antonio da Luz Jr, and\\nArnaldo de Albuquerque Ara´ujo. Vsumm: A mechanism designed to produce static\\nvideo summaries and a novel evaluation method. Pattern Recognition Letters, 32(1):56–\\n68, 2011.\\n71\\n[70] Antonio de Marvao, Timothy JW Dawes, Wenzhe Shi, Christopher Minas, Niall G\\nKeenan, Tamara Diamond, Giuliana Durighel, Giovanni Montana, Daniel Rueckert,\\nStuart A Cook, et al. Population-based studies of myocardial hypertrophy: high reso-\\nlution cardiovascular magnetic resonance atlases improve statistical power. Journal of\\ncardiovascular magnetic resonance, 16(1):16, 2014.\\n[71] M. P. Deisenroth, P. Englert, J. Peters, and D. Fox.\\nMulti-task policy search for\\nrobotics. In 2014 IEEE International Conference on Robotics and Automation (ICRA),\\npages 3876–3881, 2014.\\n[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision\\nand pattern recognition, pages 248–255. Ieee, 2009.\\n[73] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive\\nangular margin loss for deep face recognition. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 4690–4699, 2019.\\n[74] Joachim Denzler and Dietrich WR Paulus. Active motion detection and object track-\\ning. In Proceedings of 1st International Conference on Image Processing, volume 3,\\npages 635–639. IEEE, 1994.\\n[75] B. Depraetere, M. Liu, G. Pinte, I. Grondman, and R. Babu˚A¡ka. Comparison of\\nmodel-free and model-based methods for time optimal hit control of a badminton\\nrobot. Mechatronics, 24(8):1021 – 1030, 2014.\\n[76] Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S Swaroop Vedula,\\nGyusung I Lee, Mija R Lee, and Gregory D Hager. Recognizing surgical activities\\nwith recurrent neural networks. In International conference on medical image comput-\\ning and computer-assisted intervention, pages 551–558. Springer, 2016.\\n[77] Piotr Doll´ar, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian detection:\\nA benchmark. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\\npages 304–311. IEEE, 2009.\\n[78] JeﬀDonahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini\\nVenugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional\\nnetworks for visual recognition and description. CoRR, abs/1411.4389, 2014.\\n[79] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,\\nVladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:\\nLearning optical ﬂow with convolutional networks. In Proceedings of the IEEE inter-\\nnational conference on computer vision, pages 2758–2766, 2015.\\n[80] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen\\nKoltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938,\\n2017.\\n72\\n[81] Yong Du, Wei Wang, and Liang Wang.\\nHierarchical recurrent neural network for\\nskeleton based action recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 1110–1118, 2015.\\n[82] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru,\\nSven Gowal, and Todd Hester. An empirical investigation of the challenges of real-\\nworld reinforcement learning, 2020.\\n[83] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru,\\nSven Gowal, and Todd Hester. An empirical investigation of the challenges of real-\\nworld reinforcement learning. 2020.\\n[84] Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti, and Christian Micheloni. Visual\\ntracking by means of deep reinforcement learning and an expert demonstrator. In\\nProceedings of the IEEE International Conference on Computer Vision Workshops,\\npages 0–0, 2019.\\n[85] Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le, and Khoa Luu. Mobiface:\\nA lightweight deep learning face recognition on mobile devices. In 2019 IEEE 10th\\nInternational Conference on Biometrics Theory, Applications and Systems (BTAS),\\npages 1–6. IEEE, 2019.\\n[86] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Le, Marios Savvides, and\\nTien D. Bui.\\nLearning from longitudinal face demonstration–where tractable deep\\nmodeling meets inverse reinforcement learning. 127(6–7), 2019.\\n[87] A. El-Fakdi and M. Carreras. Policy gradient based reinforcement learning for real\\nautonomous underwater cable tracking. In 2008 IEEE/RSJ International Conference\\non Intelligent Robots and Systems, pages 3635–3640, 2008.\\n[88] Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking at a few: Sparse\\nmodeling for ﬁnding representative objects. In 2012 IEEE conference on computer\\nvision and pattern recognition, pages 1600–1607. IEEE, 2012.\\n[89] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scal-\\nable object detection using deep neural networks. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 2147–2154, 2014.\\n[90] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew\\nZisserman. The pascal visual object classes challenge 2007 (voc2007) results. 2007.\\n[91] Mark Everingham and John Winn. The pascal visual object classes challenge 2012\\n(voc2012) development kit. Pattern Analysis, Statistical Modelling and Computational\\nLearning, Tech. Rep, 8, 2011.\\n73\\n[92] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu,\\nChunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale\\nsingle object tracking. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 5374–5383, 2019.\\n[93] Heng Fan and Haibin Ling. Parallel tracking and verifying: A framework for real-time\\nand high accuracy visual tracking. In Proceedings of the IEEE International Conference\\non Computer Vision, pages 5486–5494, 2017.\\n[94] Jialue Fan, Wei Xu, Ying Wu, and Yihong Gong. Human tracking using convolutional\\nneural networks. IEEE Transactions on Neural Networks, 21(10):1610–1623, 2010.\\n[95] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter\\nAbbeel. Deep spatial autoencoders for visuomotor learning. In Danica Kragic, Anto-\\nnio Bicchi, and Alessandro De Luca, editors, 2016 IEEE International Conference on\\nRobotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, pages\\n512–519.\\n[96] J Michael Fitzpatrick and Jay B West. The distribution of target registration error in\\nrigid-body point-based registration. IEEE transactions on medical imaging, 20(9):917–\\n927, 2001.\\n[97] Vincent Fran¸cois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and\\nJoelle Pineau.\\nAn introduction to deep reinforcement learning.\\narXiv preprint\\narXiv:1811.12560, 2018.\\n[98] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami, and Oriol Vinyals. Syn-\\nthesizing programs for images using reinforced adversarial learning. arXiv preprint\\narXiv:1804.01118, 2018.\\n[99] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S Davis. Dynamic zoom-in\\nnetwork for fast object detection in large images. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, pages 6926–6935, 2018.\\n[100] Yixin Gao, S Swaroop Vedula, Carol E Reiley, Narges Ahmidi, Balakrishnan Varadara-\\njan, Henry C Lin, Lingling Tao, Luca Zappella, Benjamın B´ejar, David D Yuh, et al.\\nJhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset\\nfor human motion modeling. In Miccai workshop: M2cai, volume 3, page 3, 2014.\\n[101] Romane Gauriau, R´emi Cuingnet, David Lesage, and Isabelle Bloch.\\nMulti-organ\\nlocalization combining global-to-local regression and conﬁdence maps. In International\\nConference on Medical Image Computing and Computer-Assisted Intervention, pages\\n337–344. Springer, 2014.\\n[102] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti\\nvision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 3354–3361, 2012.\\n74\\n[103] Micha¨el Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoﬀ, and Fr´edo\\nDurand. Deep bilateral learning for real-time image enhancement. ACM Transactions\\non Graphics (TOG), 36(4):1–12, 2017.\\n[104] Florin C Ghesu, Edward Krubasik, Bogdan Georgescu, Vivek Singh, Yefeng Zheng,\\nJoachim Hornegger, and Dorin Comaniciu. Marginal space deep learning: eﬃcient\\narchitecture for volumetric image parsing.\\nIEEE transactions on medical imaging,\\n35(5):1217–1228, 2016.\\n[105] Florin-Cristian Ghesu, Bogdan Georgescu, Yefeng Zheng, Sasa Grbic, Andreas Maier,\\nJoachim Hornegger, and Dorin Comaniciu. Multi-scale deep reinforcement learning for\\nreal-time 3d-landmark detection in ct scans. IEEE transactions on pattern analysis\\nand machine intelligence, 41(1):176–189, 2017.\\n[106] M Giles. Mit technology review. Google researchers have reportedly achieved” quantum\\nsupremacy” URL: https:/www.technologyreview. com/f, 614416, 2017.\\n[107] Justin Girard and M Reza Emami. Concurrent markov decision processes for robot\\nteam learning. Engineering applications of artiﬁcial intelligence, 39:223–234, 2015.\\n[108] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on\\ncomputer vision, pages 1440–1448, 2015.\\n[109] Ross Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik. Rich feature hierar-\\nchies for accurate object detection and semantic segmentation. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pages 580–587, 2014.\\n[110] Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action recognition\\nwith r* cnn. In Proceedings of the IEEE international conference on computer vision,\\npages 1080–1088, 2015.\\n[111] Vikash Goel, Jameson Weng, and Pascal Poupart. Unsupervised video object segmen-\\ntation for deep reinforcement learning. In Advances in Neural Information Processing\\nSystems, pages 5683–5694, 2018.\\n[112] Abel Gonzalez-Garcia, Alexander Vezhnevets, and Vittorio Ferrari. An active search\\nstrategy for eﬃcient object class detection. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, pages 3022–3031, 2015.\\n[113] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In\\nAdvances in neural information processing systems, pages 2672–2680, 2014.\\n[114] Leo Grady. Random walks for image segmentation. IEEE transactions on pattern\\nanalysis and machine intelligence, 28(11):1768–1783, 2006.\\n75\\n[115] Alex Graves, Abdel-rahman Mohamed, and Geoﬀrey E. Hinton. Speech recognition\\nwith deep recurrent neural networks. CoRR, abs/1303.5778, 2013.\\n[116] Albert Gubern-M´erida, Robert Mart´ı, Jaime Melendez, Jakob L Hauth, Ritse M Mann,\\nNico Karssemeijer, and Bram Platel. Automated localization of breast cancer in dce-\\nmri. Medical image analysis, 20(1):265–274, 2015.\\n[117] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\\nCourville. Improved training of wasserstein gans. In Advances in neural information\\nprocessing systems, pages 5767–5777, 2017.\\n[118] Minghao Guo, Jiwen Lu, and Jie Zhou. Dual-agent deep reinforcement learning for\\ndeformable face tracking. In Proceedings of the European Conference on Computer\\nVision (ECCV), pages 768–783, 2018.\\n[119] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.\\nMeta-reinforcement learning of structured exploration strategies. In Advances in Neu-\\nral Information Processing Systems, pages 5302–5311, 2018.\\n[120] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization and\\nrecognition of indoor scenes from rgb-d images. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 564–571, 2013.\\n[121] Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, and Jitendra Malik. Learning rich fea-\\ntures from rgb-d images for object detection and segmentation. In European conference\\non computer vision, pages 345–360. Springer, 2014.\\n[122] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating\\nsummaries from user videos. In European conference on computer vision, pages 505–\\n520. Springer, 2014.\\n[123] Seyed Hamid Rezatoﬁghi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony Dick, and\\nIan Reid. Joint probabilistic data association revisited. In Proceedings of the IEEE\\ninternational conference on computer vision, pages 3047–3055, 2015.\\n[124] Junwei Han, Le Yang, Dingwen Zhang, Xiaojun Chang, and Xiaodan Liang. Reinforce-\\nment cutting-agent learning for video object segmentation. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, pages 9080–9089, 2018.\\n[125] Robert M Haralick and Linda G Shapiro. Image segmentation techniques. Computer\\nvision, graphics, and image processing, 29(1):100–132, 1985.\\n[126] Sam Hare, Stuart Golodetz, Amir Saﬀari, Vibhav Vineet, Ming-Ming Cheng, Stephen L\\nHicks, and Philip HS Torr. Struck: Structured output tracking with kernels. IEEE\\ntransactions on pattern analysis and machine intelligence, 38(10):2096–2109, 2015.\\n76\\n[127] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Jitendra Malik. Simultaneous\\ndetection and segmentation. In European Conference on Computer Vision, pages 297–\\n312. Springer, 2014.\\n[128] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Jitendra Malik. Hypercolumns\\nfor object segmentation and ﬁne-grained localization.\\nIn Proceedings of the IEEE\\nconference on computer vision and pattern recognition, pages 447–456, 2015.\\n[129] Hannes Hase, Mohammad Farid Azampour, Maria Tirindelli, Magdalini Paschali, Wal-\\nter Simson, Emad Fatemizadeh, and Nassir Navab. Ultrasound-guided robotic naviga-\\ntion with deep reinforcement learning. arXiv preprint arXiv:2003.13321, 2020.\\n[130] Hado V Hasselt. Double q-learning. In Advances in neural information processing\\nsystems, pages 2613–2621, 2010.\\n[131] Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for partially ob-\\nservable mdps. CoRR, abs/1507.06527, 2015.\\n[132] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick.\\nMask r-cnn.\\nIn\\nProceedings of the IEEE international conference on computer vision, pages 2961–2969,\\n2017.\\n[133] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 770–778, 2016.\\n[134] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking\\nwith kernelized correlation ﬁlters. IEEE transactions on pattern analysis and machine\\nintelligence, 37(3):583–596, 2014.\\n[135] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A\\nsurvey of learning in multiagent environments: Dealing with non-stationarity. CoRR,\\nabs/1707.09183, 2017.\\n[136] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will\\nDabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Com-\\nbining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298,\\n2017.\\n[137] Todd Hester, Michael Quinlan, and Peter Stone. A real-time model-based reinforce-\\nment learning architecture for robot control. CoRR, abs/1105.1749, 2011.\\n[138] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston.\\nThe goldilocks\\nprinciple: Reading children’s books with explicit memory representations.\\nCoRR,\\nabs/1511.02301, 2015.\\n77\\n[139] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural compu-\\ntation, 9(8):1735–1780, 1997.\\n[140] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recovering surface layout from an\\nimage. International Journal of Computer Vision, 75(1):151–172, 2007.\\n[141] James B. Holliday and Ngan T.H. Le.\\nFollow then forage exploration: Improving\\nasynchronous advantage actor critic.\\nInternational Conference on Soft Computing,\\nArtiﬁcial Intelligence and Applications (SAI 2020), pages 107–118, 2020.\\n[142] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang,\\nHsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching\\nHsiao, et al.\\nVirtual-to-real: Learning to control in visual semantic segmentation.\\narXiv preprint arXiv:1802.00285, 2018.\\n[143] Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online multi-\\nobject tracking via structural constraint event aggregation. In Proceedings of the IEEE\\nConference on computer vision and pattern recognition, pages 1392–1400, 2016.\\n[144] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In 2018 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 7132–7141, 2018.\\n[145] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly learning\\nheterogeneous features for rgb-d activity recognition. In Proceedings of the IEEE con-\\nference on computer vision and pattern recognition, pages 5344–5352, 2015.\\n[146] Weiming Hu, Xi Li, Wenhan Luo, Xiaoqin Zhang, Stephen Maybank, and Zhongfei\\nZhang. Single and multiple object tracking using log-euclidean riemannian subspace\\nand block-division appearance model. IEEE transactions on pattern analysis and ma-\\nchine intelligence, 34(12):2420–2440, 2012.\\n[147] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely\\nconnected convolutional networks. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 4700–4708, 2017.\\n[148] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity bench-\\nmark for generic object tracking in the wild. IEEE Transactions on Pattern Analysis\\nand Machine Intelligence, 2019.\\n[149] Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based\\ndeep reinforcement learning. In Proceedings of the IEEE International Conference on\\nComputer Vision, pages 8709–8718, 2019.\\n[150] Zhiheng Huang, Wei Xu, and Kai Yu.\\nBidirectional lstm-crf models for sequence\\ntagging. arXiv preprint arXiv:1508.01991, 2015.\\n78\\n[151] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning on\\nlie groups for skeleton-based action recognition. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 6099–6108, 2017.\\n[152] Gabriel Efrain Humpire-Mamani, Arnaud Arindra Adiyoso Setio, Bram van Ginneken,\\nand Colin Jacobs. Eﬃcient organ localization using multi-label convolutional neural\\nnetworks in thorax-abdomen ct scans. Physics in Medicine & Biology, 63(8):085003,\\n2018.\\n[153] Luis Ibanez, Will Schroeder, Lydia Ng, and Josh Cates. The itk software guide: up-\\ndated for itk version 2.4, 2005.\\n[154] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source multi-\\nscale counting in extremely dense crowd images. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 2547–2554, 2013.\\n[155] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed,\\nNasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map esti-\\nmation and localization in dense crowds. In Proceedings of the European Conference\\non Computer Vision (ECCV), pages 532–546, 2018.\\n[156] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and\\nThomas Brox. Flownet 2.0: Evolution of optical ﬂow estimation with deep networks.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 2462–2470, 2017.\\n[157] Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\\n[158] Cliﬀord R Jack Jr, Matt A Bernstein, Nick C Fox, Paul Thompson, Gene Alexander,\\nDanielle Harvey, Bret Borowski, Paula J Britson, Jennifer L. Whitwell, Chadwick\\nWard, et al. The alzheimer’s disease neuroimaging initiative (adni): Mri methods.\\nJournal of Magnetic Resonance Imaging: An Oﬃcial Journal of the International\\nSociety for Magnetic Resonance in Medicine, 27(4):685–691, 2008.\\n[159] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer net-\\nworks. In Advances in neural information processing systems, pages 2017–2025, 2015.\\n[160] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for text spot-\\nting. In European conference on computer vision, pages 512–528. Springer, 2014.\\n[161] Arjit Jain, Alexander Powers, and Hans J Johnson. Robust automatic multiple land-\\nmark detection. In 2020 IEEE 17th International Symposium on Biomedical Imaging\\n(ISBI), pages 1178–1182. IEEE, 2020.\\n[162] Suyog Dutt Jain and Kristen Grauman. Supervoxel-consistent foreground propagation\\nin video. In European conference on computer vision, pages 656–671. Springer, 2014.\\n79\\n[163] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to combine\\nmotion and appearance for fully automatic segmentation of generic objects in videos.\\nIn 2017 IEEE conference on computer vision and pattern recognition (CVPR), pages\\n2117–2126. IEEE, 2017.\\n[164] Varun Jampani, Raghudeep Gadde, and Peter V Gehler. Video propagation networks.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npages 451–461, 2017.\\n[165] Won-Dong Jang and Chang-Su Kim. Online video object segmentation via convolu-\\ntional trident network. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 5849–5858, 2017.\\n[166] Simon J´egou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua Bengio.\\nThe one hundred layers tiramisu: Fully convolutional densenets for semantic segmenta-\\ntion. In Proceedings of the IEEE conference on computer vision and pattern recognition\\nworkshops, pages 11–19, 2017.\\n[167] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang.\\nModel-based reinforce-\\nment learning with value-targeted regression. In Proceedings of the 2nd Conference on\\nLearning for Dynamics and Control, volume 120 of Proceedings of Machine Learning\\nResearch, pages 666–686, The Cloud, 10–11 Jun 2020.\\n[168] Ming-xin Jiang, Chao Deng, Zhi-geng Pan, Lan-fang Wang, and Xing Sun. Multiobject\\ntracking in videos based on lstm and deep reinforcement learning. Complexity, 2018,\\n2018.\\n[169] Mingxin Jiang, Tao Hai, Zhigeng Pan, Haiyan Wang, Yinjie Jia, and Chao Deng. Multi-\\nagent deep reinforcement learning for multi-object tracker. IEEE Access, 7:32400–\\n32407, 2019.\\n[170] Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu, and Shuicheng Yan.\\nTree-structured reinforcement learning for sequential object localization. In Advances\\nin Neural Information Processing Systems, pages 127–135, 2016.\\n[171] Oscar Jimenez-del Toro, Henning M¨uller, Markus Krenn, Katharina Gruenberg, Ab-\\ndel Aziz Taha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-Rodr´ıguez, Or-\\ncun Goksel, Andr´as Jakab, et al. Cloud-based evaluation of anatomical structure seg-\\nmentation and landmark detection algorithms: Visceral anatomy benchmarks. IEEE\\ntransactions on medical imaging, 35(11):2459–2475, 2016.\\n[172] V Craig Jordan.\\nLong-term adjuvant tamoxifen therapy for breast cancer. Breast\\ncancer research and treatment, 15(3):125–136, 1990.\\n[173] Yeong Jun Koh and Chang-Su Kim. Primary object segmentation in videos based\\non region augmentation and reduction.\\nIn Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 3442–3450, 2017.\\n80\\n[174] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas.\\nTracking-learning-detection.\\nIEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422,\\n2011.\\n[175] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. Asso-\\nciation for Computational Linguistics, October 2013.\\n[176] Micha l Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech\\nJa´skowski.\\nVizdoom: A doom-based ai research platform for visual reinforcement\\nlearning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG),\\npages 1–8. IEEE, 2016.\\n[177] Du Yong Kim and Moongu Jeon. Data fusion of radar and image measurements for\\nmulti-object tracking via kalman ﬁltering. Information Sciences, 278:641–652, 2014.\\n[178] Kye Kyung Kim, Soo Hyun Cho, Hae Jin Kim, and Jae Yeon Lee. Detecting and\\ntracking moving object using an active camera. In The 7th International Conference\\non Advanced Communication Technology, 2005, ICACT 2005., volume 2, pages 817–\\n820. IEEE, 2005.\\n[179] Donna Kirwan.\\nNhs fetal anomaly screening programme.\\nNational Standards and\\nGuidance for England, 18(0), 2010.\\n[180] Stefan Klein, Marius Staring, Keelin Murphy, Max A Viergever, and Josien PW Pluim.\\nElastix: a toolbox for intensity-based medical image registration. IEEE transactions\\non medical imaging, 29(1):196–205, 2009.\\n[181] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics:\\nA survey. The International Journal of Robotics Research, 32(11):1238–1274, 2013.\\n[182] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural\\ninformation processing systems, pages 1008–1014, 2000.\\n[183] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Eﬃcient inference in fully connected crfs\\nwith gaussian edge potentials. In Advances in neural information processing systems,\\npages 109–117, 2011.\\n[184] Julian Krebs, Tommaso Mansi, Herv´e Delingette, Li Zhang, Florin C Ghesu, Shun\\nMiao, Andreas K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-\\nrigid registration through agent-based action learning. In International Conference\\non Medical Image Computing and Computer-Assisted Intervention, pages 344–352.\\nSpringer, 2017.\\n[185] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pﬂugfelder, Luka\\nCehovin Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,\\net al. The sixth visual object tracking vot2018 challenge results. In Proceedings of the\\nEuropean Conference on Computer Vision (ECCV), pages 0–0, 2018.\\n81\\n[186] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin, Gustavo\\nFernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, and Roman Pﬂugfelder. The\\nvisual object tracking vot2015 challenge results. In Proceedings of the IEEE interna-\\ntional conference on computer vision workshops, pages 1–23, 2015.\\n[187] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\\ndeep convolutional neural networks.\\nIn Advances in neural information processing\\nsystems, pages 1097–1105, 2012.\\n[188] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\\ndeep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\\n[189] A. Kupcsik, M. Deisenroth, Jan Peters, and G. Neumann. Data-eﬃcient generalization\\nof robot skills with contextual policy search. In AAAI, 2013.\\n[190] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-\\nensemble trust-region policy optimization. 02 2018.\\n[191] Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu, and Marios Savides.\\nOﬀset curves loss for imbalanced problem in medical segmentation. arXiv preprint\\narXiv:2012.02463, 2020.\\n[192] Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, and Marios Savvides. A\\nmulti-task contextual atrous residual network for brain tumor detection & segmenta-\\ntion. arXiv preprint arXiv:2012.02073, 2020.\\n[193] T Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Kha Gia Quach, and\\nMarios Savvides. Deep contextual recurrent residual networks for scene labeling. Pat-\\ntern Recognition, 80:32–41, 2018.\\n[194] T Hoang Ngan Le, Kha Gia Quach, Khoa Luu, Chi Nhan Duong, and Marios Sav-\\nvides. Reformulating level sets as deep recurrent neural network approach to semantic\\nsegmentation. IEEE Transactions on Image Processing, 27(5):2393–2407, 2018.\\n[195] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Tem-\\nporal convolutional networks for action segmentation and detection. In proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 156–165,\\n2017.\\n[196] Colin Lea, Austin Reiter, Ren´e Vidal, and Gregory D Hager. Segmental spatiotempo-\\nral cnns for ﬁne-grained action segmentation. In European Conference on Computer\\nVision, pages 36–52. Springer, 2016.\\n[197] Colin Lea, Ren´e Vidal, and Gregory D Hager. Learning convolutional action primitives\\nfor ﬁne-grained action recognition. In 2016 IEEE international conference on robotics\\nand automation (ICRA), pages 1642–1649. IEEE, 2016.\\n82\\n[198] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional\\nnetworks: A uniﬁed approach to action segmentation. In European Conference on\\nComputer Vision, pages 47–54. Springer, 2016.\\n[199] Laura Leal-Taix´e, Cristian Canton-Ferrer, and Konrad Schindler. Learning by tracking:\\nSiamese cnn for robust target association. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition Workshops, pages 33–40, 2016.\\n[200] Laura Leal-Taix´e, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, and Silvio\\nSavarese. Learning an image-based motion context for multiple people tracking. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npages 3542–3549, 2014.\\n[201] Laura Leal-Taix´e, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler.\\nMotchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint\\narXiv:1504.01942, 2015.\\n[202] Yann LeCun.\\nThe mnist database of handwritten digits.\\nhttp://yann. lecun.\\ncom/exdb/mnist/, 1998.\\n[203] Yann LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller.\\nEﬃcient\\nbackprop. In Neural networks: Tricks of the trade, pages 9–50. Springer, 1998.\\n[204] Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for\\nback-propagation.\\nIn Proceedings of the 1988 connectionist models summer school,\\npages 21–28. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.\\n[205] Hyunjae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration\\nmodule for convolutional neural networks. pages 1854–1862, 10 2019.\\n[206] Jae Won Lee, Jonghun Park, Jangmin O, Jongwoo Lee, and Euyseok Hong. A multi-\\nagent approach to q-learning for daily stock trading. Trans. Sys. Man Cyber. Part A,\\n37(6):864–877, November 2007.\\n[207] Joel Z. Leibo, Vin´ıcius Flores Zambaldi, Marc Lanctot, Janusz Marecki, and Thore\\nGraepel. Multi-agent reinforcement learning in sequential social dilemmas. CoRR,\\nabs/1702.03037, 2017.\\n[208] Sergey Levine and Vladlen Koltun. Learning complex neural network policies with tra-\\njectory optimization. In Proceedings of the 31st International Conference on Machine\\nLearning, pages 829–837, 2014.\\n[209] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual\\ntracking with siamese region proposal network. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 8971–8980, 2018.\\n83\\n[210] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang. GS3D: an\\neﬃcient 3d object detection framework for autonomous driving. In IEEE Conference\\non Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,\\nJune 16-20, 2019, pages 1019–1028. Computer Vision Foundation / IEEE, 2019.\\n[211] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Co-occurrence feature learning\\nfrom skeleton data for action recognition and detection with hierarchical aggregation.\\narXiv preprint arXiv:1804.06055, 2018.\\n[212] Duo Li and Qifeng Chen. Deep reinforced attention learning for quality-aware visual\\nrecognition. In European Conference on Computer Vision, pages 493–509, 2020.\\n[213] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. arXiv\\npreprint arXiv:1503.08663, 2015.\\n[214] Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder\\nfor paragraphs and documents. CoRR, abs/1506.01057, 2015.\\n[215] K. Li, M. Rath, and J. W. Burdick. Inverse reinforcement learning via function ap-\\nproximation for clinical motion analysis. In 2018 IEEE International Conference on\\nRobotics and Automation (ICRA), pages 610–617, 2018.\\n[216] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit\\napproach to personalized news article recommendation. In Proceedings of the 19th\\ninternational conference on World wide web, pages 661–670, 2010.\\n[217] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep visual tracking: Review\\nand experimental comparison. Pattern Recognition, 76:323–338, 2018.\\n[218] Yingbo Li and Bernard Merialdo. Multi-video summarization based on video-mmr. In\\n11th International Workshop on Image Analysis for Multimedia Interactive Services\\nWIAMIS 10, pages 1–4. IEEE, 2010.\\n[219] Yuanwei Li, Amir Alansary, Juan J Cerrolaza, Bishesh Khanal, Matthew Sinclair,\\nJacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz, and Daniel\\nRueckert. Fast multiple landmark localisation using a patch-based iterative network.\\nIn International Conference on Medical Image Computing and Computer-Assisted In-\\ntervention, pages 563–571. Springer, 2018.\\n[220] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information for vi-\\nsual tracking: Algorithms and benchmark. IEEE Transactions on Image Processing,\\n24(12):5630–5644, 2015.\\n[221] Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen, Tommaso Mansi,\\nand Dorin Comaniciu. An artiﬁcial agent for robust image registration. In Thirty-First\\nAAAI Conference on Artiﬁcial Intelligence, 2017.\\n84\\n[222] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng\\nWang, and Ya Zhang. Iteratively-reﬁned interactive 3d medical image segmentation\\nwith multi-agent reinforcement learning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 9394–9402, 2020.\\n[223] Timothy P. Lillicrap, Jonathan J. Hunt, Alexand er Pritzel, Nicolas Heess, Tom Erez,\\nYuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep rein-\\nforcement learning. arXiv e-prints, page arXiv:1509.02971, September 2015.\\n[224] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\\nYuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep rein-\\nforcement learning. arXiv preprint arXiv:1509.02971, 2015.\\n[225] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss\\nfor dense object detection. In Proceedings of the IEEE international conference on\\ncomputer vision, pages 2980–2988, 2017.\\n[226] Tony Lindeberg. Scale-space theory in computer vision, volume 256. Springer Science\\n& Business Media, 2013.\\n[227] Geert Litjens, Robert Toth, Wendy van de Ven, Caroline Hoeks, Sjoerd Kerkstra,\\nBram van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang,\\net al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge.\\nMedical image analysis, 18(2):359–373, 2014.\\n[228] Daochang Liu and Tingting Jiang. Deep reinforcement learning for surgical gesture\\nsegmentation and classiﬁcation. In International conference on medical image comput-\\ning and computer-assisted intervention, pages 247–255. Springer, 2018.\\n[229] Hao Liu, Richard Socher, and Caiming Xiong.\\nTaming maml: Eﬃcient unbiased\\nmeta-reinforcement learning. In International Conference on Machine Learning, pages\\n4061–4071, 2019.\\n[230] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua Shen.\\nWeighing counts: Sequential crowd counting by reinforcement learning. 2020.\\n[231] Lijie Liu, Chufan Wu, Jiwen Lu, Lingxi Xie, Jie Zhou, and Qi Tian. Reinforced axial\\nreﬁnement network for monocular 3d object detection. In European Conference on\\nComputer Vision ECCV, pages 540–556, 2020.\\n[232] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd count-\\ning using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601, 2018.\\n[233] Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel Rueckert, and\\nBernhard Kainz. Ultrasound video summarization using deep reinforcement learning.\\narXiv preprint arXiv:2005.09531, 2020.\\n85\\n[234] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npages 5099–5108, 2019.\\n[235] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes\\nin the wild. In Proceedings of the IEEE international conference on computer vision,\\npages 3730–3738, 2015.\\n[236] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for\\nsemantic segmentation. In Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 3431–3440, 2015.\\n[237] Marco Lorenzi, Nicholas Ayache, Giovanni B Frisoni, Xavier Pennec, Alzheimer’s Dis-\\nease Neuroimaging Initiative (ADNI, et al. Lcc-demons: a robust and accurate sym-\\nmetric diﬀeomorphic registration algorithm. NeuroImage, 81:470–483, 2013.\\n[238] Tayebeh Lotﬁ, Lisa Tang, Shawn Andrews, and Ghassan Hamarneh. Improving prob-\\nabilistic image registration via reinforcement learning and uncertainty evaluation. In\\nInternational Workshop on Machine Learning in Medical Imaging, pages 187–194.\\nSpringer, 2013.\\n[239] David G Lowe. Distinctive image features from scale-invariant keypoints. International\\njournal of computer vision, 60(2):91–110, 2004.\\n[240] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou\\nWang. End-to-end active object tracking via reinforcement learning. arXiv preprint\\narXiv:1705.10561, 2017.\\n[241] Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Ad-\\ndressing the rare word problem in neural machine translation. CoRR, abs/1410.8206,\\n2014.\\n[242] Khoa Luu, Chenchen Zhu, Chandrasekhar Bhagavatula, T Hoang Ngan Le, and Marios\\nSavvides.\\nA deep learning approach to joint face detection and segmentation.\\nIn\\nAdvances in Face Detection and Facial Image Analysis, pages 1–12. Springer, 2016.\\n[243] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical convo-\\nlutional features for visual tracking. In Proceedings of the IEEE international confer-\\nence on computer vision, pages 3074–3082, 2015.\\n[244] Kai Ma, Jiangping Wang, Vivek Singh, Birgi Tamersoy, Yao-Jen Chang, Andreas\\nWimmer, and Terrence Chen. Multimodal image registration with deep context re-\\ninforcement learning. In International Conference on Medical Image Computing and\\nComputer-Assisted Intervention, pages 240–248. Springer, 2017.\\n86\\n[245] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video sum-\\nmarization with adversarial lstm networks. In Proceedings of the IEEE conference on\\nComputer Vision and Pattern Recognition, pages 202–211, 2017.\\n[246] Gabriel Maicas, Gustavo Carneiro, Andrew P Bradley, Jacinto C Nascimento, and\\nIan Reid. Deep reinforcement learning for active breast lesion detection from dce-\\nmri. In International conference on medical image computing and computer-assisted\\nintervention, pages 665–673. Springer, 2017.\\n[247] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Deep captioning with\\nmultimodal recurrent neural networks (m-rnn). CoRR, abs/1412.6632, 2014.\\n[248] Nicolas M¨arki, Federico Perazzi, Oliver Wang, and Alexander Sorkine-Hornung. Bilat-\\neral space video segmentation. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 743–751, 2016.\\n[249] T. Martinez-Marin and T. Duckett.\\nFast reinforcement learning for vision-guided\\nmobile robots. In Proceedings of the 2005 IEEE International Conference on Robotics\\nand Automation, pages 4170–4175, 2005.\\n[250] Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement learning\\nfor deformable object manipulation. arXiv preprint arXiv:1806.07851, 2018.\\n[251] Stefan Mathe, Aleksis Pirinen, and Cristian Sminchisescu. Reinforcement learning for\\nvisual object detection. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 2894–2902, 2016.\\n[252] George K Matsopoulos, Nicolaos A Mouravliansky, Konstantinos K Delibasis, and\\nKonstantina S Nikita. Automatic retinal image registration scheme using global opti-\\nmization techniques. IEEE Transactions on Information Technology in Biomedicine,\\n3(1):47–60, 1999.\\n[253] Darryl McClymont, Andrew Mehnert, Adnan Trakic, Dominic Kennedy, and Stuart\\nCrozier. Fully automatic lesion segmentation in breast mri using mean-shift and graph-\\ncuts on a region adjacency graph. Journal of Magnetic Resonance Imaging, 39(4):795–\\n804, 2014.\\n[254] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan\\nFarahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest,\\net al. The multimodal brain tumor image segmentation benchmark (brats). IEEE\\ntransactions on medical imaging, 34(10):1993–2024, 2014.\\n[255] Shun Miao, Rui Liao, Marcus Pﬁster, Li Zhang, and Vincent Ordy. System and method\\nfor 3-d/3-d registration between non-contrast-enhanced cbct and contrast-enhanced ct\\nfor abdominal aortic aneurysm stenting. In International Conference on Medical Image\\nComputing and Computer-Assisted Intervention, pages 380–387. Springer, 2013.\\n87\\n[256] Shun Miao, Z Jane Wang, and Rui Liao. A cnn regression approach for real-time 2d/3d\\nregistration. IEEE transactions on medical imaging, 35(5):1352–1363, 2016.\\n[257] Tomas Mikolov, Stefan Kombrink, Luk´as Burget, Jan Cernock´y, and Sanjeev Khu-\\ndanpur. Extensions of recurrent neural network language model. In ICASSP, pages\\n5528–5531, 2011.\\n[258] Anton Milan, Laura Leal-Taix´e, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16:\\nA benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831, 2016.\\n[259] Anton Milan, Laura Leal-Taix´e, Konrad Schindler, and Ian Reid. Joint tracking and\\nsegmentation of multiple targets. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 5397–5406, 2015.\\n[260] Anton Milan, S Hamid Rezatoﬁghi, Anthony Dick, Ian Reid, and Konrad Schindler.\\nOnline multi-target tracking using recurrent neural networks. In Thirty-First AAAI\\nConference on Artiﬁcial Intelligence, 2017.\\n[261] Shervin Minaee, AmirAli Abdolrashidi, Hang Su, Mohammed Bennamoun, and David\\nZhang. Biometric recognition using deep learning: A survey. CoRR, abs/1912.00271,\\n2019.\\n[262] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy\\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods\\nfor deep reinforcement learning. In International conference on machine learning, pages\\n1928–1937, 2016.\\n[263] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy\\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods\\nfor deep reinforcement learning. In Proceedings of The 33rd International Conference\\non Machine Learning, pages 1928–1937, 20–22 Jun 2016.\\n[264] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Os-\\ntrovski, et al.\\nHuman-level control through deep reinforcement learning.\\nNature,\\n518(7540):529–533, 2015.\\n[265] I. Mordatch, N. Mishra, C. Eppner, and P. Abbeel. Combining model-based policy\\nsearch with online model learning for control of physical humanoids. In 2016 IEEE\\nInternational Conference on Robotics and Automation (ICRA), pages 242–248, 2016.\\n[266] J. Morimoto, G. Zeglin, and C. G. Atkeson.\\nMinimax diﬀerential dynamic pro-\\ngramming: application to a biped walking robot.\\nIn Proceedings 2003 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems (IROS 2003) (Cat.\\nNo.03CH37453), volume 2, pages 1927–1932, 2003.\\n88\\n[267] Jun Morimoto and Christopher G. Atkeson. Nonparametric representation of an ap-\\nproximated poincar´e map for learning biped locomotion. In Autonomous Robots, page\\n131–144, 2009.\\n[268] A. Mousavian, D. Anguelov, J. Flynn, and J. Koˇseck´a. 3d bounding box estimation\\nusing deep learning and geometry. In 2017 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 5632–5640, 2017.\\n[269] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator for\\nuav tracking. In European conference on computer vision, pages 445–461. Springer,\\n2016.\\n[270] Don Murray and Anup Basu. Motion tracking with an active camera. IEEE transac-\\ntions on pattern analysis and machine intelligence, 16(5):449–459, 1994.\\n[271] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey\\nLevine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments\\nthrough meta-reinforcement learning. arXiv preprint arXiv:1803.11347, 2018.\\n[272] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming\\nexploration in reinforcement learning with demonstrations. In 2018 IEEE International\\nConference on Robotics and Automation (ICRA), pages 6292–6299, 2018.\\n[273] Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural net-\\nworks for visual tracking. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition, pages 4293–4302, 2016.\\n[274] Ali Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, and Khaled Shaalan.\\nSpeech recognition using deep neural networks: A systematic review. IEEE Access,\\n7:19143–19165, 2019.\\n[275] Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C Peeken,\\nStephanie E Combs, and Bjoern H Menze.\\nDeep reinforcement learning for organ\\nlocalization in ct. arXiv preprint arXiv:2005.04974, 2020.\\n[276] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y\\nNg. Reading digits in natural images with unsupervised feature learning. 2011.\\n[277] Dominik Neumann, Saˇsa Grbi´c, Matthias John, Nassir Navab, Joachim Hornegger, and\\nRazvan Ionasec. Probabilistic sparse matching for robust 3d/3d fusion in minimally\\ninvasive surgery. IEEE transactions on medical imaging, 34(1):49–60, 2014.\\n[278] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In\\nProceedings of the Seventeenth International Conference on Machine Learning, ICML\\n’00, page 663–670, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.\\n89\\n[279] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning.\\nIn Icml, volume 1, page 2, 2000.\\n[280] Trung Thanh Nguyen, Zhuoru Li, Tomi Silander, and Tze-Yun Leong. Online feature\\nselection for model-based reinforcement learning. In Proceedings of the 30th Inter-\\nnational Conference on International Conference on Machine Learning - Volume 28,\\npage I–498–I–506, 2013.\\n[281] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Ngan Le, and Marios Savvides. Tem-\\nporal non-volume preserving approach to facial age-progression and age-invariant face\\nrecognition. In Proceedings of the IEEE International Conference on Computer Vision,\\npages 3735–3743, 2017.\\n[282] C. Niedzwiedz, I. Elhanany, Zhenzhen Liu, and S. Livingston. A consolidated actor-\\ncritic model with function approximation for high-dimensional pomdps.\\nIn AAAI\\n2008Workshop for Advancement in POMDP Solvers, 2008.\\n[283] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang.\\nA\\nreview of deep learning based speech synthesis. Applied Sciences, 9(19), 2019.\\n[284] Kenji Okuma, Ali Taleghani, Nando De Freitas, James J Little, and David G Lowe. A\\nboosted particle ﬁlter: Multitarget detection and tracking. In European conference on\\ncomputer vision, pages 28–39. Springer, 2004.\\n[285] Jos´e Ignacio Orlando, Huazhu Fu, Jo˜ao Barbosa Breda, Karel van Keer, Deepti R\\nBathula, Andr´es Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo\\nLee, et al. Refuge challenge: A uniﬁed framework for evaluating automated methods\\nfor glaucoma assessment from fundus photographs. Medical image analysis, 59:101570,\\n2020.\\n[286] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters. 2018.\\n[287] Rameswar Panda, Abir Das, Ziyan Wu, Jan Ernst, and Amit K Roy-Chowdhury.\\nWeakly supervised summarization of web videos. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision, pages 3657–3666, 2017.\\n[288] Anestis Papazoglou and Vittorio Ferrari. Fast object segmentation in unconstrained\\nvideo. In Proceedings of the IEEE international conference on computer vision, pages\\n1777–1784, 2013.\\n[289] I. C. Paschalidis, K. Li, and R. Moazzez Estanjini. An actor-critic method using least\\nsquares temporal diﬀerence learning. In Proceedings of the 48h IEEE Conference on\\nDecision and Control (CDC) held jointly with 2009 28th Chinese Control Conference,\\npages 2564–2569, 2009.\\n90\\n[290] Massimiliano Patacchiola and Angelo Cangelosi. Head pose estimation in the wild using\\nconvolutional neural networks and adaptive gradient methods. Pattern Recognition,\\n71:132–143, 2017.\\n[291] Hanchuan Peng, Zongcai Ruan, Fuhui Long, Julie H Simpson, and Eugene W Myers.\\nV3d enables real-time 3d visualization and quantitative analysis of large-scale biological\\nimage data sets. Nature biotechnology, 28(4):348–353, 2010.\\n[292] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander\\nSorkine-Hornung. Learning video object segmentation from static images. In Pro-\\nceedings of the IEEE conference on computer vision and pattern recognition, pages\\n2663–2672, 2017.\\n[293] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross,\\nand Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology\\nfor video object segmentation. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 724–732, 2016.\\n[294] Jan Peters and Stefan Schaal.\\nReinforcement learning of motor skills with policy\\ngradients. Neural Networks, 21(4):682 – 697, 2008.\\n[295] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning of region pro-\\nposal networks for object detection. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pages 6945–6954, 2018.\\n[296] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal greedy\\nalgorithms for tracking a variable number of objects. In CVPR 2011, pages 1201–1208.\\nIEEE, 2011.\\n[297] Aske Plaat, Walter Kosters, and Mike Preuss. Deep model-based reinforcement learn-\\ning for high-dimensional problems, a survey, 2020.\\n[298] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid. Category-\\nspeciﬁc video summarization. In European conference on computer vision, pages 540–\\n555. Springer, 2014.\\n[299] Reza Pourreza-Shahri and Nasser Kehtarnavaz. Exposure bracketing via automatic ex-\\nposure selection. In 2015 IEEE International Conference on Image Processing (ICIP),\\npages 320–323. IEEE, 2015.\\n[300] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio\\nFerrari. Learning object class detectors from weakly annotated video. In 2012 IEEE\\nConference on Computer Vision and Pattern Recognition, pages 3282–3289. IEEE,\\n2012.\\n91\\n[301] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo Lim,\\nand Ming-Hsuan Yang. Hedged deep tracking. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 4303–4311, 2016.\\n[302] Zengyi Qin, Jinglu Wang, and Yan Lu. Monogrnet: A geometric reasoning network for\\nmonocular 3d object localization. Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, 33(01):8851–8858, Jul. 2019.\\n[303] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Ef-\\nﬁcient oﬀ-policy meta-reinforcement learning via probabilistic context variables. In\\nInternational conference on machine learning, pages 5331–5340, 2019.\\n[304] Vidhiwar Singh Rathour, Kashu Yamakazi, and T Le. Roughness index and roughness\\ndistance for benchmarking medical segmentation. arXiv preprint arXiv:2103.12350,\\n2021.\\n[305] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once:\\nUniﬁed, real-time object detection. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 779–788, 2016.\\n[306] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint\\narXiv:1804.02767, 2018.\\n[307] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and Jie Zhou. Collaborative deep\\nreinforcement learning for multi-object tracking. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 586–602, 2018.\\n[308] Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, and Jie Zhou. Deep reinforcement\\nlearning with iterative shift for visual tracking. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 684–700, 2018.\\n[309] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-\\ntime object detection with region proposal networks. In Advances in neural information\\nprocessing systems, pages 91–99, 2015.\\n[310] Md Reza, Jana Kosecka, et al. Reinforcement learning for semantic segmentation in\\nindoor scenes. arXiv preprint arXiv:1606.01178, 2016.\\n[311] Alexander Richard and Juergen Gall. Temporal action detection using a statistical\\nlanguage model.\\nIn Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3131–3140, 2016.\\n[312] Mrigank Rochan, Linwei Ye, and Yang Wang. Video summarization using fully convo-\\nlutional sequence networks. In Proceedings of the European Conference on Computer\\nVision (ECCV), pages 347–363, 2018.\\n92\\n[313] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\\nfor biomedical image segmentation.\\nIn International Conference on Medical image\\ncomputing and computer-assisted intervention, pages 234–241. Springer, 2015.\\n[314] German Ros, Vladfen Koltun, Felipe Codevilla, and Antonio Lopez. The carla au-\\ntonomous driving challenge, 2019.\\n[315] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\\n” grabcut” interactive\\nforeground extraction using iterated graph cuts. ACM transactions on graphics (TOG),\\n23(3):309–314, 2004.\\n[316] David Rotman. Mit technology review. Retrieved from Meet the Man with a Cheap and\\nEasy Plan to Stop Global Warming: http://www. technologyreview. com/featuredstor\\ny/511016/a-cheap-and-easy-plan-to-stop-globalwarming, 2013.\\n[317] J-M Rouet, J-J Jacq, and Christian Roux. Genetic algorithms for a robust 3-d mr-ct\\nregistration. IEEE transactions on information technology in biomedicine, 4(2):126–\\n136, 2000.\\n[318] David E Rumelhart.\\nThe architecture of mind: A connectionist approach.\\nMind\\nreadings, pages 207–238, 1998.\\n[319] T. P. Runarsson and S. M. Lucas. Imitating play from game trajectories: Temporal\\ndiﬀerence learning versus preference learning. In 2012 IEEE Conference on Computa-\\ntional Intelligence and Games (CIG), pages 79–82, 2012.\\n[320] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\\nlarge scale visual recognition challenge. International Journal of Computer Vision,\\n115(3):211–252, 2015.\\n[321] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese.\\nTracking the untrackable:\\nLearning to track multiple cues with long-term dependencies. In Proceedings of the\\nIEEE International Conference on Computer Vision, pages 300–311, 2017.\\n[322] Steind´or Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth.\\nMeta re-\\ninforcement learning with latent variable gaussian processes.\\narXiv preprint\\narXiv:1803.07551, 2018.\\n[323] Farhang Sahba.\\nDeep reinforcement learning for object segmentation in video se-\\nquences. In 2016 International Conference on Computational Science and Computa-\\ntional Intelligence (CSCI), pages 857–860. IEEE, 2016.\\n[324] Farhang Sahba, Hamid R Tizhoosh, and Magdy MA Salama. A reinforcement learning\\nframework for medical image segmentation. In The 2006 IEEE International Joint\\nConference on Neural Network Proceedings, pages 511–517. IEEE, 2006.\\n93\\n[325] Farhang Sahba, Hamid R Tizhoosh, and Magdy MMA Salama.\\nApplication of\\nopposition-based reinforcement learning in image segmentation. In 2007 IEEE Sym-\\nposium on Computational Intelligence in Image and Signal Processing, pages 246–251.\\nIEEE, 2007.\\n[326] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.\\nTrust region policy optimization. In International conference on machine learning,\\npages 1889–1897, 2015.\\n[327] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel.\\nTrust Region Policy Optimization. arXiv e-prints, February 2015.\\n[328] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal Policy Optimization Algorithms. arXiv e-prints, July 2017.\\n[329] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n[330] Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural\\nNetworks, 16(1):5–9, 2003.\\n[331] Shahin Sefati, Noah J Cowan, and Ren´e Vidal. Learning shared, discriminative dic-\\ntionaries for surgical gesture segmentation and classiﬁcation. In MICCAI Workshop:\\nM2CAI, volume 4, 2015.\\n[332] Mohammad Javad Shaﬁee, Brendan Chywl, Francis Li, and Alexander Wong. Fast\\nyolo: A fast you only look once system for real-time embedded object detection in\\nvideo. arXiv preprint arXiv:1709.05943, 2017.\\n[333] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A large\\nscale dataset for 3d human activity analysis. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 1010–1019, 2016.\\n[334] M. R. Shaker, Shigang Yue, and T. Duckett. Vision-based reinforcement learning using\\napproximate policy iteration. In 2009 International Conference on Advanced Robotics,\\npages 1–6, 2009.\\n[335] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, re-\\ninforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016.\\n[336] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kossaiﬁ, Georgios Tzimiropou-\\nlos, and Maja Pantic. The ﬁrst facial landmark tracking in-the-wild challenge: Bench-\\nmark and results. In Proceedings of the IEEE international conference on computer\\nvision workshops, pages 50–58, 2015.\\n94\\n[337] Y. Shi, L. Cui, Z. Qi, F. Meng, and Z. Chen. Automatic road crack detection using\\nrandom structured forests. IEEE Transactions on Intelligent Transportation Systems,\\n17(12):3434–3445, 2016.\\n[338] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmen-\\ntation and support inference from rgbd images. In European conference on computer\\nvision, pages 746–760. Springer, 2012.\\n[339] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin\\nRiedmiller. Deterministic policy gradient algorithms. 2014.\\n[340] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\\nscale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n[341] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom\\nfeature fusion for crowd counting. In Proceedings of the IEEE International Conference\\non Computer Vision, pages 1002–1012, 2019.\\n[342] Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan,\\nand Bal´azs Guly´as. 3d deep learning on medical images: A review, 2020.\\n[343] Gwangmo Song, Heesoo Myeong, and Kyoung Mu Lee.\\nSeednet: Automatic seed\\ngeneration with deep reinforcement learning for robust interactive segmentation. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\\n1760–1768, 2018.\\n[344] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Sum-\\nmarizing web videos using titles. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 5179–5187, 2015.\\n[345] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson WH Lau, and Ming-Hsuan\\nYang. Crest: Convolutional residual learning for visual tracking. In Proceedings of the\\nIEEE International Conference on Computer Vision, pages 2555–2564, 2017.\\n[346] Concetto Spampinato, Simone Palazzo, and Daniela Giordano. Gamifying video ob-\\nject segmentation. IEEE transactions on pattern analysis and machine intelligence,\\n39(10):1942–1958, 2016.\\n[347] Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning.\\nCoRR, abs/1703.01703, 2017.\\n[348] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary\\nmean-ﬁeld games. page 251–259. International Foundation for Autonomous Agents\\nand Multiagent Systems, 2019.\\n95\\n[349] Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi Song, and\\nXi Wu.\\nRobust multimodal image registration using deep recurrent reinforcement\\nlearning. In Asian Conference on Computer Vision, pages 511–526. Springer, 2018.\\n[350] Kalaivani Sundararajan and Damon L. Woodard. Deep learning for biometrics: A\\nsurvey. 51(3), 2018.\\n[351] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.\\nMIT press, 2018.\\n[352] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy\\ngradient methods for reinforcement learning with function approximation. In Proceed-\\nings of the 12th International Conference on Neural Information Processing Systems,\\nNIPS’99, page 1057–1063, 1999.\\n[353] Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy\\ngradient methods for reinforcement learning with function approximation. In Advances\\nin Neural Information Processing Systems 12, pages 1057–1063. 2000.\\n[354] Christian Szegedy, Sergey Ioﬀe, Vincent Vanhoucke, and Alexander A Alemi.\\nInception-v4, inception-resnet and the impact of residual connections on learning. In\\nThirty-ﬁrst AAAI conference on artiﬁcial intelligence, 2017.\\n[355] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper\\nwith convolutions.\\nIn Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 1–9, 2015.\\n[356] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for\\nobject detection. In Advances in neural information processing systems, pages 2553–\\n2561, 2013.\\n[357] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive rein-\\nforcement learning for skeleton-based action recognition. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, pages 5323–5332, 2018.\\n[358] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.\\nSiamese instance search\\nfor tracking. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 1420–1429, 2016.\\n[359] Philippe Th´evenaz and Michael Unser. Optimization of mutual information for mul-\\ntiresolution image registration. IEEE transactions on image processing, 9(12):2083–\\n2099, 2000.\\n[360] Zhiqiang Tian, Xiangyu Si, Yaoyue Zheng, Zhang Chen, and Xiaojian Li.\\nMulti-\\nstep medical image segmentation based on reinforcement learning. JOURNAL OF\\nAMBIENT INTELLIGENCE AND HUMANIZED COMPUTING, 2020.\\n96\\n[361] Marin Toromanoﬀ, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free re-\\ninforcement learning for urban driving using implicit aﬀordances. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7153–\\n7162, 2020.\\n[362] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep\\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 1653–1660, 2014.\\n[363] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation via object\\nﬂow. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 3899–3908, 2016.\\n[364] Y. Tsurumine, Y. Cui, K. Yamazaki, and T. Matsubara. Generative adversarial im-\\nitation learning with deep p-network for robotic cloth manipulation. In 2019 IEEE-\\nRAS 19th International Conference on Humanoid Robots (Humanoids), pages 274–280,\\n2019.\\n[365] Yoshihisa Tsurumine, Yunduan Cui, Eiji Uchibe, and Takamitsu Matsubara. Deep\\nreinforcement learning with smooth policy update: Application to robotic cloth ma-\\nnipulation. Robotics and Autonomous Systems, 112:72 – 83, 2019.\\n[366] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeul-\\nders. Selective search for object recognition. International journal of computer vision,\\n104(2):154–171, 2013.\\n[367] Burak Uzkent, Christopher Yeh, and Stefano Ermon.\\nEﬃcient object detection in\\nlarge images using deep reinforcement learning. In The IEEE Winter Conference on\\nApplications of Computer Vision, pages 1824–1833, 2020.\\n[368] Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip HS Torr.\\nEnd-to-end representation learning for correlation ﬁlter based tracking. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition, pages 2805–\\n2813, 2017.\\n[369] Peter van Beek. Improved image selection for stack-based hdr imaging. arXiv preprint\\narXiv:1806.07420, 2018.\\n[370] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with\\nDouble Q-learning. arXiv e-prints, page arXiv:1509.06461, September 2015.\\n[371] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with\\ndouble q-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.\\n[372] Leo Van Hove. Optimal denominations for coins and bank notes: in defense of the\\nprinciple of least eﬀort. Journal of Money, Credit and Banking, pages 1015–1021, 2001.\\n97\\n[373] Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo, and Concetto\\nSpampinato. Mask-rl: Multiagent video object segmentation framework through re-\\ninforcement learning. IEEE Transactions on Neural Networks and Learning Systems,\\n2020.\\n[374] Kashu Yamakazi Akihiro Sugimoto Viet-Khoa Vo-Ho, Ngan T.H. Le and Triet Tran.\\nAgent-environment network for temporal action proposal generation. In International\\nConference on Acoustics, Speech and Signal Processing. 2021.\\n[375] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar,\\nand Katerina Fragkiadaki. Sfm-net: Learning of structure and motion from video.\\narXiv preprint arXiv:1704.07804, 2017.\\n[376] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jader-\\nberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard\\nPowell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Aga-\\npiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky,\\nSasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar\\nGulcehre, Ziyu Wang, Tobias Pfaﬀ, Toby Pohlen, Dani Yogatama, Julia Cohen,\\nKatrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps,\\nKoray Kavukcuoglu, Demis Hassabis, and David Silver.\\nAlphaStar:\\nMaster-\\ning the Real-Time Strategy Game StarCraft II.\\nhttps://deepmind.com/blog/\\nalphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.\\n[377] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert, and\\nBernhard Kainz. Multiple landmark detection using multi-agent reinforcement learn-\\ning. In International Conference on Medical Image Computing and Computer-Assisted\\nIntervention, pages 262–270. Springer, 2019.\\n[378] Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt, Premal A Patel, Michael\\nAertsen, Tom Doel, Anna L David, Jan Deprest, S´ebastien Ourselin, et al. Deepi-\\ngeos: a deep interactive geodesic framework for medical image segmentation. IEEE\\ntransactions on pattern analysis and machine intelligence, 41(7):1559–1572, 2018.\\n[379] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng\\nLi, and Wei Liu.\\nCosface: Large margin cosine loss for deep face recognition.\\nIn\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npages 5265–5274, 2018.\\n[380] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R´emi\\nMunos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to\\nreinforcement learn. CoRR, abs/1611.05763, 2016.\\n[381] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Deep networks for\\nsaliency detection via local estimation and global search. In Computer Vision and Pat-\\ntern Recognition (CVPR), 2015 IEEE Conference on, pages 3183–3192. IEEE, 2015.\\n98\\n[382] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware\\nreinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 9322–9331, 2020.\\n[383] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces\\nin the wild: Reducing racial bias by information maximization adaptation network. In\\nProceedings of the IEEE International Conference on Computer Vision, pages 692–702,\\n2019.\\n[384] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact image representation for\\nvisual tracking. In Advances in neural information processing systems, pages 809–817,\\n2013.\\n[385] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois,\\nShunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-\\nbased reinforcement learning. CoRR, abs/1907.02057, 2019.\\n[386] Yan Wang, Lei Zhang, Lituan Wang, and Zizhou Wang. Multitask learning for object\\nlocalization with deep reinforcement learning. IEEE Transactions on Cognitive and\\nDevelopmental Systems, 11(4):573–580, 2018.\\n[387] Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, and Maja Pantic.\\nDynamic face video segmentation via reinforcement learning. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6959–\\n6969, 2020.\\n[388] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality\\nassessment: from error visibility to structural similarity. IEEE transactions on image\\nprocessing, 13(4):600–612, 2004.\\n[389] Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, and Jimmy Ren.\\nLearning a reinforced agent for ﬂexible exposure bracketing selection. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n1820–1828, 2020.\\n[390] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando\\nDe Freitas.\\nDueling network architectures for deep reinforcement learning.\\narXiv\\npreprint arXiv:1511.06581, 2015.\\n[391] Wayne A Wickelgren.\\nThe long and the short of memory.\\nPsychological Bulletin,\\n80(6):425, 1973.\\n[392] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist\\nreinforcement learning. Machine learning, 8(3-4):229–256, 1992.\\n99\\n[393] Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data to improve\\nbayesian optimization for reinforcement learning. Journal of Machine Learning Re-\\nsearch, 15(8):253–282, 2014.\\n[394] C. Wirth and J. F¨urnkranz. On learning from game annotations. IEEE Transactions\\non Computational Intelligence and AI in Games, 7(3):304–316, 2015.\\n[395] Paul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition and\\n3d pose estimation. In Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 3109–3118, 2015.\\n[396] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convo-\\nlutional block attention module. In European Conference on Computer Vision, pages\\n3–19, 2018.\\n[397] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 2411–2418, 2013.\\n[398] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, 37(9):1834–1848, 2015.\\n[399] Lu Xia, Chia-Chih Chen, and Jake K Aggarwal. View invariant human action recog-\\nnition using histograms of 3d joints. In 2012 IEEE Computer Society Conference on\\nComputer Vision and Pattern Recognition Workshops, pages 20–27. IEEE, 2012.\\n[400] Sitao Xiang and Hao Li. On the eﬀects of batch and weight normalization in generative\\nadversarial networks. arXiv preprint arXiv:1704.03971, 2017.\\n[401] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online multi-object\\ntracking by decision making. In Proceedings of the IEEE international conference on\\ncomputer vision, pages 4705–4713, 2015.\\n[402] Fanyi Xiao and Yong Jae Lee. Track and segment: An iterative unsupervised approach\\nfor video object proposals. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition, pages 933–942, 2016.\\n[403] Hang Xiao and Hanchuan Peng. App2: automatic tracing of 3d neuron morphology\\nbased on hierarchical pruning of a gray-weighted image distance-tree. Bioinformatics,\\n29(11):1448–1454, 2013.\\n[404] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy\\nstudent improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 10687–10698, 2020.\\n100\\n[405] Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, and Chunhua Shen.\\nFrom open set to closed set: Counting objects by spatial divide-and-conquer.\\nIn\\nProceedings of the IEEE International Conference on Computer Vision, pages 8362–\\n8371, 2019.\\n[406] Hailiang Xu and Feng Su. Robust seed localization and growing with deep convolu-\\ntional features for scene text detection. In Proceedings of the 5th ACM on International\\nConference on Multimedia Retrieval, pages 387–394. ACM, 2015.\\n[407] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep interactive\\nobject selection.\\nIn Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 373–381, 2016.\\n[408] Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, and Josef Kittler. Learning adaptive\\ndiscriminative correlation ﬁlters via temporal consistency preserving spatial feature\\nselection for robust visual object tracking. IEEE Transactions on Image Processing,\\n28(11):5596–5609, 2019.\\n[409] Xuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Eﬃcient multiple\\norgan localization in ct image using 3d region proposal network. IEEE transactions\\non medical imaging, 38(8):1885–1898, 2019.\\n[410] Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, and Chun-Yi Lee.\\nDynamic video\\nsegmentation network. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 6556–6565, 2018.\\n[411] Kashu Yamazaki, Vidhiwar Singh Rathour, and T Le.\\nInvertible residual net-\\nwork with regularization for eﬀective medical image segmentation.\\narXiv preprint\\narXiv:2103.09042, 2021.\\n[412] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen,\\nand Errui Ding.\\nPerspective-guided convolution networks for crowd counting.\\nIn\\nProceedings of the IEEE International Conference on Computer Vision, pages 952–\\n961, 2019.\\n[413] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory Zelinsky,\\nDimitris Samaras, and Minh Hoai. Predicting goal-directed human attention using\\ninverse reinforcement learning. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), June 2020.\\n[414] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: a\\nsurvey. arXiv preprint arXiv:1908.08796, 2019.\\n[415] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,\\nand Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta\\nreinforcement learning. In Conference on Robot Learning, pages 1094–1100, 2020.\\n101\\n[416] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, and Jin Young Choi.\\nAction-decision networks for visual tracking with deep reinforcement learning. In Pro-\\nceedings of the IEEE conference on computer vision and pattern recognition, pages\\n2711–2720, 2017.\\n[417] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, and Xia Hu. Experience replay opti-\\nmization. arXiv preprint arXiv:1906.08387, 2019.\\n[418] Da Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang. Deep reinforcement learning\\nfor visual object tracking in videos. arXiv preprint arXiv:1701.08936, 2017.\\n[419] Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, and Junwei Han. Spftn: A self-paced\\nﬁne-tuning network for segmenting objects in weakly labelled videos. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 4429–4437,\\n2017.\\n[420] Jing Zhang, Wanqing Li, Philip O Ogunbona, Pichao Wang, and Chang Tang. Rgb-\\nd-based action recognition datasets: A survey. Pattern Recognition, 60:86–105, 2016.\\n[421] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization with\\nlong short-term memory. In European conference on computer vision, pages 766–782.\\nSpringer, 2016.\\n[422] Pengyu Zhang, Dong Wang, and Huchuan Lu. Multi-modal visual tracking: Review\\nand experimental comparison, 2020.\\n[423] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image\\ncrowd counting via multi-column convolutional neural network. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition, pages 589–597, 2016.\\n[424] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet\\nfor real-time semantic segmentation on high-resolution images. In Proceedings of the\\nEuropean Conference on Computer Vision (ECCV), pages 405–420, 2018.\\n[425] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid\\nscene parsing network. In Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 2881–2890, 2017.\\n[426] Zhong-Qiu Zhao, Shou-Tao Xu, Dian Liu, Wei-Dong Tian, and Zhi-Da Jiang. A review\\nof image set classiﬁcation. Neurocomputing, 335:251–260, 2019.\\n[427] Yefeng Zheng, David Liu, Bogdan Georgescu, Hien Nguyen, and Dorin Comaniciu.\\n3d deep learning for eﬃcient and robust landmark detection in volumetric data. In\\nInternational Conference on Medical Image Computing and Computer-Assisted Inter-\\nvention, pages 565–572. Springer, 2015.\\n102\\n[428] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Tor-\\nralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 633–641, 2017.\\n[429] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for unsupervised\\nvideo summarization with diversity-representativeness reward. In Thirty-Second AAAI\\nConference on Artiﬁcial Intelligence, 2018.\\n[430] Kaiyang Zhou, Tao Xiang, and Andrea Cavallaro. Video summarisation by classiﬁca-\\ntion with deep reinforcement learning. arXiv preprint arXiv:1807.03089, 2018.\\n[431] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow\\nfor video recognition. In Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 2349–2358, 2017.\\n[432] Xiahai Zhuang and Juan Shen. Multi-scale patch and multi-modality atlases for whole\\nheart segmentation of mri. Medical image analysis, 31:77–87, 2016.\\n[433] Will Y Zou, Xiaoyu Wang, Miao Sun, and Yuanqing Lin. Generic object detection\\nwith dense neural patterns and regionlets. arXiv preprint arXiv:1404.4316, 2014.\\n103\\n',\n",
       " '2112.01590v3.pdf': 'The Art and Practice of Data Science Pipelines\\nA Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large\\nSumon Biswas\\nIowa State University\\nAmes, IA, USA\\nsumon@iastate.edu\\nMohammad Wardat\\nIowa State University\\nAmes, IA, USA\\nwardat@iastate.edu\\nHridesh Rajan\\nIowa State University\\nAmes, IA, USA\\nhridesh@iastate.edu\\nABSTRACT\\nIncreasingly larger number of software systems today are including\\ndata science components for descriptive, predictive, and prescriptive\\nanalytics. The collection of data science stages from acquisition, to\\ncleaning/curation, to modeling, and so on are referred to as data\\nscience pipelines. To facilitate research and practice on data science\\npipelines, it is essential to understand their nature. What are the\\ntypical stages of a data science pipeline? How are they connected?\\nDo the pipelines differ in the theoretical representations and that in\\nthe practice? Today we do not fully understand these architectural\\ncharacteristics of data science pipelines. In this work, we present a\\nthree-pronged comprehensive study to answer this for the state-\\nof-the-art, data science in-the-small, and data science in-the-large.\\nOur study analyzes three datasets: a collection of 71 proposals for\\ndata science pipelines and related concepts in theory, a collection\\nof over 105 implementations of curated data science pipelines from\\nKaggle competitions to understand data science in-the-small, and\\na collection of 21 mature data science projects from GitHub to\\nunderstand data science in-the-large. Our study has led to three\\nrepresentations of data science pipelines that capture the essence\\nof our subjects in theory, in-the-small, and in-the-large.\\nCCS CONCEPTS\\n• Software and its engineering →Software creation and man-\\nagement; • Computing methodologies →Machine learning.\\nKEYWORDS\\ndata science pipelines, data science processes, descriptive, predictive\\nACM Reference Format:\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan. 2022. The Art\\nand Practice of Data Science Pipelines: A Comprehensive Study of Data\\nScience Pipelines In Theory, In-The-Small, and In-The-Large. In 44th Inter-\\nnational Conference on Software Engineering (ICSE ’22), May 21–29, 2022,\\nPittsburgh, PA, USA. ACM, New York, NY, USA, 16 pages. https://doi.org/10.\\n1145/3510003.3510057\\n1\\nINTRODUCTION\\nData science processes, also called data science stages as in stages of\\na pipeline, for descriptive, predictive, and prescriptive analytics are\\nbecoming integral components of many software systems today.\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\n© 2022 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-9221-1/22/05.\\nhttps://doi.org/10.1145/3510003.3510057\\nThe data science stages are organized into a data science pipeline,\\nwhere data might flow from one stage in the pipeline to the next.\\nThese data science stages generally perform different tasks such\\nas data acquisition, data preparation, storage, feature engineering,\\nmodeling, training, evaluation of the machine learning model, etc.\\nIn order to design and build software systems with data science\\nstages effectively, we must understand the structure of the data\\nscience pipelines. Previous work has shown that understanding the\\nstructure and patterns used in existing systems and literature can\\nhelp build better systems [34, 107]. In this work, we have taken the\\nfirst step to understand the structure and patterns of DS pipelines.\\nFortunately, we have a number of instances in both the state-of-\\nthe-art and practice to draw observations. In the literature, there\\nhave been a number of proposals to organize data science pipelines.\\nWe call such proposals DS Pipelines in theory. Another source of\\ninformation is Kaggle, a widely known platform for data scientists\\nto host and participate in DS competitions, share datasets, machine\\nlearning models, and code. Kaggle contains a large number of data\\nscience pipelines, but these pipelines are typically developed by a\\nsingle data scientist as small standalone programs. We call such\\ninstances DS Pipelines in-the-small. The third source of DS pipelines\\nare mature data science projects on GitHub developed by teams,\\nsuitable for reuse. We call such instances DS Pipelines in-the-large.\\nThis work presents a study of DS pipelines in theory, in-the-\\nsmall, and in-the-large. We studied 71 different proposals for DS\\npipelines and related concepts from the literature. We also studied\\n105 instances of DS pipelines from Kaggle. Finally, we studied 21\\nmatured open-source data science projects from GitHub. For both\\nKaggle and GitHub, we selected projects that make use of Python to\\nease comparative analysis. In each setting, we answer the following\\noverarching questions.\\n(1) Representative pipeline: What are the stages in DS pipeline\\nand how frequently they appear?\\n(2) Organization: How are the pipeline stages organized?\\n(3) Characteristics: What are the characteristics of the pipelines\\nin a setting and how does that compare with the others?\\nThis work attempts to inform the terminology and practice for\\ndesigning DS pipeline. We found that DS pipelines differ signifi-\\ncantly in terms of detailed structures and patterns among theory,\\nin-the-small, and in-the-large. Specifically, a number of stages are\\nabsent in-the-small, and the pipelines have a more linear structure\\nwith an emphasis on data exploration. Out of the eleven stages\\nseen in theory, only six stages are present in pipeline in-the-small,\\nnamely data collection, data preparation, modeling, training, eval-\\nuation, and prediction. In addition, pipelines in-the-small do not\\nhave clear separation between stages which makes the maintenance\\nharder. On the other hand, the DS pipelines in-the-large have a\\narXiv:2112.01590v3  [cs.SE]  14 Feb 2022\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\nmore complex structure with feedback loops and sub-pipelines.\\nWe identified different pipeline patterns followed in specific phase\\n(development/post-development) of the large DS projects. The ab-\\nstraction of stages are stricter in-the-large having both loosely- and\\ntightly-coupled structure.\\nOur investigation also suggest that DS pipeline is a well used\\nsoftware architecture but often built in ad hoc manner. We demon-\\nstrated the importance of standardization and analysis framework\\nfor DS pipeline following the traditional software engineering re-\\nsearch on software architecture and design patterns [70, 84, 107].\\nWe contributed three representations of DS pipelines that capture\\nthe essence of our subjects in theory, in-the-small, and in-the-large\\nthat would facilitate building new DS systems. We anticipate our\\nresults to inform design decisions made by the pipeline architects,\\npractitioners, and software engineering teams. Our results will also\\nhelp the DS researchers and developers to identify whether the\\npipeline is missing any important stage or feedback loops (e.g.,\\nstorage and evaluation are missed in many pipelines).\\nThe rest of this paper is organized as follows: in section §2, we\\npresent our study of DS pipelines in theory. Section §3 describes\\nour study of DS pipelines in-the-small. In section §4, we describe\\nour study of DS pipelines in-the-large. Section §5 discusses the\\nimplications, section §6 describes the threats to the validity, section\\n§7 describes related work, and section §8 concludes.\\n2\\nDS PIPELINE IN THEORY\\nData Science. Data Science (DS) is a broad area that brings together\\ncomputational understanding, inferential thinking, and the knowl-\\nedge of the application area. Wing [129] argues that DS studies how\\nto extract value out of data. However, the value of data and extrac-\\ntion process depends on the application and context. DS includes a\\nbroad set of traditional disciplines such as data management, data\\ninfrastructure building, data-intensive algorithm development, AI\\n(machine learning and deep learning), etc., that covers both the\\nfundamental and practical perspectives from computer science,\\nmathematics, statistics, and domain-specific knowledge [13, 116].\\nDS also incorporates the business, organization, policy and privacy\\nissues of data and data-related processes. Any DS project involves\\nthree main stages: data collection and preparation, analysis and\\nmodeling, and finally deployment [128]. DS is also more than statis-\\ntics or data mining since it incorporates understanding of data and\\nits pattern, developing important questions and answering them,\\nand communicating results [116].\\nData Science Pipeline. The term pipeline was introduced by\\nGarlan with box-and-line diagrams and explanatory prose that as-\\nsist software developers to design and describe complex systems so\\nthat the software becomes intelligible [37]. Shaw and Garlan have\\nprovided the pipes-and-filter design pattern that involves stages\\nwith processing units (filters) and ordered connections (pipes) [107].\\nThey also argued that pipeline gives proper semantics and vo-\\ncabulary which helps to describe the concerns, constraints, re-\\nlationship between the sub-systems, and overall computational\\nparadigm [37, 107]. By data science pipeline (DS pipeline), we are\\nreferring to a series of processing stages that interact with data,\\nusually acquisition, management, analysis, and reasoning [77, 79].\\nThe sequential DS stages from acquisition, to cleaning/curation, to\\nmodeling, and so on are referred to as data science pipeline. A DS\\npipeline may consist of several stages and connections between\\nthem. The stages are defined to perform particular tasks and con-\\nnected to other stage(s) with input-output relations [6]. However,\\nthe definitions of the stages are not consistent across studies in\\nthe literature. The terminology vary depending on the application\\ncontext and focus.\\nDifferent study in the literature presented DS pipeline based\\non their context and desiderata. No study has been conducted to\\nunify the notions DS pipeline and collect the concepts [103]. While\\ndesigning a new DS pipeline [130], dividing roles in DS teams [65],\\ndefining software process in data-intensive setting [123], identi-\\nfying best practices in AI and modularizing DS components [6],\\nit is important to understand the current state of the DS pipeline,\\nits variations and different stages. To understand the DS pipelines\\nand compare them, we collected the available pipelines from the\\nliterature and conducted an empirical study to unify the stages with\\ntheir subtasks. Then we created a representative DS pipeline with\\nthe definitions of the stages. Next, we present the methodology and\\nresults of our analysis of DS pipelines in theory.\\n2.1\\nMethodology\\n2.1.1\\nCollecting Data Science Pipelines. We searched for the stud-\\nies published in the literature and popular press that describes DS\\npipelines. We considered the studies that described both end-to-end\\nDS pipeline or a partial DS pipeline specific to a context. First, we\\nsearched for peer-reviewed papers published in the last decade i.e.,\\nfrom 2010 to 2020. We searched the terms “data science pipeline”,\\n“machine learning pipeline”, “big data lifecycle”, “deep learning work-\\nflow”, and the permutation of these keywords in IEEE Xplore, ACM\\nDigital Library and Google Scholar. From a large pool, we selected\\n1,566 papers that fall broadly in the area of computer science, soft-\\nware engineering and data science. Then we analyzed each article\\nin this pool to select the ones that propose or describe a DS pipeline.\\nWe found many papers in this collection use the terms (e.g., ML\\nlifecycle), but do not contain a DS pipeline. We selected the ones\\nthat contain DS pipeline and extracted the pipelines (screenshot/de-\\nscription) as evidence from the article. The extracted raw pipelines\\nare available in the artifact accompanied by this paper [7]. Thus,\\nwe found 46 DS pipelines that were published in the last decade.\\nBesides peer-reviewed papers, by searching the keywords on\\nweb, we collected the DS pipelines from US patent, industry blogs\\n(e.g., Microsoft, GoogleCloud, IBM blogs), and popular press pub-\\nlished between 2010 and 2020. After manual inspection, we found\\n25 DS pipelines from this grey literature. Thus, we collected 71\\nsubjects (46 from peer reviewed articles and 25 from grey litera-\\nture) that contain DS pipeline. We used an open-coding method to\\nanalyze these DS pipelines in theory [7] .\\n2.1.2\\nLabeling Data Science Pipelines. In the collected references,\\nDS pipeline is defined with a set of stages (data acquisition, data\\npreparation, modeling, etc.) and connections among them. Each\\nstage in the pipeline is defined for performing a specific task and\\nconnected to other stages. However, not all the studies depict DS\\npipelines with the same set of stages and connections. The studies\\nuse different terminologies for defining the stages depending on the\\ncontext. To be able to compare the pipelines, we had to understand\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nEnd\\nTraining\\nIndependent Labeling\\nTrain the raters\\nLabel one subject\\nDiscussion meeting\\nReconciling\\ndisagreements\\nFinished\\ntraining\\nLabel next pipeline\\nPerfect\\nagreement\\nIndependent\\nre-labeling\\nNo\\nYes\\nFinished\\nYes\\nNo\\nReconcile all\\ndisagreements\\xa0\\nNo\\nYes\\nFigure 1: Labeling method for DS pipelines in theory\\nthe definitions and transform them into a canonical form. For a\\ngiven DS pipeline, identifying their stages and mapping them to a\\ncanonical form is often challenging. The sub-tasks, overall goal of\\nthe project, utilities affect the understanding of the pipeline stages.\\nTo counter these challenges, we used an open-coding method to\\nlabel the stages of the pipelines.\\nTwo authors labeled the collected DS pipelines into different\\ncriteria. Each author read the article, understood the pipeline, iden-\\ntified the stages, and labeled them. In each iteration, the raters\\nlabeled 10% of the subjects (7-8 pipelines). The first 8 subjects were\\nused for training and forming the initial labels. After each iteration,\\nwe calculated the Cohen’s Kappa coefficient [121], identified the\\nmismatches, and resolved them in the presence of a moderator, who\\nis another author. Thus, we found the representative DS pipeline\\nafter rigorous discussions among the raters and the moderator. The\\nmethodology of this open-coding procedure is shown in Figure 1.\\nThe entire labeling process was divided into two phases: 1) training,\\nand 2) independent labeling.\\nTraining: The two raters were trained on the goal of this project\\nand their roles. We randomly selected eight subjects for training.\\nFirst, the raters and the moderator had discussions on three sub-\\njects and identified the stages in their DS pipeline. Thus, we formed\\nthe commonly occurred stages and their definitions, which were\\nupdated through the entire labeling and reconciliation process later.\\nAfter the initial discussion and training, the raters were given the\\nalready created definitions of the stages and one pipeline from the\\nremaining five for training. The raters labeled this pipeline indepen-\\ndently. After labeling the pipeline, we calculated the agreement and\\nconducted a discussion session among the raters and the moderator.\\nIn this session, we reconciled the disagreements and updated the\\nlabels with the definitions. We continued the training session until\\nwe got perfect agreement independently. The inter-rater agreement\\nwas calculated using Cohen’s Kappa coefficient [121]. A higher 𝜅\\n([0, 1]) indicates a better agreement. The interpretation of of 𝜅is\\nshown in Figure 2a. In the discussion meetings, the raters discussed\\neach label (both agreed and disagreed ones) with the other rater and\\nmoderator, argued for the disagreed ones and reconciled them. In\\nthis way, we came up with most of the stages and a representative\\nterminology for each stage including the sub-tasks.\\nIndependent labeling: After completing the training session,\\nthe rest of the subjects were labeled independently by the raters.\\nThe raters labeled the remaining 63 labels: 7 subjects (10%) in each\\nRange (𝜅)\\nAgreement level\\n0.00 - 0.20\\nSlight agreement\\n0.21 - 0.40\\nFair agreement\\n0.41 - 0.60\\nModerate agreement\\n0.61 - 0.80\\nSubstantial agreement\\n0.81 - 1.00\\nPerfect agreement\\n(a) Interpretation of Kappa (𝜅)\\nIteration #\\n𝜅\\nIteration #\\n𝜅\\n1\\n0.67\\n6\\n0.91\\n2\\n0.74\\n7\\n0.87\\n3\\n0.82\\n8\\n0.90\\n4\\n0.84\\n9\\n0.94\\n5\\n0.84\\n10\\n0.91\\n(b) Agreement in different stages\\nFigure 2: Labeling agreement calculation\\nof the 9 iterations. The distribution of 𝜅after each independent\\nlabeling iteration is shown in Figure 2b. In each iteration, first, the\\nraters had the labeling session, and then the raters and moderator\\nhad the reconciliation session.\\nLabeling. The raters labeled separately so that their labels were\\nprivate, and they did not discuss while labeling. The raters identi-\\nfied the stages and connections between them, and finally labeled\\nwhether the DS pipeline involves processes related to cyber, physi-\\ncal or human component in it. In independent labeling, we found\\nalmost perfect agreement (𝜅= 0.83) on average. Even after high\\nagreement, there were very few disagreements in the labels, which\\nwere reconciled after each iteration.\\nReconciling. Reconciliation happened for each label for the sub-\\nject studies in the training session, and the disagreed labels for the\\nstudies in independent labeling session. In training session, the\\nreconciliation was done in discussion meetings among the raters\\nand the moderator, whereas for the independent labels, reconcilia-\\ntion was done by the moderator after separate meetings with the\\ntwo raters. For reconciliation, the raters described their arguments\\nfor the mislabeled stages. For a few cases, we had straightforward\\nsolution to go for one label. For others, both the raters had good\\narguments for their labels, and we had to decide on that label by\\nupdating the stages in the definition of the pipeline. All the labeled\\npipelines from the subjects are shared in our paper artifact [7].\\nFurthermore, after finishing labeling the pipelines stages, we\\nalso classified the subject references into four classes based on the\\noverall purpose of the article. First, after a few discussions, the\\nraters and moderator came up with the classes. Then, the raters\\nclassified each pipeline into one class. We found disagreements in 6\\nout of 71 references, which the moderator reconciled with separate\\nmeetings with the two raters. Based on our labeling, the literature\\nthat we collected are divided into four classes: describe or propose\\nDS pipeline, survey or review, DS optimization, and introduce new\\nmethod or application. Next, we are going to discuss the result of\\nanalyzing the DS pipelines in theory.\\n2.2\\nRepresentative Pipeline in Theory\\nThe labeled pipelines with their stages are visually illustrated in\\nthe artifact Table 3. We found that pipelines in theory can be both\\nsoftware architecture and team processes unlike pipelines in-the-\\nsmall and in-the-large. Through the labeling process, we separated\\nthose team processes (25 out of 71), which are discussed in §2.4.\\nRQ1a: What is a representative definition of the DS pipe-\\nline in theory? From the empirical study, we created a represen-\\ntative rendition of DS pipeline with 3 layers, 11 stages and possible\\nconnections between stages as shown in Figure 3. Each shaded box\\nrepresents a DS stage that performs certain sub-tasks (listed under\\nthe box). In the preprocessing layer, the stages are data acquisition,\\npreparation, and storage. The preprocessing stage study design only\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\nData \\npreparation\\nFeature \\nengineering\\nCommunication\\nInterpretation\\nPost-processing Layer\\nModel Building Layer\\nData \\nacquisition\\nStorage\\nModeling\\nTraining\\nEvaluation\\nPrediction\\nLoad\\nCollect\\nObtain\\nCapture\\nSurvey\\nExplore\\nWrangle\\nClean\\nFilter\\nOrganize\\nPreserve\\nArchive\\nWarehouse\\nLog\\nRecycle\\nFeature\\n  - select\\n  - construct\\nLabel\\nAnnotate\\nClassify\\nCluster\\nMine\\nAnalyze\\nProcess\\nTune\\nOptimize\\nValidate\\nTest\\nVerify\\nReview\\nDiscover\\nDerive\\nDetermine\\nTransform\\nVisualize\\nRender\\nTranslate\\nExplain\\nTransfer\\nShare\\nDistribute\\nTransmit\\nPublish\\nInstall\\nExploit\\nServe\\nMonitor\\nDeployment\\nStudy design Pre-processing Layer\\nFigure 3: Concepts in a data science pipeline. The sub-tasks are listed below each stage. The stages are connected with feedback\\nloops denoted with arrows. Solid arrows are always present in the lifecycle, while the dashed arrows are optional. Distant\\nfeedback loops (e.g., from deployment to data acquisition) are also possible through intermediate stage(s).\\nStages of Data Science Pipeline\\nData Acquisition (ACQ): In the beginning of DS pipeline, data are collected\\nfrom appropriate sources. Data can be acquired manually or automatically. Data\\nacquisition also involves understanding the nature of the data, collecting relevant\\ndata, and integrating available datasets.\\nData Preparation (PRP): Data are generally acquired in a raw format that needs\\ncertain preprocessing steps. This involves exploration and filtering, which helps\\nidentify the correct data for further processing. Well prepared data reduces the\\ntime required for data analysis and contributes to the success of the DS pipeline.\\nStorage (STR): It is important to find an appropriate hardware-software combina-\\ntion to preserve data so that it can be processed efficiently. For example, Miao et al.\\nused graph database system Neo4j [75] to build a collaborative analysis pipeline\\n[72], since Neo4J supports querying graph data properties.\\nFeature Engineering (FTR): The entire dataset might not contribute equally to\\ndecision making. In this stage, appropriate features that are useful to build the\\nmodel are identified or constructed. Features that are not readily available in the\\ndataset, require engineering to create them from raw data.\\nModeling (MDL): When data are preprocessed and features are extracted, a model\\nis built to analyze the data. Model building includes model planning, model selection,\\nmining and deriving important properties of data. Appropriate data processing\\nstrategies and algorithms are selected to create a good model.\\nTraining (TRN): For a specific model, we need to train the model with available\\nlabeled data. By each training iteration, we optimize the model and try to make it\\nbetter. The quality of the training dataset contributes to the training accuracy of\\nthe model.\\nEvaluation (EVL): After training the model, it is tested with a new dataset which\\nhas not been used as training data. Also, the model can be evaluated in real-life\\nscenarios and compared with other competing models. Existing metrics are used\\nor new metrics are created to evaluate the model.\\nPrediction (PRD): The success of the model depends on how good a model can\\npredict in an unknown setup. After a satisfactory evaluation, we employ the model\\nto solve the problem and see how it works. There are many prediction metrics such\\nas classification accuracy, log-loss, F1-score, to measure the success of the model.\\nInterpretation (INT): The prediction result might not be enough to make a deci-\\nsion. We often need a transformation of the prediction result and post-processing\\nto translate predictions into knowledge. For example, only numerical results do\\nnot help much but a good visualization can help to make a decision.\\nCommunication (CMN): Different components of the DS system might reside\\nin a distributed environment. So, we might need to communicate with the in-\\nvolved parties (e.g., devices, persons, systems) to share and accumulate information.\\nCommunication might take place in different geographical locations or the same.\\nDeployment (DPL): The built DS solution is installed in its problem domain to\\nserve the application. Over time, the performance of the model is monitored so that\\nthe model can be improved to handle new situations. Deployment also includes\\nmodel maintenance and sending feedback to the model building layer.\\nTable 1: Description of the stages in DS pipeline\\nappeared in team process pipelines that comprise requirement for-\\nmulation, specification, and planning, which are often challenging\\nin data science. The algorithmic steps and data processing are done\\nin the model building layer. Modeling does not necessarily imply the\\nexistence of an ML component, since DS can involve custom data\\nprocessing or statistical modeling. Post-processing layer includes\\nthe tasks that take place after the results have been generated. The\\nDS pipeline stages are described in Table 1.\\nRQ1b: What are the frequent and rare stages of the DS\\npipeline in theory? The frequency of stage can depend on the\\nFigure 4: Frequency of pipeline stages in theory\\nfocus of the pipeline or its importance in certain context (ML, big-\\ndata management). Among 46 DS pipelines (which are not team\\nprocesses), Figure 4 shows the number of times each stage appears.\\nA few pipelines present stages with broad terminology that fit mul-\\ntiple stage-definitions. In those cases, the pipelines were labeled\\nwith the fitted stages and counted multiple times. Modeling, data\\npreparation, and feature engineering appear most frequently in the\\nliterature. While modeling is present in 93% of the pipelines, other\\nmodel related stages (feature engineering, training, evaluation, pre-\\ndiction) are not used consistently. Often training is not considered\\nas a separate stage and included inside the modeling stage. Similarly,\\nwe found that evaluation and prediction are often not depicted as\\nseparate stages. However, by separating the stages and modulariz-\\ning the tasks, the DS process can be maintained better [6, 103]. The\\npipeline created with the most number of stages (11) is provided\\nby Ashmore et al. [9]. On the other hand, about 15% of the pipelines\\nfrom the literature are created with a minimal number (3) of stages.\\nAmong them, 80% are ML processes and falls in the category of\\nDS optimizations. We found that these pipelines are very specific\\nto particular applications, which include context-specific stages\\nlike data sampling, querying, visualization, etc., but do not cover\\nmost of the representative stages. A pipeline in theory may not\\nrequire all representative stages, since it can have novelty in certain\\nstages and exclude the others. However, the representative pipeline\\nprovides common terminology and facilitate comparative analysis.\\nFinding 1: Post-processing layers are included infrequently (52%)\\ncompared to pre-processing (96%) and model building (96%) layers\\nof pipelines in theory.\\nClearly, preprocessing and model building layers are considered\\nin almost all of the studies. In most of the cases, the pipelines\\ndo not consider the post-processing activities (interpretation, com-\\nmunication, deployment). These pipelines often end with the pre-\\ndictive process and thus do not follow up with the later stages\\nwhich entails how the result is interpreted, communicated and de-\\nployed to the external environment. Miao et al. argued that overall\\nlifecycle management tasks (e.g., model versioning, sharing) are\\nlargely ignored for deep learning systems [73]. Previous studies\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nalso showed that significant amount of cost and effort is spent in the\\npost-development phases in traditional software lifecycle [70, 90].\\nIn data-intensive software, the maintenance cost can go even higher\\nwith the high-interest technical debt in the pipeline [102]. Therefore,\\npost-processing stages should be incorporated for a better under-\\nstanding of the impact of the proposed approach on maintenance\\nof the DS pipeline.\\n2.3\\nOrganization of Pipeline Stages in Theory\\nRQ2: How are pipeline stages connected to each other? In Fig-\\nure 3, for simplicity, we depicted the DS pipeline as a mostly linear\\nchain. However, our subject DS pipelines often have non-linear\\nbehavior. In any stage, the system might have to return to the pre-\\nvious stage for refinement and upgrade, e.g., if a system faces a\\nreal-world challenge in modeling, it has to update the algorithm\\nwhich might affect the data pre-processing and feature engineering\\nas well. Furthermore, the stages do not have strict boundaries in\\nthe DS lifecycle. In Figure 3, two backward arrows, from feature\\nengineering and evaluation, indicate feedback to any of the previous\\nstages. Although in traditional software engineering processes (e.g.,\\nwaterfall model, agile development, etc.), feedback loop is not un-\\ncommon, in DS lifecycle, there are multiple stakeholders and models\\nin a distributed environment which makes the feedback loops more\\nfrequent and complex. Sculley et al. pointed that DS tasks such\\nas sampling, learning, hyperparameter choice, etc. are entangled\\ntogether so that Changing Anything Changes Everything (CACE\\nprinciple) [103], which in turn creates implicit feedback loops that\\nare not depicted in the pipelines [20, 35, 91, 120]. The feedback\\nloops inside any specific layer are more frequent than the feedback\\nloops from one layer to another. Also, a feedback loop to a distant\\nprevious stage is expensive. For example, if we do data preparation\\nafter evaluation then the intermediate stages also require updates.\\n2.4\\nCharacteristics of the Pipelines in Theory\\nRQ3: What are the different types of pipelines available in\\ntheory? The context and requirements of the project can influence\\npipeline design and architecture [36]. Here, we present the types of\\npipelines with different characteristics that are available in theory.\\nWe classified each subject in our study into four classes based on the\\noverall goal of the article. The most of the pipelines in theory (39%)\\nare describing or proposing new pipelines to solve a new or existing\\nproblem. About 31% of the pipelines are on reviewing or comparing\\nthe existing pipelines. The third group of DS pipelines (14%) are\\nintended to optimize a certain part of the pipeline. For example, Van\\nDer Weide et al. proposes a pipeline for managing multiple versions\\nof pipelines and optimize performance [120]. Most of the pipelines\\nin this category are application specific and include very few stages\\nthat are necessary for the optimization. Fourth, some research\\nintroduce new application or method and present within the pipeline.\\nWe observed that there is no standard methodology to develop\\ncomparable and inter-operable DS pipelines. Using the labeling\\nmethodology shown in Figure 1, we labeled each pipeline and found\\nthree types of DS pipelines in the literature: 1) ML process, 2) big\\ndata management process, and 3) team process.\\nML process: 46% of all the pipelines we found in the literature\\nare describing machine learning processes. The recent advent of\\nartificial intelligence, supervised learning and deep learning has\\nled to more DS systems that involve ML components. The pipelines\\nin this category emphasize the algorithmic process, learning pat-\\nterns, and building predictive models. However, the post-processing\\nstages are rare in these type of pipelines. The ML pipelines are often\\nthought of as algorithmic process in the laboratory scenario. But as\\nmentioned in [9], incorporating the post-processing stages would\\nbe desired to ensure safe real-world deployment of such pipelines.\\nBig data management: The references in this category present\\nDS pipelines that manage a large amount of data or describes a\\nframework (software-hardware infrastructure) for data processing\\nbut do not contain machine learning components in the pipeline.\\nProcessing large amount data often requires specific algorithms\\nand engineering methods for efficiency and further processing. We\\nfound that 18% of all the subject studies fall in this category.\\nTeam process: We also found some DS pipelines that are not\\ndescribing DS software architecture. These pipelines describe work-\\nflow of human activities that needs to be followed in a DS pipeline.\\nThese studies present a high-level view for building DS component\\nin a team environment. The data science teams require specific\\nexpertise and management to build successful DS pipelines [6, 65].\\nIn this paper, in §3 and §4, we are only focusing on DS pipeline as\\nsoftware architecture, and therefore, we did not compare the team\\nprocess pipelines in the rest of this section.\\nFinding 2: Most of the pipelines in-theory involve cyber and phys-\\nical components, only a few with human processes in the loop.\\nWe identified whether the pipelines involve cyber, physical or hu-\\nman process, using our labeling process described in section §2.1.2.\\nCyber processes refer to activities that involve automated systems\\nand machinery computations. Since modern DS systems involves\\nlarge amount of data and requires extensive computation, all of\\nthe pipelines include cyber component in it. Physical processes\\ninclude the activities which require real-world connections with the\\nsystem. For example, collecting data using mobile sensors or cam-\\neras is a physical process. Although 23% of the big data pipelines\\ninclude physical processes, only 9% of the ML pipelines include\\nthat in the pipeline. In many DS systems, developers or researchers\\nparticipate in the pipelines actively to make decisions that need\\nhuman interventions [116, 120]. For example, in many DS systems,\\nanalytical model validation, troubleshooting, data interpretation is\\nnecessary which requires human involvement. However, only 13%\\nof the pipelines acknowledged human involvement in the pipeline.\\n3\\nDS PIPELINE IN-THE-SMALL\\nSimilar to the DS pipelines in large systems and frameworks, for\\na very specific data science task (e.g., object recognition, weather\\nforecasting, etc.), programmers build pipeline. Different stages of\\nthe program perform a specific sub-task and connect with the other\\nstages using data-flow or control-flow relations. In this section, we\\ndescribed such DS pipelines in-the-small.\\n3.1\\nMethodology\\nWe collected 105 DS programs from Kaggle competition notebooks\\n[53]. Kaggle is one of the most popular crowd-sourced platforms\\nfor DS competitions, owned by Google. Besides participating in\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\n1. Featured\\n2. Research\\n3. Recruitment\\n4. Masters\\n5. Analytics\\n6. Playground\\n7. Getting started\\n105 top-rated\\nDS programs\\nAPI:Stage\\nDictionary\\nParse and\\nextract APIs\\nCreate\\nPipeline of\\nStages\\nCompetitions\\nPRP\\nPRD\\nMDL\\nTRN\\nMDL\\nPRP\\nACQ\\nFor each\\ncompetition, select\\nmost voted solution\\nFilter out solutions\\n\\xa0 - votes < 10\\n\\xa0 - not end-to-end\\xa0 \\xa0\\n\\xa0 \\xa0 pipeline\\nFigure 5: The pipeline creation process for Kaggle programs\\ncompetitions, data scientists, researchers, developers collaborate to\\nlearn and share DS knowledge in variety of domains. The users and\\norganizations can host a DS competition in Kaggle to solve real-\\nworld problems. A competition is accompanied by a dataset and\\nprize money. Many Kaggle solutions have resulted in impactful DS\\nalgorithms and research such as neural networks used by Hinton\\nand Dahl [23], improving the search for the Higgs Boson at CERN\\n[51], etc. We chose Kaggle solutions to analyze DS pipeline for three\\nreasons: 1) all programs perform a DS task and provide solution to\\na well specified problem associated with a dataset, 2) solutions with\\nthe highest number of votes are well accepted solutions for a specific\\nproblem, and 3) the problems cover a wide range of domains.\\nThere are 331 completed competitions in Kaggle to date. They\\ncategorized the competitions into Featured, Research, Recruitment,\\nMasters, Analytics, Playground and Getting started. We collected\\nsolutions of all the competitions from each category except Getting\\nstarted and Playground (these two categories are intended to serve\\nas DS tutorials and toy projects). First, we filtered the competitions\\nfor which there are solutions available (many old competitions do\\nnot contain any public solution). We found 138 such competitions.\\nFor a given competition problem, we selected the most voted so-\\nlution which has at least 10 votes. Thus, we got 105 top-rated DS\\nsolutions for analyzing pipelines in-the-small. This selection and\\npipeline creation process is shown in Figure 5.\\nAll of the DS programs are written in Python using ML libraries\\nlike Keras, Scikit-learn, Tensorflow, etc. These packages provide\\nhigh-level Application Programming Interfaces (APIs) for perform-\\ning a specific task on data or model. We parsed the programs into\\nAbstract Syntax Tree (AST) and collected all the API calls from the\\nprograms. Then the functionality of an API is used to identify the\\nstage of the pipeline. We extracted the temporal order of API calls\\nto identify the stages. Standard static analysis of the Python pro-\\ngrams facilitate the extraction process. Our analysis suggests that\\nthe DS programs follow a linear structure with less than 4% AST\\nnodes being conditional or loops. Wang et al. proposed a similar ap-\\nproach for extracting external dependencies in Jupyter Notebooks\\nby creating an API database and analyzing AST [125].\\nWe created a dictionary by mapping each API collected from the\\nprograms, to one of the 11 stages of the DS pipeline described in\\nsection §2. During the mapping, we excluded the generic APIs from\\nthe dictionary. For example, model.summary() is used to print the\\nmodel parameters and does not represent any stage of the pipeline.\\nFor creating the dictionary, we taken a two-fold approach. First,\\nwe understand the context of the program and API usage. Second,\\nData \\nAcquisition\\nData \\nPreparation\\nModeling\\nTraining\\nEvaluation\\nPrediction\\nFigure 6: Pipeline in-the-small extracted from API usages\\n98\\n102\\n73\\n76\\n38\\n74\\nACQ\\nPRP\\nMDL\\nTRN\\nEVL\\nPRD\\nFigure 7: Frequency of pipeline stages in-the-small\\nwe look at the API documentation to confirm the corresponding\\npipeline stage. We found that DS APIs are definitive in their oper-\\nations and well-categorized by the library. For example, the APIs\\nin Keras [60] and Scikit-learn [61] are grouped into preprocess-\\ning, models, etc. Our API-dictionary was manually validated by a\\nsecond-rater and moderator who labeled DS pipelines in section\\n§2. Then, we built a tool which takes the API dictionary and DS\\nprogram, and automatically creates the DS pipeline. For a sequence\\nof APIs with the same stage, we abstracted them into a single stage.\\nAs an example, Figure 5 shows a DS pipeline created from a Kaggle\\nsolution [54]. Each stage in the pipeline (e.g., ACQ, PRP) represents\\none or more API usages. The arrows in the pipeline denote the tem-\\nporal sequence of stages. Note that, one stage can appear multiple\\ntimes in a pipeline. The API dictionary, Kaggle programs, and tool\\nto generate the pipelines is shared in the paper artifact [7].\\n3.2\\nRepresentative Pipeline in-the-Small\\nRQ4: What are the stages of DS pipeline in-the-small? Among\\nthe 11 pipeline stages described in Figure 3, we found only 6 stages\\nin the DS programs that are depicted in Figure 6. Other stages (e.g.,\\nstorage, feature engineering, interpretation, communication, deploy-\\nment) are not found in these programs because these stages occur\\nwhile building a production-scale large DS system and often not\\npresent in the DS notebooks. Therefore, the pipeline in DS programs\\nconsists of the subset of pipeline stages in theory.\\nWe summarized the frequency of each stage of the DS programs\\nin Figure 7. Among 105 programs, data acquisition and data prepa-\\nration are present in almost all of them. Surprisingly, modeling is\\npresent in only 70% of the programs. We found that, in many pro-\\ngrams, no modeling APIs had been used because developers did not\\nuse any built-in ML algorithm from libraries, e.g., LogisticRegres-\\nsion, LSTM, etc. In these cases, the developers use data-processing\\nAPIs on the training data to build custom model, e.g., this note-\\nbook [55] uses data preparation APIs to produce results. To enable\\nmore abstraction of the stages in these pipelines, further modular-\\nization is necessary, which has been investigated in RQ8.\\nFinding 3: Evaluation stage is infrequent, appearing only in 36%\\nof the pipelines in-the-small.\\nEvaluation is a tricky stage of the DS pipeline. Developers have to\\nchoose the appropriate metric and methodology to evaluate their\\nmodel. Based on the evaluation result, the model is updated over\\nmultiple iterations. We found that, besides using metrics, in many\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nBefore After Before After Before After Before After Before After Before After\\nACQ\\nPRP\\nMDL\\nTRN\\nEVL\\nPRD\\nACQ\\nPRP\\nMDL\\nTRN\\nEVL\\nPRD\\nFigure 8: Stages occurring before and after each stage\\ncases, evaluation requires human understanding and comparison\\nof the result produced by the model. The reason for having less\\nnumber of evaluation stage in the pipeline is that often the develop-\\ners evaluate the performance by plotting and visualizing the result.\\nSince the visualization APIs are not considered as evaluation stage,\\nwe found this stage less frequently in pipelines. Also, many pro-\\ngrams directly go to the prediction stage without going to evaluation\\nstage at all. Furthermore, notebooks are often used for experimen-\\ntation purposes so that many computations are performed during\\ndevelopment but eliminated when the notebooks are shared [62].\\nFor example, one developer might try a number of classifiers and\\nevaluate their accuracy. After finding the best performing classi-\\nfier, it can be the only one shared in the notebook. Therefore, we\\nexperienced many missing stages in the pipeline in-the-small. The\\ncomplex DS tasks require several computations which might not\\nbe used in producing the final prediction, but definitely should be\\nconsidered as part of the pipeline.\\n3.3\\nPipeline Organization in the Small\\nRQ5: How are the stages connected with each other in pipe-\\nline in-the-small? To answer RQ5, we considered each occurrence\\nof the stages in a DS program and looked at its previous and next\\nstage. In Figure 8, we showed which stages are followed or preceded\\nby each stage. We found that data preparation can occur before or\\nafter all other stages. Apart from that, data acquisition is followed\\nby data preparation most of the time, which in turn is followed\\nby modeling. Modeling is followed mostly by training, which in\\nturn is followed by prediction. Evaluation is mostly surrounded\\nby prediction and data preparation. From Figure 8, we can also\\nfind some most occurring feedback loop: evaluation to preparation,\\nevaluation to modeling and prediction to modeling.\\nData preparation tasks (e.g., formatting, reshaping, sorting) are\\nnot limited to just before the modeling stage, rather it is done on a\\nwhenever-needed basis. For example, in the following code snippet\\nfrom a Kaggle competition [56], while creating model-layers, data\\npreprocessing API has been called in line 2.\\n1\\nx = Conv2D ( mid ,\\n( 4 ,\\n1) ,\\na c t i v a t i o n = \\' r e l u \\' ,\\npadding= \\' v a l i d \\' ) ( x )\\n2\\nx = Reshape ( ( branch_model . output_shape [ 1 ] ,\\nmid ,\\n1) ) ( x )\\n3\\nx = Conv2D ( 1 ,\\n( 1 ,\\nmid ) ,\\na c t i v a t i o n = \\' l i n e a r \\' ,\\npadding= \\' v a l i d \\' ) ( x )\\n4\\nx = F l a t t e n ( name= \\' f l a t t e n \\' ) ( x )\\n5\\nhead_model = Model ( [ xa_inp ,\\nxb_inp ] , x ,\\nname= \\' head \\'\\nThe modeling stage is always surrounded by other stages of the\\npipeline. However, there is often a loop around modeling, training,\\nevaluation, and prediction. Modeling often repeats many times to\\nimprove the model over multiple iterations. For example, in the\\nfollowing Kaggle code snippet [57], the model is created and trained\\nmultiple times to find the best one.\\n1\\nrandom_forest = R a n d o m F o r e s t C l a s s i f i e r ( n_estimators =100 ,\\nrandom_state =50 ,\\nverbose =1 ,\\nn_jobs = −1) # Modeling\\n2\\nrandom_forest . f i t ( train ,\\nt r a i n _ l a b e l s )\\n# Train\\n3\\n. . .\\n4\\np o l y _ f e a t u r e s =\\ns c a l e r . f i t _ t r a n s f o r m ( p o l y _ f e a t u r e s )\\n5\\np o l y _ f e a t u r e s _ t e s t =\\ns c a l e r . transform ( p o l y _ f e a t u r e s _ t e s t )\\n6\\nrandom_forest_poly = R a n d o m F o r e s t C l a s s i f i e r ( n_estimators =100 ,\\nrandom_state =50 ,\\nverbose =1 ,\\nn_jobs = −1) # Modeling\\n7\\nrandom_forest_poly . f i t ( poly_features ,\\nt r a i n _ l a b e l s )\\n#\\nTraining\\n8\\npred = random_forest_poly . pr e di c t _p r o ba ( p o l y _ f e a t u r e s _ t e s t ) [ : , 1 ]\\nFinding 4: Stages of pipelines in-the-small are often tangled with\\neach other.\\nAll of the DS programs fail to maintain a good separation of con-\\ncerns [28] between stages. Strong abstraction boundaries help to\\nmake the program modular and easy-to-maintain [81, 82, 84]. In\\naddition, a good DS solution should not only compute better pre-\\ndictive result, but also facilitate software engineering activities e.g.,\\ndebugging, testing, monitoring [42]. However, we found that stages\\nare often tangled with other stages [18, 64, 88] across the pipelines.\\nThe code for one stage is interspersed with the code for other stages.\\nFor example, while building the deep learning network (modeling),\\nthe developers often switch to different data preparation tasks, e.g.,\\nreshaping, resizing [48, 49], which tangles data preparation concern\\nwith the modeling concern. We observed some early attempts to\\nadopt modular design practices. For instance, this notebook [58]\\nseparated code into different high-level stages, namely, prepara-\\ntion, feature extraction, exploratory data analysis (EDA), topic model,\\netc. These high-level pipelines can improve the abstraction, which\\nfurther enable the maintainability, and reusability [97]. In some\\nscenarios, reuse or maintenance might not be desired for pipelines\\nin-the-small. However, to enhance readability [62] and repeatability\\n[42] and ease of testing, debugging or repairing [126, 127], more\\nattention on modular design practices is needed for DS pipelines.\\nFinding 5: Data preparation stage is occurring significant number\\nof times between any two stages of pipelines in-the-small, which is\\ncausing pipeline jungles.\\nWe found that new data sources are added, new features are identi-\\nfied, and new values are calculated incrementally in the pipeline\\nwhich evolves organically. This results in a large number of data\\npreprocessing tasks like sampling, joining, resizing along with ran-\\ndom file input-output. This is called pipeline jungles [103], which\\ncauses technical debt for DS systems in the long run. Pipeline jun-\\ngles are hard to test and any small change in the pipeline will\\ntake a lot of effort to integrate. The situation gets worse in case of\\nlarger DS pipelines, where several data management activities (e.g.,\\nclean, serve, validate) are necessary through the pipeline in differ-\\nent stages [85, 86]. The recommended way is to think about the\\npipeline holistically and scrape the pipeline jungle by redesigning it,\\nwhich in turn takes further engineering effort [103]. We found that\\nthe large DS projects, which are discussed in §4, isolate the data\\npreparation tasks into separate files and modules [17, 100, 118, 134],\\nwhich alleviates the pipeline jungles problem. So, DS pipeline in-\\nthe–small needs further IDE (e.g., Jupyter Notebook, etc.) support\\nand methodologies for code isolation and modularization.\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\nLoad Data \\nand Library\\nFeature \\nEngineering\\nModeling\\nTraining\\nEvaluation\\n& Visualization\\nPrediction\\nEDA\\n& Visualization\\nData \\nPreparation\\nFigure 9: Representative data science pipeline in-the-small\\n3.4\\nCharacteristics of Pipelines in-the-Small\\nRQ6: What are the patterns in pipeline in-the-small and how\\nit compares to pipeline in theory? We have not found many\\nstages from Figure 3, e.g., feature engineering, interpretation, com-\\nmunication, in pipeline in-the-small. One reason is that the low-\\nlevel pipeline extracted from the API usages cannot capture some\\nstages. For example, even if a developer is conducting feature engi-\\nneering, the used APIs might be from the data preparation stage.\\nFortunately, we found many Kaggle notebooks that are organized by\\nthe pipeline stages. We visited all the 105 Kaggle notebooks in our\\ncollection and extracted these high-level pipelines manually. Unlike\\nthe low-level pipelines (extracted using API usages), a high-level\\npipeline consists of the stages abstracted by the developers.\\nThe Kaggle notebooks follow literate programming paradigm [97,\\n122], which allows the developers to describe code using rich text\\nand separate them into sections. We found that 34 out of 105 note-\\nbooks divided the code into stages. We collected those stages from\\nthe Kaggle notebooks. Furthermore, we labeled these notebooks\\ninto the 11 stages from DS pipeline in theory by two raters, and\\nextracted the stages that are not present in theory. The extracted\\nhigh-level pipelines and labels are available in the paper artifact [7].\\nWe observed that no notebooks specify these stages: storage,\\ninterpretation, communication, and deployment. These DS programs\\nare not production-scale projects. Therefore, they do not include the\\npost-processing stages in the pipeline. The most common stages are\\nmodeling (79%), data preparation (62%), data acquisition (53%), and\\nfeature engineering(35%), which is aligned with the finding of DS\\npipeline in theory. In addition, we found these stages which are not\\npresent in theory: library loading, exploratory data analysis (EDA),\\nvisualization. Among them EDA has been used most of the times\\n(43%) and covered the most part of those pipeline. Before going\\nto the modeling and successive stages, a lot of effort is given on\\nunderstanding the data, compute feature importance, and visualize\\nthe patterns, which help to build models quickly in later stages [9].\\nFurthermore, some notebooks present library loading as separate\\nstage. We observed that choosing appropriate library/framework\\nand setting up the environment is an important step while devel-\\noping pipeline in-the-small. We also found that data visualization\\nis an recurring stage mentioned by the developers. Visualization\\ncan be done for EDA or feature engineering (before modeling), or\\nfor evaluation (after modeling). Based on these observations we\\nupdated the representative pipeline in-the-small in Figure 9. The\\nhigh-level pipeline provides an overall representation of the sys-\\ntem, which can be leveraged to design software process. It would be\\nbeneficial for the developers to close the gap between the low-level\\nand the high-level pipeline by identifying the tangled stages.\\n4\\nDS PIPELINE IN-THE-LARGE\\nThe DS solutions described in the previous section are specific to a\\ngiven dataset and a well-defined problem. However, there are many\\nTable 2: GitHub projects for analyzing pipeline in-the-large\\nProject Name\\nPurpose\\n#Files #AST\\nLOC\\nAutopilot [5]\\nPilot a car using computer vision\\n36\\n11185\\n348\\nCNN-Text-Classification [17] Sentence classification\\n69\\n47797\\n11.4K\\nDarkflow [118]\\nReal-time object detection and classification\\n1025\\n655670\\n8.6K\\nDeep ANPR [31]\\nAutomatic number plate recognition\\n64\\n70464\\n10.8K\\nDeep Text Corrector [80]\\nCorrect input errors in short text\\n47\\n50770\\n3.0K\\nFace Classification [8]\\nReal-time face and emotion/gender detection\\n292\\n117901\\n35.3K\\nFaceNet [100]\\nFace recognition\\n1352 1889529\\n18.2K\\nKittiSeg [115]\\nRoad segmentation\\n276\\n187143\\n4.8K\\nLSTM-Neural-Network [10] Predict time series steps and sequences\\n24\\n11434\\n1.2K\\nMask R-CNN [2]\\nObject detection and instance segmentation\\n256 1567786\\n15.6K\\nMobileNet SSD [89]\\nObject detection network\\n28\\n21272\\n25.6K\\nMTCNN [21]\\nJoint face detection and alignment\\n153\\n121138 219.7K\\nObject-Detector-App [24]\\nReal-time object recognition\\n215\\n318534\\n47.9K\\nPassword-Analysis [98]\\nAnalyze a large corpus of text passwords\\n148\\n67870\\n3.6K\\nPerson Blocker [132]\\nBlock people in images\\n12\\n44517\\n977\\nQANet [134]\\nMachine reading comprehension\\n83\\n107669\\n2K\\nSpeech-to-Text-WaveNet\\nSentence level english speech recognition\\n32\\n18626\\n5.1K\\nTacotron [83]\\nText-to-speech synthesis\\n114\\n58845\\n1.4K\\nText-Detection-CTPN [95]\\nText detection\\n640\\n257083\\n18.4K\\nTF-Recomm [112]\\nRecommendation systems\\n17\\n7789\\n535\\nXLNet [137]\\nLanguage understanding\\n36\\n143172\\n11.5K\\nDS projects which are large, not limited to a single source file, and\\ncontains multiple modules. These solutions are intended to solve\\nmore general problems which might not be specific to a dataset. For\\nexample, the objective of the Face Classification project in GitHub\\n[8] is to detect face from images or videos and classify them based\\non gender and emotion. This problem is not specific to a particular\\ndataset and the scope is broader compared to the Kaggle solutions.\\nWe collected such top-rated DS projects from GitHub to analyze\\nDS pipeline in-the-large.\\n4.1\\nMethodology\\nBiswas et al. published a dataset containing top rated DS projects\\nfrom GitHub [14]. From the list of projects in this dataset, we filtered\\nmature DS projects having more than 1000 stars. Thus, we found\\n269 mature GitHub projects. However, there are many projects in\\nthis list which are DS libraries, frameworks or utilities. Since we\\nwant to analyze the pipeline of data science software, we removed\\nthose projects. Finally, we also removed the repositories which\\nserve educational purposes. Thus, we found a list of 21 mature\\nopen-source DS projects. The list of projects, and their purpose are\\nshown in Table 2.\\nFor each project, we created two pipelines: high-level pipeline\\nand low-level pipeline. For creating the high-level pipeline, we\\nmanually checked the project architecture, module structure and\\nexecution process. This gave us a good understanding of the source\\nfile organization and linkage between modules. After identifying the\\nhigh-level pipeline and execution sequences of the source files, we\\nused the same API based method used to analyze Kaggle programs\\nin the previous section, to create low-level pipeline of these GitHub\\nprojects. The methodology of selecting and extracting pipelines\\nfrom the GitHub projects is shown in Figure 10.\\nFor example, the project QANet [134] is intended to do machine\\nreading comprehension. Here, Python has been used as the primary\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nprojects for data\\nscience\\nHigh level\\npipeline\\n21 Mature\\nGithub\\nProjects\\nLow level\\npipeline\\nFilter out projects\\n\\xa0 \\xa0 - stars < 1000\\n\\xa0 \\xa0 - libraries\\xa0\\n\\xa0 \\xa0 -\\xa0utility\\n\\xa0 \\xa0 - tutorials\\nAnalyze\\npipeline\\narchitecture\\nSource code\\nFigure 10: Pipeline creation process for GitHub projects\\nlanguage, and shell script has been used for data downloading and\\nproject setup. The high-level pipeline for QANet includes the stages:\\ndata acquisition, data preparation, modeling, training, evaluation and\\nprediction. In the beginning, config.py file integrates the modules\\n(preparation, modeling, and training) and provides an interface\\nto configure a model by specifying dataset and other parameters.\\nThen, the file evaluate.py is executed to perform the evaluation\\nand prediction. For the low-level pipeline, for a specific file, we\\nused the API based analysis to generate the pipeline, which was\\nused to analyze pipeline in-the-small. For instance, in the project\\nQANet, although model.py serves modeling at a high level, it also\\ndoes data preparation, training, and evaluation, when APIs are\\nconsidered. In addition to the pipeline stages, we also identified\\na few other properties of each project: 1) number of contributors,\\n2) AST count, 2) technology/language used, 3) entry points and 4)\\nexecution sequence. We leveraged the Boa infrastructure [29, 30]\\nto analyze the different properties of the projects. These properties\\nhelped us to categorize and analyze the pipeline in-the-large. The\\ndetails of the projects are available in the paper artifact [7].\\nThe projects are from various domains: object detection, face\\nclassification, automated driving, speech synthesis, number plate\\nrecognition, predict time series sequence, etc. The number of devel-\\nopers in each project ranges between 1 and 40 with an average of\\n8. Among 21 projects, 16 of them are developed by teams and 5 of\\nthem are developed by individuals. The primary language used to\\ndevelop these projects is Python.\\n4.2\\nRepresentative Pipeline in-the-Large\\nCompared to the Kaggle programs, we found a significant differ-\\nence in the pipeline of large DS projects. Because of the larger\\nsize of the projects, the pipeline architecture is different. All the\\nprojects contain multiple source files for handling different tasks\\n(e.g., modeling, training) and about 50% of the projects organize the\\nsource files into modules (e.g., utils, preprocessing, model, etc.).\\nRQ7: What is the representative DS pipeline in-the-large?\\nEach of the projects contains six stages described in Figure 6: ac-\\nquisition, preparation, modeling, training, evaluation, and prediction.\\nHowever, since the projects are not coupled to a specific dataset\\nand they solve a more general problem, the projects are not lim-\\nited to one single pipeline. We found that the pipeline of each\\nproject is divided into two phases: 1) development phase and 2)\\npost-development phase, which is depicted in Figure 11.\\nIn development phase, the main goal is to build a model that\\nsolves the problem in general. A base dataset is used to build the\\nmodel that would be used for other future datasets. After completing\\na modeling, training, evaluation loop, the final model is created and\\nsaved as an artifact. Afterwards, the projects also create model\\ninterfaces, which lets the user modify and exploit the model in\\nthe post-development phase. Finally, the model artifact is saved as\\na source file or some model archiving formats. For example, the\\nModify model\\nAcquisition\\nTraining\\nModeling\\nModel artifact\\nTraining\\nPreparation\\nPrediction\\nEvaluation\\nEvaluation\\nTraining\\nEvaluation\\nTrained model\\nDevelopment phase\\nPost-development phase\\nFigure 11: DS pipeline in-the-large. Development phase (top)\\nruns during model building and post-development phase\\n(bottom) runs for making prediction.\\nproject Person-Blocker [132] and Speech-to-Text-WaveNet [66] saved\\nthe model in the source file (model.py) and lets the users train the\\nmodel in the next phase. On the other hand, the project KittiSeg\\n[115] and Autopilot [5] saved the built model artifact in JSON format\\n(.json) and checkpoint format (.ckpt) respectively. We observed\\nthat the evaluation and prediction is often not the main goal in\\nthis phase; rather, building an appropriate model and making it\\navailable for further usage is the central activity.\\nIn post-development phase, the users access the pre-built\\nmodel and use that for prediction. After acquiring data, a few\\npreprocessing steps are needed to feed the model. In all of the\\nprojects under this study, we found that the development phase\\nis similar. However, we identified three different patterns in the\\npost-development phase which are shown in Figure 11. First, the\\nusers can modify the model by setting its hyperparameters and use\\nthat to make prediction on a new dataset. Second, the users can\\nuse the model as-it-is and train the model on the new dataset to\\nmake prediction. Third, the users can also download the pre-trained\\nmodel and directly leverage that for prediction. Finally, at the end\\nof this phase, the prediction result is obtained.\\nThe post-development phase in the pipeline enabled software\\nreusability of the models. All of these projects have instructions in\\ntheir readme or documentation explaining the usage and customiza-\\ntion. For example, the project Deep ANPR [31] provides instructions\\nfor obtaining large training data, retraining the models, and build\\nit for prediction. However, not all the projects enable reusability\\nin the development pipelines. Only a few of them provides access\\nto the modules by importing in new development scenario. For\\ninstance, Darkflow [118] let users access the darkflow.net.build\\nmodule and use it in new application development. To increase the\\nreusability of DS programs, it would be desired to consider similar\\naccess to the development pipeline of these large projects.\\n4.3\\nOrganization of DS Pipeline in-the-Large\\nRQ8: How are the stages connected in pipeline in-the-large?\\nThe abstraction in DS projects is stricter than the DS programs\\ndescribed in §3. The projects are built in a modular fashion, i.e.,\\none source file for a broad task (e.g., train.py, model.py). How-\\never, inside one specific file, there are many other possible stages,\\nespecially data preprocessing appears inside all the source files. In\\naddition, the module connectivity is not linear. All of the modules\\nuse external libraries for performing different tasks. As a result,\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\nthere are a lot of interdependencies (both internal and external) in\\nthe DS pipeline. One immediate difference of these pipelines with\\ntraditional software is DS pipelines are heavily dependant on the\\ndata. For example, the project Speech-to-Text-WaveNet [66] requires\\na certain format of data. When we want to use that in a new situa-\\ntion, the data properties might be different. So, the usage pipelines\\nwould have a few additional stages. In some cases, the original\\npipeline is modified. Here, there are many sub-pipelines work to-\\ngether to build a large pipeline. However, we have not found any\\nframework or common methodology these software are using. The\\ndifferent patterns of DS pipelines seek more advanced methodology\\nor framework to build DS pipeline and release for production.\\n4.4\\nCharacteristics of Pipelines in-the-Large\\nRQ9: What are the patterns found in the pipelines? The pipe-\\nlines found in this setting can be categorized into 1) loosely coupled\\nand 2) tightly coupled, based on their modularity. A high number of\\ncontributors in the project resulted in loosely coupled pipelines. We\\nfound the loosely coupled ones are designed in a modular fashion\\nand one module (e.g., data cleaning, modeling) is designed to be\\nused by other modules. Usually, there are multiple entry-points in a\\nloosely coupled pipeline and user has more flexibility. On the other\\nhand, in a tightly coupled pipeline, the modules are stricter and\\nintegrated tightly with other modules. There is only one or two\\nentry-points to the pipeline, which automatically calls the other\\nmodules. We found that the projects with 6 or more contributors\\n(∼75%) followed a loosely coupled architecture and projects with 1\\nto 5 contributors followed a tightly coupled architecture.\\nFinding 6: There is need for integration and deployment tools for\\npipelines in-the-large but no common framework is used in practice.\\nAlthough all the project under this study are written using\\nPython, no project is using any common tool that integrates the\\nDS modules and provides interface to the pipeline. Today, contin-\\nuous integration and deployment (CI/CD) tools are widely used\\nin traditional software lifecycle to automate compilation, build-\\ning, and testing [43, 59]. Additionally, from our subject studies\\nof pipelines in theory, we found some CI/CD tools designed for\\nML pipelines available [44, 74, 76]. Surprisingly, here we found no\\nprojects in pipeline in-the-large are using any CI/CD tools. How-\\never, the projects demonstrate the need of CI/CD in the repositories.\\nIn most of the projects, the environment setup and access to func-\\ntionalities are configured through command lines scripts [5, 95].\\nSome projects used docker container [8, 66, 100, 132] to set up the\\nenvironment and run the pipeline. A few others used Python note-\\nbooks that call different modules to integrate the pipeline stages\\n[2, 24, 80]. 7 out of 21 projects used shell script for integration\\n(e.g., sending HTTP request to download data, model reuse, etc.)\\n[89, 134]. Although CI/CD frameworks e.g., TravisCI, GitHub Ac-\\ntions, Microsoft Azure DevOps are well established for traditional\\nsoftware such as web applications, several challenges remain for\\nDS pipelines. Karlaš et al. outlined the probabilistic nature of ML\\ntesting as a major CI/CD challenge and pointed out the gap between\\nrecent theoretical development of CI/CD in DS and their usage in\\npractice [59]. Hence, further research is needed to investigate the\\npractical challenges of using CI/CD in data science projects.\\n5\\nDISCUSSION\\nThrough our survey, empirical study, and analysis, we presented\\nthe state of data science pipeline that describes its semantics, design\\nconcerns, and the overall computational paradigm. Furthermore,\\nthe findings show the importance of studying the pipeline structure\\nreminiscing the traditional software engineering works on design\\npatterns and architecture.\\nIn Theory: We presented all the representative stages and sub-\\ntasks that inform the terminology of DS pipelines to be used in\\nfuture works. By comparing with the available pipeline categories\\ne.g., ML process, big data, and team processes, similarities and diver-\\ngences can be directly identified. The presence of implicit feedback\\nloops and lack of post-processing stages suggest ad hoc pipeline\\nconstruction at the present time. This paper takes the first step\\ntowards comparable and reusable pipeline construction.\\nIn-The-Small: The novel API-based analysis can be utilized for\\nmining, extracting, and statically analyzing pipelines. We also\\nelicited the notion of high-level and low-level pipelines, where\\nthe high-level abstraction has more similarity with that in theory.\\nHowever, low-level pipelines exhibit many differences such as miss-\\ning some stages, sparse data preparation, lack of modularization.\\nThe gap between low-level pipeline and its presentation in high-\\nlevel can be reduced by making pipeline specific features available\\nin development environment e.g., pipeline template in Jupyter Note-\\nbook. Additionally, the low-level pipelines often had an important\\nstage exploratory data analysis missing which incurs much time\\nand effort. Pipeline versioning techniques that consider data, model,\\nand source code will facilitate storing such intermediate stages.\\nIn-The-Large: Different pipeline patterns emerged in develop-\\nment and post-development phase of the large projects, which\\nsuggest creating separate developer-centric and user-centric pipeline\\nstructure. In tightly-coupled projects, the abstraction of stages are\\ncontingent upon the project-specific requirements and internal/ex-\\nternal dependencies, whereas, in loosely-coupled projects, opportu-\\nnities remain to build reusable sub-pipelines that span over project\\nboundaries. Finally, there is a need for building automated CI/CD\\ntools for data science specific testing, deployment, and maintenance.\\nTo researchers and tool builders. (1) Modularization of DS\\npipeline into stages is challenging over all three representations.\\nFurther works are needed for standardization of pipeline architec-\\nture e.g., defining the interfaces of stages, enumerating externally\\nvisible properties, identifying domain-specific constraints, to de-\\nvelop reusable and interoperable DS pipelines. (2) We showed po-\\ntentials for automatic pipeline analysis framework based on static\\nanalysis and API specifications. A few future directions would\\nbe mining (sub-)pipelines patterns, build AutoML pipelines [78],\\nand analyzing evolution. (3) We confirmed several antipatterns of\\npipelines that call for actions e.g., CACE principle, pipeline jungles,\\nscarce post-processing, implicit feedback loop, CI/CD challenges.\\n(4) Pipeline specific tool support is needed such as version control\\nfor data and models, storing intermediate results between stages.\\nTo data scientists and engineers. (1) Pipelines are often built\\nfor a prototype in-the-small, which might not scale to a production\\nlevel system. A well-designed pipeline in the early stage will help\\nto identify key components, estimate cost, optimize, and manage\\nrisks better in the lifecycle. (2) The representative views of pipelines\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nwill serve as a checklist of stages and their connections. (3) Data\\nand algorithms being the focus of DS pipeline, preprocessing and\\nmodeling activities are well understood and practiced by data scien-\\ntists. However, they should emphasize more on including rigorous\\nevaluation beyond accuracy such as robustness and fairness [15, 16].\\n(4) Many people with diverse backgrounds are involved in a DS\\npipeline. A pipeline with human-in-the-loop approach will benefit\\nidentifying collaboration points, decomposing tasks, and manage\\ntransdisciplinary teams. For example, a pipeline can encourage\\ndata scientists to choose a modeling technique that is maintain-\\nable. (5) Future work is necessary to identify the interactions of DS\\npipeline with the real world i.e., which stages receive inputs, when\\na checkpoint is saved, how results are disseminated, etc.\\n6\\nTHREAT TO VALIDITY\\nFor building the pipelines from DS programs, we relied on the APIs.\\nOne threat might be, what happens if the developer does not use\\nany API for completing a stage in the program. We examined this\\npossibility and found that DS programs are heavily dependent on\\nlibraries and external APIs and ML tasks are always performed\\nusing library APIs. Additionally, we validated the API-to-stage\\ndictionary with the API documentation and manual verification.\\nAnother possible threat is that the Kaggle solutions might not\\nbe representative. We adopted a two-fold strategy to mitigate that\\nthreat. First, we selected the solutions with the most number of\\nvotes and at least 10 votes. Second, we manually verified each\\nprogram whether it is an end-to-end DS solution. Since some most\\nvoted solutions are only for introduction and exploratory analysis\\nof the dataset, by manual verification, we excluded those programs.\\nThe GitHub projects are also taken from a previously published\\ndataset containing DS repositories. We further filtered them based\\non the number of stars and whether they perform a DS task.\\nMoreover, since the chosen DS programs from Kaggle and GitHub\\nare using Python as the primary language, another question might\\nbe on the generalization of them as DS programs. According to\\nGitHub and Stack Overflow, Python has become the most growing\\nlanguage in recent times [47, 93]. In data science, Python is the\\nmost used language because of the availability of numerous ML, DL\\nand data analysis packages such as Pandas, NumPy, TensorFlow,\\nKeras, Caffe, Theano, Scikit-Learn and many more.\\n7\\nRELATED WORK\\nMany studies presented ML pipeline in their own context, which\\ncan not be generalized for all DS systems. Garcia et al. focused\\non building an iterative process with three main phases: develop-\\nment, training and inference. They described the interpretation of\\ndata and code while integrating the whole lifecycle [36]. Polyzotis\\net al. presented the challenges of data management in building\\nproduction-level ML pipeline in Google around three broad themes:\\ndata understanding, data validation and cleaning, and data prepa-\\nration [85, 86]. They also provided an overview of an end-to-end\\nlarge-scale ML pipeline with a data point of view. Carlton E. Sapp\\ndefined ML concepts, business challenges, stages in the lifecycle,\\nroles of DS teams with comprehensive end-to-end ML architec-\\nture [101]. This gives us a holistic understanding of the business\\nprocesses (e.g., acquire, organize, analyze, deliver) of a DS project.\\nA few other studies try to capture the DS process by surveying\\nand interviewing developers. Roh et al. surveyed the data collection\\ntechniques in the field of big data. They presented the workflow\\nof data collection answering how to improve data or models in an\\nML system [94]. Another study identified the software engineering\\npractices and challenges in building AI applications inside Microsoft\\ndevelopment teams [6]. They found some key differences in AI\\nsoftware process compared to other domains. They considered a 9-\\nstage workflow for DS software development. Hill et al. interviewed\\nexperienced AI developers and identified problems they face in\\neach stage [42]. They also tried to compare the traditional software\\nprocess and the AI process. Zhou presented her own view to build\\na better ML pipeline [139]. They presented three challenges in\\nbuilding ML pipelines: data quality, reliability and accessibility.\\nSome articles described ML applications and frameworks which\\npresent DS pipelines from industry. For example, Databricks pro-\\nvides high-level APIs for programming languages [44]. Team Data\\nScience Process (TDSP) is an agile and iterative process to build\\nintelligent applications inside Microsoft corporation [104]. In a US\\npatent, the authors compared two data analytic lifecycles [116], and\\npresented the difference in the set of parameters with respect to time\\nand cost. CRoss Industry Standard Process for Data Mining (CRISP-\\nDM) is a 6-stage comprehensive process model for data mining\\nprojects across any industry [130]. Google Cloud Blog described the\\nworkflow of an AI platform [40]. They explained tasks completed\\nin each stage with respect to Google Cloud and TensorFlow[1].\\nAlthough there are many papers in the literature presenting DS\\npipeline, there is no comprehensive study that tries to understand\\nand compare DS pipelines in theory and practice.\\n8\\nCONCLUSION\\nMany software systems today are incorporating a data science\\npipeline as their integral part. In this work, we argued that to facili-\\ntate research and practice on data science pipelines, it is essential to\\nunderstand their nature. To that end, we presented a three-pronged\\ncomprehensive study of data science pipelines in theory, data sci-\\nence pipelines in-the-small, and data science pipelines in-the-large.\\nOur study analyzed three datasets: a collection of 71 proposals for\\ndata science pipelines and related concepts in theory, a collection\\nof 105 implementations of data science pipelines from Kaggle com-\\npetitions to understand data science in-the-small, and a collection\\nof 21 mature data science projects from GitHub to understand data\\nscience in-the-large. We have found that DS pipelines differ signifi-\\ncantly between these settings. Specifically, a number of stages are\\nabsent in-the-small, and the DS pipelines have a more linear struc-\\nture. The DS pipelines in-the-large have a more complex structure\\nand feedback loops compared to the theoretical representations. We\\nalso contribute three representations of DS pipelines that capture\\nthe essence of our subjects in theory, in-the-small, and in-the-large.\\nACKNOWLEDGMENTS\\nThis work was supported in part by US NSF grants CNS-21-20448\\nand CCF-19-34884. We also thank the reviewers for their insightful\\ncomments. All opinions are of the authors and do not reflect the\\nview of sponsors.\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\nREFERENCES\\n[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.\\n2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX\\nSymposium on Operating Systems Design and Implementation (OSDI 16). 265–283.\\n[2] Waleed Abdulla. 2017. Mask R-CNN for object detection and instance segmen-\\ntation on Keras and TensorFlow. https://github.com/matterport/Mask_RCNN.\\n[3] Sudeep Agarwal. 2018. Understanding the Data Science Lifecycle. http://sudeep.\\nco/data-science/Understanding-the-Data-Science-Lifecycle.\\n[4] Charu Aggarwal, Djallel Bouneffouf, Horst Samulowitz, Beat Buesser, Thanh\\nHoang, Udayan Khurana, Sijia Liu, Tejaswini Pedapati, Parikshit Ram, Ambrish\\nRawat, et al. 2019. How can ai automate end-to-end data science? arXiv preprint\\narXiv:1910.14436 (2019).\\n[5] Jesse Hu Alexis Chan, Octavio Arriaga. 2017. Autopilot-TensorFlow. https:\\n//github.com/SullyChen/Autopilot-TensorFlow.\\n[6] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,\\nEce Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.\\n2019. Software Engineering for Machine Learning: A Case Study. In Proceedings\\nof the 41st International Conference on Software Engineering. ACM.\\n[7] Anonymous. 2021.\\nData Science Pipline Artifact.\\nhttps://github.com/\\nanonymous-authorss/DS-Pipeline.\\n[8] Octavio Arriaga. 2018. Face classification and detectionn. https://github.com/\\noarriaga/face_classification.\\n[9] Rob Ashmore, Radu Calinescu, and Colin Paterson. 2021. Assuring the Machine\\nLearning Lifecycle: Desiderata, Methods, and Challenges. ACM Comput. Surv.\\n54, 5, Article 111 (may 2021). https://doi.org/10.1145/3453444\\n[10] Jakob Aungiers. 2019. LSTM Neural Network for Time Series Prediction. https:\\n//github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction.\\n[11] Alex Ball. 2012. Review of data management lifecycle models. University of Bath,\\nIDMRC.\\n[12] Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria\\nHaque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, et al. 2017. TFX: A\\ntensorflow-based production-scale machine learning platform. In Proceedings\\nof the 23rd ACM SIGKDD International Conference on Knowledge Discovery and\\nData Mining. ACM, 1387–1395.\\n[13] Francine Berman, Rob Rutenbar, Brent Hailpern, Henrik Christensen, Susan\\nDavidson, Deborah Estrin, Michael Franklin, Margaret Martonosi, Padma Ragha-\\nvan, Victoria Stodden, et al. 2018. Realizing the potential of data science. Com-\\nmun. ACM 61, 4 (2018), 67–72.\\n[14] Sumon Biswas, Md Johirul Islam, Yijia Huang, and Hridesh Rajan. 2019. Boa\\nmeets Python: a Boa dataset of data science software in Python language. In\\nProceedings of the 16th International Conference on Mining Software Repositories.\\nIEEE Press, 577–581.\\n[15] Sumon Biswas and Hridesh Rajan. 2020. Do the Machine Learning Models on a\\nCrowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness.\\nIn Proceedings of the 28th ACM Joint Meeting on European Software Engineering\\nConference and Symposium on the Foundations of Software Engineering (Virtual\\nEvent, USA). 642–653. https://doi.org/10.1145/3368089.3409704\\n[16] Sumon Biswas and Hridesh Rajan. 2021. Fair Preprocessing: Towards Under-\\nstanding Compositional Fairness of Data Transformers in Machine Learning\\nPipeline. In ESEC/FSE’2021: The 29th ACM Joint European Software Engineering\\nConference and Symposium on the Foundations of Software Engineering (Athens,\\nGreece).\\n[17] Denny Britz. 2018. Convolutional Neural Network for Text Classification in\\nTensorflow. https://github.com/dennybritz/cnn-text-classification-tf.\\n[18] Muffy Calder, Mario Kolberg, Evan H. Magill, and Stephan Reiff-Marganiec.\\n2003. Feature Interaction: A Critical Review and Considered Forecast. Comput.\\nNetw. 41, 1 (Jan. 2003), 115–141. https://doi.org/10.1016/S1389-1286(02)00352-3\\n[19] Maurice Chang. 2017. 4 Stages of the Machine Learning (ML) Modeling Cy-\\ncle. https://www.linkedin.com/pulse/4-stages-machine-learning-ml-modeling-\\ncycle-maurice-chang.\\n[20] CL Philip Chen and Chun-Yang Zhang. 2014. Data-intensive applications,\\nchallenges, techniques and technologies: A survey on Big Data. Information\\nsciences 275 (2014), 314–347.\\n[21] Z Ming Chen Mengda. 2018. reproduce MTCNN,a Joint Face Detection and\\nAlignment using Multi-task Cascaded Convolutional Networks. https://github.\\ncom/AITTSMD/MTCNN-Tensorflow.\\n[22] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.\\n2014. Project Adam: Building an efficient and scalable deep learning train-\\ning system. In 11th {USENIX} Symposium on Operating Systems Design and\\nImplementation (𝑂𝑆𝐷𝐼14). 571–582.\\n[23] George E Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov. 2014. Multi-task\\nneural networks for QSAR predictions. arXiv preprint arXiv:1406.1231 (2014).\\n[24] Sam Crane Dat Tran. 2018. Real-Time Object Recognition App with Tensorflow\\nand OpenCV. https://github.com/datitran/object_detector_app.\\n[25] Hal Daumé III. 2016. What Is a Machine Learning Pipeline? https://nlpers.\\nblogspot.com/2016/08/debugging-machine-learning.html.\\n[26] Yuri Demchenko, Fatih Turkmen, Cees de Laat, Christophe Blanchet, and Charles\\nLoomis. 2016. Cloud based big data infrastructure: Architectural components\\nand automated provisioning. In 2016 International Conference on High Perfor-\\nmance Computing & Simulation (HPCS). IEEE, 628–636.\\n[27] Yuri Demchenko, Zhiming Zhao, Paola Grosso, Adianto Wibisono, and Cees\\nDe Laat. 2012. Addressing big data challenges for scientific data infrastructure.\\nIn 4th IEEE International Conference on Cloud Computing Technology and Science\\nProceedings. IEEE, 614–617.\\n[28] Edsger W Dijkstra. 1982. On the role of scientific thought. In Selected writings\\non computing: a personal perspective. Springer, 60–66.\\n[29] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2013.\\nBoa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software\\nRepositories. In Proceedings of the 35th International Conference on Software\\nEngineering (San Francisco, CA) (ICSE’13). 422–431.\\n[30] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2015. Boa:\\nUltra-Large-Scale Software Repository and Source-Code Mining. ACM Trans.\\nSoftw. Eng. Methodol. 25, 1, Article 7 (Dec. 2015), 34 pages. https://doi.org/10.\\n1145/2803171\\n[31] Matthew Earl. 2016. Using neural networks to build an automatic number plate\\nrecognition system. https://github.com/matthewearl/deep-anpr.\\n[32] Mohammed El Arass and Nissrine Souissi. 2018. Data lifecycle: from big data to\\nSmartData. In 2018 IEEE 5th international congress on information science and\\ntechnology (CiSt). IEEE, 80–87.\\n[33] Douglas Fisher. 2017. A selected summary of AI for computational sustainability.\\nIn Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 31.\\n[34] Erich Gamma, Richard Helm, Ralph E. Johnson, and John M. Vlissides. 1993. De-\\nsign Patterns: Abstraction and Reuse of Object-Oriented Design. In Proceedings\\nof the 7th European Conference on Object-Oriented Programming (ECOOP ’93).\\nSpringer-Verlag, Berlin, Heidelberg, 406–431.\\n[35] Amir Gandomi and Murtaza Haider. 2015. Beyond the hype: Big data concepts,\\nmethods, and analytics. International journal of information management 35, 2\\n(2015), 137–144.\\n[36] Rolando Garcia, Vikram Sreekanti, Neeraja Yadwadkar, Daniel Crankshaw,\\nJoseph E Gonzalez, and Joseph M Hellerstein. 2018. Context: The missing piece\\nin the machine learning lifecycle. In KDD CMI Workshop, Vol. 114.\\n[37] David Garlan. 2000. Software architecture: a roadmap. In Proceedings of the\\nConference on the Future of Software Engineering. 91–101.\\n[38] Yolanda Gil, Ke-Thia Yao, Varun Ratnakar, Daniel Garijo, Greg Ver Steeg, Pedro\\nSzekely, Rob Brekelmans, Mayank Kejriwal, Fanghao Luo, and I-Hui Huang.\\n2018. P4ML: A phased performance-based pipeline planner for automated\\nmachine learning. In AutoML Workshop at ICML.\\n[39] Stephanie Glen. 2019. The Lifecycle of Data. https://www.datasciencecentral.\\ncom/profiles/blogs/the-lifecycle-of-data.\\n[40] Google Cloud Blog. 2019. Machine Learning Workflow. https://cloud.google.\\ncom/ml-engine/docs/tensorflow/ml-solutions-overview.\\n[41] Yufeng Guo. 2017. The 7 Steps of Machine Learning. https://towardsdatascience.\\ncom/the-7-steps-of-machine-learning-2877d7e5548e.\\n[42] Charles Hill, Rachel Bellamy, Thomas Erickson, and Margaret Burnett. 2016.\\nTrials and tribulations of developers of intelligent systems: A field study. In 2016\\nIEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).\\nIEEE, 162–170.\\n[43] Michael Hilton, Nicholas Nelson, Timothy Tunnell, Darko Marinov, and Danny\\nDig. 2017. Trade-Offs in Continuous Integration: Assurance, Security, and\\nFlexibility. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software\\nEngineering (Paderborn, Germany) (ESEC/FSE 2017). Association for Computing\\nMachinery, New York, NY, USA, 197–207.\\nhttps://doi.org/10.1145/3106237.\\n3106270\\n[44] Sue Ann Hong and Tim Hunter. 2017. Build, Scale, and Deploy Deep Learning\\nPipelines with Ease. https://databricks.com/blog/2017/09/06/build-scale-deploy-\\ndeep-learning-pipelines-ease.html.\\n[45] Han Hu, Yonggang Wen, Tat-Seng Chua, and Xuelong Li. 2014. Toward scalable\\nsystems for big data analytics: A technology tutorial. IEEE access 2 (2014),\\n652–687.\\n[46] Waldemar Hummer, Vinod Muthusamy, Thomas Rausch, Parijat Dube, Kaoutar\\nEl Maghraoui, Anupama Murthi, and Punleuk Oum. 2019. Modelops: Cloud-\\nbased lifecycle management for reliable and trusted ai. In 2019 IEEE International\\nConference on Cloud Engineering (IC2E). IEEE, 113–120.\\n[47] GitHub Inc. 2019. Octoverse 2018. https://octoverse.github.com/projects.\\n[48] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A\\nComprehensive Study on Deep Learning Bug Characteristics. In ESEC/FSE’19:\\nThe ACM Joint European Software Engineering Conference and Symposium on\\nthe Foundations of Software Engineering (ESEC/FSE).\\n[49] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-\\ning Deep Neural Networks: Fix Patterns and Challenges. In ICSE’20: The 42nd\\nInternational Conference on Software Engineering (Seoul, South Korea).\\n[50] HV Jagadish. 2015. Big data and science: Myths and reality. Big Data Research\\n2, 2 (2015), 49–52.\\n[51] Kathryn Jepsen. 2014.\\nThe machine learning community takes on the\\nHiggs. https://www.symmetrymagazine.org/article/july-2014/the-machine-\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nlearning-community-takes-on-the-higgs.\\n[52] M. Tim Jones. 2018.\\nData, structure, and the data science pipeline.\\nhttps://developer.ibm.com/technologies/data-science/articles/ba-intro-data-\\nscience-1/.\\n[53] Kaggle. 2021. Kaggle Notebook. www.kaggle.com/competitions.\\n[54] Kaggle. 2021. Kaggle Notebook. www.kaggle.com/thousandvoices/simple-lstm.\\n[55] Kaggle. 2021. Kaggle Notebook. https://www.kaggle.com/zfturbo/simple-ru-\\nbaseline-lb-0-9627.\\n[56] Kaggle. 2021. Kaggle Notebook. www.kaggle.com/seesee/siamese-pretrained-\\n0-822.\\n[57] Kaggle. 2021. Kaggle Notebook. www.kaggle.com/willkoehrsen/start-here-a-\\ngentle-introduction.\\n[58] Kaggle. 2021.\\nKaggle Notebook.\\nhttps://www.kaggle.com/danielbecker/\\ncareervillage-org-recommendation-engine.\\n[59] Bojan Karlaš, Matteo Interlandi, Cedric Renggli, Wentao Wu, Ce Zhang, Deepak\\nMukunthu Iyappan Babu, Jordan Edwards, Chris Lauren, Andy Xu, and Markus\\nWeimer. 2020. Building continuous integration services for machine learning.\\nIn Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\\nDiscovery & Data Mining. 2407–2415. https://doi.org/10.1145/3394486.3403290\\n[60] Keras. 2021. Keras API Reference. https://keras.io/api/.\\n[61] Keras. 2021. Scikit-Learn API Reference. https://scikit-learn.org/stable/modules/\\nclasses.html.\\n[62] Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A\\nMyers. 2018. The story in the notebook: Exploratory data science using a literate\\nprogramming tool. In Proceedings of the 2018 CHI Conference on Human Factors\\nin Computing Systems. 1–11.\\n[63] Samiya Khan, Xiufeng Liu, Kashish A Shakil, and Mansaf Alam. 2017. A sur-\\nvey on scholarly data: From big data perspective. Information Processing &\\nManagement 53, 4 (2017), 923–944.\\n[64] Gregor Kiczales, John Lamping, Anurag Mendhekar, Chris Maeda, Cristina\\nLopes, Jean-Marc Loingtier, and John Irwin. 1997. Aspect-oriented program-\\nming. In ECOOP’97 — Object-Oriented Programming, Mehmet Akşit and Satoshi\\nMatsuoka (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 220–242.\\n[65] Miryung Kim, Thomas Zimmermann, Robert DeLine, and Andrew Begel. 2016.\\nThe emerging role of data scientists on software development teams. In Pro-\\nceedings of the 38th International Conference on Software Engineering. ACM,\\n96–107.\\n[66] Namju Kim. 2018. Speech-to-Text-WaveNet : End-to-end sentence level English\\nspeech recognition based on DeepMind’s WaveNet and tensorflow. https://\\ngithub.com/buriburisuri/speech-to-text-wavenet.\\n[67] Tim Kraska, Ameet Talwalkar, John C Duchi, Rean Griffith, Michael J Franklin,\\nand Michael I Jordan. 2013. MLbase: A Distributed Machine-learning System..\\nIn Cidr, Vol. 1. 2–1.\\n[68] Sara Landset, Taghi M Khoshgoftaar, Aaron N Richter, and Tawfiq Hasanin.\\n2015. A survey of open source tools for machine learning with big data in the\\nHadoop ecosystem. Journal of Big Data 2, 1 (2015), 24.\\n[69] Deanne Larson and Victor Chang. 2016. A review and future direction of\\nagile, business intelligence, analytics and data science. International Journal of\\nInformation Management 36, 5 (2016), 700–710.\\n[70] Bennet P Lientz, E. Burton Swanson, and Gail E Tompkins. 1978. Characteristics\\nof application software maintenance. Commun. ACM 21, 6 (1978), 466–471.\\n[71] Sin Kit Lo, Qinghua Lu, Chen Wang, Helen Paik, and Liming Zhu. 2020. A\\nsystematic literature review on federated machine learning: From a software\\nengineering perspective. arXiv preprint arXiv:2007.11354 (2020).\\n[72] Hui Miao, Amit Chavan, and Amol Deshpande. 2017. Provdb: Lifecycle manage-\\nment of collaborative analysis workflows. In Proceedings of the 2nd Workshop\\non Human-In-the-Loop Data Analytics. ACM, 7.\\n[73] Hui Miao, Ang Li, Larry S Davis, and Amol Deshpande. 2017. Towards unified\\ndata and lifecycle management for deep learning. In 2017 IEEE 33rd International\\nConference on Data Engineering (ICDE). IEEE, 571–582.\\n[74] Microsoft Blog. 2019. What are ML pipelines in Azure Machine Learning\\nservice? https://docs.microsoft.com/en-us/azure/machine-learning/service/\\nconcept-ml-pipelines.\\n[75] Justin J Miller. 2013. Graph database applications and concepts with Neo4j.\\nIn Proceedings of the Southern Association for Information Systems Conference,\\nAtlanta, GA, USA, Vol. 2324.\\n[76] Valohai MLOps. 2020. What Is a Machine Learning Pipeline? https://valohai.\\ncom/machine-learning-pipeline/.\\n[77] Giang Nguyen, Stefan Dlugolinsky, Martin Bobák, Viet Tran, Álvaro López\\nGarcía, Ignacio Heredia, Peter Malík, and Ladislav Hluch`y. 2019. Machine\\nLearning and Deep Learning frameworks and libraries for large-scale data\\nmining: a survey. Artificial Intelligence Review (2019), 1–48.\\n[78] Giang Nguyen, Johir Islam, Rangeet Pan, and Hridesh Rajan. 2022. Manas: Min-\\ning Software Repositories to Assist AutoML. In ICSE’22: The 44th International\\nConference on Software Engineering (Pittsburgh, PA, USA).\\n[79] Randal S Olson, Nathan Bartley, Ryan J Urbanowicz, and Jason H Moore. 2016.\\nEvaluation of a tree-based pipeline optimization tool for automating data science.\\nIn Proceedings of the Genetic and Evolutionary Computation Conference 2016.\\nACM, 485–492.\\n[80] Alex Paino. 2017. Deep learning models trained to correct input errors in short,\\nmessage-like text. https://github.com/atpaino/deep-text-corrector.\\n[81] Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural Network\\ninto Modules. In ESEC/FSE’2020: The 28th ACM Joint European Software Engi-\\nneering Conference and Symposium on the Foundations of Software Engineering\\n(Sacramento, California, United States).\\n[82] Rangeet Pan and Hridesh Rajan. 2022. Decomposing Convolutional Neural Net-\\nworks into Reusable and Replaceable Modules. In ICSE’22: The 44th International\\nConference on Software Engineering (Pittsburgh, PA, USA).\\n[83] Kyubyong Park. 2018. A TensorFlow Implementation of Tacotron: A Fully\\nEnd-to-End Text-To-Speech Synthesis Model. https://github.com/Kyubyong/\\ntacotron.\\n[84] David Lorge Parnas, Paul C Clements, and David M Weiss. 1985. The modular\\nstructure of complex systems. IEEE Transactions on software Engineering 3\\n(1985), 259–266.\\n[85] Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich.\\n2017. Data management challenges in production machine learning. In Proceed-\\nings of the 2017 ACM International Conference on Management of Data. ACM,\\n1723–1726.\\n[86] Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich.\\n2018. Data Lifecycle Challenges in Production Machine Learning: A Survey.\\nACM SIGMOD Record 47, 2 (2018), 17–28.\\n[87] Line Pouchard. 2016. Revisiting the data lifecycle with big data curation. Inter-\\nnational Journal of Digital Curation 10, 2 (2016), 176–192.\\n[88] Christian Prehofer. 1997. Feature-oriented programming: A fresh look at ob-\\njects. In ECOOP’97 — Object-Oriented Programming, Mehmet Akşit and Satoshi\\nMatsuoka (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 419–443.\\n[89] Chuan Qi. 2019. Caffe implementation of Google MobileNet SSD detection\\nnetwork. https://github.com/chuanqi305/MobileNet-SSD.\\n[90] Václav Rajlich. 2014. Software evolution and maintenance. In Future of Software\\nEngineering Proceedings. 133–144.\\n[91] Muhammad Habib Rehman, Victor Chang, Aisha Batool, and Teh Ying Wah.\\n2016. Big data reduction framework for value creation in sustainable enterprises.\\nInternational Journal of Information Management 36, 6 (2016), 917–928.\\n[92] Syed Ali Asad Rizvi, Elmarie Van Heerden, Arnold Salas, Favour Nyikosa,\\nStephen J Roberts, Michael A Osborne, and Elmer Rodriguez. 2017. Identi-\\nfying Sources of Discrimination Risk in the Life Cycle of Machine Intelligence\\nApplications under New European Union Regulations. In 2017 AAAI Spring\\nSymposium Series.\\n[93] David Robinson. 2017. The Incredible Growth of Python. https://stackoverflow.\\nblog/2017/09/06/incredible-growth-python/.\\n[94] Yuji Roh, Geon Heo, and Steven Euijong Whang. 2019. A Survey on Data\\nCollection for Machine Learning: a Big Data-AI Integration Perspective. IEEE\\nTransactions on Knowledge and Data Engineering (2019).\\n[95] Eragon Ruan. 2019. Scene text detection based on ctpn (connectionist text\\nproposal network). https://github.com/eragonruan/text-detection-ctpn.\\n[96] Janine Rüegg, Corinna Gries, Ben Bond-Lamberty, Gabriel J Bowen, Benjamin S\\nFelzer, Nancy E McIntyre, Patricia A Soranno, Kristin L Vanderbilt, and Kath-\\nleen C Weathers. 2014.\\nCompleting the data life cycle: using information\\nmanagement in macrosystems ecology research. Frontiers in Ecology and the\\nEnvironment 12, 1 (2014), 24–30.\\n[97] Adam Rule, Aurélien Tabard, and James D Hollan. 2018. Exploration and expla-\\nnation in computational notebooks. In Proceedings of the 2018 CHI Conference\\non Human Factors in Computing Systems. 1–12.\\n[98] Philippe Rémy. 2018. Deep Learning model to analyze a large corpus of clear text\\npasswords. https://github.com/philipperemy/tensorflow-1.4-billion-password-\\nanalysis.\\n[99] Shazia Sadiq, Tamraparni Dasu, Xin Luna Dong, Juliana Freire, Ihab F Ilyas,\\nSebastian Link, Miller J Miller, Felix Naumann, Xiaofang Zhou, and Divesh\\nSrivastava. 2018. Data quality: The role of empiricism. ACM SIGMOD Record\\n46, 4 (2018), 35–43.\\n[100] David Sandberg. 2018. Face Recognition using Tensorflow. https://github.com/\\ndavidsandberg/facenet.\\n[101] Carlton E Sapp. 2017. Preparing and architecting machine learning. Gartner\\nTechnical Professional Advice (2017), 1–37.\\n[102] David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,\\nDietmar Ebner, Vinay Chaudhary, and Michael Young. 2014. Machine learning:\\nThe high interest credit card of technical debt. (2014).\\n[103] David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,\\nDietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and\\nDan Dennison. 2015. Hidden technical debt in machine learning systems. In\\nAdvances in neural information processing systems. 2503–2511.\\n[104] Roald Bradley Severtson. 2017.\\nWhat is the Team Data Science Pro-\\ncess? https://docs.microsoft.com/en-us/azure/machine-learning/team-data-\\nscience-process/overview.\\n[105] Zeyuan Shang, Emanuel Zgraggen, Benedetto Buratti, Ferdinand Kossmann,\\nPhilipp Eichmann, Yeounoh Chung, Carsten Binnig, Eli Upfal, and Tim Kraska.\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\n2019. Democratizing data science through interactive curation of ML pipelines.\\nIn Proceedings of the 2019 International Conference on Management of Data. ACM,\\n1171–1188.\\n[106] M Shashanka. 2019. What is a Pipeline in Machine Learning? How to create\\none? https://medium.com/analytics-vidhya/what-is-a-pipeline-in-machine-\\nlearning-how-to-create-one-bda91d0ceaca.\\n[107] Mary Shaw and David Garlan. 1996. Software Architecture: Perspectives on an\\nEmerging Discipline. Prentice-Hall, Inc., USA.\\n[108] Naoki Shibuya. 2017.\\nPipelines, Mind Maps and Convolutional Neu-\\nral Networks.\\nhttps://towardsdatascience.com/pipelines-mind-maps-and-\\nconvolutional-neural-networks-34bfc94db10c.\\n[109] Amir Sinaeepourfard, Jordi Garcia, Xavier Masip-Bruin, and Eva Marín-Torder.\\n2016. Towards a comprehensive data lifecycle model for big data environ-\\nments. In Proceedings of the 3rd IEEE/ACM International Conference on Big Data\\nComputing, Applications and Technologies. ACM, 100–106.\\n[110] Sivakar Siva. 2020.\\nThe “Generic” Data Science Life-Cycle.\\nhttps://\\ntowardsdatascience.com/stoend-to-end-data-science-life-cycle-6387523b5afc.\\n[111] Micah J Smith, Roy Wedge, and Kalyan Veeramachaneni. 2017. FeatureHub:\\nTowards collaborative data science. In 2017 IEEE International Conference on\\nData Science and Advanced Analytics (DSAA). IEEE, 590–600.\\n[112] Guocong Song. 2017. Tensorflow-based Recommendation systems. https://\\ngithub.com/songgc/TF-recomm.\\n[113] Evan R Sparks, Shivaram Venkataraman, Tomer Kaftan, Michael J Franklin,\\nand Benjamin Recht. 2017. Keystoneml: Optimizing pipelines for large-scale\\nadvanced analytics. In 2017 IEEE 33rd international conference on data engineering\\n(ICDE). IEEE, 535–546.\\n[114] Victoria Stodden. 2020. The data science life cycle: a disciplined approach to\\nadvancing data science as a science. Commun. ACM 63, 7 (2020), 58–66.\\n[115] Marvin Teichmann. 2018. A Kitti Road Segmentation Model Implemented in\\nTensorflow. https://github.com/MarvinTeichmann/KittiSeg.\\n[116] Stephen Todd and David Dietrich. 2017. Computing resource re-provisioning\\nduring data analytic lifecycle. US Patent 9,619,550.\\n[117] Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gon-\\nzalez Zelaya, and Aad van Moorsel. 2020. The relationship between trust in\\nAI and trustworthy machine learning technologies. In Proceedings of the 2020\\nConference on Fairness, Accountability, and Transparency. 272–283.\\n[118] Andrew Bagshaw Trieu. 2018. Real-time object detection and classification.\\nhttps://github.com/thtrieu/darkflow.\\n[119] Cagatay Turkay, Nicola Pezzotti, Carsten Binnig, Hendrik Strobelt, Barbara\\nHammer, Daniel A Keim, Jean-Daniel Fekete, Themis Palpanas, Yunhai Wang,\\nand Florin Rusu. 2018. Progressive data science: Potential and challenges. arXiv\\npreprint arXiv:1812.08032 (2018).\\n[120] Tom Van Der Weide, Dimitris Papadopoulos, Oleg Smirnov, Michal Zielinski, and\\nTim Van Kasteren. 2017. Versioning for end-to-end machine learning pipelines.\\nIn Proceedings of the 1st Workshop on Data Management for End-to-End Machine\\nLearning. ACM, 2.\\n[121] Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding interobserver\\nagreement: the kappa statistic. Fam med 37, 5 (2005), 360–363.\\n[122] Ben Wagner. 2020. Accountability by design in technology research. Computer\\nLaw & Security Review 37 (2020), 105398.\\n[123] Zhiyuan Wan, Xin Xia, David Lo, and Gail C Murphy. 2019. How does machine\\nlearning change software development practices? IEEE Transactions on Software\\nEngineering (2019).\\n[124] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer,\\nCasey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019.\\nHuman-AI collaboration in data science: Exploring data scientists’ perceptions\\nof automated AI. Proceedings of the ACM on Human-Computer Interaction 3,\\nCSCW (2019), 1–24.\\n[125] Jiawei Wang, Li Li, and Andreas Zeller. 2021. Restoring Execution Environ-\\nments of Jupyter Notebooks. In 2021 IEEE/ACM 43rd International Conference on\\nSoftware Engineering (ICSE). IEEE, 1622–1633.\\n[126] Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2022. Deep-\\nDiagnosis: Automatically Diagnosing Faults and Recommending Actionable\\nFixes in Deep Learning Programs. In ICSE’22: The 44th International Conference\\non Software Engineering (Pittsburgh, PA, USA).\\n[127] Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: Fault\\nLocalization for Deep Neural Networks. In ICSE’21: The 43nd International\\nConference on Software Engineering (Virtual Conference).\\n[128] Hadley Wickham. 2019. Data science: how is it different to statistics? IMS\\nBulletin 48 (2019).\\n[129] Jeannette M Wing. 2019. The Data Life Cycle. Harvard Data Science Review\\n(2019).\\n[130] Rüdiger Wirth and Jochen Hipp. 2000. CRISP-DM: Towards a standard process\\nmodel for data mining. In Proceedings of the 4th international conference on the\\npractical applications of knowledge discovery and data mining. Citeseer, 29–39.\\n[131] Christof Wolf, Dominique Joye, Tom W Smith, and Yang-chih Fu. 2016. The\\nSAGE handbook of survey methodology. Sage.\\n[132] Max Woolf. 2018. Automatically \"block\" people in images (like Black Mirror) us-\\ning a pretrained neural network. https://github.com/minimaxir/person-blocker.\\n[133] Mehmet Yildiz. 2020. Big Data Lifecycle Management. https://medium.com/\\ntechnology-hits/big-data-lifecycle-management-629dfe16b78d.\\n[134] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Moham-\\nmad Norouzi, and Quoc V Le. 2018. A Tensorflow implementation of QANet for\\nmachine reading comprehension. https://github.com/NLPLearn/QANet.\\n[135] Yuyu Zhang, Mohammad Taha Bahadori, Hang Su, and Jimeng Sun. 2016. FLASH:\\nfast Bayesian optimization for data analytic pipelines. In Proceedings of the 22nd\\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\\nACM, 2065–2074.\\n[136] Yingfeng Zhang, Shan Ren, Yang Liu, Tomohiko Sakao, and Donald Huisingh.\\n2017. A framework for Big Data driven product lifecycle management. Journal\\nof Cleaner Production 159 (2017), 229–240.\\n[137] Charlie Bickerton Zhilin Yang, Zihang Dai. 2019. XLNet: Generalized Autore-\\ngressive Pretraining for Language Understanding. https://github.com/zihangdai/\\nxlnet.\\n[138] Baifan Zhou, Yulia Svetashova, Tim Pychynski, Ildar Baimuratov, Ahmet Soylu,\\nand Evgeny Kharlamov. 2020. SemFE: facilitating ML pipeline development\\nwith semantics. In Proceedings of the 29th ACM International Conference on\\nInformation & Knowledge Management. 3489–3492.\\n[139] Linda Zhou. 2019.\\nHow to Build a Better Machine Learning Pipeline.\\nhttps://www.datanami.com/2018/09/05/how-to-build-a-better-machine-\\nlearning-pipeline.\\nThe Art and Practice of Data Science Pipelines\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nTable 3: Labeled data science pipelines from the subject studies. ACQ: Data acquisition, PRP: Data preparation, STR: Data\\nstorage, FTR: Feature engineering, MDL: Modeling, TRN: Training, EVL: Evaluation, PRD: Prediction, INT: Interpretation,\\nCMN: Communication, DPL: Deployment.\\nOverall goal:\\nDescribe/propose pipeline,\\nSurvey/compare/review,\\nDS optimization,\\nIntroduce new method/application\\nPreprocessing\\nModeling\\nPost-processing\\nInvolves\\nType\\nReferences\\nACQ\\nPRP\\nSTR\\nFTR\\nMDL\\nTRN\\nEVL\\nPRD\\nINT\\nCMN\\nDPL\\nCyber\\nPhysical Human\\nOlson et al., 2016 [79]\\n-\\n-\\n-\\n-\\n-\\n-\\nMiao et al., 2017b [73]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nGarcia et al., 2018 [36]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nHong and Hunter, 2017 [44]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMicrosoft Blog, 2019 [74]\\n-\\n-\\n-\\n-\\n-\\nZhou, 2019 [139]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nShibuya, 2017 [108]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nPolyzotis et al., 2018 [86]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nRoh et al., 2019 [94]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMiao et al., 2017a [72]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSparks et al., 2017 [113]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nGuo, 2017 [41]\\n-\\n-\\n-\\n-\\n-\\n-\\nBaylor et al., 2017 [12]\\n-\\n-\\n-\\n-\\n-\\n-\\nAbadi et al., 2016 [1]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nChilimbi et al., 2014 [22]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nKraska et al., 2013 [67]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSculley et al., 2015 [103]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nChang, 2017 [19]\\n-\\n-\\n-\\n-\\n-\\nGoogle Cloud Blog, 2019 [40]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nAmershi et al., 2019 [6]\\n-\\n-\\n-\\n-\\n-\\nVan Der Weide et al., 2017 [120]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nHill et al., 2016 [42]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nShang et al., 2019 [105]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nZhang et al., 2016 [135]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nGil et al., 2018 [38]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSadiq et al., 2018 [99]\\n-\\n-\\n-\\n-\\n-\\n-\\nZhou et al., 2020 [138]\\n-\\n-\\n-\\n-\\n-\\n-\\nAggarwal et al., 2019 [4]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nToreini et al., 2020 [117]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nAshmore et al., 2021 [9]\\nShashanka, 2019 [106]\\n-\\n–\\n–\\n–\\n–\\nMLOps, 2020 [76]\\n-\\n-\\n–\\n–\\n–\\n–\\nMachine learning process\\nDaumé III, 2016 [25]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nTodd and Dietrich, 2017 [116]\\n-\\n-\\n-\\n-\\n-\\nZhang et al., 2017 [136]\\n-\\n-\\n-\\n-\\n-\\n-\\nSapp, 2017 [101]\\n-\\nLandset et al., 2015 [68]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nPolyzotis et al., 2017 [85]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nHu et al., 2014 [45]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nDemchenko et al., 2012 [27]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nKhan et al., 2017 [63]\\n-\\n-\\n-\\n-\\n-\\n-\\nEl Arass and Souissi, 2018 [32]\\n-\\n-\\n-\\n–\\n–\\n-\\n-\\nHummer et al., 2019 [46]\\n-\\n-\\n-\\n-\\n-\\nYildiz, 2020 [133]\\n-\\n-\\n-\\n-\\nGlen, 2019 [39]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nBig data management\\nJones, 2018 [52]\\n-\\n-\\n-\\n–\\n-\\n-\\nICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA\\nSumon Biswas, Mohammad Wardat, and Hridesh Rajan\\nPreprocessing\\nModeling\\nPost-processing\\nInvolves\\nType\\nReferences\\nACQ\\nPRP\\nSTR\\nFTR\\nMDL\\nTRN\\nEVL\\nPRD\\nINT\\nCMN\\nDPL\\nCyber\\nPhysical Human\\nPouchard, 2016 [87]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSevertson, 2017 [104]\\n-\\n-\\nBerman et al., 2018 [13]\\n-\\n-\\n-\\n-\\n-\\nAgarwal, 2018 [3]\\n-\\n-\\n-\\nNguyen et al., 2019 [77]\\n-\\n-\\n-\\n-\\n-\\n-\\nRüegg et al., 2014 [96]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nGandomi and Haider, 2015 [35]\\n-\\n-\\n-\\n-\\n-\\n-\\nBall, 2012 [11]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nWing, 2019 [129]\\n-\\n-\\n-\\n-\\n-\\n-\\nRehman et al., 2016 [91]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nChen and Zhang, 2014 [20]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nJagadish, 2015 [50]\\n-\\n-\\n-\\n-\\n-\\n-\\nLarson and Chang, 2016 [69]\\n-\\n-\\n-\\n-\\n-\\n-\\nRizvi et al., 2017 [92]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nDemchenko et al., 2016 [26]\\n-\\n-\\n-\\n-\\n-\\nWolf et al., 2016 [131]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSinaeepourfard et al., 2016 [109]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nKim et al., 2016 [65]\\n-\\n-\\n-\\n-\\nFisher, 2017 [33]\\n-\\n-\\n-\\n-\\n-\\nTurkay et al., 2018 [119]\\n-\\n-\\n-\\n-\\n-\\n-\\nSmith et al., 2017 [111]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nWang et al., 2019 [124]\\n-\\n-\\n-\\n-\\n-\\n-\\nLo et al., 2020 [71]\\n-\\n-\\n-\\n-\\n-\\nSiva, 2020 [110]\\n-\\n-\\n-\\nTeam process\\nStodden, 2020 [114]\\n-\\n',\n",
       " '2201.05852v1.pdf': 'Data Science in Perspective \\nRogério Rossi \\nCenter for Mathematics, Computing and Cognition (CMCC) \\nFederal University of ABC (UFABC) \\nSanto André, São Paulo, Brazil \\nrogeriorossi8@gmail.com \\n \\n \\nAbstract\\nData and Science has stood out in the generation of \\nresults, whether in the projects of the scientific domain or \\nbusiness domain. CERN Project, Scientific Institutes, companies \\nlike Walmart, Google, Apple, among others, need data to present \\ntheir results and make predictions in the competitive data world. \\nData and Science are words that together culminated in a \\nglobally recognized term called Data Science. Data Science is in \\nits initial phase, possibly being part of formal sciences and also \\nbeing presented as part of applied sciences, capable of generating \\nvalue and supporting decision making. Data Science considers \\nscience and, consequently, the scientific method to promote \\ndecision making through data intelligence. In many cases, the \\napplication of the method (or part of it) is considered in Data \\nScience \\nprojects \\nin \\nscientific \\ndomain \\n(social \\nsciences, \\nbioinformatics, geospatial projects) or business domain (finance, \\nlogistic, retail), among others. In this sense, this article addresses \\nthe perspectives of Data Science as a multidisciplinary area, \\nconsidering science and the scientific method, and its formal \\nstructure which integrate Statistics, Computer Science, and \\nBusiness Science, also taking into account Artificial Intelligence, \\nemphasizing Machine Learning, among others. The article also \\ndeals with the perspective of applied Data Science, since Data \\nScience is used for generating value through scientific and \\nbusiness projects. Data Science persona is also discussed in the \\narticle, concerning the education of Data Science professionals \\nand its corresponding profiles, since its projection changes the \\nfield of data in the world. \\nKeywords-Applied Data Science; Data Science; Data Science \\nPersona; Formal Data Science \\n1. \\n INTRODUCTION \\nData and Science can be considered integrated per se, since \\nscience depends on data in order to present its results. Data and \\nScience words favor the presentation of a term that is currently \\nglobally recognized as Data Science. Data Science has \\ntransformed scientific, academic, and business environments in \\nmany aspects; by its structuring phase, by its interdisciplinarity, \\nby the necessity of combining new technologies and massive \\ndata, and by the necessity of specialized professionals. \\nMany concepts referring to Data Science is proposed by \\n[1], [2] and [3]. For [4] Data Science might therefore imply a \\nfocus involving data and, by extension, statistics, or the \\nsystematic study of the organization, properties, and analysis of \\ndata and its role in inference, including our confidence in the \\n \\nData Science can also be considered as a new formal \\nknowledge area [2] that is in its initial phase of \\nconceptualization \\nand \\nfoundation, \\nstrongly \\nintegrating \\nStatistics and Computer Science [5], also considering Artificial \\nIntelligence, Machine Learning and Business Science. It is also \\npossible to highlight Data Science as an applied discipline [6] \\nwhich addresses the management and analysis of data in its \\nvarious phases, whether for projects in the scientific or \\nbusiness domains. \\nData Science is a new field, focused on the process and \\nsystems that enable the organization to extract knowledge or \\ninsight from data and translate it into action [7], which \\nemphasizes Data Science as an applied area capable of \\ngenerating results in organizations of different industries, \\ncategories or size. \\nFormal or applied, Data Science has to be well structured, \\nboth in the aspect of managing data (attribution of the Data \\nScience Professional), as well as in the aspect of decision \\nmaking based on achieved results (attribution of the decision \\nmakers). \\nMany questions are posed as more people seek Data \\nScience knowledge: What are the foundations of Data Science? \\nWhat are the activities and methods used in the Data Science \\narea? And what are the activities of a Data Science \\nProfessional? What are the tools and methods that enable Data \\nScience feasible? In this sense, students of undergraduate and \\ngraduate programs; professionals who are in the industry and \\nwho work with Data Intelligence; academics and scientists, \\nwho are involved in data analysis; business executives from \\ndifferent sectors and levels; they seek answers regarding Data \\nScience, about its concepts, the interdisciplinarity that involves \\nData Science, the technologies and techniques, the tools and \\nmethods that are capable of making Data Science practicable. \\nIn addition, two relevant questions can be highlighted: 1. \\nHow science  impacts formal Data Science? and, 2. How Data \\nScience is applied in Scientific and Business Projects? These \\nquestions permeate this research, which using the exploratory \\nmethod, address the \\nformal and \\napplied Data Science, highlighting \\nscientific method for Data Science; either as a formal \\nknowledge area, which is based on formal sciences \\n(Mathematics, Statistics and Computer Theory), and as an \\napplied area, to support decision making, based on Business \\nScience, Data Management and (Big) Data Mining. \\nScience implies knowledge gained through systematically \\nstudy [4]. Science is strictly linked to scientific method; \\nscience is built from scientific research and through scientific \\nmethod and with scientific resources. Science takes place \\nthrough the scientific method that can be associated with \\nseveral areas that are guided by the scientific domain, whether \\nin formal, natural, social or applied sciences. \\nConsidering that science also implies aspects of curiosity \\nand systematization, specific to a scientific researcher or a \\nscientist, [8] considers that a Data Scientist, in addition to \\ntechnical expertise, must be curious to distill problems into a \\nclear set of hypotheses that can be testable and [9] states that \\nData Scientist is the sexiest job of the 21st century.  \\nBased on this introduction that move toward Data Science, \\nScientific Method and Data Science projects in Scientific and \\nBusiness domain, this article is structured as follows: section \\ntwo discusses the integration of scientific method and Data \\nScience; section three presents the aspects of Data Science as \\nformal and applied area; section four deals with Data Science \\nProfessional, its education and its main activities; and, section \\nfive presents the final thoughts. \\n2. DATA SCIENCE AND THE SCIENTIFIC METHOD \\nThis section introduces some applicable issues on the \\nscientific method in order to observe that this method can be \\nconsidered as part of Data Science activities, as an area that \\nneeds to accomplish its activities in an organized and \\nsystematic way. \\nContemporary science considers specific procedures that \\nare connected to the scientific method. The method assists the \\nresearche\\nin order to determine the path and \\nactivities to be followed to generate scientific results that, in \\nthe end, can be validated or refuted. \\nThis section does not address the issues regarding the \\nscientific method and how it is used and applied nowadays. \\nThe main objective is simply the approach that a method exists \\nto guide scientific activities and, consequently, the activities of \\na scientist, emphasizing that it can also be applied by a Data \\nScience Professional. \\nScience implies organization and systematization to \\ngenerate knowledge [4], thus, this statement implies a strong \\nrelationship with Data Science; to begin with because Data \\nScience supports knowledge generation from data; and, in \\naddition, because a systematic method can be applied in Data \\nScience activities. \\nAccordingly [10], the scientific method is built around \\ntestable hypotheses. Models are tested and experiments \\nconfirm or falsify them. This statement summarizes how \\nscience has been treated for years. For [11] the scientific \\nmethod attempts to remove from the subjective domain using \\nhypotheses to be tested to validate models and eventually \\nimproving knowledge. These statements generate significant \\nconvergences to Data Science activities, even if the scientific \\nmethod is used partially or, in a customized way, to manage \\nData Science activities. \\nDodig-Crnkovic [12] presents a logical scheme used to \\nproduce scientific theories. These are the steps that compose \\nthe scientific method: 1. Pose the question in the context of \\nexisting knowledge; 2. Formulate a hypothesis; 3. Deduce \\nconsequences and make predictions; 4. Test the hypothesis; \\nand 5. Provide a set of propositions to define a new \\nphenomenon or a new theoretical concept. Considering this \\nmethod, it is possible to verify that the first four steps provide \\nimportant subsidies for Data Science, as well as for Data \\nScience professionals, who can base their activities according \\nto them. \\nThe scientific method is associated with the determination \\nof new theories or phenomena, it is carried out to offer results \\naccording to data analysis, since science requires data to \\ndetermine its results. In the same way, it occurs for Data \\nScience, which in the scope of business, is not bound to \\ngenerate new theories or phenomena but, from data \\nintelligence, \\nsupport \\ndecision-making. \\nThus, \\ndata \\nare \\nfundamental in the scientific domain for the foundation of new \\ntheories and, in the business domain, to support decision \\nmaking; strengthening the approach of using the scientific \\nmethod for Data Science.  \\nBoyd and Crawford [11] states that researchers interpret \\ndata and have linked the domain of research and the scientific \\nmethod to Big Data, given that massive and varied datasets can \\nprovide better results from analyzes carried out by researchers \\nfrom various scientific fields, be it natural sciences, social \\nsciences, or logic and mathematics.  \\nFollowing the analysis of the relationship between science, \\nthe scientific method and massive data, [10] considers that the \\nconceptual approach to science that deals with - hypothesize, \\nmodel and test - (Hypothetical-Deductive Scientific Method) \\nhas become obsolete, considering that statistical algorithms are \\nable to find patterns in massive datasets where science could \\nnot find them. \\nVan der Aalst [13] also considers that new researchers and \\nscientists have primarily used data analysis and interpretation \\nand not models, possibly due to the amount of data available \\nand the ease use of data analysis tools.  \\nAgarwal and Dhar [14] point out that whereas the scientific \\nprocess corresponds to a cycle of hypothesis generation, \\nexperimentation, hypothesis testing and inference, with several \\nstarting points, massive data (or Big Data) can be useful both in \\ngeneration and in hypothesis testing.  \\nBrodie [15] considers that the scientific method favors data \\nanalysis when it comes to a small number of variables \\nassociated with the research domain or the phenomenon to be \\nevaluated. However, for Data Science, in some problems, an \\nunlimited number of correlations between a large set of \\nvariables associated with the problem can be considered, thus \\ncarrying out the analysis due to the high computational \\ncapacity and the efficiency of the algorithms, not necessarily \\nby the method. \\nEDISON Data Science Framework (EDSF) [16] also deals \\nwith the application of Scientific Method to Data Science. \\nEDSF presents in the Data Science Competence Framework \\n(CF-DS) [17] a Competence Group called Research Method \\nand Project Management  which emphasizes as main objective \\nding and capabilities by using the \\nscientific method (hypothesis, test/artefact, evaluation) or \\nsimilar engineering methods to discover new approaches to \\ncreate new knowledge and achieve research or organizational \\n This Competence Group determines the use of the \\nscientific method or similar engineering methods for Data \\nScience, emphasizing the use of specific methods as part of \\nData Science activities. \\nIn this sense, the questions about the application of \\nscientific method to Data Science activities are relevant, they \\nare also sometimes conflicting. [2] mentions an agenda for \\nData Science, considering that governments, industries, \\nresearch and education institutions seek to promote Data \\nScience as a new field of science. \\n3. DATA SCIENCE  FORMAL AND APPLIED \\nData Science should become a formal area of knowledge as \\npart of formal sciences such as Mathematics, Statistics and \\nComputer Science and, on the other hand, Data Science tends \\nalso to present itself as an applied area, capable of generating \\nvalue from the analysis of massive data through powerful \\ncomputational algorithms. \\nIn this way, this section mentions initial foundations \\nregarding Data Science as formal science, presenting some \\nspecific issues that denote the structuring of the Data Science \\narea. The section also discusses Data Science as an applied area \\nthat is useful for data intelligence in institutions, companies or \\ncorporations of any domain, category or size. \\nConsidering Data Science as a formal area of knowledge, \\nfor [15] Data Science is in its infancy, [2] also mention that \\nData Science is in its initial phase, and [18] considers that Data \\nScience is in an embryonic stage. For [5], Data Science is the \\nchild of Statistics and Computer Science, as well as for [19]. \\n[20] adds that in its embryonic phase, Data Science must also \\nconsider the aspects of Business Sciences. These statements \\npresent some convergence and they also allow us to ponder \\nthat, in some way, that Data Science is not yet a structured and \\nsedimented formal science.  \\nCleveland [19] presents a plan for technical work in the \\nfield of statistics, and mentions that given the substantial \\nchange in this field, including multidisciplinary research, this \\nwould imply a new field, called Data Science. [19] also \\npresents a set of actions as a way to implement this plan, each \\nof these actions with percentages of impact to the expansion \\nplan of the technical areas of the statistics field evolving to \\nwhat can be called Data Science, emphasizing the creation and \\nthe structure of a new formal area of knowledge nominated \\nData Science.  \\nCao [2] presents an evolutionary view of Data Science as \\nan area of knowledge, considering a natural evolution of Data \\nScience from Statistics and addressing some of the main (key) \\nterms in Data Science, such as: Advanced Analytics, Data \\nAnalytics, Predictive Analytics, Deep Analytics. [2] adds that \\nthe evolution from Data Analysis to Data Science begins with \\nthe community of mathematicians and statisticians.  \\nVan der Aalst [13] mentions three main ingredients for \\nData Science: 1) Infrastructure (Big Data, Distributed Systems, \\nprogramming, etc.), 2) Analysis (Statistics, Machine Learning, \\netc.), and 3) Effect (business models, operations, etc.). The \\nAnalysis ingredient determines the formal part of Data Science \\nand, in addition to these ingredients, [13] presents a vision of \\nData Science applied in several domains (medicine, logistics, \\nsocial sciences, etc.) emphasizing Data Science as part of \\napplied sciences.  \\nIn this sense, Data Science as an applied area must be able \\nto generate value and results, whether in the scientific domain \\nor business domain, and implies an organization of human and \\nnon-human resources, such as: methods, processes, tools, \\ntechnologies and concepts from Statistics, Computer Science \\nand Machine Learning as part of AI. \\nData Science implies the capacity that the decision maker \\nhas on the data, thus requiring the science of data intelligence \\nacquired based on data value chain that can be considered, but \\nis not limited to these phases: Data Collection, Data \\nPreparation, Data Analysis and Data Visualization. This \\ngeneral value chain for data lifecycle has favored the proposal \\nof different methods, workflows, or pipelines to be used as part \\nof Data Science activities. \\nBrodie [15] highlights the proposal of a specific method for \\nData Science activities - \\nFor \\n[15], the method was created based on the scientific method, as \\nthis is currently the method capable of supporting Data Science \\nactivities. The method considers eight steps that make Data \\nScience activities feasible, highlighting steps that deal with the \\nformulation of the problem, the formulation of hypotheses, the \\nvalidation of the model until the conclusion and the validation \\nof the results. \\nAt a high conceptual level, it is possible to verify a \\ncorrespondence considering the scientific method presented by \\n[12] and the Generic Data Science Method presented by [15], \\nwhich, in a formal perspective, links the concept of science, \\nscientific method and Data Science.  \\nBlei and Smyth [5] also proposes a set of activities \\nfavorable for managing Data Science activities. A possible \\npipeline can be derived from these activities: 1) understanding \\na problem domain; 2) deciding which data to acquire and how \\nto process it; 3) exploring and visualizing the data; 4) selecting \\nappropriate statistical models and computational methods, and \\n5) communicating the results of the analysis. \\nMatsudaira [21] emphasizes that in order to generate \\nresults, Data Science, as an applied area, needs a clear process \\nto deliver better. [21] also mentions that a group of Data \\nScientists from a Data Science area proposes ideas, investigates \\nhunches, and tests hypotheses, and that these are complex \\nactivities that imply difficulties in estimating the work to be \\nperformed and guaranteeing the results. [21] adds that the steps \\nof an established process for Data Science activities cannot be \\nbased on existing process models, such as those applied to \\nSoftware Engineering, such as the Agile Methods, like Scrum, \\nbecause the research activities associated with Data Science do \\nnot correspond to the activities proposed by these recent \\nmethods. This is yet another example that points to the need for \\na real closer approximation \\nnce  concept, scientific \\nmethod and Data Science, since Data Science is able to \\ngenerate value, in the scientific and business domain projects. \\nIn closing, further studies and in-depth research are \\nrequired to form, through science, the concepts and foundations \\nof Data Science. Based on this, theories, concepts, and methods \\ncould offer better conditions to applied Data Science. Data \\nScience, as a formal area, has its initial concepts and theories \\ndetermined by Statistics and Computer Theory, however its \\nstrong interdisciplinarity also integrates other disciplines, such \\nas Artificial Intelligence, with more emphasis on Machine \\nLearning methods and models, Business Science, Data Mining, \\namong others. \\n4. DATA SCIENCE PERSONA \\nFor Oberski [22] Data Science is mostly about humans. The \\nData Science professional, commonly called Data Scientist, is \\nthe professional who holds the knowledge regarding Data \\nScience activities, methods, and technologies to be applied in \\nprojects that are carried out in scientific or business domains, in \\norder to generate value and results that support the \\norganization s intelligence actions. \\nA diversity of professional profiles for Data Science can be \\nverified in the Data Science Profession Profile (DSPP) [23], a \\ncomponent of EDSF framework [16], but possibly the most \\ncommon is Data Scientist. Whether in the managerial, \\ntechnical, or specialist areas, there are specific professional \\nprofiles for Data Science, such as: Data Science Manager, Data \\nStewards, Large Scale Database Designers, Big Data Facilities \\nOperator, among others.  \\nSmith [1] consider that Data Science include \\nthe capture of data, their analysis, metadata, fast retrieval, \\narchiving, exchange, mining to find unexpected knowledge and \\ndata relationships, visualization in two- and three-dimensions \\nincluding movement, an\\n. This specific \\ndefinition presents a set of activities that can be considered for \\na Data Science Professional. For the specific concept of Data \\nScientist, [24] considers that it is an emerging job title to \\npeople who are able to tackle big data problems. [25] considers \\nData Scientist as a practitioner with business domain expertise \\nas also analytical skills, and programming and systems \\nexpertise to manage scientific methods in the big data lifecycle \\nto deliver value to science or industry. \\nMany questions are posed about Data Science professionals \\n- What is their ideal education? What degree of knowledge is \\nrequired to be a Data Scientist? What are the abilities and \\nresponsibilities of a Data Scientist? How to train people for this \\nprofession? What scientific domain should they have? These \\nquestions have gradually been answered by the academic \\ncommunity or by the industry, and have enabled the generation \\nof professionals to handle and analyze data. \\nMany efforts can be noted to meet the goal of Data Science \\neducation, whether by the industry itself, by governments, by \\nacademia, by associations and educational institutions. [2], [7], \\n[22], [26], [27] collaborate for a better understanding of the \\nefforts to address issues related to Data Science and education. \\nSong and Zhu [24] and Brunner and Kim [28] are more \\nspecifically concerned with issues related to how to teach Data \\nScience; [26] concerns with the formative structuring of \\nknowledge and profiles for Data Science professionals \\npresenting the EDISON Data Science Framework (EDSF) [16] \\nthrough the EDISON Project [29].  \\nAs [24] in the US, Data Science programs are available in \\nfour categories: Bachelor, Master, Specialization, and \\nCertificate Programs. For t\\n Programs are \\nat the beginning and most well-known universities that were \\ninvestigated are often programs at the graduate level. \\nCao [2] is emphatic in proposing the relationship between \\ndata education and data innovation and econom\\ndat\\n- represents the \\nrequirements for industry and governments to recognize the \\nvalue of data for decision making. [2] presents some possible \\nactivities for a Data Scientist: 1. Learn the business problem \\ndomain, 2. Understand data complexity, 3. Set up analytical \\nprocesses, 4. Transform business problems into analytical \\ntasks, 5. Mine relevant data, 6. Write coherent reports and \\npresentations  among others.  \\nPatil [8], emphasizes the possible nomenclatures adopted \\nfor Data Science professi\\nnalyst , \\n \\nwhich has been a nomenclature used by some companies, but \\nconsiders that the ideal term found for these professionals is \\nData Sci\\ny use data and science to create \\nsomething new. [8] proposes some characteristics for the \\nprofile of a Data Science Professional: 1. Technical expertise - \\ndeep expertise in some specific discipline, 2. Curiosity - distill \\na problem into a very clear set of hypotheses that can be tested, \\n3. Storytelling - using data to tell a story, 4. Cleverness - look \\nat a problem in creative ways. \\nIn order to provide the results based on data and science, \\nspecific competences proposed according to the EDSF [16] can \\nbe verified, specifically according to the Data Science \\nCompetence Framework (CF-DS) [17], which refers to the fact \\nthat Data Scientist must use research methods and principles in \\ndeveloping data driven applications and implementing the \\nwhole lifecycle of data handling. \\n5. CONCLUSIONS \\nData and Science are two words that together form the term \\nData Science, more than just two words or one term, Data \\nScience generates value through data intelligence in scientific \\nresearch institutes and business organizations from different \\ndomains, size or category. \\nData Science can represent a connection of science , the \\nscientific method and (big) data. This connection can be \\n per se, but in fact, \\nscience  can formalize and structure what is Data Science \\ntoday or what it will be in the future. The foundations of Data \\nScience are related to Statistics and Computer theories, \\npossibly turning Data Science part of the formal sciences. \\nThe scientific method can be applied in Data Science, even \\npartially, contributing to Data Science activities in both \\ndomains, scientific and business. In the domain of scientific \\nprojects, which are generally guided by the scientific method, \\nless effort is applied in the usage of the method; however, in \\nthe business-oriented domain projects, which is more \\naccelerated and competitive, it can sometimes neglect the \\nmethod and, consequently, the data-based results. \\nAnother relevant consideration regarding the scientific \\nmethod and Data Science can be observed regarding the use of \\nscientific method that can sometimes be neglected in both \\ndomains, due to the avalanche of existing data [10], [13], that \\nprocessed by powerful algorithms can present satisfactory \\npredictive results, disadvantaging the effective use of the \\nscientific method as part of data science activities. \\nApplied Data Science serves scientific and business \\nprojects, and it also collaborates with a particularity of Data \\nScience activities in both types of projects. Given that the \\nprojects in the scientific domain have a greater connection with \\nscience and the scientific method, and business projects are \\ncarried out in the competitive business world, these differences \\nin both applied Data Science areas can generate different \\nstructures of Data Science in the future. \\nQuestions regarding Data Science are still proliferating, and \\nthe answers, sometimes, are not conclusive, possibly due to the \\nlack of formalization and foundations regarding Data Science \\nas a new formal knowledge area. The answers to the questions \\nare provided through different statements and biases, \\ndepending on the group, community or professionals that \\nprovide such answers. Statisticians, Computer Scientists, \\nBusiness Executives and Data Scientists have different \\nconcepts and insights about Data Science. It is even possible to \\nverify a relevant conceptual variation on the term Data Science \\nby itself. \\nIt is also necessary to comment on the characteristics of the \\nprofessional in the field of Data Science, who has varied \\nprofiles as verified in the EDSF [16]. Education and \\ndevelopment of these professionals also incurs variations, with \\nno formative standard for Data Science Professional. Although \\nit is possible to observe the effort of the EDISON Project [29] \\nin order to present a standard framework for Data Science \\nprofiles and knowledge. Especially as Data Science is at an \\nearly stage, graduate programs are more common for Data \\nScience education, with fewer options for undergraduate \\nprograms around the world, up to the moment [24]. MOOCs \\nand short-term programs have also been presented as \\neducational options for Data Science professionals. \\nTo conclude, studies, reflections and discussions about \\nformal or applied Data Science are necessary and urgent, but \\nthere is a feeling that its progress is slow, especially in the \\nformal area; but, on the other hand, applied Data Science, in \\nthe scientific and business domains, is fully working. \\nREFERENCES \\n[1] F. J. Smith, \\nata science as an academi\\n Data Science \\nJournal, vol. 5, pp. 163-164, 2006.  \\n[2] L. Cao, \"Data science: a comprehensive overview,\" ACM Computing \\nSurveys (CSUR), vol. 50, no. 3, pp. 1-42, 2017.  \\n[3] M. L. Brodie, Understanding Data Science: An Emerging Discipline \\nfor Data Intensive Discovery,\\nShannon Cutt (Eds.), Getting Data \\nRight: Tackling the Challenges of Big Data Volume and Variety, \\nO\\nlly Media, Sebastopol, CA, USA, June 2015.  \\n[4] V. Dhar, \"Data science and prediction,\" Communications of the ACM, \\nvol. 56, no. 12, pp. 64-73, 2013.  \\n[5] D. M. Blei, and P. Smyth, \"Science and data science,\" Proceedings of \\nthe National Academy of Sciences, 2017, pp. 8689-8692, 2017.  \\n[6] F. Provost, and T. Fawcett, Data Science for Business: What you need to \\nknow about data mining and data-analytic thinking; MA: O\\'Reilly \\nMedia, Inc., 2013.  \\n[7] F. \\nBerman, \\net \\nal., \\n\"Realizing \\nthe \\npotential \\nof \\ndata \\nscience,\" Communications of the ACM, vol. 61, no. 4, pp. 67-72, 2018.  \\n[8] D. J. Patil, Building Data Science Teams; MA: O\\'Reilly Media, Inc., \\n2011, ch 1.  \\n[9] T. Davenport, and D. J. Patil, \"Data scientist,\" Harvard Business \\nReview, vol. 90, no. 5, pp. 70-76, 2012.  \\n[10] C. Anderson, \"The end of theory: The data deluge makes the scientific \\nmethod obsolete,\" Wired Magazine, vol. 16, no. 7, 2008.  \\n[11] D. Boyd, and K. Crawford, \"Critical questions for big data: Provocations \\nfor a cultural, technological, and scholarly phenomenon,\" Information, \\nCommunication & Society, vol. 15, no.5, pp. 662-679, 2012.  \\n[12] G. \\nDodig-Crnkovic, \\n\"Scientific \\nmethods \\nin \\ncomputer \\nscience,\" Proceedings of the Conference for the Promotion of Research \\nin IT at New Universities and at University Colleges in Sweden, Skövde, \\nSweden, 2002, pp. 126-130.  \\n[13] W. M. P. van der Aalst, The Data Science Revolution, In: Unimagined \\nFutures ICT Opportunities and Challenges, Springer, Cham, 2020, pp. \\n5-19.  \\n[14] R. Ag\\nBig Data, Data Science, and Analytics: The \\nOpportunity and Challenge for IS \\nInformation Systems \\nResearch, \\nvol. \\n25, \\nno. \\n3, \\npp. \\n443-448, \\n2014. \\nhttps://doi.org/10.1287/isre.2014.0546 (Access date: 24 October 2021).  \\n[15] M. L. Brodie, What is Data Science? , Applied Data Science, Springer, \\nCham, pp. 101-130, 2019.  \\n[16] EDISON \\nData \\nScience \\nFramework \\n(EDSF). \\nhttps://edison-\\nproject.eu/edison/edison-data-science-framework-edsf/ (Access date: 24 \\nOctober 2021).  \\n[17] Data Science Competence Framework. https://edison-project.eu/data-\\nscience-competence-framework-cf-ds/ (Access date: 24 October 2021).  \\n[18] L. Liu, \\nBuilding a community of data scientists: an explorative \\nanalysis  Data Science Journal, vol. 8, pp. 201-208, 2009.  \\n[19] W. S. Cleveland, \"Data science: an action plan for expanding the \\ntechnical areas of the field of statistics,\" International Statistical Review, \\nvol. 69, no.1, pp. 21-26, 2001.  \\n[20] F. Provost, and T. Fawcett, \"Data science and its relationship to big data \\nand data-driven decision making,\" Big Data, vol. 1, no. 1, pp. 51-59, \\n2013.  \\n[21] K. \\nMatsudaira, \\n\"The \\nscience \\nof \\nmanaging \\ndata \\nscience,\" Communications of the ACM, vol. 58, no. 6, pp. 44-47, 2015.  \\n[22] D. L. Oberski, \"Human Data Science,\" Patterns, vol. 1, no. 4, pp. 1-4, \\n2020.  \\n[23] Data Science Professional Profiles. https://edison-project.eu/data-\\nscience-professional-profiles-definition-dsp/ (Access date: 24 October \\n2021).  \\n[24] I. Song, and Y. Zhu, \"Big data and data science: what should we \\nteach?,\" Expert Systems, vol. 33, no. 4, pp. 364-373, 2016.  \\n[25] NIST \\nBig \\nData \\nWorking \\nGroup \\n(NBD-WG). \\nhttp://bigdatawg.nist.gov/home.php (Access date: 24 October 2021).  \\n[26] Y. Demchenko, et al., \"EDISON data science framework: a foundation \\nfor building data science profession for research and industry,\" 2016 \\nIEEE International Conference on Cloud Computing Technology and \\nScience (CloudCom), 2016, pp. 620-626.  \\n[27] A. Manieri, et al., \"Data Science Professional uncovered: How the \\nEDISON Project will contribute to a widely accepted profile for Data \\nScientists.\" 2015 IEEE 7th International Conference on Cloud \\nComputing Technology and Science (CloudCom), 2015, pp. 588-593.  \\n[28] R. J. Brunner, and E. J. Kim, \"Teaching data science,\" Procedia \\nComputer Science, vol. 80, pp. 1947-1956, 2016.  \\n[29] EDISON Project. https://edison-project.eu/ (Access date: 24 October \\n2021).  \\n',\n",
       " '2201.05867v1.pdf': 'Transferability in Deep Learning: A Survey\\nTransferability in Deep Learning: A Survey\\nJunguang Jiang\\njiangjunguang1123@outlook.com\\nSchool of Software, BNRist, Tsinghua University\\nBeijing 100084, China\\nYang Shu ∗\\nshu-y18@mails.tsinghua.edu.cn\\nSchool of Software, BNRist, Tsinghua University\\nBeijing 100084, China\\nJianmin Wang\\njimwang@tsinghua.edu.cn\\nSchool of Software, BNRist, Tsinghua University\\nBeijing 100084, China\\nMingsheng Long †\\nmingsheng@tsinghua.edu.cn\\nSchool of Software, BNRist, Tsinghua University\\nBeijing 100084, China\\nEditor: Leslie Pack Kaelbling\\nAbstract\\nThe success of deep learning algorithms generally depends on large-scale data, while humans\\nappear to have inherent ability of knowledge transfer, by recognizing and applying relevant\\nknowledge from previous learning experiences when encountering and solving unseen tasks.\\nSuch an ability to acquire and reuse knowledge is known as transferability in deep learning.\\nIt has formed the long-term quest towards making deep learning as data-eﬃcient as human\\nlearning, and has been motivating fruitful design of more powerful deep learning algorithms.\\nWe present this survey to connect diﬀerent isolated areas in deep learning with their relation\\nto transferability, and to provide a uniﬁed and complete view to investigating transferability\\nthrough the whole lifecycle of deep learning. The survey elaborates the fundamental goals\\nand challenges in parallel with the core principles and methods, covering recent cornerstones\\nin deep architectures, pre-training, task adaptation and domain adaptation. This highlights\\nunanswered questions on the appropriate objectives for learning transferable knowledge and\\nfor adapting the knowledge to new tasks and domains, avoiding catastrophic forgetting and\\nnegative transfer. Finally, we implement a benchmark and an open-source library, enabling\\na fair evaluation of deep learning methods in terms of transferability.\\nKeywords:\\nDeep learning, transferability, pre-training, adaptation, library, benchmark\\n∗. Equal contribution\\n†. Correspondence to: Mingsheng Long <mingsheng@tsinghua.edu.cn>.\\n©2000 Marina Meil˘a and Michael I. Jordan.\\narXiv:2201.05867v1  [cs.LG]  15 Jan 2022\\nJiang et al.\\nContents\\n1\\nIntroduction\\n3\\n1.1\\nTerminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n1.2\\nOverview\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2\\nPre-Training\\n7\\n2.1\\nPre-Training Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.2\\nSupervised Pre-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2.2.1\\nMeta-Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2.2.2\\nCausal Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n2.3\\nUnsupervised Pre-Training\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3.1\\nGenerative Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3.2\\nContrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n2.4\\nRemarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3\\nAdaptation\\n21\\n3.1\\nTask Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.1.1\\nCatastrophic Forgetting . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n3.1.2\\nNegative Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n24\\n3.1.3\\nParameter Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.1.4\\nData Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.1.5\\nRemarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.2\\nDomain Adaptation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n3.2.1\\nStatistics Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n34\\n3.2.2\\nDomain Adversarial Learning . . . . . . . . . . . . . . . . . . . . . .\\n36\\n3.2.3\\nHypothesis Adversarial Learning . . . . . . . . . . . . . . . . . . . .\\n39\\n3.2.4\\nDomain Translation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n3.2.5\\nSemi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n3.2.6\\nRemarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n4\\nEvaluation\\n45\\n4.1\\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n4.2\\nLibrary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n4.3\\nBenchmark\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n4.3.1\\nPre-Training\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n4.3.2\\nTask Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n4.3.3\\nDomain Adaptation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n5\\nConclusion\\n50\\n2\\nTransferability in Deep Learning: A Survey\\n1. Introduction\\nDeep learning (LeCun et al., 2015) is a class of machine learning algorithms that utilize\\nmultiple processing layers to learn representations of data with multiple levels of abstraction.\\nThese multiple processing layers, also called deep neural networks (DNNs), are empowered\\nwith the ability to discover diﬀerent explanatory factors of variation behind the intricate\\nstructured data (Bengio et al., 2013). With essential advances in network architectures,\\ntraining strategies and computation devices, deep learning has made breakthroughs or even\\nrevolutions in various areas, such as computer vision (Krizhevsky et al., 2012; He et al.,\\n2016), natural language processing (Radford et al., 2018), speech processing (Amodei et al.,\\n2016), computational biology (Senior et al., 2020), games (Silver et al., 2016; Vinyals et al.,\\n2019) and so forth. Despite its great success in these important areas, deep learning is still\\nfaced with the grand challenge of data eﬃciency. Most mainstream deep learning methods\\nrequire big datasets in the order of millions or even trillions to achieve good performance,\\nyet collecting and annotating such huge amount of data for each new task or domain are\\nexpensive and even prohibitive. This data eﬃciency challenge heavily impedes the adoption\\nof deep learning to a wider spectrum of application scenarios.\\nAn eﬀective solution to this challenge is to explore the transferability in deep learning.\\nTransferability is a foundational ability of human learning: human beings can gain relevant\\nknowledge from other related problems and apply it to handle new problems with extremely\\nfew samples (Thrun and Pratt, 1998). In deep learning, transferability refers to the ability\\nof deep neural networks to extract transferable representations from some source tasks and\\nthen adapt the gained representations to improve learning in related target tasks (Bengio,\\n2012). Recent advances in deep learning reveal that deep models trained via upstream tasks\\non large-scale data tend to yield good transferability to a variety of downstream tasks, such\\nas visual object detection (Ren et al., 2015), natural language understanding (Devlin et al.,\\n2019), to name a few. Transferability has become the central property of deep learning for\\nimproving data eﬃciency. It is on par with generalizability, interpretability, and robustness\\nfor bridging the gap between machine learning and human learning.\\nPre-Trained \\nModel\\nLabeled / \\nUnlabeled\\nPre-Training\\nUpstream Task\\nAdapted \\nModel\\nAdaptation\\nDownstream Task\\nTarget Domain\\nSource Domain\\nUpstream\\nData\\nDownstream\\nData\\nFigure 1: The two-stage lifecycle of most deep learning applications. In the ﬁrst stage, the\\ndeep model is pre-trained on an upstream task with large-scale data (labeled or unlabeled)\\nfor gaining transferable knowledge. In the second stage, the pre-trained model is adapted\\nto a downstream task in the target domain with labeled data; If the downstream task only\\nhas unlabeled data, then additional labeled data from another source domain of identical\\nlearning task but diﬀerent data distribution will be used to improve performance.\\n3\\nJiang et al.\\nTowards gaining and applying knowledge with good transferability, the lifecycle of many\\ndeep learning applications is divided into two stages: pre-training and adaptation (Figure 1).\\nThe goal of the pre-training stage is to gain the transferable knowledge. The deep models are\\npre-trained on an upstream task with large-scale data (either labeled or unlabeled) to learn\\ndisentangled representations or reusable parameters that are transferable to a variety of\\ndownstream tasks. The goal of the adaptation stage is to reuse the transferable knowledge.\\nThe pre-trained models are adapted to a downstream task in the target domain with labeled\\ndata, and the previously learned knowledge enables better generalization with fewer labeled\\nsamples. When the downstream task only has unlabeled data, additional labeled data from\\nanother source domain of identical learning task but diﬀerent data distribution will be used\\nto improve the data eﬃciency of the adapted model (Ganin and Lempitsky, 2015).\\nIt is helpful to highlight the diﬀerence underlying the transferability in the two stages.\\nThe pre-training stage focuses mainly on the generic transferability, i.e., obtaining a general\\ntransferable representation that can improve the performance of as many downstream tasks\\nas possible. In contrast, the adaptation stage pays attention to the speciﬁc transferability,\\ni.e., how to exploit the transferable knowledge in pre-trained models for a speciﬁc kind of\\ndownstream tasks, or how to improve the transferability between related domains of the\\nsame downstream task. The generic transferability is attractive since it may beneﬁt many\\ndownstream tasks without additional cost or special design. Yet it may ignore the special\\nstructures of downstream tasks that are crucial for stronger transferability, thus the speciﬁc\\ntransferability is still necessary in many cases. Recently, the gap between the pre-training\\nstage and the adaptation stage is getting closer. Several pre-training methods are designed\\nto obtain fast model adaptation ability in the adaptation stage (Finn et al., 2017), while\\nsome adaptation methods try to convert downstream tasks into pre-training tasks to make\\nfull use of the generic transferability of pre-trained models (Brown et al., 2020).\\nTransferability lies at the core of the whole lifecycle of deep learning, yet diﬀerent areas\\nsuch as domain adaptation (Zhuang et al., 2021) and continual learning (Delange et al.,\\n2021), mainly explore transferability in a partial regime of the lifecycle. This is not enough\\nto achieve a complete picture of transferability. Thereby, we present this survey to connect\\ndiﬀerent isolated areas in deep learning with their relation to transferability, and to provide\\na uniﬁed and complete view to investigate transferability through the whole lifecycle of deep\\nlearning. Due to the broadness of the scope and the limitation of the space, we do not aim\\nto cover all methods towards transferability. Instead, we elaborate on the core principles\\nand methods and then give a brief review of the expanded literature. We further implement\\nTLlib, a high-quality open library to provide a fair evaluation of typical methods. We hope\\nthis survey can highlight the grand picture of transferability in deep learning, and provide a\\nuseful navigation to researchers interested in improving the data eﬃciency of deep learning.\\n1.1 Terminology\\nForemost, we give several deﬁnitions related to transferability, and the summary of notations\\nand their descriptions used in this survey can be found in Table 1. Denote the input space\\nas X and the output space as Y, and assume that there exists an unknown labeling function\\nf : X 7→Y. Formally, a task corresponds to learning an underlying labeling function f. To\\nlearn a task, we ﬁrst collect a set of samples bD = {x1, ..., xn}, which are drawn independently\\n4\\nTransferability in Deep Learning: A Survey\\nTable 1: Notations and descriptions used in the survey.\\nX\\nInput space\\nY\\nOutput space\\nD\\nA ﬁxed but unknown distribution over X\\nbD\\nEmpirical distribution of a sample drawn i.i.d. from D\\nP(·)\\nProbability of an event\\nE(·)\\nExpectation of a random variable\\nU\\nUpstream data\\nS\\nSource domain in downstream data\\nT\\nTarget domain in downstream data\\nH\\nHypothesis space\\nh\\nA hypothesis in the hypothesis space H\\nψ\\nFeature generator\\nθ\\nHypothesis parameter\\nx\\nModel input\\ny\\nModel output\\nz\\nHidden activation of the feature generator\\nD\\nA discriminator to distinguish diﬀerent distributions\\nand identically distributed (i.i.d.) from some ﬁxed but unknown distribution D. Formally,\\na domain is a marginal probability distribution P(X) deﬁned on a certain input space X.\\nConsider a set of hypotheses H and a speciﬁc loss function ℓ: Y × Y 7→R+, the objective\\nof the learner is to select a hypothesis h ∈H that yields the lowest generalization error,\\nminh∈H Ex∼Dℓ(h(x), f(x)).\\nDeﬁnition 1 (Transferability) Given a source domain S with learning task tS and a\\ntarget domain T with learning task tT , transferability is the ability of gaining transferable\\nknowledge from tS on S and reusing the knowledge to decrease the generalization error of\\ntT on T , under the distribution shift S ̸= T or the task discrepancy tS ̸= tT .\\nIn the deep learning lifecycle (Figure 1), the pre-training stage aims to gain transferable\\nknowledge via learning on upstream task with large-scale data, while the adaptation stage\\naims to reuse the pre-trained knowledge to improve the data eﬃciency in downstream tasks.\\nThe upstream and downstream are diﬀerent in both learning tasks and data distributions.\\nTo conform with the literature, in the pre-training stage, we will replace the notions of source\\ndomain/task with the widely-used upstream data/task, denoted as U and tU respectively.\\n1.2 Overview\\nThe survey is organized around how to acquire and utilize the transferability in deep learning\\nthroughout its whole lifecycle, including pre-training, adaptation, and evaluation (Figure 2).\\n• Pre-Training. We ﬁrst brieﬂy discuss some important model architectures that make\\npre-trained representations transferable. Then we elaborate on supervised pre-training\\nand unsupervised pre-training, which are distinguished by the availability of labeled\\nor unlabeled data for pre-training. In supervised pre-training, we cover both standard\\npractices commonly used in the industry and research advances in academia to acquire\\n5\\nJiang et al.\\nLifecycle\\nArchitecture\\nPre-Training\\nSupervised\\nPre-Training\\nUnsupervised\\nPre-Training\\nStandard\\nPre-Training\\nMeta\\nLearning\\nCasual\\nLearning\\nGenerative\\nLearning\\nContrastive\\nLearning\\nAdaptation\\nTask\\nAdaptation\\nDomain\\nAdaptation\\nCatastrophic\\nForgetting\\nNegative\\nTransfer\\nParameter\\nEfficiency\\nStatistics\\nMatching\\nDomain\\nAdversarial\\nHypothesis\\nAdversarial\\nDomain\\nTranslation\\nSemi-Supervised\\nLearning\\nEvaluation\\nDatasets\\nBenchmark\\nData\\nEfficiency\\nCore\\nMethod\\nLearning\\nSetup\\nDomain\\nGeneralization\\nOOD\\nGeneralization\\nFew-shot\\nLearning\\nZero-shot\\nLearning\\nLibrary\\nPrompt\\nLearning\\nFigure 2: Overview of this survey. The survey is organized around the lifecycle (pre-training,\\nadaptation, and evaluation) of deep learning applications and focuses on the core problems\\nand methods towards transferability. Besides, we brieﬂy review related learning setups.\\ntransferability on the labeled data. In unsupervised pre-training, we cover the latest\\ndesigns of proper pre-training tasks on unlabeled data to gain transferability.\\n• Adaptation. We mainly elaborate on task adaptation and domain adaptation, which\\nare divided by whether there exists another related source domain in addition to the\\npre-trained model for boosting the downstream task performance. In task adaptation,\\nwe ﬁrst pinpoint several open problems caused by the discrepancy between upstream\\ntasks and downstream tasks, then illustrate how diﬀerent task adaptation paradigms\\n(Yosinski et al., 2014; Brown et al., 2020) close the task discrepancy to better utilize\\nthe transferability. In domain adaptation, we ﬁrst pinpoint the most inﬂuential theo-\\nries for closing the distribution shift (Ben-David et al., 2006, 2010a), then elaborate\\nhow to derive solid learning algorithms (Long et al., 2015; Ganin and Lempitsky, 2015)\\nfrom these theories to enhance the transferability of deep models across domains.\\n• Evaluation. We mainly investigate the transferability gained and reused by diﬀerent\\npre-training and adaptation methods on several large-scale datasets released recently\\nin the literature. Note that we omit some small-scale and relatively obsolete datasets\\nto make our benchmark concise and easy to report. To facilitate fair evaluation and full\\nreproduction of existing algorithms, we open source TLlib, a high-quality library along\\nwith this survey at https://github.com/thuml/Transfer-Learning-Library.\\nPre-training and adaptation lie at the core methods towards transferability. In parallel\\nwith them, there are some ﬁelds that are also closely related to the transferability in deep\\nlearning, such as domain generalization (Gulrajani and Lopez-Paz, 2021), out-of-distribution\\n(OOD) generalization (Bengio et al., 2021), few-shot learning (Chen et al., 2019a), etc.\\nRecent evaluation shows that these learning setups can largely beneﬁt from the advancement\\nin pre-training and adaptation and we will give a brief review to them in the related sections.\\n6\\nTransferability in Deep Learning: A Survey\\n2. Pre-Training\\nDespite yielding unprecedented performances on various machine learning tasks, the deep\\nlearning methods require large amounts of labeled data to generalize well. This data hungry\\nnature limits their application to a wide variety of domains and tasks, especially to scenarios\\nshort of data and annotations. Pre-training, which obtains transferable representations or\\nmodels from upstream tasks with large-scale data to boost the performance on downstream\\ntasks, is one of the most common and practical solutions to the problem of data scarcity. In\\nthis section, we will ﬁrst review some important model architectures that have a great impact\\non the transferability of pre-trained representations in Section 2.1. Then we elaborate on\\nhow to gain knowledge of improved transferability via supervised pre-training on large-scale\\nlabeled data in Section 2.2 and via unsupervised pre-training on much larger unlabeled data\\nin Section 2.3. Figure 3 overviews the recent cornerstones of pre-training methods.\\n2.1 Pre-Training Model\\nPre-training has a big interplay with the model architecture. On the one hand, pre-training\\ntechniques, such as greedy layerwise unsupervised pre-training (Bengio et al., 2007), have\\neased the training of many deep architectures. On the other hand, as neural networks evolve\\nfrom shallow to deep, they have a larger capacity to capture knowledge by pre-training from\\nlarge-scale data, which increases their transferability to downstream tasks.\\nModel architecture has a great inﬂuence on the transferability of knowledge obtained via\\npre-training. Kornblith et al. (2019) ﬁnd that the performance of the pre-trained models on\\nthe downstream tasks is highly correlated with the accuracy on the pre-training tasks, which\\nsuggests that improving the performance on the pre-training task serves as a direct way for\\nimproving transferability. The depth of the architecture, or more precisely, the capacity of\\n2015\\n2017\\n2016\\n2018\\n2019\\n2020\\n2021\\nBatchNorm\\nIRM\\nGPT\\nTransformer\\nBERT\\nPre-Training\\nModel\\nSupervised\\nPre-Training\\nUnsupervised\\nPre-Training\\nResNet\\nViT\\nBiT\\nMANN\\nMAML\\nRIM\\nGPT3\\nMAE\\nCPC\\nMoCo\\nSSP\\nInstDisc\\nSimCLR\\nDeep InfoMax\\nRoBERTA\\nT5\\nCLIP\\nLayerNorm\\nGroupNorm\\nImageNet\\nDAT\\nWSP\\nSIN\\nMeta Transfer\\nXLNet\\nXLM\\nALBERT\\nBART\\nSimSiam\\nFigure 3: Cornerstones of pre-training methods for gaining knowledge of transferability.\\n7\\nJiang et al.\\nthe model, is deemed the most critical factor to its transferability. However, training very\\ndeep neural networks have remained a grand diﬃculty for decades. He et al. (2016) observe\\na degradation of training accuracy by increasing the network depth, which implies that\\ndeeper models are more diﬃcult to optimize. Instead of ﬁtting a desired mapping h(x) by\\na few stacked layers, they proposed Residual Network (ResNet) to explicitly ﬁt a residual\\nmapping δ(x) := h(x) −x and then recast the original mapping into δ(x) + x. As a result,\\nResNet improves feature and gradient ﬂows and enables end-to-end training of hundreds of\\nand even thousands of layers, allowing the capacity of pre-trained models to scale up easily.\\nIoﬀe and Szegedy (2015) hypothesize that the optimization diﬃculty also comes from the\\ninternal covariate shift caused by layerwise transformation. To stabilize training very deep\\nmodels, they proposed Batch Normalization (BatchNorm) (Ioﬀe and Szegedy, 2015), which\\nperforms normalization for each training mini-batch within the architecture. This design is\\nextensively used by ResNet. Kolesnikov et al. (2020) ﬁnd that BatchNorm is suboptimal for\\ntransfer due to the requirement of distribution-dependent moving averaged statistics. They\\nproposed Big Transfer (BiT) to replace BatchNorm by GroupNorm (Wu and He, 2018),\\nwhich generates pre-trained models of strong performance on downstream tasks.\\nThe pre-training paradigm also reshapes the design of model architectures. In classic\\nsupervised learning, models usually have strong inductive bias such as the local connectivity\\nassumption in Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN).\\nA strong inductive bias makes pre-training of deep models more data-eﬃcient and generalize\\nbetter when training data is scarce, yet on the other hand, it also limits the expressiveness\\nand transferability of the deep models when there is large-scale data for pre-training. Thus,\\nTransformer (Vaswani et al., 2017) removes the local connectivity assumption and models\\nthe global dependencies between every two tokens. The connection weights are dynamically\\ncomputed by the self-attention mechanism and then the feature aggregation in Transformer\\ndepends on these attentions calculated from the input sequence, while the token positions in\\nthe sequence are encoded by positional embedding. Transformers are powerful for sequence\\nmodeling in natural language processing, and Vision Transformer (ViT) (Dosovitskiy et al.,\\n2021) extends them to computer vision. ViT splits an image into ﬁxed-size patches, linearly\\nembeds each of them, adds positional embeddings, and feeds the resulting sequence of vec-\\ntors to a standard Transformer encoder. In summary, Transformer makes least assumptions\\non the structural information of data, which makes Transformer an expressive architecture\\nfor storing the transferable knowledge extracted by pre-training on large amounts of training\\ndata (Devlin et al., 2019; Radford et al., 2018).\\nAllowed Hypotheses\\nAll Hypotheses\\nInductive Learning\\nInductive Transfer\\nAll Hypotheses\\nAllowed Hypotheses\\nSearch\\nSearch\\nFigure 4: Designed inductive bias (left) and learned inductive bias from pre-training (right).\\n8\\nTransferability in Deep Learning: A Survey\\nIn some sense, pre-training provides a learned inductive bias for the downstream tasks\\n(Torrey and Shavlik, 2010). Many downstream tasks only have hundreds or thousands of\\nlabeled samples, yet the pre-trained Transformers with hundreds of millions of parameters\\ncan generalize well after ﬁne-tuning on such small data. To explain this phenomenon, Agha-\\njanyan et al. (2021) empirically show that pre-training minimizes the intrinsic dimension\\n(Li et al., 2018), which measures the number of parameters required to closely approximate\\nthe optimization problem. Further, an intrinsic-dimension generalization bound is given,\\nindicating that the pre-trained parameters implicitly aﬀect the inductive bias of models\\nand a larger pre-trained model might correspond to a smaller allowed hypothesis space dur-\\ning ﬁne-tuning (see Figure 4). The success of Transformer reveals that as the amount of\\npre-training data increases, the learned inductive bias is able to outperform the manually\\ndesigned inductive bias in terms of transferability.\\n2.2 Supervised Pre-Training\\nSupervised pre-training aims to obtain models on large-scale labeled data and then transfers\\nthese models to boost downstream tasks (see Figure 5). Supervised pre-training is commonly\\nemployed in computer vision, where image classiﬁcation on ImageNet (Deng et al., 2009;\\nRussakovsky et al., 2015) is often used as the pre-training task. The pre-trained models can\\nbe transferred to downstream tasks by reusing the representations from the feature generator\\n(Sermanet et al., 2013). Donahue et al. (2014) ﬁnd that the generic visual representations\\npre-trained on ImageNet outperforms many conventional feature descriptors on various\\nobject recognition tasks. Yosinski et al. (2014) ﬁnd that transferring the pre-trained models\\nby ﬁne-tuning the whole models yields better generalization performance on new tasks.\\ngenerator\\nhead\\nlabeling\\nfunction\\n!\\n\"\\n#\\n$#\\n%!\"#\\nFigure 5: Standard supervised pre-training. The model is composed of a feature generator\\nand a task-speciﬁc head. The goal is to obtain a feature generator capturing transferable\\nknowledge from large-scale labeled data. After pre-training, the feature generator is adapted\\nto downstream tasks, while the task-speciﬁc head is usually discarded.\\nAmong the factors that inﬂuence the transferability of pre-trained models, the quantity\\nand quality of the pre-training data might be the most important. BiT (Kolesnikov et al.,\\n2020) emphasizes that training on larger datasets is vital for better transferability. Yet the\\ndata labeling is labor-exhaustive and time-consuming, which limits the possible size of the\\nannotation data. To break this limitation, Mahajan et al. (2018) explore Weakly Supervised\\nPre-training (WSP) on IG-1B-Targeted, a dataset of billions of images with social media\\nhashtags. Yalniz et al. (2019) further explore web-scale Semi-Supervised Pre-training (SSP)\\non YFCC100M, a dataset of billions of unlabeled images along with a relatively smaller set\\nof task-speciﬁc labeled data. These methods improve clearly against the counterpart trained\\n9\\nJiang et al.\\nwith only clean labeled data and achieve stronger transfer performance. On the other hand,\\nDomain Adaptive Transfer (DAT) (Ngiam et al., 2018) studies the inﬂuence of data quality\\nand ﬁnds that using more data does not necessarily lead to better transferability, especially\\nwhen the dataset is extremely large. Thus, an importance weighting strategy is proposed\\nto carefully choose the pre-training data that are most relevant to the target task. Cui et al.\\n(2018) also ﬁnd that pre-training on more similar upstream data improves transferability to\\nﬁne-grained downstream tasks. They propose to estimate domain similarity via the Earth\\nMover’s Distance to choose proper pre-training data. Geirhos et al. (2019) ﬁnd that models\\ntrained supervisedly on ImageNet are biased towards textures in images, and propose to\\npre-train with a Stylized ImageNet (SIN), which ﬁxes the texture bias and encourages the\\nmodels to learn shape-based representations of better transferability.\\nWhile standard supervised pre-training is powerful when there are enough labeled data,\\nit still has drawbacks that may limit the transferability of the model. For instance, standard\\nsupervised pre-trained models are vulnerable to adversarial examples (Goodfellow et al.,\\n2015), and Salman et al. (2020) enhance the adversarial robustness of the pre-trained models\\nto achieve better transferability. In addition, there are alternative pre-training methods for\\nimproving the transferability of deep models. Section 2.2.1 will elaborate on meta-learning,\\nwhich aims to obtain pre-trained models that adapt to downstream tasks with less training\\ntime and less training data. Section 2.2.2 will review causal learning, which aims to obtain\\ndistributionally robust and generalizable pre-trained models.\\n2.2.1 Meta-Learning\\nStandard supervised pre-training gains transferable representations to boost the learning of\\nnew tasks. However, it still requires to ﬁne-tune the pre-trained models with hundreds or\\nthousands of labeled data and with many gradient updates when adapting to the new task.\\nIn contrast, people have the ability to quickly adapt to diﬀerent related new tasks with few\\nlabeled data. Meta-learning, also known as learning to learn (Schmidhuber, 1987), aims to\\npursue such kind of eﬃcient transferability in the pre-training stage.\\nThe core idea of meta-learning is to equip the model with some meta knowledge φ that\\ncaptures intrinsic properties of diﬀerent learning tasks, which is called meta-training. When\\nfacing a new task, the learned meta knowledge could help the target model θ adapt to the\\ntask faster, which is called meta-testing. Meta-learning is based on a simple machine learning\\nprinciple that test and training conditions should be matched. As shown in Figure 6(a),\\nto simulate the fast adaptation condition during meta-testing, the meta-training data is\\nconstructed into a collection of n learning tasks, and each task i ∈[n] contains a training\\nset Dtr\\ni for adaptation to this task and a test set Dts\\ni for evaluation1. As shown in Figure 6(b),\\nthe learning objective of meta-training is a bi-level optimization problem,\\nφ∗= arg max\\nφ\\nn\\nX\\ni=1\\nlog P(θi(φ)|Dts\\ni ),\\nwhere θi(φ) = arg max\\nθ\\nlog P(θ|Dtr\\ni , φ).\\n(1)\\nHere the inner level optimization updates the model θ with the training set Dtr\\ni using meta\\nknowledge φ, and the outer level optimization evaluates the updated model with the test\\n1. Dts is a surrogate test set used during meta-training to simulate diﬀerent tasks and improve the model.\\nIt is diﬀerent from the true test set in the general setting in machine learning.\\n10\\nTransferability in Deep Learning: A Survey\\ntraining set\\ntest set\\nmeta-\\ntraining\\n…\\n…\\nmeta-\\ntest\\n…\\n…\\n(a) Learning Setup\\ntest set\\ntraining set\\nmeta knowledge\\n&$\\n%&\\n&$\\n%!\\n\\'\\n(\\n!%&\\n#%&\\n%\\'(()&\\n$#%&\\n#%!\\n!%!\\n%*\"%)&\\n$#%!\\n(b) Architecture\\nFigure 6: Learning setup and architecture for meta-learning. (a) Meta-learning consists of\\ntwo phases, meta-training and meta-testing. Meta-training gains meta knowledge φ from\\ntraining tasks to help the model θ adapt quickly to a new task in meta-testing, where each\\ntask consists of a training set and a test set. (b) In the inner level optimization, the model θ\\nis updated with the training set Dtr\\ni using meta knowledge φ. In the outer level optimization,\\nthe updated model is evaluated on the test set Dts\\ni to ﬁnd better meta knowledge φ.\\nset Dts\\ni to ﬁnd better meta knowledge of stronger transferability. The key to enhancing the\\ntransferability of meta-learning methods is to design a proper form of meta knowledge.\\nMemory-Based Meta-Learning considers memory mechanisms as the meta knowl-\\nedge. A controller writes knowledge extracted from training data Dtr\\ni into the memory, and\\nreads from the memory to adapt the base learner θ to make predictions on test data Dts\\ni . The\\nparameter of the controller is updated to ﬁnd transferable knowledge. Memory-Augmented\\nNeural Network (MANN) (Santoro et al., 2016) stores bound sample representation-class la-\\nbel information in the external memory, which can then be retrieved as features for making\\npredictions when a sample from the same class is presented. Meta Network (Munkhdalai\\nand Yu, 2017) designs another memory mechanism where a base learner provides informa-\\ntion about the status of the current task while the meta learner interacts with the external\\nmemory to generate parameters for the base learner to quickly learn the new task. Memory-\\nbased meta-learning methods improve transferability in various downstream tasks, such as\\nfew-shot classiﬁcation and reinforcement learning. However, they require a careful design of\\nthe black-box architecture to incorporate the memory mechanism, and it is unclearer what\\nis stored and retrieved in the memory and why it helps adapt the model.\\nOptimization-Based Meta-Learning considers a good initialization of the model\\nas the meta knowledge. The motivation of Model-Agnostic Meta-Learning (MAML) (Finn\\net al., 2017) is to explicitly seek for an initialization that is most transferable for ﬁne-tuning,\\ni.e., only a small amount of gradient steps and a few labeled data are needed for the model\\nto generalize to a new task. To learn such an initialization, for each sampled task i ∈[n],\\nthe model φ is ﬁrst updated on its training data Dtr\\ni using one gradient step of size α,\\nθi = φ −α∇φL(φ, Dtr\\ni ).\\n(2)\\nwhich mimics the situation of ﬁne-tuning the model from the starting point of φ. As meta\\nknowledge, φ should have good transferablity, such that for all tasks i ∈[n], the ﬁne-tuned\\n11\\nJiang et al.\\nparameters θi could perform well on the test set Dts\\ni ,\\nmin\\nφ\\nn\\nX\\ni=1\\nL(θi(φ), Dts\\ni ) =\\nn\\nX\\ni=1\\nL(φ −α∇φL(φ, Dtr\\ni ), Dts\\ni ).\\n(3)\\nThe meta knowledge of MAML is high-dimensional, hindering MAML from deeper models.\\nTo tackle it, Meta Transfer (Sun et al., 2019a) uses standard pre-training for initialization\\nand performs meta-training with light-weight neuron operations (e.g. scaling and shifting\\nover tasks), which reduces the training tasks needed to acquire the meta knowledge. Raghu\\net al. (2020) ﬁnd that feature reuse of the backbone is the predominant reason for eﬃcient\\nlearning on downstream tasks with MAML. They thus propose the Almost No Inner Loop\\nalgorithm, which performs inner loop updates and task adaptation only on the task-speciﬁc\\nhead layer. Another limitation of MAML is that the ﬁxed meta knowledge is globally shared\\nby all tasks. To break this, Latent Embedding Optimization (Rusu et al., 2019) performs\\ngradient-based meta-learning in a low-dimensional latent space, and learns data-dependent\\nlatent embedding as meta knowledge to generate target model parameters. Yao et al. (2019)\\nperform Hierarchically Structured Meta-Learning over hierarchical tasks based on clustering\\nstructures and learns to tailor transferable meta knowledge to diﬀerent tasks.\\nWhile meta-learning methods enable fast model adaptation across tasks, they are weak\\nin transferring to data from diﬀerent domains, and some sophisticated methods even perform\\nworse than standard pre-training baselines (Chen et al., 2019a). Thus, Omni-Training (Shu\\net al., 2021a) incorporates both standard pre-training and meta-training in a framework\\nwith a tri-ﬂow architecture to equip the pre-trained model with both domain transferability\\nacross diﬀerent distributions and task transferability for fast adaptation across related tasks.\\n2.2.2 Causal Learning\\nIt remains diﬃcult for supervised pre-training to obtain a transferable representation that\\ngeneralizes well to an out-of-distribution (OOD) domain (Bengio et al., 2021). In contrast,\\nhumans have the ability to adapt to diﬀerent domains or new environments. Causal learning\\naims to pursue such kind of extrapolated transferability in the pre-training stage.\\nThe core idea of causal learning is to equip the model with some causal mechanisms\\nthat capture independent and disentangled aspects of the complex real-world distributions.\\nWhen the distribution changes, only one or several causal mechanisms change, with others\\nremaining invariant, which could result in better out-of-distribution (OOD) generalization.\\nThe causal mechanisms are described by Structural Causal Models. As shown in Figure 7,\\ncausal mechanisms consider a set of variables as the vertices of a directed acyclic graph, and\\neach edge represents a mechanism of direct causation in that the parents directly aﬀect the\\nassignment of the child. This induces a canonical factorization of the joint distribution of\\nthese variables into the disentangled distribution of them conditioned on their parents. The\\nindependent causal mechanism principle states that given its mechanism, the conditional\\ndistribution of each variable does not inform or inﬂuence the other mechanisms (Sch¨olkopf\\net al., 2012; Peters et al., 2017). This implies that small distribution changes should only\\naﬀect the causal mechanisms along with the disentangled factorization in a sparse and local\\nway (Sch¨olkopf et al., 2021), thereby enabling transferability towards diﬀerent distributions.\\nThe key problem of causal learning is to obtain the variables governed by independent causal\\n12\\nTransferability in Deep Learning: A Survey\\nhead\\ncausal relation\\nvariable\\ngenerator\\naffected\\nvariable\\ndifferent\\nenvironments\\nhead\\ncausal mechanisms\\n!+\\n&,\\n&-\\n!.\\n$#+\\n$#.\\nFigure 7: Causal mechanisms consider a set of observations or variables as the vertices of\\na directed acyclic graph, where each edge corresponds to a mechanism of direct causation.\\nCausal learning seeks a model with variables governed by certain causal mechanisms, and if\\nthe environment or distribution changes, only part of the causal mechanisms will be aﬀected.\\nmechanisms. One way is to explicitly introduce independence with the modular models.\\nAnother common practice is to leverage the invariance assumption that causal relationships\\nremain invariant across distributions.\\nModular Model. Recurrent Independent Mechanism (RIM) (Goyal et al., 2021) takes\\na modular model composed of several modules of diﬀerent functions, where each module is a\\nrecurrent cell such as LSTM or GRU (Cho et al., 2014) and represents a causal mechanism.\\nTo obtain independence in distinct modules, RIM introduces attention between the hidden\\nstates of each module and the current inputs. For speciﬁc inputs, only the most relevant\\nmodules with larger attention are activated and updated, which forms competition between\\ndiﬀerent modules and encourages their independence. RIM is shown to capture independent\\ncausal mechanisms and generalize well over diﬀerent temporal patterns.\\nInvariant Learning. The invariance assumption indicates that the conditional proba-\\nbility of the target output given its direct cause should be invariant across all environments\\nor distributions. Invariant Causal Prediction (ICP) (Peters et al., 2016) uncovers indepen-\\ndent causal mechanisms by performing a statistical test to ﬁnd the subset of the variables\\nsatisfying the invariance assumption. Invariant Risk Minimization (IRM) (Arjovsky et al.,\\n2019) extends this idea to representation learning and learns a good representation such\\nthat the conditional probability of the target output given the representation should be\\ninvariant across training environments. Formally, given a data representation ψ : X →Z\\nand training environments Etr, the conditional probability between the representation and\\nthe output is invariant if there is a classiﬁer h : Z →Y simultaneously optimal for all the\\nenvironments. This can be formalized as the following constrained optimization problem,\\nmin\\nψ:X→Z,h:Z→Y\\nX\\ne∈Etr\\nϵe(h ◦ψ),\\nsubject to h ∈arg min\\n¯h:Z→Y\\nϵe(¯h ◦ψ), for all e ∈Etr,\\n(4)\\nwhere ϵe(h◦ψ) refers to the expected error of the predictor h◦ψ on the environment e. The\\ntransferability across environments relies on how the invariance across training environments\\nimplies invariance across all environments. Thus, the diversity of training environments is\\nimportant for gaining transferability. IRM can be extended to complex situations where the\\ncausal relations are deﬁned on some latent variables that need to be extracted from data.\\n13\\nJiang et al.\\n2.3 Unsupervised Pre-Training\\nBeing a canonical successful approach, supervised pre-training still requires a large amount\\nof labeled data which are expensive to annotate and only available in certain ﬁelds. This\\nhinders pre-training on huge-scale data and limits its transferability to particular tasks.\\nTo break this shackle, unsupervised learning (Bengio, 2012), typically in the form of self-\\nsupervised learning, is used for pre-training on very large unlabeled data to acquire generally\\ntransferable knowledge. To improve the transferability on downstream tasks, it is crucial to\\ndesign a proper self-supervised task for pre-training. According to the type of task, we can\\ndivide common unsupervised pre-training methods into generative learning and contrastive\\nlearning, which will be discussed in Sections 2.3.1 and 2.3.2 respectively.\\n2.3.1 Generative Learning\\nGenerative learning is underpinned by the idea of learning to generate data distribution\\nP(X) for unsupervised pre-training. It aims to learn the intrinsic representation in data\\nand has been commonly used for pre-training deep neural networks (Bengio et al., 2007). As\\nshown in Figure 8, we employ an encoder fθ that maps the perturbed input ˜x into a latent\\nrepresentation z = fθ(˜x) and a decoder gθ that maps the representation back to derive a\\nreconstructed version of the input bx = gθ(z). The model is then optimized by minimizing\\nthe reconstruction error Lgen(bx, x). Most generative pre-training methods are based on two\\nmodels: Autoregressive Model, which generates future inputs given only past inputs, and\\nAutoencoding Model, which generates full inputs given partial inputs.\\n \\nencoder\\ndecoder\\nperturbation\\n!\\n)!\\n\"\\n$!\\n%/)(\\nFigure 8: Generative pre-training tries to reconstruct the original input x from a perturbed\\ninput ˜x. The generative learning task shall encourage the learned representation z to capture\\nthe intrinsic and transferable explanatory factors from the data.\\nAutoregressive Model approximates the distribution of a sequence by predicting each\\nentry conditioned on its previous context, which is called Language Modeling (LM) task\\nin NLP. As shown in Figure 9, given a text sequence x1:T = [x1, x2, ..., xT ], the learning\\nobjective of LM is to maximize the conditional probability of each entry xt,\\nmax\\nθ\\nT\\nX\\nt=1\\nlog Pθ(xt|xt−k, · · · , xt−1),\\n(5)\\nwhere k is the size of the context window and θ is the parameter of the neural network.\\nGenerative Pre-Training (GPT) (Radford et al., 2018) explores unsupervised pre-training\\nof Transformer with LM on the BooksCorpus (Zhu et al., 2015) dataset with over 7000\\n14\\nTransferability in Deep Learning: A Survey\\nunpublished books. This equips the model with great transferability to various NLP tasks,\\nsuch as question answering, commonsense reasoning, and so on. The advantage of LM is\\nthat it models the context dependency while the drawback is that it only encodes contextual\\ninformation from one direction, yet contextual representations encoded in both directions\\nmay be more suitable to many downstream tasks, such as natural language inference.\\nAutoencoding Model approximates the data distribution by generating original data\\nfrom encoded representations. Vincent et al. (2008) hypothesize that a good representation\\nshould also be robust to partial corruption of the input. Thus Denoising Autoencoder (Vin-\\ncent et al., 2008) is trained to reconstruct the original input x with the corrupted input\\n˜x. Inspired from Denoising Autoencoder, BERT (Devlin et al., 2019) adopts the Masked\\nLanguage Modeling (MLM) task as a pre-training task to overcome the drawback of the\\nunidirectional LM. As shown in Figure 9, MLM ﬁrst randomly masks out some tokens\\nm(x) from the input sentences x with a special [MASK] token and then trains the models\\nto predict the masked tokens by the rest of the tokens x\\\\m(x),\\nmax\\nθ\\nX\\nx∈m(x)\\nlog Pθ(x|x\\\\m(x)).\\n(6)\\nMasked pre-training has also been used in many other areas. For instance, Masked Au-\\ntoencoders (MAE) (He et al., 2021) pre-trains vision transformers on large-scale unlabeled\\nimage datasets using the image generation task. The diﬃculty is that the signals are highly\\nredundant in images, thus it is hard for generative tasks, such as ﬁlling a few missing pixels,\\nto capture high-level knowledge from data. To tackle this issue, MAE randomly masks a\\nvery large portion of patches, forcing the model to go beyond low-level understanding and\\nreconstruct the whole image based on a small subset of visible patches, which improves\\nits transferability to semantic-level tasks. For another instance, to pre-train Graph Neu-\\nral Network (GNN) (Garcia and Bruna, 2018) for transferable representations, Attribute\\nMasking (Hu et al., 2020) conceals node or edge attributes and asks GNNs to predict those\\nattributes based on neighboring structures, which can capture the regularities of attributes\\ndistribution over diﬀerent graph structures, such as the chemistry rules in molecular graphs,\\nand improve transferability on the downstream node or edge classiﬁcation tasks.\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\n(b) MLM\\n(a) LM\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\n(c) PLM\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\n(d) Seq2Seq MLM\\n*,\\n$*-\\n*-\\n$*0\\n*0\\n$*1\\n*,\\n$*,\\n[MASK]\\n$*-\\n*0\\n$*0\\n*-\\n$*1\\n*1\\n$*0\\n*0\\n$*,\\n*,\\n$*-\\n[MASK]\\n$*0\\n*0\\n$+,\\n+,\\n$+-\\nFigure 9: Attention visibility in Transformer (Trm) for language models. (a) LM maximizes\\nthe probabilities of all words conditioned on their previous words. (b) MLM maximizes the\\nprobabilities of random masked words conditioned on all unmasked words. (c) PLM per-\\nmutes the original sequence and then performs autoregression. (d) Seq2Seq MLM encodes\\nthe input masked sequence x and then decodes the output masked tokens y sequentially.\\n15\\nJiang et al.\\nCombining Autoregressive and Autoencoding Models. In MLM, some special\\ntokens, such as [MASK], are only used in pre-training while absent in the downstream tasks,\\nleading to the mismatch between the pre-training phase and the ﬁne-tuning phase. To mit-\\nigate this discrepancy, Permuted Language Modeling (PLM) (Yang et al., 2019) randomly\\nsamples a permutation of the sequence and then performs autoregression on the permuted\\nsequence to predict the last few tokens. To explore the limits of transferability of knowledge\\ngained in diﬀerent generative pre-training methods, T5 (Raﬀel et al., 2020) uniﬁes all text-\\nbased language tasks into the text-to-text format and then adopts a Sequence-to-Sequence\\nMLM (Seq2Seq MLM), where the encoder processes a masked sequence and the decoder\\nsequentially generates the masked tokens in an autoregression manner.\\nThe design of unsupervised pre-training tasks has a great inﬂuence on the transferability\\nto the downstream tasks, thus many eﬀorts have been made to optimize the pre-training\\ntasks and exploit better training objectives.\\nRoBERTa (Liu et al., 2019b) explores the\\nunder-training issue of BERT and highlights that training with more data, longer sequences,\\nand dynamically changed masking patterns helps the model transfer better. Besides, MLM\\nrandomly masks out some independent words, which are the smallest semantic units in\\nEnglish but may not have complete semantics in other languages, such as Chinese. Thus,\\nERNIE (Baidu) (Sun et al., 2019b) introduces entity-level and phrase-level masking, where\\nmultiple words that represent the same semantic meaning are masked. This achieves good\\ntransferability on Chinese NLP tasks. To improve transferability to tasks where span selec-\\ntion is important, such as question answering and coreference resolution, SpanBERT (Joshi\\net al., 2020) masks a random variable length of span in the text and trains the span boundary\\nrepresentations to predict the entire content of the masked span. BART (Lewis et al., 2020)\\nintroduces more perturbation functions such as sentence permutation, document rotation,\\ntoken deletion, and text inﬁlling for more transferable pre-trained models.\\nThe generative pre-training on large-scale data greatly improves the transferability of\\nmodels and even enables few-shot task transfer.\\nBy scaling up the model size to 175B\\nand pre-training on the corpus over 500GB, GPT-3 (Brown et al., 2020) obtains impressive\\ntransferability. Using only task demonstrations and a few examples, GPT-3 achieves better\\nperformance than prior state-of-the-art ﬁne-tuning approaches on some tasks. The success\\nof GPT-3 comes from the fact that the web-scale corpus contains a vast amount of natu-\\nral language sentences, which potentially demonstrate diﬀerent tasks without explicit task\\nsymbols. A high-capacity language model trained on such data would perform unsupervised\\nmulti-task learning and absorb transferable knowledge to handle downstream tasks. The\\ngenerative pre-training on large-scale data also improves the transferability across domains.\\nMultilingual BERT (Pires et al., 2019) is pre-trained with MLM on Wikipedia texts from\\n104 languages and then achieves great cross-lingual transferability in the downstream tasks,\\nwhere each language can be considered as a domain. Further, XLM (Lample and Conneau,\\n2019) introduces the translation language modeling task, which extends MLM to parallel\\nbilingual sentence pairs, encouraging more transferable representations across language.\\n2.3.2 Contrastive Learning\\nContrastive learning utilizes the idea of learning to compare for unsupervised pre-training.\\nAs shown in Figure 10, two diﬀerent views, query xq and key xk, are constructed from the\\n16\\nTransferability in Deep Learning: A Survey\\noriginal data x. Encoders will map diﬀerent views into latent representations and decoders\\nwill further map the representation to the metric space. The model is learned by minimizing\\nthe distance between query and key of the same instance. We will review three typical con-\\ntrastive learning methods widely used in pre-training: Mutual Information Maximization\\n(uses the global context and the local features as diﬀerent views), Relative Position Predic-\\ntion (uses diﬀerent local components as diﬀerent views), and Instance Discrimination (uses\\ndata augmentations to generate diﬀerent views of the same instance). Diﬀerent ways of\\ngenerating and comparing diﬀerent views encourage these methods to respectively capture\\nthe global-local relation, local-local relation and global-global relation of the training data.\\n \\nquery\\nencoder\\nview 1\\nview 2\\nquery\\ndecoder\\nkey\\nencoder\\nkey\\ndecoder\\n%2*(%\\n!\\n!3\\n!4\\n\"3\\n\"4\\n,\\n-\\nFigure 10: Contrastive pre-training aims to minimize the similarity between the query q\\nand the key k that are generated from diﬀerent views of the same data input x.\\nMutual Information Maximization.\\nDeep InfoMax (Hjelm et al., 2019) aims to ac-\\nquire transferable representations from the relation between the high-level global context\\nand the low-level local features. Given input x, Deep InfoMax learns an encoder ψ to maxi-\\nmize the mutual information between its input and output of the same instance. The mutual\\ninformation can be estimated and bounded by training a discriminator to distinguish be-\\ntween their joint distribution and the product of their marginals. Using Noise-Contrastive\\nEstimation (NCE), the training objective of Deep InfoMax becomes,\\nmax\\nψ\\nEx∼U\\n\"\\nD(x, ψ(x)) −Ex′∼eU\\n\\x10\\nlog\\nX\\nx′\\neD(x′,ψ(x))\\x11#\\n,\\n(7)\\nwhere x is the input sampled from the training distribution U of upstream task, x′ is an-\\nother input sampled from eU = U, and D is the discriminator to distinguish between the joint\\ndistribution and the product of marginals. A parallel work, Contrastive Predictive Coding\\n(CPC) (Oord et al., 2019), also maximizes the mutual information between pairs of global\\nrepresentation and local representation. Given a sequence input, CPC processes it with an\\nencoder and summarizes the results into a context by an autoregression model. Then it\\nmaximizes the mutual information between the summarized context and the hidden repre-\\nsentation of the future observation in the sequence, which guides the learned representations\\nto capture information for predicting future samples.\\nMutual information maximization has been used to obtain pre-trained models on many\\ndata formats, such as Deep InfoMax on image data and CPC on sequence data. On graph\\ndata, Deep Graph Infomax (Veliˇckovi´c et al., 2019) maximizes the mutual information be-\\ntween a node’s local representations and the k-hop neighborhoods’ context representations.\\n17\\nJiang et al.\\nOn multimodal data, Contrastive Language-Image Pre-training (CLIP) (Radford et al.,\\n2021) maximizes the mutual information between the image and the corresponding text\\nin a multimodal embedding space. After training with a large-scale dataset of image-text\\npairs from the Internet, it enables the zero-shot transfer of the model to downstream tasks,\\ncompetitive with the prior task-speciﬁc supervised models.\\nRelative Position Prediction.\\nNext Sentence Prediction (NSP) (Devlin et al., 2019),\\nwhich is ﬁrst introduced in BERT, acquires transferable representations from the relation\\nbetween local parts. Speciﬁcally, NSP uses a binary classiﬁer to predict whether two sen-\\ntences are coherent from the training corpus, aiming to enhance the transferability to tasks\\nwith multiple sentences, such as question answering and natural language inference. How-\\never, subsequent work questions the necessity of NSP tasks (Yang et al., 2019; Liu et al.,\\n2019c) and Lan et al. (2020) conjecture that NSP only forces the model to learn topic pre-\\ndiction, rather than more diﬃcult coherence prediction. Since inter-sentence coherence is\\nimportant to many downstream tasks, ALBERT (Lan et al., 2020) introduces a sentence-\\norder prediction task, where two consecutive segments from the same document are taken\\nas positive examples, and the same segments with order swapped are taken as negative\\nexamples. Similar ideas are also explored in vision, where the pre-training task is to predict\\nrelative positions of two patches from an image (Doersch et al., 2015).\\nInstance Discrimination.\\nInstDisc (Wu et al., 2018) aims to learn transferable repre-\\nsentations from the relation between instances. Given n instances, an encoder ψ is trained\\nto distinguish each instance from others, i.e., minimize the distance between the query q\\nand key k+ from the same instance (also called positive samples) and maximize the distance\\nbetween that of diﬀerent instances (also called negative samples),\\nmin\\nψ −log\\nexp(q · k+/τ)\\nPK\\nj=0 exp(q · kj/τ)\\n,\\n(8)\\nwhere τ is a temperature hyper-parameter and the sum is over one positive and K negative\\nsamples. Note that the computation of the features of all samples and the non-parametric\\nsoftmax is costly especially when the number of training instances n is extremely large. To\\ntackle this issue, negative sampling is used to approximate the softmax, i.e., K < n.\\nThe discriminability of representations to contrast one instance from another instance\\nis closely related to the transferability on downstream tasks. Thus, many eﬀorts have been\\nmade to increase the number and improve the quality of keys. As shown in Figure 11, Inst-\\nDisc (Wu et al., 2018) uses a memory bank to store the latest updated representations for\\neach key, which increases the number of negative samples, yet may result in less consistent\\nrepresentations. Momentum Contrast (MoCo) (He et al., 2020) maintains a dynamic queue\\nof encoded features to enlarge the size of negative samples and encodes the keys with a\\nmomentum-updated encoder, which increases encoding consistency between diﬀerent sam-\\nples in the queue and improves the quality of keys. The way how the positive samples and\\nnegative samples are constructed is also important for transferability. Contrastive Multiview\\nCoding (CMC) (Tian et al., 2020) takes multiple views, rather than multiple augmentations,\\nof the same instance as positive samples and achieves better transferability. SimCLR (Chen\\net al., 2020) emphasizes that data augmentations play a crucial role in implicitly deﬁning\\n18\\nTransferability in Deep Learning: A Survey\\nsimilarity &\\ndissimilarity\\nencoder\\nmemory\\nbank\\nsampling\\nencoder\\nmomentum\\nencoder\\nmoving\\naverage\\nsimilarity &\\ndissimilarity\\nencoder\\nsimilarity &\\ndissimilarity\\nencoder\\nencoder\\nsimilarity\\nencoder\\npredictor\\n(a) InstDisc\\n(b) MoCo\\n(c) SimCLR\\n(d) SimSiam\\nstop gradient \\npredictor\\n!\\n!3\\n,\\n-\\n!\\n!3\\n,\\n-\\n!4\\n!\\n!3\\n,\\n-\\n!4\\n!\\n!3\\n,\\n-\\n!4\\nFigure 11: Comparison of diﬀerent contrastive learning mechanisms. (a) InstDisc samples\\nthe keys from a memory bank. (b) MoCo encodes the new keys on the ﬂy by a momentum\\nencoder and maintains a queue of keys. (c) SimCLR encodes the keys and queries in the same\\nbatch with the same encoder and adds a nonlinear predictor to improve the representation.\\n(d) SimSiam applies an MLP predictor on one side and applies a stop-gradient operation\\non the other side, and maximizes the similarity in two views without using negative pairs.\\ndiﬀerent pretext tasks, and the composition of stronger augmentations leads to better trans-\\nferability even without the need for a memory bank or a queue. The introduction of negative\\nsamples is to avoid trivial solutions that all outputs collapse to a constant. However, BYOL\\n(Grill et al., 2020) ﬁnds that when maximizing the similarity between two augmentations\\nof one image, negative sample pairs are not necessary. Further, SimSiam (Chen and He,\\n2021) ﬁnds that momentum encoder is also not necessary while a stop-gradient operation\\napplied on one side is enough for learning transferable representations.\\nCompared with supervised pre-training, contrastive pre-training leads to competitive\\nperformance on downstream classiﬁcation tasks and even better performance on various\\nother downstream tasks, such as object detection and semantic segmentation. To explain the\\nstronger transferability of contrastive pre-training, Zhao et al. (2021) observe that standard\\nsupervised pre-training usually transfers high-level semantic knowledge, while contrastive\\npre-training usually transfers low-level and mid-level representations. When the target tasks\\nare diﬀerent from the supervised pre-trained tasks, the supervised pre-training methods\\nhave the risk of over-ﬁtting the semantic discriminative parts of objects deﬁned by the class\\nlabels, which hurts the transferability. On the contrary, the contrastive pre-training tasks\\nlead to more holistic modeling of the objects, which relaxes the task misalignment issue and\\nachieves better transferability for widespread downstream tasks.\\n2.4 Remarks\\nWhile standard supervised pre-training is well established, its transferability also depends on\\nthe relationship between the pre-training task and the target task, and no pre-training task\\ncan dominate all downstream tasks. He et al. (2019) show that compared with the random\\ninitialization, supervised pre-training on ImageNet only speeds up the convergence of object\\ndetection on the COCO dataset, but does not lead to better ﬁnal accuracy. Raghu et al.\\n(2019) observe similar phenomena in medical imaging, where training lightweight models\\nfrom scratch perform comparably with transferring from ImageNet pre-trained models.\\nAbnar et al. (2022) explore the limits of large-scale supervised pre-training and ﬁnd that\\nas the pre-training accuracy increases by scaling up data, model size and training time,\\n19\\nJiang et al.\\nthe performance of downstream tasks gradually saturates and there are even some extreme\\nscenarios where performance on pre-training and downstream tasks are at odds with each\\nother. These controversial results encourage us to rethink the common practice of supervised\\npre-training and design new supervised pre-training strategies for speciﬁc ﬁelds, especially\\nwhen large gaps exist between the pre-training and target tasks.\\nTable 2: Comparison between diﬀerent pre-training methods.\\nMethod\\nModality Scalability1\\nTask Scalability2\\nData Eﬃciency3\\nLabeling Cost4\\nStandard Pre-Training\\n⋆⋆⋆\\n⋆⋆\\n⋆⋆⋆\\n⋆\\nMeta-Learning\\n⋆⋆⋆\\n⋆\\n⋆\\n⋆\\nCausal Learning\\n⋆⋆\\n⋆\\n⋆\\n⋆\\nGenerative Learning\\n⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\nContrastive Learning\\n⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n1 Modality Scalability: whether models can be pre-trained on various modalities, such as text, graph.\\n2 Task Scalability: whether pre-trained models can be easily transferred to diﬀerent downstream tasks.\\n3 Data Eﬃciency: whether stronger transferability can be yielded from large-scale pre-training.\\n4 Labeling Cost: whether relies on manual data labeling.\\nTable 2 compares pre-training methods from four perspectives: modality scalability, task\\nscalability, data eﬃciency, and labeling cost. Though meta-learning enables fast adaptation\\nto new tasks, it mainly considers related tasks such as reinforcement learning under envi-\\nronments with small changing factors, while standard pre-training can transfer to broader\\ntask gaps such as from image classiﬁcation to object detection. Besides, the existing meta-\\nlearning and causal learning methods are empirically veriﬁed only on small datasets, and it\\nremains unclear whether they can acquire stronger transferability via pre-training on large-\\nscale data. Despite the promising performance without manual labeling, unsupervised pre-\\ntrained models require a large number of gradient steps for ﬁne-tuning to downstream tasks.\\nAlso, strong data augmentations are required by contrastive learning to gain transferability,\\nbut they are not easy to design in other modalities, such as text and graphs. Finally, the\\ndesign of unsupervised pre-training tasks remains heuristic, lacking solid analysis on how\\nthe task shift is bridged and what enables the transferability of these models.\\nAcquiring transferability only through the pre-training stage may limit our horizon. As\\nthe shift in tasks and domains naturally exists between the pre-training and adaptation\\nstages, many pre-training methods are tailored to adaptation. Unsupervised pre-training\\naims to improve the transferability to downstream tasks by exploring diﬀerent kinds of self-\\nsupervised tasks to reduce the task discrepancy between pre-training and adaptation, or by\\nenlarging the size and diversity of the upstream data to reduce the upstream-downstream\\ndiscrepancy. The distribution shift commonly tackled by domain adaptation (Ganin and\\nLempitsky, 2015) also inﬂuences the transferability of the pre-trained model. For instance,\\nthe data distribution in a speciﬁc domain, such as biological and scientiﬁc literature, is quite\\ndiﬀerent from that in the general pre-training domain and may degrade transferability, thus\\nBioBert (Lee et al., 2020b) and SciBERT (Beltagy et al., 2019) perform pre-training on\\ndomain-speciﬁc data to improve the transferability on domain-speciﬁc tasks.\\n20\\nTransferability in Deep Learning: A Survey\\n3. Adaptation\\nWhile pre-training on large-scale datasets can gain transferable knowledge in deep models,\\nperforming task adaptation with the target data is still necessary for most applications, as\\nthe target task is usually diﬀerent from the pre-training task. When the labeled data for\\nthe target task is not enough, domain adaptation from a related source domain with labeled\\ndata to boost the performance on the target domain is also necessary in many applications.\\nWe will review task adaptation and domain adaptation in Sections 3.1 and 3.2 respectively.\\n3.1 Task Adaptation\\nIn task adaptation, there exist a pre-trained model hθ0 and a target domain bT = {xi, yi}m\\ni=1\\nof m labeled samples. The goal is to ﬁnd a hypothesis hθ : X 7→Y in the space H using the\\npre-trained model and target data to achieve a low generalization risk ϵT (hθ). In general,\\nthere are two simple ways to adapt a pre-trained model to the downstream tasks: feature\\ntransfer and ﬁne-tuning. Feature transfer freezes the weights of the pre-trained models and\\ntrains a linear classiﬁer on top of that. In contrast, ﬁne-tuning uses the pre-trained models\\nto initialize the target model parameters and update these parameters during training.\\nFeature transfer is fast in training and eﬃcient in parameter storage, yet ﬁne-tuning yields\\nbetter performance (Yosinski et al., 2014), and has become a common practice for task\\nadaptation in both vision and NLP (Girshick et al., 2014; Devlin et al., 2019).\\nVanilla ﬁne-tuning, which tunes the pre-trained models by empirical risk minimization\\non the target data, has been widely used in various downstream tasks and scenarios. How-\\never, vanilla ﬁne-tuning still suﬀers from several issues, including catastrophic forgetting and\\nnegative transfer. We will introduce how to alleviate these issues in Sections 3.1.1 and 3.1.2.\\nBesides, as the parameters of deep models keep increasing and some of them reach billions\\nor trillions, parameter eﬃciency and data eﬃciency have become increasingly important in\\ntask adaptation. We will give an introduction on how to explore the transferability in the\\npre-trained models to solve these problems in Sections 3.1.3 and 3.1.4. Overall, Figure 12\\nshows the progress made by the task adaptation algorithms to solve diﬀerent problems.\\n2017\\n2016\\n2018\\n2019\\n2020\\n2021\\nLogME\\nLEEP\\nDiff Pruning\\nTaskonomy\\nBSS\\nSide-Tuning\\nAdapter Tuning\\nResidual Adapter\\nCatastrophic\\nForgetting\\nNegative\\nTransfer\\nParameter\\nEfﬁciency\\nData\\nEfﬁciency\\nLWF\\nULMFiT\\nDELTA\\nCo-Tuning\\nDAPT\\nZoo-Tuning\\nGPT3\\nPreﬁx Tuning\\nInstruction-Tuning\\nEWC\\nPiggyBack\\nMatching Net\\nProtoNet\\nT5\\nFigure 12: Cornerstones of task adaptation methods for applying transferable knowledge.\\n21\\nJiang et al.\\n3.1.1 Catastrophic Forgetting\\nCatastrophic forgetting, which was ﬁrst studied in lifelong learning, refers to the tendency\\nof neural networks to lose knowledge acquired from previous tasks when learning new tasks\\n(Kirkpatrick et al., 2017). In the ﬁne-tuning scenario where labeled data is usually scarce,\\nit will lead to the overﬁtting of models on the target data. This phenomenon is also called\\nrepresentational collapse, i.e., the degradation of generalizable representations during the\\nﬁne-tuning stages (Aghajanyan et al., 2021). The most simple way to avoid catastrophic\\nforgetting might be selecting a small learning rate and adopting an early-stopping strategy,\\nwhich avoids updating the parameters too much.\\nHowever, this strategy may lead the\\nmodel to falling into the local minimal, especially when there is a large gap between the\\npre-training parameters and the optimal parameters for the downstream task.\\nYosinski et al. (2014) ﬁnd that the transferability of diﬀerent layers is not the same —\\nthe ﬁrst layers learn general features, the middle layers learn semantic features and the last\\nlayers learn task-speciﬁc features. Thus, to make the model retain the knowledge acquired in\\nthe pre-training task and ﬁt the target task well at the same time, diﬀerent layers should not\\nbe treated the same. Speciﬁcally, the ﬁrst layers should retain more pre-trained knowledge\\nwhile the last layers should adapt more to the downstream tasks. Inspired by this ﬁnding,\\nDAN (Long et al., 2015) sets the learning rate of the task-speciﬁc head to be 10 times larger\\nthan that of the lower layers, which is simple yet eﬀective when the labeled data is scarce\\nor the target domain is close with the pre-training domain. ULMFiT (Howard and Ruder,\\n2018) gradually unfreezes the model starting from the last layers to the ﬁrst layers, which\\neﬀectively retains general knowledge in the ﬁrst layers. To automatically determine which\\nlayers should be ﬁne-tuned or frozen for each sample, Spottune (Guo et al., 2019) proposes\\na policy network that is deployed to output the routing decision based on the input of each\\nsample and is jointly trained with the main model during ﬁne-tuning.\\nDomain Adaptive Tuning reveals that an important source of catastrophic forgetting\\nis the dataset shift between the pre-training and the target domain. To bridge such shift,\\nULMFiT (Howard and Ruder, 2018) and DAPT (Gururangan et al., 2020) ﬁrst tune the\\npre-trained model on data related to the target domain, or simply data of the target task,\\nwith the pre-training task. Then they ﬁne-tune the adaptive-tuned model on the target\\ntask (Figure 13). Usually, the pre-training task is unsupervised, thus further pre-training\\nwith in-domain data can provide rich information about the target data distribution for\\nbetter task adaptation with no additional labeling costs. The two stages, domain adaptive\\ntuning and regular ﬁne-tuning, in the above methods can also be done jointly via multi-\\ntask learning. SiATL (Chronopoulou et al., 2019) adds an auxiliary language model loss to\\nthe task-speciﬁc optimization function, which alleviates catastrophic forgetting and learns\\ntask-speciﬁc features at the same time.\\nRegularization Tuning is another way to prevent the models from deviating far away\\nfrom the pretrained ones. The optimization objective with a general regularization is\\nmin\\nθ\\nm\\nX\\ni=1\\nL(hθ(xi), yi) + λ · Ω(θ),\\n(9)\\nwhere L is the loss function, Ωis a general form of regularization, and λ is the trade-\\noﬀbetween them. A typical regularization in supervised learning is L2 penalty, Ω(θ) =\\n22\\nTransferability in Deep Learning: A Survey\\nInit\\nModel\\nPre-trained\\nModel\\nAdaptive\\nPre-trained\\nFinal\\nModel\\npre-train\\n \\nadaptive\\ntune\\nﬁne-tune\\n.′\\n.\\n0\\nFigure 13: Domain Adaptive Tuning often consists of two consecutive steps: ﬁrst, adaptive-\\ntune on an auxiliary domain T ′ that is related to the target domain using the pre-training\\ntask; second, ﬁne-tune on the target domain T using the target learning task.\\n1\\n2||θ||2\\n2, which drives the weights θ to zero to control the model complexity. Diﬀerent from\\ntypical supervised learning, in ﬁne-tuning, there exists a pre-trained model hθ0 setting a\\nreference that can be used to deﬁne the hypothesis space (Figure 4). Thus, Elastic Weight\\nConsolidation (EWC) (Kirkpatrick et al., 2017) constrains the distance between the weights\\nof the pre-trained and ﬁne-tuned networks (Figure 14) to overcome catastrophic forgetting,\\nΩ(θ) =\\nX\\nj\\n1\\n2Fj\\n\\r\\rθj −θ0\\nj\\n\\r\\r2\\n2 ,\\n(10)\\nwhere F is the estimated Fisher information matrix. EWC is based on the assumption\\nthat the networks with similar weights should produce similar outputs. However, due to\\nthe complex structures of deep networks, similar parameters do not necessarily produce\\nthe same output, and the same output may also come from completely diﬀerent model\\nparameters. Thus, DELTA (Li et al., 2019) constraints the behavior, i.e., the feature maps\\nof the model by selecting the discriminative features with a supervised attention mechanism\\nand regularizing the distance of these features between pre-trained and ﬁne-tuned networks.\\nLearning Without Forgetting (LWF) (Li and Hoiem, 2018) constrains the output prediction\\nof the model by encouraging the model’s response for old tasks to keep the same throughout\\nthe ﬁne-tuning process (Figure 14). Regularization on the output often performs better than\\nregularization on the parameters or the features, yet the latter two have better scalability\\nand versatility to more complex downstream tasks.\\nbackbone\\ntarget\\nhead\\npre-trained\\nhead\\nnoise\\npre-trained\\nbackbone\\ninitialize\\nL2\\nL\\n2\\n…\\n…\\npre-trained models\\nﬁne-tuned models\\n…\\nL\\n2\\n…\\n…\\npre-trained models\\nﬁne-tuned models\\n…\\nL2\\n(c) LWF\\n(a) EWC\\n(b) DELTA\\n&\\n&=\\nℎ=\\nℎ\\n#$\\n$\\n#$=\\n$=\\n%!&\\'\\n%>+!\"+??\\n)=\\n#\\n)=\\n@\\n)#\\n)@\\n!\\n)=\\n#\\n)=\\n@\\n)#\\n)@\\n\"=\\n#\\n\"=\\n@A#\\n\"=\\n#\\n\"=\\n@A#\\n!\\n,!\\n!\\nFigure 14: Regularization methods for task adaptation that avoids catastrophic forgetting.\\nBlue: pre-trained parameters; Red: ﬁne-tuned parameters. (a) EWC regularizes the param-\\neters of the new models θ and that of the pre-trained models θ0 with weighted L2-penalty.\\n(b) DELTA regularizes the feature maps of the new models z and that of the pre-trained\\nmodels z0. (c) LWF enforces the output of the old tasks by0 close to the initial response y0.\\n23\\nJiang et al.\\nAn explanation for the eﬀect of regularization is that it makes the hypothesis smoother.\\nTherefore, TRADES (Zhang et al., 2019a) and SMART (Jiang et al., 2020) directly enforce\\nthe smoothness of the hypothesis by encouraging the output of the model to not change\\nmuch when injecting a small perturbation to the input,\\nΩ(θ) =\\nm\\nX\\ni=1\\nmax\\n||exi−xi||p≤ϵ Ls(hθ(exi), hθ(xi)),\\n(11)\\nwhere ϵ > 0 is a small positive, Ls is the distance between two predictions, such as the\\nsymmetrized KL-divergence in classiﬁcation and the squared loss in regression.\\nApart from the training objective, an alternative regularization approach is through\\nthe parameter updating strategy. Stochastic Normalization (Kou et al., 2020) randomly\\nreplaces the target statistics in the batch-normalization layer (Ioﬀe and Szegedy, 2015)\\nwith their pre-trained statistics, which serves as an implicit regularization by avoiding over-\\ndepending on the target statistics. Mixout (Lee et al., 2020a) randomly replaces part of the\\nmodel parameters with their pre-trained weights during ﬁne-tuning to mitigate catastrophic\\nforgetting. Child-Tuning (Xu et al., 2021) selects a subset of parameters (child network) by\\nsome criterion and only updates them during ﬁne-tuning. In some senses, the above methods\\ndecrease the hypothesis space to preserve the transferability in pre-trained models.\\n3.1.2 Negative Transfer\\nAlthough the paradigm of pre-training and ﬁne-tuning has been used in various downstream\\ntasks, it does not necessarily produce a positive eﬀect, which is known as negative transfer\\n(Rosenstein, 2005). Wang et al. (2019d) propose to quantitatively measure the degree of\\nnegative transfer across diﬀerent domains and we extend this idea to the paradigm of pre-\\ntraining and ﬁne-tuning.\\nDeﬁnition 2 (Negative Transfer Gap) Let hθ(U, T ) be a hypothesis obtained by adapt-\\ning the model pre-trained from the upstream data U to the target data T , and hθ(∅, T ) be a\\nhypothesis obtained by training from scratch on T , then negative transfer gap is deﬁned as\\nNTG = ϵT (hθ(U, T )) −ϵT (hθ(∅, T )),\\n(12)\\nand negative transfer occurs if NTG is positive and vice versa.\\nFirst, negative transfer will happen when the relatedness between the upstream task\\nand downstream task is not strong, e.g. Next Sentence Prediction pre-training task will hurt\\ntoken-level classiﬁcation tasks (Liu et al., 2019b). Negative transfer will also happen when\\nthere is a large shift between the pre-training domain and the target domain, e.g. for legal\\ndocuments classiﬁcation, pre-training only on legal documents is better than pre-training\\non more diverse datasets (Zheng et al., 2021). Second, negative transfer depends on the size\\nof the labeled target dataset (Wang et al., 2019d). For example, He et al. (2019) empirically\\nshow that on large-scale object detection datasets (e.g. COCO), ImageNet pre-training is\\nnot beneﬁcial when training for enough iterations. Third, negative transfer depends on the\\ntask adaptation algorithms. An ideal adaptation algorithm should promote positive transfer\\nbetween related tasks while avoiding negative transfer between unrelated tasks. In practice,\\nhowever, these two goals are often contradictory and result in the dilemma: approaches that\\npromote larger positive transfer will suﬀer from severer negative transfer (Figure 15).\\n24\\nTransferability in Deep Learning: A Survey\\ntask \\nrelatedness\\naggressive\\nconservative\\nnegative\\ntransfer\\npositive\\ntransfer\\ntransfer\\nperformance\\nFigure 15: Dilemma to promote positive transfer and avoid negative transfer: Aggressive\\nstrategies that promote larger positive transfer will suﬀer from severer negative transfer;\\nConservative strategies can decrease negative transfer, yet lead to smaller positive transfer.\\nEnhancing Safe Transfer.\\nOne way to avoid negative transfer is to recognize and re-\\nject harmful knowledge in the pre-trained model. Chen et al. (2019b) observe that with\\nsuﬃcient training data, the spectral components with small singular values vanish dur-\\ning ﬁne-tuning, indicating that small singular values correspond to detrimental pre-trained\\nknowledge and may cause negative transfer. Thus BSS penalizes smaller singular values to\\nsuppress untransferable spectral components for safe transfer. Jang et al. (2019) meta-learns\\nthe weights determining which pairs of layers should be matched and to what extent the\\nknowledge should be transferred, which rejects irrelevant information during transfer. Zoo-\\ntuning (Shu et al., 2021b) enables adaptive transfer from a zoo of models, which adaptively\\naggregates multiple pre-trained models to derive the target model using data-dependent\\ngating mechanisms to highlight transferable parameters. Another way to mitigate negative\\ntransfer of the pre-trained model is to fully explore the target data. Self-Tuning (Wang et al.,\\n2021) proposes a pseudo group contrastive mechanism to explore the intrinsic structure of\\nthe target data in the process of ﬁne-tuning with standard supervised objective.\\nChoosing Pre-trained Models.\\nWith the fast development of deep learning, a large zoo\\nof pre-trained models are available, thus a simpler way to avoid negative transfer is to select a\\nmodel that is pre-trained on the upstream data/task relevant to the downstream data/task.\\nThe most common practice to choose pre-trained models is based on rich past experience or\\nthrough heavy experiments. To facilitate faster selection, Taskonomy (Zamir et al., 2018)\\nproposes a fully computational approach for explicitly modeling the relationship between\\n26 diﬀerent visual tasks. Another more eﬃcient strategy to select pre-trained models is to\\npredict the transferability of pre-trained models. LEEP (Nguyen et al., 2020) constructs\\nan empirical predictor by estimating the joint distribution over pre-trained labels and the\\ntarget labels, and then uses the log expectation of the empirical predictor to measure the\\ntransferability. LogME (You et al., 2021) proposes to predict the ﬁne-tuning performance\\nfrom the compatibility of features {zi = ψ(xi)}m\\ni=1 and labels {yi}m\\ni=1. Still, these methods\\nmay underestimate strong but non-linear features. He et al. (2021) show that features from\\ncontrastive pre-training, such as MoCo v3 (Chen et al., 2021a), have higher linear probing\\naccuracy while worse fully ﬁne-tuning results than generative pre-training, such as MAE\\n(He et al., 2021), indicating that the linear separability of the pre-trained features is not\\nthe sole metric for evaluating transferability.\\n25\\nJiang et al.\\n(a) Feature Transfer\\n(b) Fine-Tuning\\n+\\n(c) Side-Tuning\\npre-trained\\nmodules\\n+\\nadapter\\nŏ\\n(d) Adapter Tuning\\nFigure 16: Comparison on how diﬀerent adaptation methods freeze (blue) and tune (red)\\npre-trained parameters. (a) Feature transfer freezes all the pre-trained parameters. (b)\\nFine-tuning re-trains all the pre-trained parameters. (c) Side-tuning trains a lightweight\\nconditioned side network that is fused with the ﬁxed pre-trained network using summation.\\n(d) Adapter-Tuning inserts adapter modules for tuning into each frozen pre-trained layer.\\n3.1.3 Parameter Efficiency\\nFine-tuning large pre-trained models yields strong performances on many downstream tasks\\n(Radford et al., 2018; Devlin et al., 2019), yet it is not parameter eﬃcient since it generates\\na full set of model parameters for each downstream task, which will cause unacceptable\\nstorage cost as the number of tasks increases. The simplest solution is Multi-task Learning\\n(Caruana, 1997), i.e., ﬁne-tuning a single model to solve multiple target tasks, which might\\nbe mutually beneﬁcial to each other (He et al., 2017; Liu et al., 2019a). Yet when diﬀerent\\ntarget tasks are weakly related, multi-task learning will underperform ﬁne-tuning for each\\ntask separately. Also, multi-task learning requires simultaneous access to all target tasks,\\nwhich is not feasible in online scenarios where the target tasks arrive in sequence. Hereafter,\\nwe will introduce new tuning paradigms proposed to improve parameter eﬃciency.\\nResidual Tuning.\\nInspired by the fact that approximation to a diﬀerence is easier than\\nthe original function (He et al., 2016), Side-Tuning (Zhang et al., 2019b) adds a small side\\nnetwork hside to adapt the frozen pre-trained model hpretrained for the target task and obtain\\na combined model h(x) = αhpretrained(x)+(1−α)hside(x), where α is a weight that changes\\nduring training. When there is a big gap between the pre-trained model and the downstream\\ntask, it may be diﬃcult to learn the residuals of the entire model. Thus, Adapter Tuning\\n(Houlsby et al., 2019) inserts residual adapter modules into each frozen layer. Residual\\nAdapter was ﬁrst introduced for learning multiple visual domains (Rebuﬃet al., 2017) and\\nconsists of a skip connection, such that it is set as a near-identity function and will not\\nimpair the whole model when the training starts. By choosing a much smaller amount of\\nparameters for the adapters, Adapter Tuning can extend pre-trained models to new tasks\\nwithout increasing much storage cost. Houlsby et al. (2019) ﬁnd that Adapter Tuning with\\nonly 3.6% tunable parameters can match the performance of the fully ﬁne-tuned BERT on\\nthe GLUE benchmark (Wang et al., 2019a), revealing the great potential of this method.\\nParameter Diﬀerence Tuning.\\nWhile residual adapter tuning changes the model acti-\\nvations by adding new modules, parameter diﬀerence tuning extends the pre-trained models\\nthrough a task-speciﬁc parameter diﬀerence vector,\\nθtask = θpretrained ⊕δtask,\\n(13)\\n26\\nTransferability in Deep Learning: A Survey\\nwhere ⊕is the element-wise addition function, θpretrained is the ﬁxed pre-trained parameters\\nand δtask is the tuned task-speciﬁc diﬀerence vector. Instead of storing a copy of θtask for\\nevery task, diﬀerence tuning only needs to store a single copy of θpretrained and a copy of δtask\\nfor every task. As long as the size of δtask can be reduced, we can achieve parameter eﬃcient\\nmodels. To this end, DiﬀPruning (Guo et al., 2021) utilizes L0-norm penalty (Louizos\\net al., 2018) to encourage sparsity of the diﬀerence vector δtask. Aghajanyan et al. (2021)\\nadopt FastFood transform M (Li et al., 2018) to convert δtask into a low-dimensional vector\\nδlow, i.e., δtask = δlowM. The element-wise addition can also be replaced by element-wise\\nmultiplication. For instance, Piggyback (Mallya and Lazebnik, 2018) multiplies real-valued\\nmask weights to the pre-trained parameters, i.e., θtask = θpretrained ⊙δtask during training.\\nAfter training, the mask weights δtask are passed through a thresholding function to obtain\\nbinary-valued masks, further reducing the parameter storage of δtask at inference.\\nThe essential diﬀerence between the above two tuning methods lies in their diﬀerent\\nassumptions about the root of transferability. Residual tuning assumes that transferability\\nis encoded in the behaviors of each module, i.e., the features output by each module. When\\nadapting to the downstream tasks, we only need to add some task-speciﬁc behaviors by\\nstacking the pre-trained modules with the residual adapter modules. In contrast, parameter\\ndiﬀerence tuning assumes that transferability lies in the pre-trained parameters. Most of the\\npre-trained parameters can be reused, and only a small part of them need to be adapted to\\nthe downstream tasks, thus we only need to store the increment. Another thing to mention\\nis that when limiting the size of the residual adapters or the complexity of the diﬀerence\\nvector, these methods naturally overcome the catastrophic forgetting issue in Section 3.1.1.\\n3.1.4 Data Efficiency\\nCurrently, when ﬁne-tuning large pre-trained models, hundreds or even thousands of labeled\\nsamples are still required to achieve strong performance on a speciﬁc downstream task,\\nwhich limits the application of the “pre-training and ﬁne-tuning” paradigm to wider range\\nof tasks where labeled data are expensive to collect. In contrast, people can adapt to a\\nnew task with extremely few labeled samples, which is known as few-shot learning, or even\\nwith no labeled samples, which is known as zero-shot learning. Considering the lifecycle of\\ndeep learning, we can tackle this problem in three ways. The ﬁrst is to improve the cross-\\ntask transferability of the pre-trained models, such as by increasing the model capacity or\\nthe pre-training dataset size, which is mentioned in Section 2. The second is to transfer\\nfrom another labeled source domain where labeled data is cheaper to collect, which will be\\ndiscussed in Section 3.2. The last is to reformulate the target task to close its gap with the\\npre-trained models, which is the focus of this part.\\nMetric Learning.\\nFine-tuning in low data regimes will easily cause over-ﬁtting as updat-\\ning the model of large-scale parameters using few labeled samples is ill-posed. In contrast,\\nmany non-parametric methods, such as nearest neighbors, can deal with low-sample regimes\\nwithout suﬀering from catastrophic forgetting. To combine the advantages of parametric\\nand non-parametric methods, Matching Net (Vinyals et al., 2016) uses an attention mech-\\nanism over the learned representations to predict the classes for the query samples, which\\ncan be interpreted as weighted nearest neighbors. Since labeled data is severely limited,\\nProtoNet (Snell et al., 2017) adds a stronger inductive bias that there exists a single pro-\\n27\\nJiang et al.\\ntotype representation for each class, where each prototype is the mean of the features of\\nthe labeled samples in each class, and classiﬁcation boils down to ﬁnding the nearest pro-\\ntotype. Since no gradient update is performed on the feature representation, choosing a\\nproper distance metric that has good transferability across tasks plays an important role.\\nA common choice is the cosine distance, which explicitly reduces the intra-class variations\\nand improves the cross-task transferability. Chen et al. (2019a) ﬁnd that by replacing the\\nlinear classiﬁer with a cosine-distance based classiﬁer, the naive feature transfer method\\nwithout ﬁne-tuning serves as a strong baseline in few-shot learning.\\nPrompt Learning.\\nPrompting, ﬁrstly proposed in GPT-3 (Brown et al., 2020), is another\\nway to reformulate the downstream task to make it similar to the solved pre-training task.\\nIn ﬁne-tuning, models will take input x and predict an output y as P(y|x). In prompting,\\nthe original input x is modiﬁed by a prompt template into a new string ˜x that has unﬁlled\\nslots, then the pre-trained language model will ﬁll ˜x to obatin a ﬁnal string bx and derive the\\noutput y from bx (Liu et al., 2021b). Table 3 provides an example of prompting methods.\\nTable 3: An example of prompting method from Liu et al. (2021b).\\nName\\nNotation\\nExample\\nInput\\nx\\nI love this ﬁlm.\\nOutput\\ny\\npositive\\nPrompt Template\\nfprompt(x)\\n[X] Overall, it was a [Z] ﬁlm.\\nPrompt\\n˜x\\nI love this ﬁlm. Overall, it was a [Z] ﬁlm.\\nFilled Prompt\\nbx\\nI love this movie. Overall, it was a good movie.\\nThe advantage of prompting is that it enables few-shot or even zero-shot task adaptation.\\nThe strong cross-task transferability stems from the implicit multiple tasks such as question-\\nanswering that language models are forced to learn on the large-scale pre-training corpus.\\nHowever, this transferability requires a large model capacity to deal with potential implicit\\ntasks and is also very sensitive to the choice of prompts. Thus, the disadvantage is that it\\nintroduces the necessity for prompt engineering, i.e., ﬁnding the best prompts to solve each\\ndownstream task, which is work-heavy and time-consuming especially on large datasets.\\nPLM\\nCLS\\nTAG\\nGEN\\nﬁne-tune\\nPLM\\nCLS\\nTAG\\nGEN\\nPLM\\nCLS\\nModel:\\nTask:\\nPrompt:\\nTAG\\nGEN\\nCLS\\nTAG\\nGEN\\nMultiple\\nInstruction\\nTasks:\\nCLS,\\nTAG,\\netc.\\nFLAN\\nCLS\\nTAG\\nGEN\\n(a) Pretrain-Finetune (BERT, T5)\\n(b) Prompting (GPT-3)\\n(c) Instruction Tuning (FLAN)\\nFigure 17: (a) Fine-tuning generates a new set of parameters for each downstream task. (b)\\nPrompting ﬁxes the pre-trained parameters and ﬁnds task-speciﬁc prompts to solve each\\ndownstream task. (c) Instruction Tuning tunes the pre-trained models on instruction-format\\ndataset and uses the obtained model to do inference with multiple downstream tasks.\\n28\\nTransferability in Deep Learning: A Survey\\nCombining prompting and ﬁne-tuning may tackle this problem (Figure 17). PET-TC\\n(Schick and Sch¨utze, 2020) tunes the parameters of pre-trained language models in prompt\\nlearning while Preﬁx-Tuning (Li and Liang, 2021) adds additional prompt-related parame-\\nters and tunes these parameters. Instruction Tuning (Wei et al., 2022) explicitly ﬁne-tunes\\nthe pre-trained models on a mixture of datasets expressed through natural language instruc-\\ntions (similar to the ﬁlled prompt) and obtain Fine-tuned LAnguage Model (FLAN), which\\nlargely increases the models’ transferability to unseen tasks. In summary, prompt learning\\nhas provided a revolutionary way on how to utilize the transferability of pre-trained models.\\n3.1.5 Remarks\\nTable 4 gives a comparison of diﬀerent task adaptation methods. Fine-tuning (including\\nvanilla ﬁne-tuning, domain adaptive tuning, and regularization tuning) has better perfor-\\nmance when there are enough labeled data in the downstream tasks. In contrast, prompt\\nlearning requires much fewer labeled data to achieve decent performance, yet its applica-\\ntions are still limited to NLP and it is still non-trivial to extend it to vision or other areas.\\nFine-tuning is the most parameter-ineﬃcient since it generates a full set of model parame-\\nters for each downstream task, while residual tuning, diﬀerence tuning, and prompt learning\\nare all parameter eﬃcient. Also, these latter methods naturally mitigate the catastrophic\\nforgetting problem, but negative transfer is still a hard problem to be resolved.\\nTable 4: Comparison between diﬀerent task adaptation methods.\\nAdaptation\\nPerformance1\\nData\\nEﬃciency2\\nParameter\\nEﬃciency3\\nModality\\nScalability4\\nTask\\nScalability5\\nFeature Transfer\\n⋆\\n⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\nVanilla Fine-tuning\\n⋆⋆⋆\\n⋆\\n⋆\\n⋆⋆⋆\\n⋆⋆⋆\\nDomain Adaptive Tuning\\n⋆⋆⋆\\n⋆⋆\\n⋆\\n⋆⋆\\n⋆⋆⋆\\nRegularization Tuning\\n⋆⋆⋆\\n⋆⋆\\n⋆\\n⋆⋆⋆\\n⋆\\nResidual Tuning\\n⋆⋆\\n⋆⋆\\n⋆⋆\\n⋆⋆\\n⋆⋆\\nParameter Diﬀerence Tuning\\n⋆⋆\\n⋆⋆\\n⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\nMetric Learning\\n⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆\\nPrompt Learning\\n⋆⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆\\n⋆\\n1 Adaptation Performance: performance when there are large-scale labeled data in downstream tasks.\\n2 Data Eﬃciency: performance when there are only small-scale labeled data in downstream tasks.\\n3 Parameter Eﬃciency: whether can control parameters when the number of downstream tasks increases.\\n4 Modality Scalability: whether can adapt pre-trained models to various modalities, such as text, graph.\\n5 Task Scalability: whether can adapt pre-trained models to diﬀerent downstream tasks, such as detection.\\nThe motivation of many task adaptation methods can be understood from the perspec-\\ntive of transferability. For instance, domain adaptive tuning aims to bridge the domain\\ndiscrepancy between the pre-training task and the downstream task by further obtaining a\\npre-trained model on the target data distribution. Prompt learning aims to bridge the task\\ndiscrepancy between the pre-training task and the downstream task by reformulating all\\nthe tasks to the same format. In this scenario, when all tasks can be expressed in the same\\nform, the diﬀerence between the pre-training task and the downstream task is only the shift\\nin data distributions, i.e., task adaptation becomes the domain adaptation problem.\\n29\\nJiang et al.\\n3.2 Domain Adaptation\\nThe pre-training and ﬁne-tuning paradigm has greatly improved the state-of-the-arts for\\ndiverse machine learning problems and applications, and the pre-trained deep networks can\\nbe easily adapted to the tasks at hand even with a small amount of labeled data. However,\\nin many practical scenarios, there is no labeled training data and thus there is the demand\\nto transfer a deep network from a source domain where labeled training data is available to\\na target domain where only unlabeled data exists (Chen et al., 2012; Glorot et al., 2011). In\\nthis situation, the deep models still suﬀer from performance degradations due to distribution\\nshift (Quionero-Candela et al., 2009). Thus, domain adaptation is proposed to reduce the\\ndistribution shift between training and testing domains (Pan and Yang, 2010).\\nMany methods have been proposed for domain adaptation in the shallow regime, either\\nby re-weighting or selecting samples from the source domain (Sugiyama et al., 2008) or\\nseeking an explicit feature space transformation from the source distribution into the target\\ndistribution (Gong et al., 2013). As seminal methods, Huang et al. (2007); Pan et al. (2011);\\nLong et al. (2013) explicitly match the distributions in the kernel-reproducing Hilbert space,\\nwhile Gong et al. (2012) map the principal axes associated with each of the distributions.\\nThis survey will focus on deep domain adaptation, where adaptation modules are embedded\\nin deep architectures to match data distributions across domains.\\nIn unsupervised domain adaptation (UDA), there is a source domain bS = {(xs\\ni, ys\\ni )}n\\ni=1\\nof n labeled samples and a target domain bT = {xt\\ni}m\\ni=1 of m unlabeled samples. The goal\\nof a learning algorithm is to ﬁnd a hypothesis h : X 7→Y in the hypothesis space H with\\na low target risk ϵT (h) = E(xt,yt)∼T [ℓ(h(xt), yt)] with no access to the labels of T , where\\nℓ: Y × Y →R+ is a loss function. Several seminal theories have been proposed to tackle\\nthis problem and the the main idea of them is to bound the target risk ϵT (h) by the source\\nrisk ϵS(h) and a distribution distance. In this survey, we will focus on the theory of H∆H-\\nDivergence (Ben-David et al., 2006, 2010a; Mansour et al., 2009) and Disparity Discrepancy\\n(Zhang et al., 2019c) and illustrate how to derive diﬀerent algorithms from these theories.\\nFirst, using triangle inequalities, we can relate the target risk to the source risk as follows.\\nTheorem 3 (Bound with Disparity) Assume that the loss function ℓis symmetric and\\nobeys the triangle inequality. Deﬁne the disparity between any two hypotheses h and h′ on\\ndistribution D as\\nϵD(h, h′) = E(x,y)∼D[ℓ(h(x), h′(x))].\\n(14)\\nThen the target risk ϵT (h) can be bounded by\\nϵT (h) ⩽ϵS (h) + [ϵS (h∗) + ϵT (h∗)] + |ϵS (h, h∗) −ϵT (h, h∗)| ,\\n(15)\\nwhere h∗= arg minh∈H [ϵS (h) + ϵT (h)] is the ideal joint hypothesis, ϵideal = ϵS (h∗) + ϵT (h∗)\\nis the ideal joint error, |ϵS (h, h∗) −ϵT (h, h∗)| is the disparity diﬀerence between S and T .\\nIt is a common assumption in domain adaptation that the ideal joint error ϵideal shall be\\nsuﬃciently small, otherwise domain adaptation will be infeasible, the impossibility theorem\\n(Ben-David et al., 2010b). The goal is reduced to bound the disparity diﬀerence. However,\\nsince the ideal hypothesis h∗is unknown due to the unavailability of labeled target data, the\\ndisparity diﬀerence cannot be estimated directly. To this end, H∆H-Divergence (Ben-David\\net al., 2006, 2010a) is proposed to measure the upper bound of the disparity diﬀerence.\\n30\\nTransferability in Deep Learning: A Survey\\nDeﬁnition 4 (H∆H-Divergence) Deﬁne H∆H ≜{h|h = h1 ⊗h2, h1, h2 ∈H} as the\\nsymmetric diﬀerence hypothesis space of H, where ⊗stands for the XOR operator. Then\\nthe H∆H-Divergence between S and T is\\ndH∆H(S, T ) ≜sup\\nh,h′∈H\\n\\x0c\\x0cϵS\\n\\x00h, h′\\x01\\n−ϵT\\n\\x00h, h′\\x01\\x0c\\x0c .\\nFor binary classiﬁcation problem with the 01-loss, ℓ(y, y′) = 1(y ̸= y′), we have\\ndH∆H(S, T ) =\\nsup\\nδ∈H∆H\\n|ES [δ(x) ̸= 0] −ET [δ (x) ̸= 0]| .\\nThe main advantage of the H∆H-Divergence is that it can be estimated from ﬁnite un-\\nlabeled samples of source and target domains. However, it is generally hard to compute and\\noptimize. Thus, it is approximated by training a domain discriminator D that separates the\\nsource and target samples (Ben-David et al., 2006; Ganin and Lempitsky, 2015). Assume\\nthat the family of the discriminators is rich enough, such as the multilayer perceptrons\\n(MLP) that is universal approximator to any functions, to contain H∆H, i.e., H∆H ⊂HD.\\nThe H∆H-Divergence can be further bounded by supD∈HD |ES [D(x) = 1] + ET [D (x) = 0]|,\\nwhich gives rise to the domain adversarial methods in Section 3.2.2. The H∆H-Divergence\\ncan also be estimated in a nonparametric way by replacing H∆H with a proper function\\nspace F, which induces the statistics matching methods in Section 3.2.1.\\nThe following theorem is the earliest theory in domain adaptation, which establishes the\\ngeneralization bound based on the H∆H-Divergence for binary classiﬁcation problems.\\nTheorem 5 (Ben-David et al. (2010a)) Let H be a binary hypothesis space of VC di-\\nmension d. If bS and bT are samples of size m each, then for any δ ∈(0, 1), with probability\\nat least 1 −δ, for every h ∈H,\\nϵT (h) ≤ϵS(h) + dH∆H( bS, bT ) + ϵideal + 4\\ns\\n2d log(2m) + log(2\\nδ)\\nm\\n.\\n(16)\\nThis bound sheds key insights into algorithm designs. However, it has the limit of being\\nbased on the particular 01-loss. Thus, Mansour et al. (2009) extend the domain adaptation\\ntheory to a general class of loss functions satisfying the symmetry and subadditivity.\\nTheorem 6 (Mansour et al. (2009)) Assume that the loss function ℓis symmetric and\\nobeys the triangle inequality, and deﬁne h∗\\nS = arg minh∈H ϵS(h) and h∗\\nT = arg minh∈H ϵT (h)\\nas the ideal hypotheses for the source and target domains, then for every h ∈H,\\nϵT (h) ≤ϵS(h, h∗\\nS) + dH∆H(S, T ) + ϵ,\\n(17)\\nwhere ϵS(h, h∗\\nS) stands for the source risk and ϵ = ϵT (h∗\\nT ) + ϵS(h∗\\nT , h∗\\nS) for the capacity to\\nadapt. Further, let ℓbe bounded, ∀(y, y′) ∈Y2, ℓ(y, y′) ≤M for some M > 0, and deﬁned as\\nℓ(y, y′) = |y −y′|q for some q. If bS and bT are samples of size n and m each, with probability\\nat least 1 −δ, we have\\ndH∆H(S, T ) ≤dH∆H( bS, bT ) + 4q(Rn,S(H) + Rm,T (H)) + 3M\\n s\\nlog 4\\nδ\\n2n\\n+\\ns\\nlog 4\\nδ\\n2m\\n!\\n,\\n(18)\\n31\\nJiang et al.\\nwhere Rn,D is the expected Rademacher Complexity (Bartlett and Mendelson, 2002) with\\nrespect to distribution D and sample size n.\\nNote that the H∆H-Divergence bounds are still loose since the supremum is taken over\\nboth h′ ∈H and h ∈H. Observing that h is known as the source classiﬁer, the Disparity\\nDiscrepancy (Zhang et al., 2019c) provides a tighter bound by computing directly on H.\\nDeﬁnition 7 (Disparity Discrepancy) Given a binary hypothesis space H and a speciﬁc\\nhypothesis h∈H, the Disparity Discrepancy induced by h′ ∈H is deﬁned by\\ndh,H(S, T ) = sup\\nh′∈H\\n\\x00ET 1[h′ ̸= h] −ES1[h′ ̸= h]\\n\\x01\\n(19)\\nSince the supremum is only take over h′ ∈H, estimating and minimizing the disparity\\ndiscrepancy jointly through a minimax game can be done much more easily. The disparity\\ndiscrepancy can well measure the distribution shift and yields a tighter generalization bound.\\nTheorem 8 (Zhang et al. (2019c)) Let bS and bT be samples of size n and m each. For\\nany δ > 0 and every binary classiﬁer h ∈H, with probability at least 1 −3δ, we have\\nϵT (h) ≤ϵ b\\nS(h) + dh,H( bS, bT ) + ϵideal + 2Rn,S(H)\\n+ 2Rn,S(H∆H) + 2\\ns\\nlog 2\\nδ\\n2n\\n+ 2Rm,T (H∆H) +\\ns\\nlog 2\\nδ\\n2m .\\n(20)\\nThe disparity discrepancy can be further extended to the multiclass classiﬁcation prob-\\nlem with hypothesis space F of scoring functions f : X × Y →R and margin loss, which is\\ngoing beyond existing bounds and closer to the choices for real tasks (Zhang et al., 2019c).\\nDeﬁnition 9 (Margin Disparity Discrepancy) Given a scoring hypothesis space F,\\ndenote the margin of a real hypothesis f at a labeled example (x, y) as ρf(x, y) ≜1\\n2(f(x, y)−\\nmaxy′̸=y f(x, y′)), the labeling function induced by f as hf : x 7→arg maxy∈Y f(x, y), and\\nthe margin loss as\\nΦρ(x) ≜\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n0\\nρ ≤x\\n1 −x/ρ\\n0 ≤x ≤ρ\\n1\\nx ≤0\\n,\\n(21)\\nthen the margin disparity between f and f′ on distribution D is\\nϵ(ρ)\\nD (f′, f) = E(x,y)∼D[Φρ(ρf′(x, hf(x))].\\n(22)\\nGiven a speciﬁc hypothesis f ∈F, the Margin Disparity Discrepancy is deﬁned by\\nd(ρ)\\nf,F(S, T ) = sup\\nf′∈F\\n[ϵ(ρ)\\nT (f′, f) −ϵ(ρ)\\nS (f′, f)].\\n(23)\\nNote that the margin disparity satisﬁes the nonnegativity and subadditivity, but not the\\nsymmetry. Thus Theorem 6 cannot apply here and a new generalization bound is derived.\\n32\\nTransferability in Deep Learning: A Survey\\nTheorem 10 (Zhang et al. (2019c)) Given the same settings with Deﬁnition 9, for any\\nδ > 0, with probability at least 1 −3δ, the following margin bound holds for all scoring\\nfunctions f ∈F,\\nϵT (f) ≤ϵ(ρ)\\nb\\nS (f) + d(ρ)\\nf,F( bS, bT ) + ϵideal + 2k2\\nρ Rn,S(Π1F)\\n+ k\\nρRn,S(ΠHF) + 2\\ns\\nlog 2\\nδ\\n2n\\n+ k\\nρRm,T (ΠHF) +\\ns\\nlog 2\\nδ\\n2m ,\\n(24)\\nwhere ΠHF ≜{x 7→f(x, h(x))|h ∈H, f ∈F} is the scoring version of the symmetric hy-\\npothesis space H∆H, Π1F ≜{x 7→f(x, y)|y ∈Y, f ∈F} and ϵideal = minf∗∈F{err(ρ)\\nS (f∗)+\\nerr(ρ)\\nT (f∗)} is the ideal joint error in terms of margin loss, k is the number of classes.\\nThis margin bound suggests that a proper margin ρ could yield better generalization on\\nthe target domain. Theorems 8 and 10 together form the theoretical basis of the hypothesis\\nadversarial methods in Section 3.2.3. Note that the supremum in both the H∆H-Divergence\\nand Disparity Discrepancy will become meaningless, when the allowed hypothesis space H\\nis too large, which is common in deep neural networks. Thus, pre-training the deep neural\\nnetworks on large-scale upstream data to decrease the allowed hypotheses is still necessary\\nfor the domain adversarial methods and hypothesis adversarial methods.\\nA ﬁnal important note is that while there are no theoretical guarantees for some well-\\nestablished methods, they have also achieved quite strong performance in practice, such as\\nthe domain translation methods in Section 3.2.4 and the semi-supervised learning methods\\nin Section 3.2.5. Figure 18 highlights the cornerstones of domain adaptation methods in\\ndeep learning, which rely on the reuse of transferability gained in pre-trained deep models.\\n2015\\n2017\\n2016\\n2018\\n2019\\n2020\\n2021\\nDAN\\nAdaBN\\nTransNorm\\nDANN\\nFCN-wild\\nD-adapt\\nBSP\\nRegDA\\nMCD\\nMDD\\nSelf-Ensemble\\nMCC\\nJDOT\\nJAN\\nAdaptSeg\\nDA-Faster\\nCDAN\\nADVENT\\nSWDA\\nSWD\\nDD\\nPixelDA\\nCycleGAN\\nDTN\\nCyCADA\\nCBST\\nStatistics\\nMatching\\nDomain\\nAdversarial\\nLearning\\nHypothesis\\nAdversarial\\nLearning\\nDomain\\nTranslation\\nSemi\\nSupervised\\nLearning\\nADDA\\nCAN\\nDeep Coral\\nDeep JDOT\\nDSN\\nCoGAN\\nRSP\\nMMT\\nFigure 18: The cornerstones of domain adaptation methods in deep learning.\\n33\\nJiang et al.\\n3.2.1 Statistics Matching\\nWe have introduced several seminal theories on the generalization bounds for domain adap-\\ntation, which are all based on the hypothesis-induced distribution distances. These distances\\nare less intuitive because they rely on unknown hypotheses and cannot be computed before\\nlearning the hypotheses. In this section, we ﬁrst introduce another family of metrics on the\\nspace of probability measures well-studied in probability theory, which provide interpretable\\nand complementary properties to the hypothesis-induced distribution distances and relate\\nclosely to a large set of domain adaptation algorithms (Long et al., 2015, 2017).\\nDeﬁnition 11 (Maximum Mean Discrepancy) Given two probability distributions S\\nand T on a measurable space X, the integral probability metric (Redko et al., 2020) is\\ndeﬁned as dF(S, T ) ≜supf∈F\\n\\x0c\\x0cEx∼S[f(x)] −Ex∼T [f(x)]\\n\\x0c\\x0c, where F is a class of bounded\\nfunctions on X. Sriperumbudur et al. (2010) further restrict F as the unit ball in Reproduc-\\ning Kernel Hilbert Space (RKHS) Hk endowed with a characteristic kernel k, F = {f ∈Hk :\\n||f||Hk ≤1}, leading to the maximum mean discrepancy (MMD) (Gretton et al., 2012a),\\nd2\\nMMD(S, T ) =\\n\\r\\rEx∼S[φ(x)] −Ex∼T [φ(x)]\\n\\r\\r2\\nHk,\\n(25)\\nwhere φ(x) is a feature map associated with kernel k such that k(x, x′) = ⟨φ(x), φ(x′)⟩. It can\\nbe proved from probability theory that S = T if and only if dF(S, T ) = 0 or d2\\nMMD(S, T ) = 0.\\nTheorem 12 (Redko et al. (2020)) Given the same settings with Deﬁnition 11, let ℓbe\\na convex loss function with a parametric form ℓ(y, y′) = |y −y′|q for some q. Then for any\\nδ > 0, with probability at least 1 −δ, the following bound holds for all h ∈F,\\nϵT (h) ≤ϵS(h) + dMMD( bS, bT ) + ϵideal\\n+ 2\\nnEx∼S[\\np\\ntr(KS)] + 2\\nmEx∼T [\\np\\ntr(KT )] +\\ns\\nlog 2\\nδ\\n2n\\n+\\ns\\nlog 2\\nδ\\n2m ,\\n(26)\\nwhere KS and KT are the kernel matrices computed on samples from S and T , respectively.\\nThis bound has several advantages compared to previous theories. First, it is hypothesis-\\nfree and does not require estimating hypotheses to measure the distribution distance. Sec-\\nond, the complexity term does not depend on the Vapnik-Chervonenkis dimension. Third,\\nthe unbiased estimate of MMD can be computed in linear time. Fourth, minimizing MMD\\nhas a nice interpretation of statistics matching in the probability space. These advantages\\nmake the bound particularly useful to underpin several seminal algorithms.\\nDeep Domain Confusion (DDC) (Tzeng et al., 2014) applies MMD with a linear kernel\\nto a single feature layer of the deep network, yet it has limited power for closing the domain\\ngap since linear kernel is not characteristic and cannot ensure MMD to be a probability\\nmetric. Thereby, Deep Adaptation Network (DAN) (Long et al., 2015, 2019) introduces\\nthe multiple-kernel variant of MMD (MK-MMD) (Gretton et al., 2012b,a) to measure the\\ndomain relatedness, employing a convex combination of multiple characteristic kernels such\\nas Gaussian kernel to make the function space F rich enough and enhance the distinguishing\\npower of MK-MMD. Besides, as shown in Figure 19, multiple domain-speciﬁc layers are\\nadapted by MK-MMD, which enables learning transferable features for domain adaptation.\\n34\\nTransferability in Deep Learning: A Survey\\nMK-\\nMMD\\nMK-\\nMMD\\nMK-\\nMMD\\nJMMD\\n…\\n…\\n×\\n×\\ntied\\ntied\\n(a) DAN\\n(b) JAN\\n!!\\n!%\\n\"!,\\n\"!-\\n\"!0\\n$#!\\n\"%,\\n\"%-\\n\"%0\\n#!\\n%!\"#\\n1\\n!!\\n!%\\n\"!,\\n\"! 5\\n$#!\\n\"%,\\n\"% 5\\n#!\\n1\\n\\',\\n\\' 5\\n\\',\\n\\' 5\\n%!\"#\\nFigure 19: The cornerstone methods of statistics matching: (a) DAN adapts the marginal\\ndistributions of activations in multiple task-speciﬁc layers with MK-MMD. (b) JAN adapts\\nthe joint distributions of the feature activations and classiﬁcation predictions with JMMD.\\nDAN mainly reduces the shift in the feature distribution and ignores that in the label\\ndistribution. Take AlexNet as example, the feature distribution shift mainly exists in layers\\nfc6 and fc7 while the label distribution shift mainly exists in layer fc8. Joint Adaptation\\nNetwork (JAN) (Long et al., 2017) proposes Joint Maximum Mean Discrepancy (JMMD) to\\nmeasure the shift in joint distributions P(Xs, Ys) and Q(Xt, Yt). Denoting the activations\\nof adapted layers L as {(zs1\\ni , . . . , zs|L|\\ni\\n)}n\\ni=1 and {(zt1\\nj , . . . , zt|L|\\nj\\n)}m\\nj=1, JMMD is deﬁned as\\nd2\\nJMMD( bS, bT ) =\\n\\r\\r\\rEi∈[n] ⊗l∈L φl(zsl\\ni ) −Ej∈[m] ⊗l∈L φl(ztl\\nj )\\n\\r\\r\\r\\n2\\nHk\\n(27)\\nwhere φl is the feature map associated with kernel kl for layer l and ⊗is the outer product.\\nA characteristic kernel widely used in MMD is the Gaussian kernel, or k(x1, x2) =\\nexp(−||x1 −x1||2/2σ2). After Taylor expansion, MMD can be considered as a weighted\\nsum of distances between all orders of statistic moments. Thus, the statistic moments can\\nbe directly used to measure the distribution distance. For instance, deep CORAL (Sun and\\nSaenko, 2016) uses the second-order statistics (covariance) to measure distribution distance,\\nwhich is frustratingly easy yet useful. Center moment discrepancy (CMD) (Zellinger et al.,\\n2017) further considers an explicit order-wise matching of higher-order moments.\\nOne disadvantage of MMD is that it cannot take into account the geometry of the data\\ndistribution when estimating the discrepancy between two domains. Thus, Joint Distribu-\\ntion Optimal Transport (JDOT) (Courty et al., 2017) is introduced into domain adaptation,\\nand Deep JDOT (Damodaran et al., 2018) further extends it to deep networks. Another dis-\\nadvantage is that minimizing MMD on the instance representation has the risk of changing\\nthe feature scale, while regression tasks are fragile to feature scaling. Thus, Representation\\nSubspace Distance (RSD) (Chen et al., 2021b) closes the domain shift through orthogonal\\nbases of the representation spaces, which are free from feature scaling.\\nInstead of explicitly matching the statistics moments of feature distributions, Adaptive\\nBatch Normalization (AdaBN) (Li et al., 2017) implicitly minimizes domain discrepancy by\\naligning BatchNorm Ioﬀe and Szegedy (2015) statistics. The hypothesis is that task-related\\nknowledge is stored in the weight matrix while domain-related knowledge is represented by\\nBatchNorm statistics. Thus AdaBN replaces the mean and variance of all BatchNorm layers\\nwith those estimated on the target domain at inference to reduce the domain shift. However,\\n35\\nJiang et al.\\ngenerator\\nclassiﬁer\\ndiscriminator\\ngradient reverse layer\\ngenerator\\nclassiﬁer\\ndiscriminator\\nstop gradient \\n×\\n×\\n(a) DANN\\n(b) CDAN\\n!!\\n!%\\n\"!\\n$#!\\n\"%\\n#!\\n%!\"#\\n1\\n2!\\n2%\\n%6788\\nℎ\\n4\\n!!\\n!%\\n\"!\\n$#!\\n\"%\\n#!\\n%!\"#\\n%9678\\nℎ\\n4\\nℎ\\n$#%\\n1\\nFigure 20: Both DANN and CDAN have a feature generator network ψ, a classiﬁer h, and\\na domain discriminator D connected to ψ via a gradient reversal layer. (a) In DANN, the\\ndiscriminator D is trained to distinguish between domains while the generator ψ tries to\\nmake the feature distributions indistinguishable for the discriminator. (b) In CDAN, the\\ndiscriminator D is conditioned on the classiﬁer prediction by via a multilinear map z ⊗by.\\nit is risky that AdaBN excludes the statistics on the target domain from training. Thus,\\nTransferable Normalization (TransNorm) (Wang et al., 2019c) applies domain-speciﬁc mean\\nand variance at both training and inference to capture suﬃcient statistics of both domains.\\nFinally, both MMD and JMMD may misalign samples from diﬀerent classes due to a\\nsuboptimal modeling of the discriminative structure. To alleviate this problem, Contrastive\\nAdaptation Network (CAN) (Kang et al., 2019) alternatively estimates the labels of target\\nsamples through clustering, and adapts the feature representations in a class-wise manner.\\nBesides, CAN uses class-aware sampling for both domains to improve adaptation eﬃciency.\\n3.2.2 Domain Adversarial Learning\\nDomain Adversarial Neural Network.\\nAn important milestone for modeling distribu-\\ntions is the Generative Adversarial Net (GAN) (Goodfellow et al., 2014). Inspired by GAN,\\nDomain Adversarial Neural Network (DANN) (Ganin and Lempitsky, 2015; Ganin et al.,\\n2016) integrates a two-player game into domain adaptation (Figure 20). The ﬁrst player is\\nthe domain discriminator D trained to distinguish the source features from the target fea-\\ntures and the second player is the feature generator ψ trained simultaneously to confuse the\\ndomain discriminator. As mentioned in Section 3.2, the upper bound of H∆H-Divergence\\nbetween feature distributions can be estimated by training the domain discriminator D,\\nLDANN(ψ) = max\\nD Exs∼b\\nS log[D(zs)] + Ext∼bT log[1 −D(zt)],\\n(28)\\nwhere z = ψ(x) is the feature representation for x. The objective of the feature generator\\nψ is to minimize the source error as well as the H∆H-Divergence bounded by Equation 28,\\nmin\\nψ,h E(xs,ys)∼b\\nSLCE(h(zs), ys) + λLDANN(ψ),\\n(29)\\nwhere LCE is the cross-entropy loss, λ is a hyper-parameter that trades oﬀsource error and\\ndomain adversary. Minimizing the cross-entropy loss will lead to discriminative representa-\\ntions while decreasing the domain adversarial loss will result in transferable representations.\\n36\\nTransferability in Deep Learning: A Survey\\nDANN aligns the marginal feature distributions through adversarial training. However,\\nthis may be insuﬃcient when the feature-label joint distributions change between domains.\\nBesides, the feature distribution is usually multimodal in multi-class classiﬁcation, thus even\\nif the discriminator is fully confused, there is no guarantee that the two feature distributions\\nare similar (Arora et al., 2017). To tackle these two issues, Conditional Domain Adversarial\\nNetwork (CDAN) (Long et al., 2018) conditions features z on classiﬁer predictions by = h(z)\\nand introduces multilinear map z ⊗by instead of z as the input to domain discriminator D:\\nLCDAN(ψ) = max\\nD Exs∼b\\nS log[D(zs ⊗bys)] + Ext∼bT log[1 −D(zt ⊗byt)].\\n(30)\\nConditioning on by, CDAN fully captures cross-variance between the feature representation\\nand classiﬁer prediction, resulting in better alignment of the joint distributions.\\nTzeng et al. (2015) align the class distributions explicitly by transferring the similarity\\nstructure in classes from the source domain to the target domain. Speciﬁcally, the average\\noutput probability of data from each class is computed over the source samples as soft labels.\\nThen the model is optimized to match the distribution over classes to the soft labels.\\nImprovements.\\nDANN integrates a gradient reverse layer (GRL) into the standard ar-\\nchitecture to implement the minimax between the feature generator and domain classiﬁer.\\nHowever, this optimizing strategy might not work well in practice due to gradient vanishing,\\nwhich is also a major problem in training GANs. For instance, when the generated target\\nfeatures zt are very distinguishable from source features such that D(zt) = 0, the gradient\\nfor the feature generator is small and vice versa. This makes the optimization of the feature\\ngenerator diﬃcult. Thus, Adversarial Discriminative Domain Adaptation (ADDA) (Tzeng\\net al., 2017) splits the optimization of feature generator ψ and domain classiﬁer D into two\\nindependent objectives, where the maximization of D remains unchanged, but the objective\\nof ψ becomes\\nmin\\nψ Ext∼bT −log[D(zt)].\\n(31)\\nThis assigns small gradients for source-like target samples and larger gradients for the other\\ntarget samples. It has the same ﬁxed-point properties as GRL while making the optimiza-\\ntion easier for the feature generator. Although adversarial domain adaptation enhances the\\nfeature transferability, i.e. the ability of feature representations to bridge the discrepancy\\nacross domains, studies (Chen et al., 2019c) reveal that it is at the expense of deteriorating\\nthe discriminability, i.e. the easiness of separating categories over the ﬁxed feature represen-\\ntations of both domains. Spectral analysis shows that only the eigenvectors corresponding\\nto the largest singular values tend to carry transferable knowledge, while other eigenvectors\\nmay reﬂect domain variations and thus be overly penalized in domain adversarial training.\\nHowever, these eigenvectors may convey crucial discriminative information, and thus the\\ndiscriminability is weakened. To tackle this transferability-discriminability dilemma, Batch\\nSpectral Penalization (BSP) (Chen et al., 2019c) penalizes the largest singular values so that\\nthe other eigenvectors can be relatively strengthened to boost the feature discriminability.\\nDomain Separation Network (DSN) (Bousmalis et al., 2016) introduces a private subspace\\nfor each domain, which preserves domain-speciﬁc information, such as background and low-\\nlevel image statistics. Then domain alignment is performed safely in the shared subspace,\\nwhich is orthogonal to the private subspace responsible for the discriminative tasks.\\n37\\nJiang et al.\\ndiscriminator\\nD\\nsegmentator\\nsegmentation output\\nD\\ndiscriminator\\nfeature extractor\\nglobal feature\\nD\\ndiscriminator\\nfeature extractor\\ninstance feature\\nD\\ndiscriminator\\nfeature extractor\\nlocal feature\\nsource domain\\ntarget domain\\n(a) global-level feature alignment \\n(b) instance-level feature alignment \\n(c) local-level feature alignment \\n(d) local-level output alignment \\nFigure 21: Where to adapt in diﬀerent methods. (a) DANN performs alignment on global\\nfeatures. (b) DA-Faster performs alignment on both image-level and instance-level features.\\n(c) SWDA performs alignment on local features. (d) Adapt-SegMap performs alignment on\\nlocal outputs.\\nDomain Adversarial Leanring in Real-World Scenarios.\\nDomain adversarial learn-\\ning has largely improved the performance on unlabeled target domain and has been widely\\nadopted in many applications (Hoﬀman et al., 2016; Chen et al., 2018). However, real-world\\nscenarios are much more complex. Here we list several situations that are often encountered\\nyet not well resolved, and review some existing solutions to them.\\nWhich part to adapt is unknown. In image recognition, we only need to classify the input.\\nYet in applications such as object detection, we need to locate the Region of Interests (RoIs)\\nﬁrst and then classify them. Due to the distribution shift across domains, the localization\\nof RoIs on the target domain is not reliable, thus which part to adapt in the adversarial\\ntraining is unknown. To alleviate this problem, as shown in Figure 21, DA-Faster (Chen\\net al., 2018) performs domain alignment at both image level and instance level, where the\\nimage-level alignment is believed to improve the localization of RoIs on the target domain.\\nSWDA (Saito et al., 2019) argues that alignment of the local features is more eﬀective than\\nthat of the global features for better localization. Although the above adversarial training\\nmethods have improved the transferability of object detectors, the discriminability might\\nget lost in the adversarial adaptation process as mentioned by BSP (Chen et al., 2019c).\\nSince discriminability is crucial for the localization of RoIs, D-adapt (Jiang et al., 2022)\\nintroduces parameter-independent category adaptor and bounding box adaptor to decouple\\nadversarial adaptation from detector training, which yields sharp improvement.\\nThere are structural dependencies between labels of each sample. In low-level classiﬁca-\\ntion tasks, such as semantic segmentation or token classiﬁcation (Named Entity Recognition,\\nParts-of-speech tagging, etc.), adaptation on features (Hoﬀman et al., 2016) may not be a\\ngood option, because the feature of each pixel or each token is high-dimensional and there\\n38\\nTransferability in Deep Learning: A Survey\\nexist many pixels or tokens in a single sample. Every coin has two sides. Compared to\\nthe high-level classiﬁcation tasks, the output space of these low-level tasks contains much\\nricher information of the distribution, e.g., scene layout and context, and thus adaptation\\non the output space can reduce the domain shift. As shown in Figure 21, Adapt-SegMap\\n(Tsai et al., 2018) trains a discriminator to distinguish whether the segmentation output is\\nfrom the source or the target, while the feature generator is encouraged to generate similar\\nsegmentation outputs across domains. It explicitly aligns the output distributions of target\\nand source domains, and implicitly adapts the feature distributions. Further, ADVENT\\n(Vu et al., 2019) minimizes the distribution distance on the self-information distributions,\\nwhere the entropy of segmentation outputs is fed to the discriminator. In this way, the con-\\nditional information is neglected while more attention is paid to the structural dependencies\\nbetween local semantics.\\n3.2.3 Hypothesis Adversarial Learning\\nInevitably, there is still a gap between the proxy distance used in previous methods and\\nthe hypothesis-induced discrepancies in the theories. To close this gap, Maximum Classiﬁer\\nDiscrepancy (MCD) (Saito et al., 2018) starts to estimate and optimize H∆H-Divergence in\\na fully parameterized way. As shown in Figure 22, MCD maximizes the discrepancy between\\ntwo classiﬁers’ outputs to detect target samples far from the support of source distribution,\\ni.e. estimate H∆H-Divergence. A feature generator then learns to generate target features\\nnear the support to minimize the discrepancy, i.e. minimize the domain discrepancy. MCD\\nuses the L1 distance to calculate the discrepancy, while Sliced Wasserstein Discrepancy\\n(SWD) (Lee et al., 2019) adopts the Wasserstein metric, which is the natural geometry for\\nprobability measures induced by the optimal transport theory. In theory MCD is closer to\\nH∆H-Divergence, yet in experiments it is slow in convergence and very sensitive to hyper-\\nparameters. The reason is that there exist two classiﬁers h and h′ in MCD that maximize\\nthe discrepancy, which makes the minimax optimization hard to reach equilibrium.\\nDisparity Discrepancy (DD) (Zhang et al., 2019c) provides a tighter bound by taking\\nsupremum in hypothesis space H rather than H∆H. This will signiﬁcantly ease the minimax\\noptimization. As shown in Figure 22, DD introduces an adversarial classiﬁer h′ sharing the\\nsame hypothesis space with h. The supremum in dh,H(S, T ) is approximated by\\nLDD(h, ψ) = max\\nh′\\nExs∼b\\nSLs \\x02\\nh′(ψ(xs)), h(ψ(xs))\\n\\x03\\n−Ext∼bT Lt \\x02\\nh′(ψ(xt)), h(ψ(xt))\\n\\x03\\n,\\n(32)\\nwhere Ls and Lt are speciﬁc loss functions deﬁned on the source domain and target domain\\nrespectively. Based on the theory (Zhang et al., 2019c), when the adversarial classiﬁer h′ is\\nclose to the supremum, minimizing the following terms will decrease the target error ϵT ,\\nmin\\nψ,h E(xs,ys)∼b\\nSLCE(h(ψ(xs)), ys) + λLDD(h, ψ),\\n(33)\\nwhere λ is a tradeoﬀhyper-parameter. An intuitive explanation is that DD is looking for an\\nadversarial classiﬁer h′ that predicts correctly on the source domain while making diﬀerent\\npredictions from h on the target domain. And then the feature generator ψ is encouraged\\nto generate features near the decision boundary to avoid such situations.\\n39\\nJiang et al.\\nfreeze\\ngenerator\\nupdate\\nclassiﬁer\\nMaximize discrepancy on target \\ngenerator\\nadversarial\\nclassiﬁer\\nclassiﬁer\\ngradient reverse layer\\nstop gradient \\n(a) MCD (Step B)\\n(c) MDD / DD \\nfreeze\\nclassiﬁer\\nMinimize discrepancy on target\\n(a) MCD (Step C)\\nupdate\\ngenerator\\n!%\\n1\\nℎ,\\nℎ-\\n$#,\\n%\\n$#-\\n%\\n!%\\n1\\nℎ,\\nℎ-\\n$#,\\n%\\n$#-\\n%\\n!\\n1\\nℎ\\nℎ′\\n$#\\n$#:\\n%!\"#\\n% ; 66\\nFigure 22: (a) The training of MCD has three steps. Step A: Both the classiﬁers and the\\nfeature generator are trained to classify the source samples correctly. Step B: The classiﬁers\\nh1 and h2 learn to maximize the discrepancy on the target samples. (b) Step C: The feature\\ngenerator ψ learns to minimize the discrepancy on the target samples. (c) DD and MDD\\nintroduce an adversarial classiﬁer h′ to maximize the discrepancy and trains the feature\\ngenerator ψ to minimize the source error as well as the discrepancy.\\nHowever, DD is still limited to the 01 loss in the classiﬁcation setting. Based on scoring\\nfunctions and margin loss, Margin Disparity Discrepancy (MDD) (Zhang et al., 2019c) goes\\na crucial step forward and provides a margin theory for the multi-class classiﬁcation setting.\\nThe margin ρ is attained by introducing parameter γ ≜exp ρ in the disparity discrepancy,\\nLMDD(h, ψ) = max\\nh′\\nγ Exs∼b\\nS log\\n\\x02\\nσh(ψ(xs))(h′(ψ(xs)))\\n\\x03\\n+ Ext∼bT log\\n\\x02\\n1 −σh(ψ(xt))(h′(ψ(xt)))\\n\\x03\\n,\\n(34)\\nwhere σ is the softmax function. A proper γ can constrain h′ in a hypothesis space of proper\\nsize to avoid overestimation of the generalization bound. Note that in Equation 34, the loss\\non the source domain is the standard cross-entropy, while that on the target domain is a\\nmodiﬁed cross-entropy to avoid gradient vanishing and ease the optimization of h′.\\nIn principle, DD can be easily extended to regression problems by replacing the classiﬁers\\nin Figure 22 with regressors and choosing Ls and Lt as the L1 or L2 loss commonly used\\nin regression. It has been extended to both keypoint detection (Jiang et al., 2021) and\\nbounding box localization task (Jiang et al., 2022). To tackle the challenge caused by the\\nhigh-dimensional output space in the keypoint detection, Regressive Domain Adaptation\\n(RegDA) (Jiang et al., 2021) introduces a spatial probability distribution to describe the\\nsparse density of the output space and uses it to guide the optimization of the adversarial\\nregressor h′. In an expectation sense, this reduces the size of the hypothesis space of h′ and\\navoids overestimation of the generalization bound in Zhang et al. (2019c).\\n3.2.4 Domain Translation\\nDomain translation is the task of mapping raw data of text, image, audio, and other data\\nmodality from the source distribution S to the target distribution T . In domain adaptation\\nproblems, we can use translation models, usually based on Generate Adversarial Networks\\n(GAN) (Goodfellow et al., 2014), to obtain labeled source domain in the target style, i.e.\\ntranslated into the target distribution. Training on such stylized source domain can yield\\nbetter transferability than models trained on the original source domain.\\n40\\nTransferability in Deep Learning: A Survey\\nclassiﬁer\\ndiscriminator\\ngenerator\\n(b) Cycle Consistency\\n(a) Domain Translation\\n(c) Semantic Consistency\\npixel space\\nsemantic space\\n!!\\n!%\\n5\\nℎ∘1\\n!!→#\\n\"#!→#\\n#!\\n4\\n%!\"#\\n%<78\\n!\\n\"\\n!\\n\"\\n#\\n$\\n#\\n%\\nFigure 23: (a) The architecture for PixelDA includes a generator network G, an adversarial\\ndiscriminator D and a task-speciﬁc classiﬁer h on feature extractor ψ. (b) Cycle-consistency\\nloss: after we translate from source domain to target domain, we should recover the source\\ndata if translating back again. (c) Semantic-consistency loss: translating between domains\\nshould not change the semantic labels of the samples, where f is the labeling function.\\nGANs reason about the marginal distribution, i.e., from a random vector, the generator\\nnetwork should synthesize data that resembles one that is drawn from the true distribution.\\nHowever, marginal distribution is not enough for domain adaptation, thus Coupled Gen-\\nerative Adversarial Networks (CoGAN) (Liu and Tuzel, 2016) learns a joint distribution\\nof multi-domain images from data, i.e., from a random vector, multiple generators should\\ngenerate paired data that are from diﬀerent distributions and share the same labels. By\\nenforcing a weight-sharing constraint between diﬀerent generators, CoGAN learns a joint\\ndistribution without the existence of corresponding images in diﬀerent domains. Then the\\nshared labels of the target samples are used to train the target model.\\nA more common objective of domain translation is to learn a mapping G : S →T such\\nthat the generated sample G(x) is indistinguishable from the training samples of the target\\ndomain. As shown in Figure 23, PixelDA (Bousmalis et al., 2017) introduces an adversarial\\ndiscriminator D to distinguish between translated samples and target samples,\\nLGAN(G) = max\\nD Ex∼b\\nS log [1 −D(G(x))] + Ex∼bT log [D(x)] .\\n(35)\\nThe generator G tries to synthesize samples G(x) that look similar to images from the target\\ndomain by minG LGAN(G). The task-speciﬁc classiﬁer h and feature extractor ψ are trained\\nsupervisedly on the target-style generated data by minψ,h E(x,y)∼b\\nSLsup(h ◦ψ(G(x)), y).\\nCycle Consistency.\\nWhile GAN can learn a mapping between two datasets, the desired\\nmapping may not be obtained. The source sample may be projected to an irrelevant target\\nsample, destroying the structure or content of the original samples. Besides, multiple source\\nsamples may be mapped to the same target sample, leading to the well-known problem of\\nmode collapse (Goodfellow et al., 2014). Therefore, CycleGAN (Zhu et al., 2017) introduces\\nan additional mapping F : T →S from target to source and adds a constraint of cycle\\nconsistency to reduce the space of possible mapping functions (Figure 23). Mathematically,\\ncycle consistency requires F and G to be bijections and inverse of each other. In practice,\\nCycleGAN constrains F(G(x)) ≈x and G(F(x)) ≈x, which preserves the structure or\\ncontent of samples to obtain more meaningful mappings. CycleGAN has been widely used\\nin domain adaptation problems, such as image classiﬁcation (Hoﬀman et al., 2018), semantic\\nsegmentation (Hoﬀman et al., 2018), person re-identiﬁcation (Wei et al., 2018), robotic\\n41\\nJiang et al.\\ngrasping (Bousmalis et al., 2018), object detection (Kim et al., 2019), etc. The idea goes\\nbeyond the ﬁeld of image translation and is widely used in other ﬁelds, such as unsupervised\\nmachine translation (Lample et al., 2017), where the cycle consistency is also called back-\\ntranslation. Unsupervised machine translation can further be used for cross-lingual domain\\nadaptation tasks (Conneau et al., 2018), where a language is a domain.\\nSemantic Consistency.\\nCycleGAN is a general-purpose translation model for vision\\ntasks and is apt at style transfer between datasets. However, it is diﬃcult for CycleGAN to\\nmaintain the semantic information. It has been experimentally shown that when mapping\\nfrom source to target, the problem of label ﬂipping will easily occur (Bousmalis et al., 2017;\\nHoﬀman et al., 2018). As a result, there will exist a lot of noisy labels in the translated\\ndataset, hurting the performance of the target model. Thus, ensuring semantic consistency\\nis important for translation-based domain adaptation. Formally, given the labeling function\\nf, the labels assigned to sample x should be consistent with that of the translated sample,\\ni.e., f(x) = f(G(x)) (Figure 23). Since function f is not accessible, several proxy functions\\nhave been proposed to approximate the semantic consistency (Taigman et al., 2017; Hoﬀman\\net al., 2018; Bousmalis et al., 2018). Given a proxy function hp and a distance measure d,\\nthe objective is to reduce the semantic inconsistency,\\nmin\\nG Lsc(G, hp) = d(hp(x), hp(G(x))).\\n(36)\\nDTN (Taigman et al., 2017) and SimGAN (Shrivastava et al., 2017) use the feature extractor\\nas a proxy function and the goal is to translate the low-level style while keeping the high-level\\nfeatures invariant. PersonGAN (Wei et al., 2018) uses the foreground crop of the person\\nimage as the proxy function of the person’s identity, which ensures that a person’s identity\\nremains the same before and after translation. However, the constraint on feature or pixel\\nspace might be too strong, making it diﬃcult to change the low-level style. Therefore, Cycle-\\nconsistent Adversarial Adaptation (CyCADA) (Hoﬀman et al., 2018) utilizes a pre-trained\\nsource model as a proxy function to encourage generating samples that have consistent\\npredictions under the function. This proxy function eﬀectively avoids label ﬂipping during\\nthe translation of handwritten digit images, yet it is still not perfect. When faced with\\nthe dataset shift of real-world problems, the predictions on the generated samples are not\\nreliable and may provide incorrect guidance to G.\\nWhen the domain diﬀerence is primarily low-level, such as textures, illumination, and\\ncolor, translation can eﬀectively close the domain gap. But when the domain is diﬀerent at\\nthe high level, such as from diﬀerent camera angles, translation may fail to adapt domains.\\nTherefore, translation methods at the low-level and adaptation methods at the high-level\\nmentioned in Section 3.2.1-3.2.3 are complementary and can be combined in practical appli-\\ncations (Hoﬀman et al., 2018). For example, Generate to Adapt (Sankaranarayanan et al.,\\n2018) directly uses the generative task as an auxiliary task to align features across domains.\\n3.2.5 Semi-Supervised Learning\\nUnsupervised Domain Adaptation (UDA) is closely related to Semi-Supervised Learning\\n(SSL) since both of them aim at generalizing from the labeled samples to the unlabeled\\nsamples. The diﬀerence is that in SSL, both the labeled and unlabeled samples come from\\nthe same distribution while in UDA, the source and target distributions diﬀer. Thus, SSL\\n42\\nTransferability in Deep Learning: A Survey\\ntasks can be considered as a special case of UDA tasks and some SSL methods can be\\napplied in UDA tasks. Since there is still no theoretical guarantee for SSL methods in the\\nUDA scenario, the ﬁrst question to answer is, under what assumptions can we use SSL in\\nUDA? There are mainly three assumptions in SSL (Chapelle et al., 2006). (1) Smoothness\\nAssumption: if two samples x1, x2 residing in a high-density region are close, then so should\\nbe their corresponding outputs y1, y2 . (2) Cluster Assumption: if points are in the same\\ncluster, they are likely to be of the same class. It can also be interpreted as the Low-density\\nSeparation Assumption, where the decision boundary should lie in the low-density regions.\\n(3) Manifold Assumption: the high-dimensional data shall lie roughly on a low-dimensional\\nmanifold. Both smoothness assumption and cluster assumption are helpful for classiﬁcation,\\nbut not for regression problems. Thus SSL is used more commonly in classiﬁer adaptation.\\nHere we review several SSL methods applied to the UDA problems.\\nConsistency Regularization encourages consistent predictions for similar data points.\\nSimilar data points are generated by performing diﬀerent data augmentations on the same\\ndata point. While many augmentation techniques are proposed for images, few are available\\nfor other data formats, such as texts and time series. Thus this type of method is limited to\\ncertain data modalities. Self-Ensemble (French et al., 2018) applies mean-teacher, a typical\\nconsistency regularization method, to image domain adaptation. The teacher model, which\\nis an Exponential Moving Average (EMA) of the student model, will generate predictions to\\ntrain the student model on the target domain. Due to the domain shift, the predictions are\\nnoisy, thus Mutual Mean-Teaching (MMT) (Ge et al., 2020) uses two collaborative networks\\njointly optimized under the supervision of mutual teacher models.\\nEntropy Minimization encourages the model to make conﬁdent (i.e., low-entropy)\\npredictions on unlabeled data. It serves as an auxiliary term in many domain adaptation\\nmethods (Long et al., 2016, 2018; Saito et al., 2018; Shu et al., 2018; Vu et al., 2019). The\\nrisk is that the predictions on the target domain are not reliable, and entropy minimization\\nmay hurt the performance of the model. Thus, Minimum Class Confusion (MCC) (Jin et al.,\\n2020) introduces a weight for each instance, where uncertain samples have smaller weights\\nto avoid minimizing entropy on the incorrectly classiﬁed samples. MCC further minimizes\\nthe instance-weighted confusion between diﬀerent classes, which is simple yet frustratingly\\neﬀective. Source Hypothesis Transfer (Liang et al., 2020) adopts an information maximiza-\\ntion loss with a fair diversity-promoting objective, which circumvents the trivial solutions\\nin entropy minimization that all unlabeled data have the same one-hot encoding.\\nPseudo-Labeling produces proxy labels on unlabeled data and uses these noisy labels\\ntogether with the labeled data to train the model. In self-training, a conﬁdence threshold\\nis used to ﬁlter out unreliable proxy labels, which may fail in UDA since the model is likely\\nto be biased towards well-transferred classes while ignoring other hard classes. Thus, Class-\\nBalanced Self-Training (CBST) (Zou et al., 2018) uses a class-wise conﬁdence threshold.\\nStill, large noise exists in the generated pseudo labels on the target domain, and the standard\\nCross-Entropy (CE) loss has been shown to be sensitive to label noise (Zhang et al., 2017).\\nTowards this problem, Zhang and Sabuncu (2018) propose the Generalized Cross-Entropy\\n(GCE) loss as an eﬀective solution (Rusak et al., 2021; Liu et al., 2021a),\\nLGCE(x, ˜y) = 1/q · (1 −h˜y(x)q),\\n(37)\\nwhere q ∈(0, 1] is a hyper-parameter to trade-oﬀbetween the CE loss and the MAE loss.\\n43\\nJiang et al.\\n3.2.6 Remarks\\nDiﬀerent domain adaptation methods are compared from several perspectives in Table 5.\\nFirst, statistics matching, domain adversarial learning, and hypothesis adversarial learning\\nmethods are derived from theory, enjoying theoretical guarantees while domain translation\\nand semi-supervised learning methods are still in the empirical regime. Second, the former\\nthree categories of methods work in the feature space or the output space and are highly re-\\nlated to speciﬁc tasks, and some are tightly integrated to speciﬁc architectures. In contrast,\\ntranslation methods work in the input space and are relatively independent of speciﬁc tasks.\\nHowever, translation models and semi-supervised learning are dependent on speciﬁc data\\nformat, and are hard to scale to diﬀerent modalities. Finally, statistics matching methods\\nare based on nonparametric distances, which are data-eﬃcient but weak in expressiveness,\\nthereby more suitable for low-data regimes. In contrast, domain adversarial learning and\\nhypothesis adversarial learning methods are based on parametric distances, which can only\\nbe measured throughout learning, but are more performant when scaling up data.\\nTable 5: Comparison between diﬀerent domain adaptation methods.\\nAdaptation\\nPerformance1\\nData\\nEﬃciency2\\nModality\\nScalability3\\nTask\\nScalability4\\nTheory\\nGuarantee5\\nStatistics Matching\\n⋆\\n⋆⋆⋆\\n⋆⋆⋆\\n⋆⋆\\n⋆⋆⋆\\nDomain Adversarial Learning\\n⋆⋆\\n⋆⋆\\n⋆⋆⋆\\n⋆⋆\\n⋆⋆⋆\\nHypothesis Adversarial Learning\\n⋆⋆⋆\\n⋆⋆\\n⋆⋆⋆\\n⋆⋆\\n⋆⋆⋆\\nDomain Translation\\n⋆⋆\\n⋆\\n⋆\\n⋆⋆⋆\\n⋆\\nSemi-Supervised Learning\\n⋆⋆\\n⋆⋆\\n⋆⋆\\n⋆\\n⋆\\n1 Performance: performance when there are large-scale data in source and target domains.\\n2 Data Eﬃciency: performance when there are only small-scale data in source and target domains.\\n3 Modality Scalability: whether can adapt the model to various modalities, such as text, time series.\\n4 Task Scalability: whether can adapt the model to diﬀerent tasks, such as regression, detection.\\n5 Theory Guarantee: whether the generalization error of target domain can be bounded in adaptation.\\nDomain adaptation is closely related to pre-training and task adaptation. First, pre-\\ntraining can boost the transferability in domain adaptation, since pre-training will reduce\\nthe allowed hypothesis space and decrease the generalization bound on the target domain,\\nas mentioned in Section 3.2. Thus pre-training on the source domain also serves as the\\nﬁrst step in many domain adaptation methods, such as RegDA (Jiang et al., 2021). Pre-\\ntraining also provides some new solutions for domain adaptation. When there exists a large\\nunlabeled target domain, a feasible solution is to ﬁrst perform unsupervised pre-training on\\nthe target domain, and then ﬁne-tune with the labeled data on the source domain. This is\\nwidely adopted in cross-lingual adaptation (Lample and Conneau, 2019).\\nWhen using pre-trained models for domain adaptation, we will also encounter the prob-\\nlems in task adaptation, such as the catastrophic forgetting mentioned in 3.1.1. Thus, many\\ndomain adaptation methods babysit the learning rates to avoid catastrophic forgetting\\n(Long et al., 2015; Ganin and Lempitsky, 2015). Compared with task adaptation, domain\\nadaptation increases the restriction on the task space, where the task of the source domain\\nand that of the target domain must be the same. Due to this restriction, domain adaptation\\nhas a strict theoretical guarantee. But this restriction is sometimes hard to satisfy in prac-\\n44\\nTransferability in Deep Learning: A Survey\\ntice since we cannot ensure whether the category on the unlabeled target domain is exactly\\nthe same as the source domain (Busto and Gall, 2017). Therefore, real-world adaptation is\\noften a mix of task adaptation and domain adaptation. How to explore the transferability\\nin such a practical open-domain scenario is a problem to be solved.\\n4. Evaluation\\nEvaluation serves as a means for (1) measuring the performance of diﬀerent architectures,\\ndiﬀerent pre-training and adaptation methods, and (2) understanding the strengths and lim-\\nitations of diﬀerent methods. This section will elaborate on the evaluation of transferability,\\nwhich is deﬁned by the performance on the target task or domain. We believe that the eval-\\nuation of diﬀerent methods should be performed on large-scale datasets for a practical and\\nmeaningful comparison. Thus, in Section 4.1 we list some large-scale datasets that are suit-\\nable for evaluating transferability in deep learning. Since diﬀerent methods are often based\\non diﬀerent codebases, a fair comparison between them is rather diﬃcult. To ﬁll this blank,\\nin Section 4.2 we propose an open-source library, TLlib, to better evaluate transferability of\\ndiﬀerent methods in a uniﬁed framework. Finally, Section 4.3 provides several benchmarks\\nfor evaluating both the cross-task transferability and cross-domain transferability.\\n4.1 Datasets\\nTo evaluate the transferability in deep learning, we list several datasets that are large-scale\\nin the number of samples and categories, the richness of tasks, and the diversity of domains.\\nThe General Language Understanding Evaluation (GLUE) (Wang et al., 2019a) is one\\nof the most famous benchmarks in NLP. As shown in Table 6, it consists of nine sentence or\\nsentence-pair language understanding tasks, covering a diverse range of dataset sizes, text\\ngenres, and degrees of diﬃculty. It is widely used to evaluate the cross-task transferability\\nof diﬀerent pre-training and task adaptation methods.\\nTable 6: Descriptions and statistics of the GLUE datasets.\\nCorpus\\n#Train\\n#Test\\nMetrics\\nTask\\nDomain\\nCoLA\\n8.5k\\n1k\\nMatthews corr\\nacceptability\\nmisc.\\nSST-2\\n67k\\n1.8k\\nacc.\\nsentiment\\nmovie reviews\\nMRPC\\n3.7k\\n1.7k\\nacc./F1\\nparaphrase\\nnews\\nSTS-B\\n7k\\n1.4k\\nPearson/Spearman corr\\nsentence similarity\\nmisc.\\nQQP\\n364k\\n391k\\nacc./F1\\nparaphrase\\nsocial QA questions\\nMNLI\\n393k\\n20k\\nmatched acc./mismatched acc.\\nNLI\\nmisc.\\nQNLI\\n105k\\n5.4k\\nacc\\nQA/NLI\\nWikipedia\\nRTE\\n2.5k\\n3k\\nacc\\nNLI\\nnews, Wikipedia\\nWNLI\\n634\\n146\\nacc\\ncoreference/NLI\\nﬁction books\\nIn contrast, there is no common benchmark to evaluate the transferability of diﬀerent\\nmethods in computer vision. Table 7 lists some of the widely used vision datasets. Food-101,\\nCIFAR-10, CIFAR-100, SUN397, Stanford Cars, FGVC Aircraft, DTD, Oxford-III Pets,\\nCaltech-101, Oxford 102 Flowers are used to evaluate the transferability of diﬀerent archi-\\ntectures under task discrepancy (Kornblith et al., 2019). ImageNet-R(endition) (Hendrycks\\net al., 2021) and ImageNet-Sketch (Wang et al., 2019b) are two variants of the ImageNet,\\n45\\nJiang et al.\\nmainly used to evaluate the cross-domain transferablity of diﬀerent architectures and pre-\\ntraining methods. DomainNet (Peng et al., 2019) has multiple domains sharing the same\\ncategory space, and is used to evaluate the cross-domain transferablity of diﬀerent domain\\nadaptation methods under large domain shift.\\nTable 7: Descriptions and statistics of typical vision datasets.\\nDataset\\n#Train\\n#Test\\n#Classes\\nMetric\\nDomain\\nFood-101\\n75,750\\n25,250\\n101\\ntop-1\\nphotos and real world images\\nCIFAR-10\\n50,000\\n10,000\\n10\\ntop-1\\nphotos and real world images\\nCIFAR-100\\n50,000\\n10,000\\n100\\ntop-1\\nphotos and real world images\\nSUN397\\n19,850\\n19,850\\n397\\ntop-1\\nphotos and real world images\\nStanford Cars\\n8,144\\n8,041\\n196\\ntop-1\\nphotos and real world images\\nFGVC Aircraft\\n6,667\\n3,333\\n100\\nmean\\nper-class\\nphotos and real world images\\nDescribable\\nTextures (DTD)\\n3,760\\n1,880\\n47\\ntop-1\\nphotos and real world images\\nOxford-III Pets\\n3,680\\n3,369\\n37\\nmean\\nper-class\\nphotos and real world images\\nCaltech-101\\n3,060\\n6,084\\n102\\nmean\\nper-class\\nphotos and real world images\\nOxford 102 Flowers\\n2,040\\n6,149\\n102\\nmean\\nper-class\\nphotos and real world images\\nImageNet-R\\n-\\n30k\\n200\\ntop-1\\nart, cartoons, deviantart, graﬃti,\\nembroidery, graphics, origami,\\npaintings, patterns, plastic objects,\\nplush objects, sculptures, sketches,\\ntattoos, toys, and video games\\nImageNet-Sketch\\n-\\n50k\\n1000\\ntop-1\\nsketch\\nDomainNet-c\\n33,525\\n14,604\\n365\\ntop-1\\nclipart images\\nDomainNet-p\\n50,416\\n21,850\\n365\\ntop-1\\nartistic paintings\\nDomainNet-r\\n120,906\\n52,041\\n365\\ntop-1\\nphotos and real world images\\nDomainNet-s\\n48,212\\n20,916\\n365\\ntop-1\\nsketch\\n4.2 Library\\nTo make up for the lack of a uniﬁed codebase in some areas, we propose an open and ongoing\\nlibrary, TLlib. This library implements many representative adaptation algorithms in a uni-\\nﬁed way, allowing quantitative, fair, reproducible comparisons between diﬀerent algorithms\\nand promoting seamless integration of diﬀerent pre-training or adaptation methods.\\nLibrary Usage.\\nFirst, we give a short description of how to use TLlib using DANN as an\\ninstance. In the original implementation of DANN, the domain adversarial loss, domain dis-\\ncriminator, feature generator, and classiﬁer are tightly coupled together in one nn.Module,\\nwhich causes the diﬃculty of reuse, e.g., the entire algorithm needs re-implementation when\\nthe input data is changed from image to text. Yet in this case, the domain adversarial loss\\nand the domain discriminator remain unchanged and shall be reused. Therefore, in TLlib,\\nmodels and loss functions are decoupled. When using DANN for any case, users need only\\nto initialize a domain discriminator and pass it to the domain adversarial loss module, and\\nthen use this module in the same way as the cross-entropy loss module deﬁned in PyTorch\\n(example code below). TLlib provides friendly and coherent APIs for supported algorithms.\\nDetailed usages of these algorithms can be found at the documentation.\\n46\\nTransferability in Deep Learning: A Survey\\n>>> # define the domain discriminator\\n>>> from dalib.modules.domain_discriminator import DomainDiscriminator\\n>>> discriminator = DomainDiscriminator(in_feature=1024, hidden_size=1024)\\n>>> # define the domain adversarial loss module\\n>>> from dalib.adptation.dann import DomainAdversarialLoss\\n>>> dann = DomainAdversarialLoss(discriminator, reduction=’mean’)\\n>>> # features from the source and target domain\\n>>> f_s, f_t = torch.randn(20, 1024), torch.randn(20, 1024)\\n>>> # calculate the final loss\\n>>> loss = dann(f_s, f_t)\\nDesign Philosophy.\\nTLlib is designed to be extendible by researchers and simple for\\npractitioners. Currently, there are mainly two types of algorithm implementations. One is to\\nencapsulate each algorithm in a Trainer, whose typical representative is PyTorch-Lighting.\\nUsers only need to feed the training data to it and do not need to care about the speciﬁc\\ntraining process. Another strategy is to encapsulate the core loss function in each algo-\\nrithm, and users need to implement the complete training process by themselves. A typical\\nrepresentative is PyTorch (Paszke et al., 2019). Although the former method is easier to\\nuse, it is less extendible. Since it is often necessary to adjust the training process in diﬀerent\\ntransfer learning scenarios, TLlib adopts the latter method for better extendibility. We try\\nour best to make TLlib easy to start with, e.g., we support the automatic download of most\\ncommon transfer learning datasets so that users do not need to spend time on data prepa-\\nration. Our code is in PyTorch-style and we provide training examples of diﬀerent transfer\\nalgorithms in diﬀerent scenarios, which allows users to quickly adapt to TLlib as long as\\nthey have learned PyTorch before. For more convenient algorithm selection, we provide a\\ncomprehensive benchmark among all those libraries. For faster algorithm reproduction, we\\nprovide training scripts for all the results in the benchmark.\\nTLlib is released under the MIT License and available at https://github.com/thuml/\\nTransfer-Learning-Library. Documentation and tutorials are available on its website.\\n4.3 Benchmark\\nThis section will present a benchmark of typical pre-training and adaptation methods on\\nthe large-scale datasets described in Section 4.1. Since such a benchmark is missing in the\\nliterature, we produce the results using the open library TLlib implemented in Section 4.2.\\n4.3.1 Pre-Training\\nProtocols.\\nThe transferability of pre-training methods is evaluated on the target task,\\nwhere the adaptation process and data augmentations are kept the same for fair comparison.\\nHyper-parameters in adaptation are selected by the performance of target validation data.\\nResults.\\nFor the pre-training methods, the transferability cross diﬀerent tasks and across\\ndiﬀerent domains should be evaluated. Tables 8 and 9 list the performance on various down-\\nstream tasks with diﬀerent architectures and pre-training tasks. It can be concluded that\\narchitectures and pre-training methods have a great impact on the cross-task transferability\\nof deep networks. Table 10 lists the performance on ImageNet-Sketch and ImageNet-R with\\n47\\nJiang et al.\\ndiﬀerent architectures and pre-training tasks. Architectures and pre-training strategies also\\ngreatly inﬂuence the cross-domain transferability in deep learning.\\nTable 8: Cross-task transferability benchmark. Results of diﬀerent architectures and pre-\\ntraining methods are reported from the GLUE leaderboard. BiLSTM+ELMo (Peters et al.,\\n2018) serves as the baseline. GPT (Radford et al., 2018), BERTLarge (Devlin et al., 2019), T5\\n(Raﬀel et al., 2020), and ERNIE (Sun et al., 2019b) have diﬀerent architectures. RoBERTa\\n(Liu et al., 2019c), XLM (Lample and Conneau, 2019), and SpanBERT (Joshi et al., 2020)\\nshare the same architecture as BERTLarge but employ diﬀerent pre-training methods.\\nModel\\nCoLA\\nSST-2\\nMRPC\\nSTS-B\\nQQP\\nMNLIm\\nMNLImm\\nQNLI\\nRTE\\nAvg\\nHuman\\nBaselines\\n66.4\\n97.8\\n86.3\\n92.7\\n80.4\\n92\\n92.8\\n91.2\\n93.6\\n88.1\\nBiLSTM\\n+ELMo\\n36.0\\n90.4\\n84.9\\n73.3\\n64.8\\n76.4\\n76.1\\n79.9\\n56.8\\n71.0\\nGPT\\n45.4\\n91.3\\n82.3\\n80.0\\n70.3\\n82.1\\n81.4\\n88.1\\n56.0\\n75.2\\nBERTLarge\\n60.5\\n94.9\\n89.3\\n86.5\\n72.1\\n86.7\\n85.9\\n92.7\\n70.1\\n82.1\\nT5\\n71.6\\n97.5\\n92.8\\n93.1\\n90.6\\n92.2\\n91.9\\n96.9\\n92.8\\n91.0\\nERNIE\\n75.5\\n97.8\\n93.9\\n93.0\\n90.9\\n92.3\\n91.7\\n97.3\\n92.6\\n91.7\\nRoBERTa\\n67.8\\n96.7\\n92.3\\n92.2\\n90.2\\n90.8\\n90.2\\n95.4\\n88.2\\n89.3\\nXLM\\n62.9\\n95.6\\n90.7\\n88.8\\n89.8\\n89.1\\n88.5\\n94.0\\n76.0\\n86.2\\nSpanBERT\\n64.3\\n94.8\\n90.9\\n89.9\\n89.5\\n88.1\\n87.7\\n94.3\\n79.0\\n86.5\\nTable 9: Cross-task transferability benchmark. Results on image recognition using diﬀerent\\npre-training methods, including SimCLR (Chen et al., 2020) and BYOL (Grill et al., 2020).\\nModel\\nPre-Training Food CIFAR10 CIFAR100 SUN397 Cars Aircraft DTD Pets Caltech101 Flowers Avg\\nResNet50\\nRandom Init 86.9\\n95.9\\n80.2\\n53.6\\n91.4\\n85.9\\n64.8\\n81.5\\n72.6\\n92.0\\n80.5\\nSimCLR\\n88.2\\n97.7\\n85.9\\n63.5\\n91.3\\n88.1\\n73.2\\n89.2\\n92.1\\n97.0\\n86.6\\nBYOL\\n88.5\\n97.8\\n86.1\\n63.7\\n91.6\\n88.1\\n76.2\\n91.7\\n93.8\\n97.0\\n87.5\\nResNet50\\nSupervised\\nPre-Trained\\non ImageNet\\n87.8\\n96.8\\n84.5\\n64.7\\n91.7\\n86.6\\n75.2\\n92.5\\n91.8\\n97.5\\n86.9\\nResNet101\\n87.6\\n97.7\\n87.0\\n64.8\\n91.7\\n85.6\\n75.4\\n94.0\\n93.1\\n97.9\\n87.5\\nResNet152\\n87.6\\n97.9\\n87.6\\n66.0\\n92.0\\n85.3\\n74.9\\n94.5\\n93.2\\n97.4\\n87.6\\nTable 10: Cross-domain transferability benchmark. Results are reported from the PyTorch-\\nImage-Models (Wightman, 2019) on ImageNet using diﬀerent architectures and pre-training\\nmethods. SSP refers to semi-supervised pre-training on YFCC100M (Yalniz et al., 2019).\\nWSP refers to weakly supervised pre-training on IG-1B-Targeted (Mahajan et al., 2018).\\nModel\\nPre-Training\\nParam Count\\nImageNet-Sketch\\nImageNetR\\ntop-1\\ntop-5\\ntop-1\\ntop-5\\nResNet50\\nStandard\\nPre-Trained\\non ImageNet\\n25.6\\n29.6\\n46.8\\n40.4\\n54.7\\nResNet152d\\n60.2\\n37.9\\n58.4\\n49.3\\n64.4\\nViTlarge, patch16\\n304.3\\n51.8\\n73.7\\n64.3\\n76.2\\nResNext101 32x8d\\nStandard\\n88.8\\n29.4\\n48.5\\n42.6\\n58.3\\nSSP\\n34.1\\n55.6\\n49.2\\n65.5\\nWSP\\n54.9\\n77.5\\n75.9\\n86.2\\nSSP+WSP\\n56.4\\n78.9\\n75.6\\n87.1\\n48\\nTransferability in Deep Learning: A Survey\\n4.3.2 Task Adaptation\\nProtocols.\\nWe follow the common practice in the community as described in Kornblith\\net al. (2019). Training iterations and data augmentations are kept the same for diﬀerent\\ntask adaptation methods for a fair comparison. Hyper-parameters, such as learning rate\\nand weight decay, of each method are selected by the performance on target validation data.\\nResults.\\nWe mainly investigate the cross-task transferability between diﬀerent task adap-\\ntation methods. Tables 11 and 12 compare the performance of downstream tasks with dif-\\nferent task adaptation methods. Note that previous methods usually only report results\\non individual datasets, such as Aircraft and Stanford Cars, where regularization tuning\\nperforms better than vanilla ﬁne-tuning by a large margin. But the average improvements\\nbrought by diﬀerent task adaptation methods on a large number of datasets are still limited.\\nThus, we can conclude that the eﬀectiveness of diﬀerent task adaptation algorithms largely\\ndepends on the relatedness between the target task and the pre-training task.\\nTable 11: Cross-task transferability benchmark. GLUE performance with diﬀerent task\\nadaptation methods, including SMART (Jiang et al., 2020), Adapter-Tuning (Houlsby et al.,\\n2019) and DiﬀPruning (Guo et al., 2021). Results are reported from their original papers.\\nModel\\nTask\\nAdaptation\\nNew Params\\nPer Task\\nCoLA SST-2 MRPC STS-B QQP MNLIm MNLImm QNLI RTE Avg\\nRoBERTa\\nvanilla\\n100%\\n67.8\\n96.7\\n92.3\\n92.2\\n90.2\\n90.8\\n90.2\\n95.4\\n88.2 89.3\\nSMART\\n100%\\n65.1\\n97.5\\n93.7\\n92.9\\n90.1\\n91.0\\n90.8\\n95.4\\n87.9 89.4\\nBERTLarge\\nvanilla\\n100%\\n60.5\\n94.9\\n89.3\\n86.5\\n72.1\\n86.7\\n85.9\\n92.7\\n70.1 82.1\\nAdapter\\n2.10%\\n59.2\\n94.3\\n88.7\\n87.3\\n89.4\\n85.4\\n85.0\\n92.4\\n71.6 83.7\\nDiﬀPruning\\n0.50%\\n61.1\\n94.1\\n89.7\\n86.0\\n-\\n86.4\\n86.0\\n93.3\\n70.6\\n-\\nTable 12: Cross-task transferability benchmark. Accuracy (%) on image classiﬁcation with\\ndiﬀerent task adaptation methods: LWF (Li and Hoiem, 2018), DELTA (Li et al., 2019),\\nBSS (Chen et al., 2019b), Bi-Tuning (Zhong et al., 2020). Results are reproduced by TLlib.\\nTask\\nAdaptation\\nFood\\nCIFAR10\\nCIFAR100\\nSUN397\\nCars\\nAircraft\\nDTD\\nPets\\nCaltech101\\nFlowers\\nAvg\\nResNet50\\n85.1\\n96.9\\n84.1\\n80.7\\n87.8\\n80.1\\n74.4\\n93.2\\n92.9\\n96.5\\n87.2\\nLWF\\n83.9\\n96.5\\n83.6\\n79.5\\n87.4\\n82.2\\n76.3\\n94.0\\n91.7\\n97.1\\n87.2\\nDELTA\\n83.8\\n95.9\\n83.7\\n73.6\\n88.1\\n82.3\\n75.6\\n94.2\\n92.5\\n97.0\\n86.7\\nBSS\\n85.0\\n96.6\\n84.2\\n80.4\\n88.4\\n81.8\\n74.3\\n93.3\\n93.0\\n96.6\\n87.4\\nBi-Tuning\\n85.7\\n97.1\\n84.3\\n80.7\\n90.3\\n84.8\\n74.6\\n93.5\\n93.4\\n97.5\\n88.2\\n4.3.3 Domain Adaptation\\nProtocols.\\nWe follow the standard protocols for unsupervised domain adaptation (Long\\net al., 2015; Ganin and Lempitsky, 2015).\\nTraining iterations and data augmentations\\nare kept the same for diﬀerent methods for a fair comparison. For each method, hyper-\\nparameters are selected on one task and then kept the same for all other tasks, requiring the\\nhyper-parameters of each method to transfer across tasks. This selection strategy is more\\n49\\nJiang et al.\\nexecutable than the importance-weighted cross-validation (Sugiyama et al., 2007) and can\\nbe applied to various practical applications, thus it is widely adopted by many competitions.\\nResults.\\nTables 13 and 14 give the classiﬁcation performance of diﬀerent domain adapta-\\ntion methods on DomainNet and ImageNet. We ﬁnd that many state-of-the-art methods on\\nsmall datasets do not perform well on large-scale datasets. This ﬁeld shall pay more atten-\\ntion to improving the cross-domain transferability of deep models on large-scale datasets.\\nTable 13: Cross-domain transferability benchmark. Accuracy (%) for unsupervised domain\\nadaptation on DomainNet. Results are reproduced from TLlib.\\nDomainNet\\nc\\x01p\\nc\\x01r\\nc\\x01s\\np\\x01c\\np\\x01r\\np\\x01s\\nr\\x01c\\nr\\x01p\\nr\\x01s\\ns\\x01c\\ns\\x01p\\ns\\x01r\\nAvg\\nResNet101\\n32.7\\n50.6\\n39.4\\n41.1\\n56.8\\n35.0\\n48.6\\n48.8\\n36.1\\n49.0\\n34.8\\n46.1\\n43.3\\nDAN (2015)\\n38.8\\n55.2\\n43.9\\n45.9\\n59.0\\n40.8\\n50.8\\n49.8\\n38.9\\n56.1\\n45.9\\n55.5\\n48.4\\nDANN (2016)\\n37.9\\n54.3\\n44.4\\n41.7\\n55.6\\n36.8\\n50.7\\n50.8\\n40.1\\n55.0\\n45.0\\n54.5\\n47.2\\nADDA (2017)\\n38.4\\n54.1\\n44.1\\n43.5\\n56.7\\n39.2\\n52.8\\n51.3\\n40.9\\n55.0\\n45.4\\n54.5\\n48.0\\nJAN (2017)\\n40.5\\n56.7\\n45.1\\n47.2\\n59.9\\n43.0\\n54.2\\n52.6\\n41.9\\n56.6\\n46.2\\n55.5\\n50.0\\nCDAN (2018)\\n40.4\\n56.8\\n46.1\\n45.1\\n58.4\\n40.5\\n55.6\\n53.6\\n43.0\\n57.2\\n46.4\\n55.7\\n49.9\\nMCD (2018)\\n37.5\\n52.9\\n44.0\\n44.6\\n54.5\\n41.6\\n52.0\\n51.5\\n39.7\\n55.5\\n44.6\\n52.0\\n47.5\\nMDD (2019c)\\n42.9\\n59.5\\n47.5\\n48.6\\n59.4\\n42.6\\n58.3\\n53.7\\n46.2\\n58.7\\n46.5\\n57.7\\n51.8\\nTable 14: Cross-domain transferability benchmark. Accuracy (%) for unsupervised domain\\nadaptation on ImageNet-scale datasets. Results are reproduced from TLlib.\\nTask\\nImageNet\\x01ImageNet-R\\nImageNet\\x01ImageNet-Sketch\\nModel\\nResNet50\\nig resnext101 32x8d\\nSource Only\\n35.6\\n54.9\\nDAN (Long et al., 2015)\\n39.8\\n55.7\\nDANN (Ganin et al., 2016)\\n52.7\\n56.5\\nJAN (Long et al., 2017)\\n41.7\\n55.7\\nCDAN (Long et al., 2018)\\n53.9\\n58.2\\nMCD (Saito et al., 2018)\\n46.7\\n55.0\\nMDD (Zhang et al., 2019c)\\n56.2\\n62.4\\n5. Conclusion\\nIn this paper, we investigate how to acquire and apply transferability in the whole lifecycle\\nof deep learning. In the pre-training section, we focus on how to improve the transferability\\nof the pre-trained models by designing architecture, pre-training task, and training strategy.\\nIn the task adaptation section, we discuss how to better preserve and utilize the transferable\\nknowledge to improve the performance of target tasks. In the domain adaptation section, we\\nillustrate how to bridge the domain gap to increase the transferability for real applications.\\nThis survey connects many isolated areas with their relation to transferability and provides\\na uniﬁed perspective to explore transferability in deep learning. We expect this study will\\nattract the community’s attention to the fundamental role of transferability in deep learning.\\nAcknowledgments\\nThis work was supported by the NSFC Grants (62022050 and 62021002), the Beijing Nova\\nProgram (Z201100006820041), and the BNRist Innovation Fund (BNR2021RC01002).\\n50\\nTransferability in Deep Learning: A Survey\\nReferences\\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the\\nlimits of large scale pre-training. In ICLR, 2022.\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains\\nthe eﬀectiveness of language model ﬁne-tuning. In ACL, 2021.\\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Bat-\\ntenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al.\\nDeep speech 2: End-to-end speech recognition in english and mandarin. In ICML, 2016.\\nMartin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz.\\nInvariant risk\\nminimization. arXiv preprint arXiv:1907.02893, 2019.\\nSanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and\\nequilibrium in generative adversarial nets (GANs). In ICML, 2017.\\nPeter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk\\nbounds and structural results. In JMLR, 2002.\\nIz Beltagy, Kyle Lo, and Arman Cohan. Scibert: Pretrained language model for scientiﬁc\\ntext. In EMNLP, 2019.\\nS. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory\\nof learning from diﬀerent domains. Machine Learning, 79, page 151–175, 2010a.\\nShai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of repre-\\nsentations for domain adaptation. In NeurIPS, 2006.\\nShai Ben-David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain\\nadaptation. In AISTATS, pages 129–136, 2010b.\\nYoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In\\nICML workshop, 2012.\\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise\\ntraining of deep networks. In NeurIPS, 2007.\\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review\\nand new perspectives. TPAMI, 35(8):1798–1828, 2013.\\nYoshua Bengio, Yann Lecun, and Geoﬀrey Hinton. Deep learning for ai. Communications\\nof the ACM, 64(7):58–65, 2021.\\nKonstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Du-\\nmitru Erhan. Domain separation networks. In NeurIPS, 2016.\\nKonstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Kr-\\nishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks.\\nIn CVPR, 2017.\\n51\\nJiang et al.\\nKonstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal\\nKalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine,\\nand Vincent Vanhoucke. Using simulation and domain adaptation to improve eﬃciency\\nof deep robotic grasping. In ICRA, 2018.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\\nLanguage models are few-shot learners. In NeurIPS, 2020.\\nPau Panareda Busto and Juergen Gall. Open set domain adaptation. In ICCV, 2017.\\nRich Caruana. Multitask learning. Technical report, 1997.\\nOlivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien.\\nSemi-Supervised Learning\\n(Adaptive Computation and Machine Learning). The MIT Press, 2006. ISBN 0262033585.\\nMinmin Chen, Zhixiang Xu, Kilian Q. Weinberger, and Fei Sha. Marginalized denoising\\nautoencoders for domain adaptation. In ICML, 2012.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple frame-\\nwork for contrastive learning of visual representations. In ICML, 2020.\\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A\\ncloser look at few-shot classiﬁcation. In ICLR, 2019a.\\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR,\\n2021.\\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\\nvision transformers. arXiv preprint arXiv:2104.02057, 2021a.\\nXinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and Jianmin Wang. Catastrophic\\nforgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning. In\\nNeurIPS, 2019b.\\nXinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang.\\nTransferability vs.\\ndiscriminability: Batch spectral penalization for adversarial domain adaptation. In ICML,\\n2019c.\\nXinyang Chen, Sinan Wang, Jianmin Wang, and Mingsheng Long. Representation subspace\\ndistance for domain adaptation regression. In ICML, 2021b.\\nYuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive\\nfaster R-CNN for object detection in the wild. In CVPR, 2018.\\nKyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi\\nBougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using\\nRNN encoder–decoder for statistical machine translation. In EMNLP, 2014.\\n52\\nTransferability in Deep Learning: A Survey\\nAlexandra Chronopoulou, Christos Baziotis, and Alexandros Potamianos. An embarrass-\\ningly simple approach for transfer learning from pretrained language models. In NAACL,\\n2019.\\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman,\\nHolger Schwenk, and Veselin Stoyanov. XNLI: evaluating cross-lingual sentence repre-\\nsentations. In EMNLP, 2018.\\nNicolas Courty, R´emi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distri-\\nbution optimal transportation for domain adaptation. In NeurIPS, 2017.\\nYin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale ﬁne-\\ngrained categorization and domain-speciﬁc transfer learning. In CVPR, pages 4109–4118,\\n2018.\\nBharath Bhushan Damodaran, Benjamin Kellenberger, R´emi Flamary, Devis Tuia, and\\nNicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised\\ndomain adaptation. In ECCV, 2018.\\nMatthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis,\\nGreg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting\\nin classiﬁcation tasks. TPAMI, page 1–20, 2021.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\\nscale hierarchical image database. In CVPR, 2009.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training\\nof deep bidirectional transformers for language understanding. In NAACL, 2019.\\nCarl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation\\nlearning by context prediction. In ICCV, 2015.\\nJeﬀDonahue, Yangqing Jia, Oriol Vinyals, Judy Hoﬀman, Ning Zhang, Eric Tzeng, and\\nTrevor Darrell. Decaf: A deep convolutional activation feature for generic visual recog-\\nnition. In ICML, 2014.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\\nGelly, Jakob Uszkoreit, and Neil Houlsby.\\nAn Image is Worth 16x16 Words: Trans-\\nformers for Image Recognition at Scale. In ICLR, 2021.\\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\\nModel-agnostic meta-learning for fast\\nadaptation of deep networks. In ICML, 2017.\\nGeoﬀrey French, Michal Mackiewicz, and Mark H. Fisher.\\nSelf-ensembling for domain\\nadaptation. In ICLR, 2018.\\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropaga-\\ntion. In ICML, 2015.\\n53\\nJiang et al.\\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle,\\nFran¸cois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of\\nneural networks. JMLR, 17(59):1–35, 2016.\\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In ICLR,\\n2018.\\nYixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label reﬁnery\\nfor unsupervised domain adaptation on person re-identiﬁcation. In ICLR, 2020.\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann,\\nand Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape\\nbias improves accuracy and robustness. In ICLR, 2019.\\nRoss Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\\nfor accurate object detection and semantic segmentation. In CVPR, 2014.\\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\\nDomain adaptation for large-scale\\nsentiment classiﬁcation: A deep learning approach. In ICML, 2011.\\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsuper-\\nvised domain adaptation. In CVPR, 2012.\\nBoqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Dis-\\ncriminatively learning domain-invariant features for unsupervised domain adaptation. In\\nICML, 2013.\\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In NeurIPS,\\n2014.\\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy.\\nExplaining and harnessing\\nadversarial examples. 2015.\\nAnirudh Goyal, Alex Lamb, Jordan Hoﬀmann, Shagun Sodhani, Sergey Levine, Yoshua\\nBengio, and Bernhard Sch¨olkopf. Recurrent independent mechanisms. In ICLR, 2021.\\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexan-\\nder Smola. A kernel two-sample test. JMLR, 13(25):723–773, 2012a.\\nArthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimil-\\niano Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for\\nlarge-scale two-sample tests. In NeurIPS, 2012b.\\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap\\nyour own latent - a new approach to self-supervised learning. In NeurIPS, 2020.\\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR,\\n2021.\\n54\\nTransferability in Deep Learning: A Survey\\nDemi Guo, Alexander Rush, and Yoon Kim. Parameter-eﬃcient transfer learning with diﬀ\\npruning. In ACL, 2021.\\nYunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio\\nFeris. Spottune: transfer learning through adaptive ﬁne-tuning. In CVPR, 2019.\\nSuchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug\\nDowney, and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains\\nand tasks. In ACL, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In CVPR, 2016.\\nKaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask r-cnn. In ICCV,\\n2017.\\nKaiming He, Ross Girshick, and Piotr Doll´ar. Rethinking imagenet pre-training. In ICCV,\\n2019.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast\\nfor unsupervised visual representation learning. In CVPR, 2020.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked\\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan\\nDorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Stein-\\nhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-\\ndistribution generalization. ICCV, 2021.\\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,\\nAdam Trischler, and Yoshua Bengio. Learning deep representations by mutual informa-\\ntion estimation and maximization. In ICLR, 2019.\\nJudy Hoﬀman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level\\nadversarial and constraint-based adaptation. 2016.\\nJudy Hoﬀman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei\\nEfros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In\\nICML, 2018.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Larous-\\nsilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-eﬃcient transfer\\nlearning for NLP. In ICML, 2019.\\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In ACL, 2018.\\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and\\nJure Leskovec. Pre-training graph neural networks. In ICLR, 2020.\\n55\\nJiang et al.\\nJiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch¨olkopf, and Alex Smola.\\nCorrecting sample selection bias by unlabeled data. In NeurIPS, 2007.\\nSergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network train-\\ning by reducing internal covariate shift. In ICML, 2015.\\nYunhun Jang, Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. Learning what and where\\nto transfer. In ICML, 2019.\\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao.\\nSMART: robust and eﬃcient ﬁne-tuning for pre-trained natural language models through\\nprincipled regularized optimization. In ACL, 2020.\\nJunguang Jiang, Yifei Ji, Ximei Wang, Yufeng Liu, Jianmin Wang, and Mingsheng Long.\\nRegressive domain adaptation for unsupervised keypoint detection. In CVPR, 2021.\\nJunguang Jiang, Baixu Chen, Jianmin Wang, and Mingsheng Long. Decoupled adaptation\\nfor cross-domain object detection. In ICLR, 2022.\\nYing Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for\\nversatile domain adaptation. In ECCV, 2020.\\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.\\nSpanBERT: Improving pre-training by representing and predicting spans. In TACL, 2020.\\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation\\nnetwork for unsupervised domain adaptation. In CVPR, 2019.\\nTaekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify\\nand match: A domain adaptive representation learning paradigm for object detection. In\\nCVPR, 2019.\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-\\nBarwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114\\n(13):3521–3526, 2017.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\\nGelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In\\nECCV, 2020.\\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer\\nbetter? In CVPR, 2019.\\nZhi Kou, Kaichao You, Mingsheng Long, and Jianmin Wang. Stochastic normalization. In\\nNeurIPS, 2020.\\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. Imagenet classiﬁcation with deep\\nconvolutional neural networks. In NeurIPS, 2012.\\n56\\nTransferability in Deep Learning: A Survey\\nGuillaume Lample and Alexis Conneau.\\nCross-lingual language model pretraining.\\nIn\\nNeurIPS, 2019.\\nGuillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine\\ntranslation using monolingual corpora only. In ICLR, 2017.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\\nRadu Soricut. Albert: A lite bert for self-supervised learning of language representations.\\nIn ICLR, 2020.\\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\\nDeep learning.\\nNature, 521(7553):\\n436–444, 2015.\\nChen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasser-\\nstein discrepancy for unsupervised domain adaptation. In CVPR, 2019.\\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Eﬀective regularization to\\nﬁnetune large-scale pretrained language models. In ICLR, 2020a.\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So,\\nand Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for\\nbiomedical text mining. Bioinformatics, 36(4):1234–1240, 2020b.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising Sequence-to-\\nSequence Pre-training for Natural Language Generation, Translation, and Comprehen-\\nsion. In ACL, 2020.\\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic\\ndimension of objective landscapes. In ICLR, 2018.\\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for genera-\\ntion. In ACL, 2021.\\nXingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu Chen, and Jun\\nHuan. Delta: Deep learning transfer using feature map with attention for convolutional\\nnetworks. In ICLR, 2019.\\nYanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch\\nnormalization for practical domain adaptation. In ICLR Workshop, 2017.\\nZhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947,\\n2018.\\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data?\\nsource hypothesis transfer for unsupervised domain adaptation. In ICML, 2020.\\nHong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation.\\nIn NeurIPS, 2021a.\\nMing-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In NeurIPS, 2016.\\n57\\nJiang et al.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neu-\\nbig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural\\nlanguage processing, 2021b.\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural\\nnetworks for natural language understanding. In ACL, 2019a.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized\\nbert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized\\nBERT pretraining approach. 2019c.\\nMingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer\\nfeature learning with joint distribution adaptation. In ICCV, 2013.\\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable\\nfeatures with deep adaptation networks. In ICML, 2015.\\nMingsheng Long, Jianmin Wang, and Michael I. Jordan. Unsupervised domain adaptation\\nwith residual transfer networks. In NeurIPS, 2016.\\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning\\nwith joint adaptation networks. In ICML, 2017.\\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adver-\\nsarial domain adaptation. In NeurIPS, 2018.\\nMingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transfer-\\nable representation learning with deep adaptation networks. TPAMI, 41(12):3071–3085,\\n2019.\\nChristos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks\\nthrough l 0 regularization. In ICLR, 2018.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yix-\\nuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly\\nsupervised pretraining. In ECCV, 2018.\\nArun Mallya and Svetlana Lazebnik. Piggyback: Adding multiple tasks to a single, ﬁxed\\nnetwork by learning to mask. In ECCV, 2018.\\nYishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning\\nbounds and algorithms. In COLT, 2009.\\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In ICML, 2017.\\n58\\nTransferability in Deep Learning: A Survey\\nJiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V Le, and Ruom-\\ning Pang.\\nDomain adaptive transfer learning with specialist models.\\narXiv preprint\\narXiv:1811.07056, 2018.\\nCuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau.\\nLeep: A new\\nmeasure to evaluate transferability of learned representations. In ICML, 2020.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\\npredictive coding. NeurIPS, 2019.\\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. TKDE, pages 1345–1359,\\n2010.\\nSinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation\\nvia transfer component analysis. TNNLS, pages 199–210, 2011.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-\\ndreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank\\nChilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\\nimperative style, high-performance deep learning library. In NeurIPS, 2019.\\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment\\nmatching for multi-source domain adaptation. In ICCV, 2019.\\nJonas Peters, Peter B¨uhlmann, and Nicolai Meinshausen. Causal inference by using invari-\\nant prediction: identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical\\nSociety. Series B (Statistical Methodology), pages 947–1012, 2016.\\nJonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of causal inference:\\nfoundations and learning algorithms. The MIT Press, 2017.\\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.\\nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? In\\nACL, 2019.\\nJ. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in\\nmachine learning. The MIT Press, 2009.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. Technical report, OpenAI, 2018.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervision. In ICML, 2021.\\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning\\nwith a uniﬁed text-to-text transformer. JMLR, 21(140):1–67, 2020.\\n59\\nJiang et al.\\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.\\nRapid learning or\\nfeature reuse? towards understanding the eﬀectiveness of maml. In ICLR, 2020.\\nMaithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Under-\\nstanding transfer learning for medical imaging. In NeurIPS, 2019.\\nS-A Rebuﬃ, H. Bilen, and A. Vedaldi.\\nLearning multiple visual domains with residual\\nadapters. In NeurIPS, 2017.\\nIevgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Youn`es Bennani. A\\nsurvey on domain adaptation theory: learning bounds and theoretical guarantees, 2020.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\\nobject detection with region proposal networks. In NeurIPS, 2015.\\nMichael T. Rosenstein. To transfer or not to transfer. In NeurIPS, 2005.\\nEvgenia. Rusak, Steﬀen Schneider, Peter Gehler, Oliver Bringmann, Wieland Brendel, and\\nMatthias Bethge. Adapting imagenet-scale models to complex distribution shifts with\\nself-learning. arXiv preprint arXiv:2104.12928, 2021.\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhi-\\nheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,\\nand Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211–\\n252, 2015.\\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon\\nOsindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In ICLR,\\n2019.\\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classi-\\nﬁer discrepancy for unsupervised domain adaptation. In CVPR, 2018.\\nKuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distri-\\nbution alignment for adaptive object detection. In CVPR, 2019.\\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do\\nadversarially robust imagenet models transfer better? In NeurIPS, 2020.\\nSwami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, and Rama Chellappa. Gener-\\nate to adapt: Aligning domains using generative adversarial networks. In CVPR, 2018.\\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lilli-\\ncrap. Meta-learning with memory-augmented neural networks. In ICML, 2016.\\nTimo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few-shot text classiﬁcation\\nand natural language inference. In EACL, 2020.\\nJ¨urgen Schmidhuber. Evolutionary principles in self-referential learning. PhD thesis, Tech-\\nnische Universit¨at M¨unchen, 1987.\\n60\\nTransferability in Deep Learning: A Survey\\nBernhard Sch¨olkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris\\nMooij. On causal and anticausal learning. In ICML, 2012.\\nBernhard Sch¨olkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalch-\\nbrenner, Anirudh Goyal, and Yoshua Bengio.\\nToward causal representation learning.\\nProceedings of the IEEE, 109(5):612–634, 2021.\\nAndrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim\\nGreen, Chongli Qin, Augustin ˇZ´ıdek, Alexander WR Nelson, Alex Bridgland, et al. Im-\\nproved protein structure prediction using potentials from deep learning.\\nNature, 577\\n(7792):706–710, 2020.\\nPierre Sermanet, David Eigen, Xiang Zhang, Micha¨el Mathieu, Rob Fergus, and Yann\\nLeCun. Overfeat: Integrated recognition, localization and detection using convolutional\\nnetworks. arXiv preprint arXiv:1312.6229, 2013.\\nAshish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russell\\nWebb. Learning from simulated and unsupervised images through adversarial training.\\nIn CVPR, 2017.\\nRui Shu, Hung H. Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsuper-\\nvised domain adaptation. In ICLR, 2018.\\nYang Shu, Zhangjie Cao, Jinghan Gao, Jianmin Wang, and Mingsheng Long. Omni-training\\nfor data-eﬃcient deep learning. arXiv preprint arXiv:2110.07510, 2021a.\\nYang Shu, Zhi Kou, Zhangjie Cao, Jianmin Wang, and Mingsheng Long.\\nZoo-tuning:\\nAdaptive transfer from a zoo of models. In ICML, 2021b.\\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc\\nLanctot, et al. Mastering the game of go with deep neural networks and tree search.\\nNature, 529(7587):484–489, 2016.\\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learn-\\ning. In NeurIPS, 2017.\\nBharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Sch¨olkopf, and\\nGert R. G. Lanckriet. Hilbert space embeddings and metrics on probability measures.\\nJMLR, 2010.\\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert M¨uller. Covariate shift adapta-\\ntion by importance weighted cross validation. JMLR, 8(35):985–1005, 2007.\\nMasashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki\\nKawanabe.\\nDirect importance estimation with model selection and its application to\\ncovariate shift adaptation. In NeurIPS, 2008.\\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adap-\\ntation. In ECCV, 2016.\\n61\\nJiang et al.\\nQianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for\\nfew-shot learning. In CVPR, 2019a.\\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danx-\\niang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge\\nintegration. arXiv preprint arXiv:1904.09223, 2019b.\\nYaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation.\\nIn ICLR, 2017.\\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media,\\n1998.\\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV,\\n2020.\\nLisa Torrey and Jude Shavlik. Transfer learning. 2010.\\nYi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and\\nManmohan Chandraker. Learning to adapt structured output space for semantic seg-\\nmentation. In CVPR, 2018.\\nEric Tzeng, Judy Hoﬀman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain\\nconfusion: Maximizing for domain invariance. 2014.\\nEric Tzeng, Judy Hoﬀman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer\\nacross domains and tasks. In ICCV, pages 4068–4076, 2015.\\nEric Tzeng, Judy Hoﬀman, Kate Saenko, and Trevor Darrell. Adversarial discriminative\\ndomain adaptation. In CVPR, 2017.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS,\\n2017.\\nPetar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and\\nR Devon Hjelm. Deep graph infomax. In ICLR, 2019.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting\\nand composing robust features with denoising autoencoders. In ICML, 2008.\\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks\\nfor one shot learning. In NeurIPS, 2016.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik,\\nJunyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al.\\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575\\n(7782):350–354, 2019.\\n62\\nTransferability in Deep Learning: A Survey\\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez. Advent:\\nAdversarial entropy minimization for domain adaptation in semantic segmentation. In\\nCVPR, 2019.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bow-\\nman. GLUE: A multi-task benchmark and analysis platform for natural language under-\\nstanding. In ICLR, 2019a.\\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.\\nLearning robust global\\nrepresentations by penalizing local predictive power. In NeurIPS, 2019b.\\nXimei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Transferable\\nnormalization: Towards improving transferability of deep neural networks. In NeurIPS,\\n2019c.\\nXimei Wang, Jinghan Gao, Mingsheng Long, and Jianmin Wang.\\nSelf-tuning for data-\\neﬃcient deep learning. In ICML, 2021.\\nZirui Wang, Zihang Dai, Barnab´as P´oczos, and Jaime G. Carbonell. Characterizing and\\navoiding negative transfer. In CVPR, 2019d.\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\\nNan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot\\nlearners. In ICLR, 2022.\\nLonghui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain\\ngap for person re-identiﬁcation. In CVPR, 2018.\\nRoss\\nWightman.\\nPytorch\\nimage\\nmodels.\\nhttps://github.com/rwightman/\\npytorch-image-models, 2019.\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\nZhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. Unsupervised feature learning\\nvia non-parametric instance discrimination. In CVPR, 2018.\\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and\\nFei Huang. Raise a child in large language model: Towards eﬀective and generalizable\\nﬁne-tuning. In EMNLP, 2021.\\nI Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale\\nsemi-supervised learning for image classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and\\nQuoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.\\nIn NeurIPS, 2019.\\nHuaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-\\nlearning. In ICML, 2019.\\n63\\nJiang et al.\\nJason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. How transferable are features\\nin deep neural networks? In NeurIPS, 2014.\\nKaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment\\nof pre-trained models for transfer learning. In ICML, 2021.\\nAmir Roshan Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik,\\nand Silvio Savarese. Taskonomy: Disentangling task transfer learning. In CVPR, 2018.\\nWerner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschl¨ager, and Susanne\\nSaminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation\\nlearning. In ICLR, 2017.\\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-\\nstanding deep learning requires rethinking generalization. In ICLR, 2017.\\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and\\nMichael I. Jordan. Theoretically principled trade-oﬀbetween robustness and accuracy.\\nIn ICML, 2019a.\\nJeﬀrey O. Zhang, Alexander Sax, Amir Zamir, Leonidas J. Guibas, and Jitendra Malik.\\nSide-tuning: Network adaptation via additive side networks. 2019b.\\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and\\nalgorithm for domain adaptation. In ICML, 2019c.\\nZhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural\\nnetworks with noisy labels. In NeurIPS, 2018.\\nNanxuan Zhao, Zhirong Wu, Rynson W. H. Lau, and Stephen Lin. What makes instance\\ndiscrimination good for transfer learning? In ICLR, 2021.\\nLucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. When\\ndoes pretraining help? assessing self-supervised learning for law and the casehold dataset.\\nIn ICAIL, 2021.\\nJincheng Zhong, Ximei Wang, Zhi Kou, Jianmin Wang, and Mingsheng Long. Bi-tuning of\\npre-trained representations. arXiv preprint arXiv:2011.06182, 2020.\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image\\ntranslation using cycle-consistent adversarial networks. In ICCV, 2017.\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio\\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla-\\nnations by watching movies and reading books. In ICCV, 2015.\\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui\\nXiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the\\nIEEE, 109(1):43–76, 2021.\\nYang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain\\nadaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018.\\n64\\n',\n",
       " '2303.01980v1.pdf': ' \\n \\n \\n \\nTowards energy-efficient Deep Learning: An overview of energy-\\nefficient approaches along the Deep Learning Lifecycle  \\n \\n \\nVanessa Mehlin \\nUniversity of Applied Science Ansbach \\nvanessamehlin@gmail.com \\nSigurd Schacht \\nUniversity of Applied Science Ansbach \\nsigurd.schacht@hs-ansbach.de \\nCarsten Lanquillon \\nUniversity of Applied Science Heilbronn \\ncarsten.lanquillon@hs-heilbronn.de \\n \\nFebruary 1, 2023 \\n \\nABSTRACT \\nDeep Learning has enabled many advances in machine learning applications in the last few years. \\nHowever, since current Deep Learning algorithms require much energy for computations, there are \\ngrowing concerns about the associated environmental costs. Energy-efficient Deep Learning has \\nreceived much attention from researchers and has already made much progress in the last couple of \\nyears. This paper aims to gather information about these advances from the literature and show how and \\nat which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, \\nDeployment, Evaluation) it is possible to reduce energy consumption.  \\n1. Introduction \\nThe last decade has seen tremendous developments in the fields of Artificial Intelligence (AI) and \\nMachine Learning (ML), mainly due to the availability of massive datasets and the advancement of \\nDeep Learning (DL). DL consists of neural networks with multiple hidden layers known as deep neural \\nnetworks (DNNs) [1]. DNNs are used in various products and services, such as speech assistance, \\nautonomous driving, or facial recognition. They provide superior performance in these tasks compared \\nto traditional models, and there is a trend for developing even larger and more powerful DNNs [2]. \\nHowever, these models generally require large amounts of data and high computing power, which is \\nassociated with high energy consumption incurring high financial and environmental costs [3]. \\nFurthermore, for emerging applications such as autonomous driving and the Internet of Things, models \\nmust be run on low-power devices [4]. Therefore, energy-efficient DL is crucial from an economic, \\nenvironmental, and application perspective.  \\nThe efficiency of DL can be divided into training efficiency and inference efficiency [5]. Model training \\ncomes at a high environmental cost, as energy is required to run it on hardware for weeks or months at \\na time [3]. However, training a DL model is only the beginning of the lifecycle. Once the model is \\ntrained, it will be implemented and used. This process, called inference, also consumes enormous \\nenergy. Inference does not last weeks or months, but unlike training, it is not a one-time event. It takes \\n \\n2 \\n \\nplace continuously and can therefore exceed the energy consumption of the training after a certain \\nnumber of inference events [6].  \\nVarious researchers have presented approaches, methods, and techniques such as mixed-precision \\ntraining, pruning or knowledge distillation that can accelerate training and inference time and reduce the \\nenergy consumption of the models. This paper aims to provide an overview of these existing approaches \\nand categorize them into the phases of the DL lifecycle. The lifecycle considered in this paper \\nencompasses IT-Infrastructure, Data, Modeling, Training, Deployment (Inference) and Evaluation. The \\nsubdivision of the lifecycle is based on the Data Science Process Model (DASC-PM) [7] as well as on \\nthe CRISP-DM [8], which is the Cross Industry Standard Data Mining Process Model. To the authors\\' \\nknowledge, there has been no previous work aimed at providing a global overview of existing energy-\\nreducing approaches along the lifecycle of DL. There have been overviews of energy-efficient DL ([5], \\n[9]–[12]); however, these overviews have neglected the evaluation part of the DL lifecycle. Thus, this \\npaper is intended not only as a holistic overview of energy-efficient approaches along the lifecycle of \\nDL, it also provides new insights beyond the information provided by these previous overviews.  \\nThe paper is structured as follows. Section II presents the overviews that served as the foundation for \\nthe present work. This is followed by Section III, in which the methodology and research questions are \\noutlined. Section IV provides an overview of the various approaches to reducing energy consumption \\nin the different phases of the DL lifecycle. Finally, a conclusion and a discussion on future work are \\ngiven. \\n2. Related Work \\nA great deal about energy-efficient DL has been written, including approaches to reduce energy \\nconsumption [13]–[19], studies about how much energy DL is consuming ([3], [20], [21]), publications \\nthat point out the need for efficiency improvements due to carbon emissions or the use of edge devices \\n([22], [23]) as well as the following overviews of energy-efficient DL:  \\nMenghani [5] gave an overview of efficient modeling techniques, infrastructure and hardware. A \\ncollection of algorithms, techniques, and tools related to efficient DL in the five areas of (1) compression \\ntechniques, (2) learning techniques, (3) automation, (4) efficient architectures, and (5) infrastructure are \\npresented. Furthermore, an experimental guide with code to show how these tools and techniques work \\nwith each other is provided. This guide helps practitioners to optimize model training and deployment \\nXu et al. [9] conducted a systematic review of energy-efficient DL technologies within the four \\ncategories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference \\napproaches, and (4) efficient data usage. For each category, they provided a taxonomy. Furthermore, \\nthey discussed the progress made and the unresolved challenges in each category.  \\nCai et al. [10] and Liu et al. [11] focused on memory and compute-limited devices (edge/mobile \\ndevices). They surveyed DL approaches including lightweight network design, network compression, \\nhardware-aware neural architecture search, adaptive models, efficient on-device learning, and efficient \\nsystem design. \\nLee et al. [12] provided an overview of resource-efficient DL techniques in terms of (1) modeling, (2) \\narithmetic, and (3) implementation techniques. The focus of this overview is on resource-efficient \\ntechniques for Convolutional Neural Network architectures, since it is one of the most commonly used \\nDL architectures. \\n \\n \\n \\n3 \\n \\n3. Methodology  \\nTo provide a holistic overview of energy-efficient approaches along the lifecycle of DL an initial \\nliterature review on existing overviews and literature reviews were conducted. As a result, four recent \\noverviews of energy-efficient DL ([5], [9]–[12]) were found (see Related Work). Two of those ([10], \\n[11]) focus on mobile/edge devices and one of those [12] focuses on resource efficiency. DL on edge \\nand mobile devices are closely related topics, since those devices have limited memory and compute \\nresources, depending on energy efficiency [10]. Resource-efficient DL is also strongly related, as some \\nresource-efficient techniques, such as mixed-precision training are associated with energy efficiency. \\nHowever, in the further course of this work, these edge/mobile- and resource-focused overviews are not \\nconsidered to the same extent as the other two surveys, due to (1) a focus on the energy efficiency of \\nDL in general and (2) due to the limited scope of the study. Since the remaining two overviews ([5], [9]) \\ntogether already comprehensively cover the topic, it was decided to synthesize the collected information \\nfrom these two overviews as a basis rather than conduct another individual literature review on energy-\\nefficient DL. \\nThe next step was to verify whether the techniques and approaches gathered in the overviews cover the \\nentire DL lifecycle (table 1). The key takeaway was that the reviewed overviews neglect the evaluation \\npart of the DL lifecycle. The topic is addressed, but no such overview is provided as in the other \\ncategories. Since the evaluation part is not equally represented, it was decided to guide this paper with \\nthe two research questions and methods presented in Table 2.  \\nTab. 1: Mapping of the content of the overviews along the DL lifecycle \\n* = it is addressed, but no such overview is provided as in the other categories \\nOverview \\nIT-Infrastr. \\nData \\nModeling \\nTraining \\nDeployment  Evaluation \\nMenghani [5] \\nü \\nü \\nü \\nü \\nü \\n* \\nXu et al. [9] \\n \\nü \\nü \\nü \\nü \\n* \\n \\nTab. 2: Research questions and methods \\nNo. \\nResearch question \\nMethod \\nRQ1 \\nWhich techniques and approaches can be applied in the \\nphases of the DL lifecycle to reduce the energy consumption \\nof DL models? \\nSynthesizing information from related \\noverviews and mapping them along the \\nDL lifecycle. \\nRQ2 \\nHow can models be evaluated in terms of their energy \\nconsumption and carbon emissions? \\n(1) Synthesizing information from the \\nrelated overviews \\n(2) Literature review \\n \\nTo answer RQ1 the techniques and approaches of the two reviewed overviews were analyzed and then \\nclassified into a suitable phase of the DL lifecycle. As mentioned by Xu et al. [9] it can be challenging \\nto provide an overview of energy-efficient DL due to the lack of a unified standard measurement for \\nenergy-efficient DL. Therefore, it is sometimes difficult to tell whether an approach is energy-efficient \\nor not. One controversial example is Neural Architecture Search Algorithms (explained in section 4). \\nRunning such algorithms usually needs large computational resources [5]. But the resulting efficient \\nmodels can significantly reduce the computational burden in downstream research and production, \\nleading to reduced energy consumption [24]. Therefore, it is debatable whether a technology is defined \\nas energy-efficient or not. However, if an approach has the potential to reduce the energy costs of DL \\nmodels, it is included in the overview. Furthermore, a clear classification of some approaches in a certain \\nphase of the DL lifecycle is not given. In some cases, an approach could be assigned to two or three \\n \\n4 \\n \\nphases, depending on the interpretation. For example: Pre-trained models could be mapped in the \\n\"Data\", \"Modeling\" or \"Training\" phase. Thus, the classification of some approaches depends on the \\nauthor\\'s interpretation, which is based on the categorization of the two studies reviewed. Not all of the \\ntechniques listed in the overviews will be presented, because some of them are just mentioned within \\nthe overview and not described as for example “K-Means Clustering” in Menghani [5]. Additionally, \\nsome of them are not relevant in the same extent as the others, for example Hyper-Parameter \\nOptimization. It is mentioned in Menghani [5], but in the remainder of this paper, the focus is on Neural \\nArchitecture Search, as this is the most recent advance in the field. To answer RQ2 a literature review \\nwas conducted. The search was performed during May 2022 and the meta search engines Web of \\nScience, Scopus as well as Google Scholar, which cover all major publishers and journals such as \\nSpringerlink, Elsevier, IEEE Xplore and Research Gate, were used. Searches utilizing the following \\nsearch terms were performed across all databases:  \\n(“deep learning” OR “DNN” OR “deep neural network” OR “machine learning”) AND (“measure*” \\nOR “metric” OR “evaluation” OR “report*”) AND (“environment” OR “energy” OR “emission” OR \\n“footprint”) \\nThese search strings were modified for the different databases, using specific search functions for each \\ndatabase. Using this search strategy, 78 journal articles were identified for further analysis. After reading \\nabstract and full text 14 relevant articles remained to answer RQ2. \\n4. Findings \\nRQ1: Which techniques and approaches can be applied in the phases of the DL lifecycle to reduce the \\nenergy consumption of DL models? \\nTable 3 summarizes the different energy-efficient techniques and approaches, extracted from the \\noverviews, at each stage of the DL lifecycle. In the following, each phase will be presented in more \\ndetail (tables 4 to 8). Since this paper aims to provide a holistic overview of which energy-efficient \\nmethods exist and in which phase of the DL lifecycle they can be used for more energy efficiency, the \\ndifferent approaches are not described in detail. A detailed description can be found in the additional \\nreferences (Add. Ref.). These references are either collected from the overviews or included by the \\nauthor.  \\nTab. 3: Energy-efficient techniques and approaches along the DL lifecycle \\nIT-Infrastr. \\nData \\nModeling \\nTraining \\nDeployment  \\nSoftware  \\n•Tensorflow \\n• PyTorch \\n• Hardware-\\noptimized \\nlibraries \\nActive Learning \\nDesign: \\n• Compact Convolution \\n• Efficient Attention \\n• Lightweight Softmax \\n• Compact Embedding \\nInitialization \\nPruning \\nHardware  \\n• GPU \\n• TPU \\n \\nData \\nAugmentation  \\nAssembling: \\n• Memory Sharing \\n• Static Weight Sharing \\n• Dynamic Weight \\nSharing \\nNormalization \\nLow-Rank \\nFactorization \\nQuantization \\nNeuromorphic \\nComputing \\nPre-trained \\nmodels \\nHyper-Parameter \\nOptimization/Neural \\nArchitecture Search  \\nProgressive \\nTraining \\nKnowledge \\nDistillation \\nMixed-Precision \\nTraining \\nDeployment \\nSharing \\n \\n \\n5 \\n \\n4.1 IT-Infrastructure \\nThe basis for running a DL model efficiently is a robust software and hardware infrastructure [5]. This \\nsection provides an overview of software and hardware components that are critical to model efficiency. \\nTab. 4: Efficient IT-Infrastructure \\nTechnique/ \\nApproach \\nDescription \\nAdd. \\nRef. \\nSoftware \\n \\n• \\nTensorflow is a machine learning framework which has some of the most \\nextensive software support for model efficiency (e.g. TF Lite, TF Model \\nOptimization toolkit) [5]. \\n• \\nPyTorch is a machine learning framework, which includes for example PyTorch \\nMobile (light-weight interpreter that enables running PyTorch models on \\nmobiles) and a model tuning guide that lists various options available to \\npractitioners such as mixed-precision training or enabling device-specific \\noptimizations [5].  \\n• \\nHardware-optimized libraries: Efficiency can be further increased by optimizing \\nfor the hardware on which the neural networks run [5]. \\n[25] \\n \\n \\n \\n[26] \\n \\n \\n \\n \\n \\nHardware \\n \\n• \\nGPU (Graphics Processing Units) were originally used for computer graphics, \\nuntil a study in 2009 [27] demonstrated that it can be used to accelerate DL models \\n[5].  \\n• \\nTPUs (Tensor Processing Units) are proprietary application-specific integrated \\ncircuits (ASICs) that Google developed to accelerate DL applications using \\nTensorflow [5].  \\n \\n \\n \\n \\n \\n[28] \\nNeuromorphic \\nComputing \\nNeuromorphic Computing refers to a variety of computers, devices, and models that \\nare inspired by the brain and contrast the widespread von Neumann computer \\narchitecture [29]. Schuman et al. 2017 [29] discovered that one of the main \\nmotivations for neuromorphic computing is speed of computation and their potential \\nfor extremely low power operation. Especially the developers of early systems \\nhighlighted that it is possible to perform much faster neural network computations \\nwith customized chips than with traditional von Neumann architectures, in part by \\nexploiting their natural parallelism, but also by building customized hardware for \\nneural computations. This early focus on speed was a forerunner of the future of using \\nneuromorphic systems as accelerators for machine learning or neural network style \\ntasks [29]. \\n[29] \\n[30] \\n \\n4.2 Data \\nIt is common to increase training data to achieve better model performance. The downside of this \\napproach is the significant increase in training costs [9]. This is one of the reasons why data efficiency \\nhas received significant attention over the years [22]. In this phase three techniques are presented that \\ncan be used to gain data efficiency, accelerate training time, and achieve competitive results with fewer \\ndata resources.  \\nTab. 5: Efficient Data Usage \\nTechnique/ \\nApproach \\nDescription \\nAdd. \\nRef. \\nData \\nAugmentation \\n \\nData augmentation is a solution to address the scarcity of labeled data. The basic idea \\nis to synthetically inflate the existing dataset through augmentation methods, so that \\nthe new label of the augmentation example does not change or can be derived in a cost-\\neffective way. An example of this is the classic dog or cat image classification task. If \\nthe image of a dog is shifted horizontally/vertically by a small number of pixels or it \\nis rotated by a small angle, the image does not change significantly, so the transformed \\nimage should still be classified as \"dog\". As a result, the classifier is forced to learn a \\n[31] \\n[32] \\n \\n6 \\n \\nrobust representation of the image that is more generalizable across these \\ntransformations. [5] \\nActive \\nLearning \\n \\nActive learning aims to achieve good results with as few samples as possible. It was \\noriginally proposed to reduce annotation costs. Today, pool-based active learning is \\nwidely used to reduce training costs by selecting the most useful samples for training \\na network. The idea behind active learning is the following: Annotated training data \\ndo not contribute equally to final performance. Thus, if only the most useful sample is \\nselected for training models, the waste of training on irrelevant samples can be largely \\nreduced. [9] \\n[33] \\n[34] \\n[35] \\n[36] \\nPre-trained \\nmodels \\nPre-trained models as initialization can be an effective approach to reduce data \\nrequirements in downstream tasks [5]. This approach is further discussed in the \\ntraining phase.  \\n[37] \\n \\n4.3 Modeling \\nContinuous improvements in model architectures lead to significant reductions in computational effort \\nrequired to achieve a given level of accuracy. For example, the Transformer architecture developed in \\n2017 required 10 to 100 times less computational effort while achieving better results than the state-of-\\nthe-art models at the time [13]. This chapter concentrates on such efficient neural networks. \\nFurthermore, assembling and automation as presented in Xu et al. [9] will be addressed. \\nTab. 6: Efficient Modeling \\nTechnique/ \\nApproach \\nDescription \\nAdd. \\nRef. \\nEfficient Architecture Design \\nCompact \\nConvolution \\n(Vision) \\nIn the following compact convolution methods that improve resource-efficiency are \\nlisted: \\n• \\nDepthwise separable convolution  \\n• \\nDownsampling  \\n• \\nFlattened convolution  \\n• \\nGroup convolution  \\n• \\nLinear bottleneck layer  \\n• \\nOctave convolution \\n• \\nShrinked convolution  \\n• \\nSqueezing channel/fire convolution \\n \\n \\n[38] \\n[39] \\n[40] \\n[41] \\n[42] \\n[43] \\n[44] \\n[45] \\nEfficient \\nAttention \\nThe attention mechanism directly aligns all tokens from sequence-to-sequence models \\ntogether, which can address long-distance dependencies to some extent. But due to the \\nfact that any two tokens have an attention score, the required computations grow \\nquadratically with the input length [9]. To address this problem, several studies \\nproposed efficient attention variants. Xu et al. [9] classified them into the following \\ncategories: \\n• \\nSparse attention: Reduces the span of attention  \\n• \\nAttention approximation: Different attention estimation formats \\n[16] \\n[46] \\n[47] \\n[48] \\n[49] \\nLightweight \\nSoftmax \\n(NLP) \\nSoftmax layer introduces embeddings for all tokens, which leads to many \\ncomputations for a large vocabulary [9]. Therefore, several efficient lightweight \\nsoftmax variants have been proposed [9]. Xu et al. [9] distinguished these variants as \\nfollows: \\n• \\nReduction of parameters: The proposal is to create a sequence at character \\nlevel instead of word level. The number of characters is much smaller than \\nthat of words, which helps to reduce the computations for softmax \\nsignificantly. \\n• \\nReduction of computations: Xu et al. [9] classified softmax variants with \\nfewer computations into five categories: \\no \\nHierarchical softmax \\no \\nSoftmax with dynamic embeddings \\no \\nSampling-based softmax \\n[50] \\n[51] \\n[52] \\n[53] \\n[54] \\n[55] \\n[56] \\n \\n7 \\n \\no \\nHashing-based softmax \\no \\nNormalization-based softmax \\nCompact \\nEmbedding \\n(NLP) \\nThe first step for NLP tasks is building token embeddings. Reducing the parameters \\nof these embeddings to make them compacter is an important topic. There are several \\napproaches to compress neural networks such as pruning, knowledge distillation, low-\\nrank approximation, and quantization. Xu et al. [9] divided approaches for compact \\nembeddings into four categories:  \\n• \\nReuse-based approaches \\n• \\nKnowledge-distillation-based approaches \\n• \\nLow-rank-based approaches \\n• \\nFine-grained vocabularies \\n[57] \\n[58] \\n[59] \\n[60] \\n[61] \\n[62] \\nAssembling \\nComponent assembling solutions aim for efficient architecture design. The key idea of \\nefficient component assembling lies in sharing [9]. \\nMemory \\nSharing \\nMemory sharing is a technique for storing large models on devices with limited \\nmemories (IoT). The idea is to share the same storage among intermediate forward \\nvectors [63] or backward vectors [64] to reduce memory requirements [9]. \\n[65] \\n[66] \\nStatic Weight \\nSharing \\nStatic weight sharing aims to explore how weights can be reused for a neural network. \\nThe weights are fixed during inference and shared among all samples. To save \\nmemory, many models choose to reuse parameters across different layers or different \\ntasks (dealing with problems involving multiple tasks, multiple domains, or \\nmultilingual problems) [9]. \\n[67] \\n[68] \\n[69] \\n[70] \\nDynamic \\nWeight \\nSharing \\n \\n \\nStatic parameter sharing usually fails when dealing with tasks that are not closely \\nrelated. To decide which layers/components should be shared by different input \\nsamples dynamic solutions can be used. Dynamic networks are neural networks with \\ndynamic computational graphs in which the computational topology or parameters are \\nspontaneously determined to reduce computation costs and improve the adaptiveness \\nof networks [9]. Xu et al. [9] provided an overview of general dynamic architectures. \\nWithin the cascading-style network architectures, for example, first smaller networks \\nand then larger ones are executed. If a smaller network can already process the input \\nsample, the model stops the execution process and does not execute any further \\nmodels. Skipping-style networks speed up inference by either skipping certain layers \\nor omitting unimportant input spans in the entire input sequence. Other general \\ndynamic architectures are early-exit-style networks and mixture-of-experts-style \\nnetworks [9]. \\n[71] \\n[72] \\n[73] \\n[74] \\n[75] \\n[76] \\n[77] \\n[78] \\n \\nAutomation \\nAutomated approaches can automatically search for ways to train more efficient models. The \\ndownside is that these methods may require large computational resources [5]. However, while \\nthe initial effort can be computationally intensive, the resulting efficient models can \\nsignificantly reduce the computational effort required in downstream production, resulting in \\nlower overall energy consumption [24]. \\nNeural \\nArchitecture \\nSearch (NAS) \\nNAS is a technique to search automatically for a global optimal efficient DNN model. \\nBut it is very time-consuming and computationally expensive [79]. To solve this \\nproblem, some new NAS algorithms are proposed [5]: \\n• \\nEvolution based search reduces the search costs. It is a two-stage search \\napproach, where several well-functioning parent architectures are selected in \\nthe first stage. In the second stage mutations are applied on these architectures \\nto select the best one. For this stage pre-trained parent networks which does \\nnot require too many computations to train child networks are used. This \\nmethod requires validation accuracy as search criterion, which still makes it \\ncomputationally expensive.  \\n• \\nDifferentiable search: This method is proposed to completely eliminate the \\ndependency on validation accuracy with re-formulating the task in a \\ndifferentiable manner and allowing efficient search using gradient descent.  \\n• \\nAnother research direction aims to represent a model in a continuous space \\nwhere there is a mapping between structures and outcomes. By doing so, the \\nmodel only learns how to predict the performance of architectures based on \\ntheir continuous representations where the downstream training is not \\nneeded.  \\n• \\nTraining-free NAS approaches: These approaches directly extract features \\nfrom randomly-initialized models and use these features as evaluation \\ncriterion to select optimal networks.  \\n[80] \\n[81] \\n[82] \\n[83] \\n[84] \\n[85] \\n[86] \\n[17] \\n \\n8 \\n \\n4.4 Training \\nIn this section, the focus is on the energy efficiency of training DL models. Many approaches have been \\nproposed to reduce training costs, including initialization, normalization, progressive training, and \\nmixed-precision training. \\nTab. 7: Efficient Training \\nTechnique/ \\nApproach \\nDescription \\nAdd. \\nRef. \\nInitialization \\nPre-trained models from other domains (or other tasks) can be used for initialization. \\nIt is generally assumed that initialization from existing models can improve the \\ngeneralization ability with fewer training iterations [9]. Xu et al. [9] categorize these \\npre-training initialization approaches as follows:  \\n• \\nFeature-based initialization: The parameters (usually from the lower or \\nmiddle layers) are borrowed from other domains/tasks as initialization.  \\n• \\nFine-tuning-based initialization: Here the target data is used to train all \\nparameters (including new parameters and borrowed parameters). It can \\nfurther optimize the target objectives by fine-tuning all parameters to better \\nfit the training data.  \\n• \\nSupervised initialization: This approach is popular in low-resource \\nenvironments and is extensively studied on domain adaptation/transfer \\nlearning. As a common solution, the target model is pre-trained using similar \\ntasks/datasets and then the pre-trained parameters are used as initialization \\nfor the target task.  \\n• \\nSelf-supervised initialization: To reduce the requirements of supervised \\ndata, previous studies have dealt with self-supervised pre-training, which \\nuses unlabeled data to construct supervision signals to learn representations. \\nSince self-supervised pre-training does not require human annotated labels, \\nit is easy to obtain sufficient training data.  \\n[87] \\n[88] \\nNormalization \\nNormalization is based on a special component which can accelerate convergence \\nand is used to normalize hidden outputs in deep neural networks. There are different \\nnormalization variants that have almost the same calculation process but are applied \\nto different dimensions or objectives, such as batch normalization, layer \\nnormalization, group normalization and weight normalization. Ioffe & Szegedy 2015 \\n[89] were able to perform 14 times fewer training steps with the same accuracy on a \\nstate-of-the-art image classification model when they used batch normalization. ([9], \\n[90]) \\n[90] \\n[91] \\nProgressive \\nTraining \\nThe idea of progressive training is to add layers constructively. When compared to \\nfull training, progressive training does not require full gradients to all parameters and \\ncan therefore reduce the computations required for training. Furthermore, the well-\\ntrained lower layers also accelerate the training of the higher layers [9]. \\n[92] \\n[93] \\nMixed-\\nPrecision \\nTraining \\nA distinction can be made between a simple precision format (FP32), which requires \\n32 bits of memory, and lower precision format (FP16), which requires 16 bits of \\nmemory. The lower precision offers numerous advantages, such as the ability to train \\nand deploy larger neural networks, the reduced load on memory, which speeds up \\ndata transfer operations, and finally, the acceleration of computations. However, \\ntraining with low precision also affects the accuracy of the model: The fewer bits \\nused, the lower the accuracy. Mixed precision training can be used as a solution. This \\napproach can reduce the memory requirements by almost half while maintaining the \\nmodel accuracy. This is achieved by identifying the steps that require full precision \\nto use FP32 for them, while using FP16 for all other steps. ([12], [94]) \\n[95] \\n \\n4.5 Deployment (Inference) \\nState-of-the-art neural network models have millions of parameters that increase the trained model\\'s size \\nand thus increase inference costs [1]. Therefore, it is important to compress and accelerate these models \\nbefore deploying them without compromising model accuracy. This section describes common model \\n \\n9 \\n \\ncompression methods for reducing inference costs, including pruning, low-rank factorization, \\nquantization, and knowledge distillation. These techniques are commonly used to compress a model \\nafter the training to accelerate inference [1]. Deployment sharing is also presented in this phase. \\nTab. 8: Efficient Inference \\nTechnique/ \\nApproach \\nDescription \\nAdd. \\nRef. \\nPruning  \\nPruning can be used to remove redundant elements in neural networks to reduce the \\nsize of the model and the computational cost. The main idea is to create a smaller \\nnetwork by removing unimportant weights, filters, channels, or even layers from the \\noriginal DNN while keeping the accuracy ([9], [96]). Generally, network pruning can \\nbe divided into (1) pruning of connections (weights) (unstructured pruning) and (2) \\npruning of filters or channels (structural pruning) [96]. \\n• \\nVery effective for reducing the number of parameters in DNNs [5] \\n• \\nRequires iteratively scoring weights and re-training the network for many \\niterations [5] \\n• \\nOften leads to non-negligible performance drop when applied to powerful \\nDNNs [5] \\n[19] \\nLow-Rank \\nFactorization \\nThis techniques uses tensor/matrix decomposition to reduce the complexity of \\nconvolutional or fully connected layers in deep neural networks [10]. The aim of low \\nrank factorization is to factorize a weight matrix into two matrices with low dimensions \\n[1]. \\n• \\nReduces memory storage and accelerates DNNs [1] \\n• \\nIn comparison to other common compression methods, it can effectively \\nreduce the size of models with a large compression ratio while maintaining \\ngood performance [9] \\n• \\nComputationally complicated [9] \\n• \\nLess effective in reducing computational cost and inference time than other \\ncommon compression methods [9] \\n[18] \\nQuantization \\nQuantization reduces computation by reducing the number of bits per weight. It \\nminimizes the bit-width of data storage and flow through the DNN. Computing and \\nstoring data with a smaller bit-width enables fast inference with lower energy \\nconsumption [96]. \\n• \\nCan help significantly reduce model size and inference latency [5] \\n• \\nEasy to implement [9] \\n• \\nPost-training quantization often causes a non-negligible drop in performance, \\nwhereas quantization-sensitive training can effectively reduce the \\nperformance drop [9] \\n[97] \\nKnowledge \\nDistillation \\nThe knowledge gained from a large-scale high performing model (teacher model), \\nwhich generalizes well on unseen data, is transferred to a smaller and lighter network \\nknown as the student model [1]. \\n• \\nImproves resource efficiency [1] \\n• \\nSame/comparable performance as the larger model [1] \\n[98] \\nDeployment \\nSharing \\n \\nThe optimal neural network architecture varies significantly with different hardware \\nresources [14]. Therefore, developing elastic or dynamic models that meet different \\nconstraints is crucial for practical applications. During inference, the appropriate \\nsubnetwork is selected to satisfy the resource constraints from different devices. By \\namortization of the one-time training cost, the total cost of the specialized design can \\nbe reduced [9]. \\n[99] \\n[100] \\n \\nRQ2: How can models be evaluated in terms of their energy consumption and carbon emissions?  \\nEven though more and more research is done in energy-efficient DL, standardized metrics of models\\' \\nenergy efficiency and carbon emissions still do not exist till do not exist [9]. However, many studies \\nencourage researchers to consider these two values when evaluating models ([3], [22], [101], [102]). \\nThis would help promote those models that achieve high accuracy with lower energy consumption. It \\n \\n10 \\n \\nwould also raise practitioners\\' awareness of their carbon emissions, which could encourage them to take \\nactive steps to reduce them. To determine the evaluation possibilities of DL models in terms of their \\nenergy consumption and carbon emissions, (1) information from the relevant overviews was collected \\nand (2) a literature review was conducted. The found insights and publications can be categorized into \\n“Metrics” and “Tools” as shown in table 9.  \\nTab. 9: Evaluation Metrics and Tools \\nEvaluation metrics \\nand tools \\nDescription \\nMetric \\n \\nCarbon emission \\nTaking carbon emissions as a metric would be the most logical, since this is what is \\nto be minimized. However, it is difficult to measure the exact amount of carbon \\nreleased by training or running a model, as this number is highly dependent on the \\nlocal power infrastructure. Therefore, this metric would not be comparable between \\nresearchers at different locations or even at the same location at different times [22]. \\nTo measure the carbon emission various online tools (see section “tools”) can be \\nused. \\nRun Time \\nMeasuring the running time of training and inference could be a valuable metric if \\nall models had the same hardware and software settings. Since this is not the case \\nand running time is highly dependent on infrastructure settings, models running with \\ndifferent settings are not comparable. Nevertheless, reporting the running time can \\nbe important for a general understanding of the impact. ([9], [22]) \\nEnergy  \\nThe energy consumption can be calculated by multiplying the maximum power \\nconsumption of the individual hardware used according to their technical \\nspecifications by the number of hardware used for training and then by the training \\ntime in hours [20].  \\nAccuracy per Joule \\n \\nThis metric captures the accuracy, latency, and energy tradeoffs between models \\n[12]. Hanafy et al. [103] propose to compute it as follows: \\nAccuracy/Joule = ModelAccuracy/Energy_per_Request.  \\nIt indicates how much energy is required for one unit of accuracy and serves as a \\nnormalized metric and a benchmark for comparison between two or more models. \\nFull-Cycle Energy \\nConsumption Metric  \\n“Greeness” \\nThis metric proposed from Li et al. [104] focuses on the energy consumption during \\nthe training and inference, the accuracy, as well as the model usage intensity. It is \\ncalculated as follows: \\nG(MUI) = Accτ/(MUI ∗ IEC + TEC) \\n• \\nTrain Energy Cost (TEC) calculates the total energy consumption of efficient \\nDL throughout the training phase, including base model training, model \\ncompression, and retraining the model.  \\n• \\nInference energy cost (IEC) refers to the energy consumed to perform inference \\nfor one time. \\n• \\nModel Usage Intensity (MUI) is defined as the average number of inferences in \\neach lifecycle. The importance of TEC and IEC varies depending on the MUI. \\nIf an AI system uses the model intensively and the number of inferences in a \\nlifecycle is large, then IEC accounts for a large portion of the energy \\nconsumption and conversely.  \\n• \\nAccuracy (Acc) refers to the accuracy of the model on a given task. Efficient DL \\nmodels usually trade accuracy for efficiency and their accuracy degradation can \\nvary significantly. For this reason, the accuracy of the models should also be \\nconsidered. \\nModel Size/Number of \\nparameters  \\n \\nModel size/Number of parameters are also a crucial factor when it comes to training \\nand inference costs [9]. It can be determined quite easily and it is usually directly \\ncorrelated with the complexity of the computations [20]. Unlike the previously \\ndescribed measures, it is not dependent on the underlying hardware. Nevertheless, \\ndifferent algorithms use its parameters differently, for example by making the model \\ndeeper or wider. As a result, different models with a similar number of parameters \\nare often not comparable [22]. \\nFLOPs \\nFloating point operations (FLOPs) count the number of operations required to run a \\nmodel when executing a specific instance. The advantage of this metric is that it is \\n \\n11 \\n \\nnearly independent of hardware and software settings and can therefore be an easy \\nway to make a fair comparison between different models [9].  \\nTools \\n \\neco2AI \\neco2AI [105] is an open-source Python library for carbon emission tracking. It allows \\nusers to monitor energy consumption of CPU and GPU devices and estimate \\nequivalent carbon emissions taking into account the regional carbon emissions \\naccounting. The eco2AI library currently includes the largest and permanently \\nenriched and maintained database of regional carbon emissions accounting based on \\nthe public available data in 209 countries.  \\nCarbontracker \\nCarbontracker [106] is an open-source Python package for tracking and predicting \\nthe energy consumption and carbon footprint of computational workloads. The \\ntraining of models can be cancelled as soon as the limit of environmental costs set by \\nthe user has been exceeded. \\nCodeCarbon \\nCodeCarbon [107] is an open-source software package that can easily be integrated \\nin the Python codebase. It estimates the amount of CO2 generated by the cloud or \\npersonal computing resources used to run the code. It further shows how the code \\ncan be optimized to reduce emissions and provides suggestions for hosting a cloud \\ninfrastructure in geographic regions that use renewable energy sources.   \\nEnergy Usage Reports \\nIt is an open-source Python package [101] for calculating the energy and carbon \\nemissions of algorithms. It provides an energy usage report in which the results are \\nput into a context that is more understandable, such as car kilometers traveled. (This \\npackage is no longer actively maintained since it has been integrated into the tool \\nCarbonCode). \\nEnergyVis \\nEnergyVis [108] is an interactive energy consumption tracker for ML models. It can \\nbe used for tracking, visualizing, and comparing model energy consumption in terms \\nof energy consumption and CO2 metrics. It further shows alternative deployment \\nlocations and hardware that can reduce the carbon footprint. \\nExperiment-Impact-\\nTracker \\nThe experiment-impact-tracker [102] is an open-source drop-in method to track \\nenergy usage, carbon emissions, and compute utilization of the system used. It also \\ngenerates carbon impact statements for enabling standardized reporting.  \\nGreen Algorithms \\nGreen Algorithms [109] is a self-reporting free online tool that allows users to \\nestimate the carbon footprint of their computations. The tool requires minimal \\ninformation and considers a wide range of hardware configurations. After providing \\nall the required information, the tool visualizes and contextualizes the estimated \\ncarbon footprint of the computations. \\nML CO2 Impact \\nThis free online tool relies on self-reporting [110]. The tool can estimate the carbon \\nfootprint of GPU computation by specifying hardware type, hours used, cloud \\nprovider, and region. \\n \\nOne important issue that needs to be kept in mind when evaluating energy-efficient models is accuracy. \\nIf two models are compared and one is significantly more energy-efficient than the other, but at the same \\ntime has significantly lower accuracy, the comparison is not fair. Therefore, for example, Lee et al. [12] \\nand Li et al. [104] include accuracy in their metrics. Furthermore, the optimal metric for energy \\nefficiency should allow a fair comparison between different models. Therefore, this metric should \\nideally be stable under different influencing factors such as infrastructure. FLOPs could be an interesting \\nmetric, since it is nearly independent of hardware and software settings. Xu et al. [6] and Douwes et al. \\n[20] suggest reporting the FLOPs during model training and inference. However, they would also \\nencourage researchers to specify the total FLOPs during all experiments, including but not limited to \\nparameter tuning and base implementation, to identify the wasted and redundant computations required \\nto develop a new model/algorithm.  \\n \\n \\n \\n \\n12 \\n \\n5. Discussion  \\nThis paper gives a holistic overview of techniques and approaches along the DL lifecycle to reduce the \\nenergy consumption of DL. Since it is an overview, the approaches are not described in detail. However, \\nrelevant references with more information are listed for each approach. It must be emphasized that the \\nanalyzed overviews are comprehensive, but not exhaustive. Due to the fact that the overviews focusing \\non resource-limited devices were not included, there are existing energy-efficient approaches (f. e. \\nfederated learning) that are not listed in this overview. However, this overview will hopefully give \\npractitioners an idea at which point along the DL lifecycle and how the energy consumption of DL \\nmodels can be reduced.  \\n6. Conclusion \\nThere are various approaches to increase the energy efficiency of DL. Such approaches have been \\nextracted from recent overviews and classified within the six defined phases of the DL lifecycle. During \\nthe classification process, it was noticeable that approaches for the evaluation phase were not listed to \\nthe same extent in the overviews. This is because this phase is mainly about evaluating the energy \\nefficiency of DL, not improving it. However, evaluating the energy efficiency of DL can also lead to \\nimproved energy efficiency, as practitioners\\' awareness about emitting CO2 increases and some \\nevaluation tools even provide recommendations on how to reduce energy consumption. The reporting \\nof the models\\' energy metrics can also help achieve more energy efficiency in further research, as \\nresearchers can compare the models concerning their energy efficiency and thus, for example, use the \\nmodel with lower energy consumption as a base or pre-trained model.   \\n7. Future Work  \\nLooking to the future of energy-efficient DL, it would be important to continue research in this direction, \\nto develop more efficient hardware and software, more efficient architectures, and more or improved \\napproaches reducing energy consumption. In addition, a standardized metric for energy efficiency \\nshould be introduced to enable the evaluation and comparison of DL models and to raise awareness \\namong researchers and practitioners. Moreover, a clear and transparent reporting of the measurements \\nis crucial. Here, not only the energy consumption of training but also that of the inference should be \\nconsidered. Further research could include defining guidelines, proposing a process model or a \\nframework that can serve as a step-by-step guide for practitioners to achieve energy improvements \\nthroughout the DL lifecycle.  \\nReferences \\n[1] \\nT. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, ‘A comprehensive survey on model \\ncompression and acceleration’, Artif. Intell. Rev., vol. 53, no. 7, pp. 5113–5155, Oct. 2020, doi: \\n10.1007/s10462-020-09816-7. \\n[2] \\nS. Wang, ‘Efficient deep learning’, Nat. Comput. Sci., vol. 1, no. 3, pp. 181–182, Mar. 2021, doi: \\n10.1038/s43588-021-00042-x. \\n[3] \\nE. Strubell, A. Ganesh, and A. McCallum, ‘Energy and Policy Considerations for Deep Learning \\nin NLP’, Jan. 2019, doi: 10.48550/arXiv.1906.02243. \\n[4] \\nM. Kumar, X. Zhang, L. Liu, Y. Wang, and W. Shi, ‘Energy-Efficient Machine Learning on the \\nEdges’, in 2020 IEEE International Parallel and Distributed Processing Symposium Workshops \\n(IPDPSW), \\nNew \\nOrleans, \\nLA, \\nUSA, \\nMay \\n2020, \\npp. \\n912–921. \\ndoi: \\n10.1109/IPDPSW50202.2020.00153. \\n \\n13 \\n \\n[5] \\nG. Menghani, ‘Efficient Deep Learning: A Survey on Making Deep Learning Models  Smaller, \\nFaster, and Better’, Jun. 2021. [Online]. Available: http://arxiv.org/pdf/2106.08962v2 \\n[6] \\nC.-J. Wu et al., ‘Sustainable AI: Environmental Implications, Challenges and Opportunities’, \\n2021, doi: 10.48550/ARXIV.2111.00364. \\n[7] \\nM. Schulz et al., ‘DASC-PM v1.0 - Ein Vorgehensmodell für Data-Science-Projekte’, Feb. 2021, \\ndoi: 10.25673/32872.2. \\n[8] \\nR. Wirth and J. Hipp, ‘CRISP-DM: Towards a Standard Process Model for Data Mining.’ 2000. \\n[9] \\nJ. Xu, W. Zhou, Z. Fu, H. Zhou, and L. Li, ‘A Survey on Green Deep Learning’, Nov. 2021. \\n[Online]. Available: http://arxiv.org/pdf/2111.05193v2 \\n[10] \\nH. Cai et al., ‘Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications’, \\nACM Trans. Des. Autom. Electron. Syst., vol. 27, no. 3, pp. 1–50, May 2022, doi: \\n10.1145/3486618. \\n[11] \\nD. Liu, H. Kong, X. Luo, W. Liu, and R. Subramaniam, ‘Bringing AI to edge: From deep \\nlearning’s perspective’, Neurocomputing, vol. 485, pp. 297–320, May 2022, doi: \\n10.1016/j.neucom.2021.04.141. \\n[12] \\nJ. Lee et al., ‘Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and \\nImplementation-Level Techniques’. arXiv, Dec. 30, 2021. Accessed: Jun. 14, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/2112.15131 \\n[13] \\nA. Vaswani et al., ‘Attention Is All You Need’. arXiv, Dec. 05, 2017. Accessed: Jul. 04, 2022. \\n[Online]. Available: http://arxiv.org/abs/1706.03762 \\n[14] \\nH. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, ‘Once-for-All: Train One Network and \\nSpecialize it for Efficient Deployment’, ArXiv190809791 Cs Stat, Apr. 2020, Accessed: May 10, \\n2022. [Online]. Available: http://arxiv.org/abs/1908.09791 \\n[15] \\nA. Fan et al., ‘Training with Quantization Noise for Extreme Model Compression’, ArXiv Learn., \\nApr. 2020. \\n[16] \\nN. Kitaev, Ł. Kaiser, and A. Levskaya, ‘Reformer: The Efficient Transformer’. arXiv, Feb. 18, \\n2020. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/2001.04451 \\n[17] \\nJingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang, ‘KNAS: \\nGreen Neural Architecture Search’, ICML, 2021. \\n[18] \\nS. Cahyawijaya et al., ‘Greenformer: Factorization Toolkit for Efficient Deep Neural Networks’, \\nArXiv Learn., Sep. 2021. \\n[19] \\nT. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, ‘Sparsity in Deep Learning: \\nPruning and growth for efficient inference and training in neural networks’, ArXiv Learn., 2021. \\n[20] \\nC. Douwes, P. Esling, and J.-P. Briot, ‘Energy Consumption of Deep Generative Audio Models’. \\narXiv, \\nOct. \\n13, \\n2021. \\nAccessed: \\nJul. \\n06, \\n2022. \\n[Online]. \\nAvailable: \\nhttp://arxiv.org/abs/2107.02621 \\n[21] \\nDavid A. Patterson et al., ‘Carbon Emissions and Large Neural Network Training’, ArXiv, 2021. \\n[22] \\nR. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, ‘Green AI’, Jul. 2019. [Online]. Available: \\nhttp://arxiv.org/pdf/1907.10597v3 \\n[23] \\nA. van Wynsberghe, ‘Sustainable AI: AI for sustainability and the sustainability of AI’, AI Ethics, \\nvol. 1, no. 3, pp. 213–218, Jan. 2021, doi: 10.1007/s43681-021-00043-6. \\n[24] \\nS. R. Young et al., ‘Evolving Energy Efficient Convolutional Neural Networks’, in 2019 IEEE \\nInternational Conference on Big Data (Big Data), Los Angeles, CA, USA, Dec. 2019, pp. 4479–\\n4485. doi: 10.1109/BigData47090.2019.9006239. \\n[25] \\nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu \\nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, \\nRajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, and Vijay \\nVasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng, ‘TensorFlow: A \\nSystem for Large-Scale Machine Learning’, 12th USENIX Symp. Oper. Syst. Des. Implement. \\nOSDI 16, pp. 265–283, 2016. \\n[26] \\nA. Paszke et al., ‘PyTorch: An Imperative Style, High-Performance Deep Learning Library’. \\narXiv, \\nDec. \\n03, \\n2019. \\nAccessed: \\nJun. \\n25, \\n2022. \\n[Online]. \\nAvailable: \\nhttp://arxiv.org/abs/1912.01703 \\n \\n14 \\n \\n[27] \\nR. Raina, A. Madhavan, and A. Y. Ng, ‘Large-scale deep unsupervised learning using graphics \\nprocessors’, in Proceedings of the 26th Annual International Conference on Machine Learning \\n- ICML ’09, Montreal, Quebec, Canada, 2009, pp. 1–8. doi: 10.1145/1553374.1553486. \\n[28] \\nN. P. Jouppi et al., ‘In-Datacenter Performance Analysis of a Tensor Processing Unit’, in \\nProceedings of the 44th Annual International Symposium on Computer Architecture, Toronto \\nON Canada, Jun. 2017, pp. 1–12. doi: 10.1145/3079856.3080246. \\n[29] \\nC. D. Schuman et al., ‘A Survey of Neuromorphic Computing and Neural Networks in \\nHardware’. arXiv, May 19, 2017. Accessed: Jun. 26, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1705.06963 \\n[30] \\nA. Shrestha, H. Fang, Z. Mei, D. P. Rider, Q. Wu, and Q. Qiu, ‘A Survey on Neuromorphic \\nComputing: Models and Hardware’, IEEE Circuits Syst. Mag., vol. 22, no. 2, pp. 6–35, 2022, \\ndoi: 10.1109/MCAS.2022.3166331. \\n[31] \\nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, ‘Randaugment: Practical automated data \\naugmentation with a reduced search space’, in 2020 IEEE/CVF Conference on Computer Vision \\nand Pattern Recognition Workshops (CVPRW), Seattle, WA, USA, Jun. 2020, pp. 3008–3017. \\ndoi: 10.1109/CVPRW50498.2020.00359. \\n[32] \\nX. Wang, H. Pham, Z. Dai, and G. Neubig, ‘SwitchOut: an Efficient Data Augmentation \\nAlgorithm for Neural Machine Translation’. arXiv, Aug. 28, 2018. Accessed: Jul. 06, 2022. \\n[Online]. Available: http://arxiv.org/abs/1808.07512 \\n[33] \\nS. Mohamadi and H. Amindavar, ‘Deep Bayesian Active Learning, A Brief Survey on Recent \\nAdvances’. arXiv, Apr. 21, 2022. Accessed: Jun. 13, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2012.08044 \\n[34] \\nP. Ren et al., ‘A Survey of Deep Active Learning’, ACM Comput. Surv., vol. 54, no. 9, pp. 1–40, \\nDec. 2022, doi: 10.1145/3472291. \\n[35] \\nP. F. Jacobs, G. M. de B. Wenniger, M. Wiering, and L. Schomaker, ‘Active learning for reducing \\nlabeling effort in text classification tasks’. Nov. 03, 2021. Accessed: Jun. 15, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/2109.04847 \\n[36] \\nK. Margatina, L. Barrault, and N. Aletras, ‘Bayesian Active Learning with Pretrained Language \\nModels’, \\nJan. \\n2021, \\nAccessed: \\nJun. \\n15, \\n2022. \\n[Online]. \\nAvailable: \\nhttps://openreview.net/forum?id=oO1QGIKkEsY \\n[37] \\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, ‘Pre-trained models for natural language \\nprocessing: A survey’, Sci. China Technol. Sci., vol. 63, no. 10, pp. 1872–1897, Oct. 2020, doi: \\n10.1007/s11431-020-1647-3. \\n[38] \\nF. Chollet, ‘Xception: Deep Learning with Depthwise Separable Convolutions’. arXiv, Apr. 04, \\n2017. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1610.02357 \\n[39] \\nZ. Qin, Z. Zhang, X. Chen, C. Wang, and Y. Peng, ‘Fd-Mobilenet: Improved Mobilenet with a \\nFast Downsampling Strategy’, in 2018 25th IEEE International Conference on Image Processing \\n(ICIP), Athens, Oct. 2018, pp. 1363–1367. doi: 10.1109/ICIP.2018.8451355. \\n[40] \\nJ. Jin, A. Dundar, and E. Culurciello, ‘Flattened Convolutional Neural Networks for Feedforward \\nAcceleration’. arXiv, Nov. 20, 2015. Accessed: Jun. 22, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1412.5474 \\n[41] \\nG. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, ‘Densely Connected Convolutional \\nNetworks’. arXiv, Jan. 28, 2018. Accessed: Jun. 22, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1608.06993 \\n[42] \\nG. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger, ‘CondenseNet: An Efficient \\nDenseNet using Learned Group Convolutions’. arXiv, Jun. 07, 2018. Accessed: Jun. 22, 2022. \\n[Online]. Available: http://arxiv.org/abs/1711.09224 \\n[43] \\nM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, ‘MobileNetV2: Inverted \\nResiduals and Linear Bottlenecks’. arXiv, Mar. 21, 2019. Accessed: Jun. 22, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/1801.04381 \\n[44] \\nA. G. Howard et al., ‘MobileNets: Efficient Convolutional Neural Networks for Mobile Vision \\nApplications’. arXiv, Apr. 16, 2017. Accessed: Jun. 22, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1704.04861 \\n \\n15 \\n \\n[45] \\nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, ‘SqueezeNet: \\nAlexNet-level accuracy with 50x fewer parameters and <0.5MB model size’. arXiv, Nov. 04, \\n2016. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1602.07360 \\n[46] \\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, ‘Transformer-XL: \\nAttentive Language Models Beyond a Fixed-Length Context’. arXiv, Jun. 02, 2019. Accessed: \\nJun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1901.02860 \\n[47] \\nG. M. Correia, V. Niculae, and A. F. T. Martins, ‘Adaptively Sparse Transformers’. arXiv, Sep. \\n06, 2019. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1909.00015 \\n[48] \\nR. Child, S. Gray, A. Radford, and I. Sutskever, ‘Generating Long Sequences with Sparse \\nTransformers’. arXiv, Apr. 23, 2019. Accessed: Jun. 22, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1904.10509 \\n[49] \\nK. Choromanski et al., ‘Rethinking Attention with Performers’. arXiv, Mar. 09, 2021. Accessed: \\nJun. 22, 2022. [Online]. Available: http://arxiv.org/abs/2009.14794 \\n[50] \\nM. R. Costa-Jussà and J. A. R. Fonollosa, ‘Character-based Neural Machine Translation’. arXiv, \\nJun. 30, 2016. Accessed: Jun. 24, 2022. [Online]. Available: http://arxiv.org/abs/1603.00810 \\n[51] \\nR. Sennrich, B. Haddow, and A. Birch, ‘Neural Machine Translation of Rare Words with \\nSubword Units’. arXiv, Jun. 10, 2016. Accessed: Jun. 24, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1508.07909 \\n[52] \\nQ. Liu et al., ‘Hierarchical Softmax for End-to-End Low-resource Multilingual Speech \\nRecognition’. arXiv, Apr. 08, 2022. Accessed: Jul. 12, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2204.03855 \\n[53] \\nW. Chen, D. Grangier, and M. Auli, ‘Strategies for Training Large Vocabulary Neural Language \\nModels’. arXiv, Dec. 15, 2015. Accessed: Jun. 24, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1512.04906 \\n[54] \\nS. Jean, K. Cho, R. Memisevic, and Y. Bengio, ‘On Using Very Large Target Vocabulary for \\nNeural Machine Translation’. arXiv, Mar. 18, 2015. Accessed: Jun. 24, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/1412.2007 \\n[55] \\nS. Vijayanarasimhan, J. Shlens, R. Monga, and J. Yagnik, ‘Deep Networks With Large Output \\nSpaces’. arXiv, Apr. 10, 2015. Accessed: Jun. 24, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1412.7479 \\n[56] \\nJ. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul, ‘Fast and Robust Neural \\nNetwork Joint Models for Statistical Machine Translation’, in Proceedings of the 52nd Annual \\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, \\nMaryland, 2014, pp. 1370–1380. doi: 10.3115/v1/P14-1129. \\n[57] \\nR. Shu and H. Nakayama, ‘Compressing Word Embeddings via Deep Compositional Code \\nLearning’. arXiv, Nov. 17, 2017. Accessed: Jun. 24, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1711.01068 \\n[58] \\nM. Joshi, E. Choi, O. Levy, D. S. Weld, and L. Zettlemoyer, ‘pair2vec: Compositional Word-\\nPair Embeddings for Cross-Sentence Inference’. arXiv, Apr. 05, 2019. Accessed: Jun. 24, 2022. \\n[Online]. Available: http://arxiv.org/abs/1810.08854 \\n[59] \\nH.-J. M. Shi, D. Mudigere, M. Naumov, and J. Yang, ‘Compositional Embeddings Using \\nComplementary Partitions for Memory-Efficient Recommendation Systems’, in Proceedings of \\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, \\nVirtual Event CA USA, Aug. 2020, pp. 165–175. doi: 10.1145/3394486.3403059. \\n[60] \\nL. Mou, R. Jia, Y. Xu, G. Li, L. Zhang, and Z. Jin, ‘Distilling Word Embeddings: An Encoding \\nApproach’, in Proceedings of the 25th ACM International on Conference on Information and \\nKnowledge Management, Indianapolis Indiana USA, Oct. 2016, pp. 1977–1980. doi: \\n10.1145/2983323.2983888. \\n[61] \\nP. H. Chen, S. Si, Y. Li, C. Chelba, and C. Hsieh, ‘GroupReduce: Block-Wise Low-Rank \\nApproximation for Neural Language Model Shrinking’. arXiv, Jun. 18, 2018. Accessed: Jul. 12, \\n2022. [Online]. Available: http://arxiv.org/abs/1806.06950 \\n[62] \\nC. Wang, K. Cho, and J. Gu, ‘Neural Machine Translation with Byte-Level Subwords’, Proc. \\nAAAI Conf. Artif. Intell., vol. 34, no. 05, pp. 9154–9160, Apr. 2020, doi: \\n10.1609/aaai.v34i05.6451. \\n \\n16 \\n \\n[63] \\nG. Pleiss, D. Chen, G. Huang, T. Li, L. van der Maaten, and K. Q. Weinberger, ‘Memory-\\nEfficient Implementation of DenseNets’. arXiv, Jul. 21, 2017. Accessed: Jun. 24, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/1707.06990 \\n[64] \\nT. Chen, B. Xu, C. Zhang, and C. Guestrin, ‘Training Deep Nets with Sublinear Memory Cost’. \\narXiv, \\nApr. \\n22, \\n2016. \\nAccessed: \\nJun. \\n24, \\n2022. \\n[Online]. \\nAvailable: \\nhttp://arxiv.org/abs/1604.06174 \\n[65] \\nL. Wang et al., ‘Superneurons: dynamic GPU memory management for training deep neural \\nnetworks’, in Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of \\nParallel Programming, Vienna Austria, Feb. 2018, pp. 41–53. doi: 10.1145/3178487.3178491. \\n[66] \\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, ‘ZeRO: Memory optimizations Toward \\nTraining Trillion Parameter Models’, in SC20: International Conference for High Performance \\nComputing, Networking, Storage and Analysis, Atlanta, GA, USA, Nov. 2020, pp. 1–16. doi: \\n10.1109/SC41405.2020.00024. \\n[67] \\nM. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser, ‘Universal Transformers’. \\narXiv, \\nMar. \\n05, \\n2019. \\nAccessed: \\nJun. \\n24, \\n2022. \\n[Online]. \\nAvailable: \\nhttp://arxiv.org/abs/1807.03819 \\n[68] \\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ‘ALBERT: A Lite BERT \\nfor Self-supervised Learning of Language Representations’. arXiv, Feb. 08, 2020. Accessed: Jun. \\n24, 2022. [Online]. Available: http://arxiv.org/abs/1909.11942 \\n[69] \\nS. Takase and S. Kiyono, ‘Lessons on Parameter Sharing across Layers in Transformers’. arXiv, \\nApr. 20, 2022. Accessed: Jun. 24, 2022. [Online]. Available: http://arxiv.org/abs/2104.06022 \\n[70] \\nK. Hashimoto, C. Xiong, Y. Tsuruoka, and R. Socher, ‘A Joint Many-Task Model: Growing a \\nNeural Network for Multiple NLP Tasks’. arXiv, Jul. 24, 2017. Accessed: Jun. 24, 2022. \\n[Online]. Available: http://arxiv.org/abs/1611.01587 \\n[71] \\nE. Park et al., ‘Big/little deep neural network for ultra low power inference’, in 2015 \\nInternational \\nConference \\non \\nHardware/Software \\nCodesign \\nand \\nSystem \\nSynthesis \\n(CODES+ISSS), \\nAmsterdam, \\nNetherlands, \\nOct. \\n2015, \\npp. \\n124–132. \\ndoi: \\n10.1109/CODESISSS.2015.7331375. \\n[72] \\nA. Gormez and E. Koyuncu, ‘Class Means as an Early Exit Decision Mechanism’. arXiv, Oct. \\n20, 2021. Accessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/2103.01148 \\n[73] \\nL. Yang, Y. Han, X. Chen, S. Song, J. Dai, and G. Huang, ‘Resolution Adaptive Networks for \\nEfficient Inference’. arXiv, May 18, 2020. Accessed: Jun. 25, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2003.07326 \\n[74] \\nX. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, ‘SkipNet: Learning Dynamic Routing \\nin Convolutional Networks’. arXiv, Jul. 25, 2018. Accessed: Jun. 25, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1711.09485 \\n[75] \\nZ. Wu et al., ‘BlockDrop: Dynamic Inference Paths in Residual Networks’. arXiv, Jan. 28, 2019. \\nAccessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/1711.08393 \\n[76] \\nA. Veit and S. Belongie, ‘Convolutional Networks with Adaptive Inference Graphs’. arXiv, May \\n08, 2020. Accessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/1711.11503 \\n[77] \\nD. Lepikhin et al., ‘GShard: Scaling Giant Models with Conditional Computation and Automatic \\nSharding’. arXiv, Jun. 30, 2020. Accessed: Jun. 25, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2006.16668 \\n[78] \\nJ. Lin et al., ‘M6: A Chinese Multimodal Pretrainer’. arXiv, May 29, 2021. Accessed: Jun. 25, \\n2022. [Online]. Available: http://arxiv.org/abs/2103.00823 \\n[79] \\nJ.-X. Mi, J. Feng, and K.-Y. Huang, ‘Designing efficient convolutional neural network structure: \\nA \\nsurvey’, \\nNeurocomputing, \\nvol. \\n489, \\npp. \\n139–156, \\nJun. \\n2022, \\ndoi: \\n10.1016/j.neucom.2021.08.158. \\n[80] \\nE. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ‘Regularized Evolution for Image Classifier \\nArchitecture Search’, Proc. AAAI Conf. Artif. Intell., vol. 33, pp. 4780–4789, Jul. 2019, doi: \\n10.1609/aaai.v33i01.33014780. \\n[81] \\nY. Jiang, C. Hu, T. Xiao, C. Zhang, and J. Zhu, ‘Improved Differentiable Architecture Search \\nfor Language Modeling and Named Entity Recognition’, in Proceedings of the 2019 Conference \\non Empirical Methods in Natural Language Processing and the 9th International Joint \\n \\n17 \\n \\nConference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, 2019, pp. \\n3583–3588. doi: 10.18653/v1/D19-1367. \\n[82] \\nA. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, and F. Hutter, ‘Understanding and \\nRobustifying Differentiable Architecture Search’. arXiv, Jan. 28, 2020. Accessed: Jun. 25, 2022. \\n[Online]. Available: http://arxiv.org/abs/1909.09656 \\n[83] \\nX. Chu and B. Zhang, ‘Noisy Differentiable Architecture Search’. arXiv, Oct. 17, 2021. \\nAccessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/2005.03566 \\n[84] \\nR. Luo, F. Tian, T. Qin, E. Chen, and T.-Y. Liu, ‘Neural Architecture Optimization’. arXiv, Sep. \\n04, 2019. Accessed: Jul. 12, 2022. [Online]. Available: http://arxiv.org/abs/1808.07233 \\n[85] \\nW. Chen, X. Gong, and Z. Wang, ‘Neural Architecture Search on ImageNet in Four GPU Hours: \\nA Theoretically Inspired Perspective’. arXiv, Mar. 15, 2021. Accessed: Jun. 25, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/2102.11535 \\n[86] \\nM. S. Abdelfattah, A. Mehrotra, Ł. Dudziak, and N. D. Lane, ‘Zero-Cost Proxies for Lightweight \\nNAS’. \\narXiv, \\nMar. \\n19, \\n2021. \\nAccessed: \\nJun. \\n25, \\n2022. \\n[Online]. \\nAvailable: \\nhttp://arxiv.org/abs/2101.08134 \\n[87] \\nZ. Alyafeai, M. S. AlShaibani, and I. Ahmad, ‘A Survey on Transfer Learning in Natural \\nLanguage Processing’. arXiv, May 31, 2020. Accessed: Jun. 15, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2007.04239 \\n[88] \\nF. Zhuang et al., ‘A Comprehensive Survey on Transfer Learning’, Proc. IEEE, vol. 109, no. 1, \\npp. 43–76, Jan. 2021, doi: 10.1109/JPROC.2020.3004555. \\n[89] \\nS. Ioffe and C. Szegedy, ‘Batch Normalization: Accelerating Deep Network Training by \\nReducing Internal Covariate Shift’. arXiv, Mar. 02, 2015. Accessed: Jul. 12, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/1502.03167 \\n[90] \\nJ. L. Ba, J. R. Kiros, and G. E. Hinton, ‘Layer Normalization’. arXiv, Jul. 21, 2016. Accessed: \\nJul. 03, 2022. [Online]. Available: http://arxiv.org/abs/1607.06450 \\n[91] \\nY. Wu and K. He, ‘Group Normalization’. arXiv, Jun. 11, 2018. Accessed: Jul. 03, 2022. \\n[Online]. Available: http://arxiv.org/abs/1803.08494 \\n[92] \\nL. Gong, D. He, Z. Li, T. Qin, L. Wang, and T.-Y. Liu, ‘Efficient Training of BERT by \\nProgressively Stacking’, presented at the Proceedings of the 36 th International Conference on \\nMachine Learnin, Long Beach, California, 2019, vol. PMLR 97. \\n[93] \\nC. Yang, S. Wang, C. Yang, Y. Li, R. He, and J. Zhang, ‘Progressively Stacking 2.0: A Multi-\\nstage Layerwise Training Method for BERT Training Speedup’. arXiv, Nov. 27, 2020. Accessed: \\nJul. 03, 2022. [Online]. Available: http://arxiv.org/abs/2011.13635 \\n[94] \\nX. He, F. Xue, X. Ren, and Y. You, ‘Large-Scale Deep Learning Optimizations: A \\nComprehensive Survey’. arXiv, Nov. 01, 2021. Accessed: Jun. 09, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2111.00856 \\n[95] \\nP. Micikevicius et al., ‘Mixed Precision Training’. arXiv, Feb. 15, 2018. Accessed: Jun. 30, 2022. \\n[Online]. Available: http://arxiv.org/abs/1710.03740 \\n[96] \\nD. Ghimire, D. Kil, and S. Kim, ‘A Survey on Efficient Convolutional Neural Networks and \\nHardware Acceleration’, Electronics, vol. 11, no. 6, p. 945, Mar. 2022, doi: \\n10.3390/electronics11060945. \\n[97] \\nA. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, ‘A Survey of \\nQuantization Methods for Efficient Neural Network Inference’, ArXiv Comput. Vis. Pattern \\nRecognit., 2021. \\n[98] \\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, ‘Knowledge Distillation: A Survey’, Int. J. Comput. \\nVis., vol. 129, no. 6, pp. 1789–1819, Jun. 2021, doi: 10.1007/s11263-021-01453-z. \\n[99] \\nJ. Yu, L. Yang, N. Xu, J. Yang, and T. S. Huang, ‘Slimmable Neural Networks.’, ArXiv Comput. \\nVis. Pattern Recognit., Dec. 2018. \\n[100] A. Fan, E. Grave, and A. Joulin, ‘Reducing Transformer Depth on Demand with Structured \\nDropout’. arXiv, Sep. 25, 2019. Accessed: Jun. 24, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/1909.11556 \\n[101] K. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, ‘Energy Usage Reports: Environmental \\nawareness as part of algorithmic accountability’, ArXiv Learn., Nov. 2019. \\n \\n18 \\n \\n[102] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau, ‘Towards the Systematic \\nReporting of the Energy and Carbon Footprints of  Machine Learning’, Jan. 2020. [Online]. \\nAvailable: http://arxiv.org/pdf/2002.05651v1 \\n[103] W. A. Hanafy, T. Molom-Ochir, and R. Shenoy, ‘Design Considerations for Energy-efficient \\nInference on Edge Devices’, E-Energy, pp. 302–308, Jun. 2021, doi: 10.1145/3447555.3465326. \\n[104] B. Li et al., ‘Full-Cycle Energy Consumption Benchmark for Low-Carbon Computer Vision’, \\nArXiv210813465 Cs, Oct. 2021, Accessed: May 10, 2022. [Online]. Available: \\nhttp://arxiv.org/abs/2108.13465 \\n[105] S. Budennyy et al., ‘Eco2AI: carbon emissions tracking of machine learning models as the first \\nstep towards sustainable AI’. arXiv, Aug. 03, 2022. Accessed: Sep. 26, 2022. [Online]. \\nAvailable: http://arxiv.org/abs/2208.00406 \\n[106] L. F. W. Anthony, B. Kanding, and R. Selvan, ‘Carbontracker: Tracking and Predicting the \\nCarbon Footprint of Training Deep Learning Models’, ArXiv200703051 Cs Eess Stat, Jul. 2020, \\nAccessed: May 10, 2022. [Online]. Available: http://arxiv.org/abs/2007.03051 \\n[107] V. Schmidt et al., mlco2/codecarbon: v2.1.4. Zenodo, 2022. doi: 10.5281/ZENODO.4658424. \\n[108] O. Shaikh et al., ‘EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML \\nModels’, ArXiv Learn., 2021. \\n[109] L. Lannelongue, J. Grealey, and M. Inouye, ‘Green Algorithms: Quantifying the carbon \\nemissions of computation.’, Jul. 2020. \\n[110] A. Lacoste, Alexandra Luccioni, A. Luccioni, V. Schmidt, and T. Dandres, ‘Quantifying the \\nCarbon Emissions of Machine Learning.’, ArXiv Comput. Soc., Oct. 2019. \\n',\n",
       " '2303.02715v1.pdf': 'Deep Learning in the Field of\\nBiometric Template Protection: An Overview\\nC. Rathgeb∗, J. Kolberg∗, A. Uhl†, C. Busch∗‡\\n∗da/sec – Biometrics and Internet Security Research Group, Hochschule Darmstadt, Germany\\n†WaveLab – Multimedia Signal Processing and Security Lab, Universit¨at Salzburg, Austria\\n‡Norwegian Biometrics Laboratory, Norwegian University of Science and Technology, Norway\\n{christian.rathgeb,jascha.kolberg,christoph.busch}@h-da.de, uhl@cs.sbg.ac.at\\nAbstract—Today, deep learning represents the most popular\\nand successful form of machine learning. Deep learning has\\nrevolutionised the ﬁeld of pattern recognition, including biometric\\nrecognition. Biometric systems utilising deep learning have been\\nshown to achieve auspicious recognition accuracy, surpassing\\nhuman performance. Apart from said breakthrough advances\\nin terms of biometric performance, the use of deep learning\\nwas reported to impact different covariates of biometrics such\\nas algorithmic fairness, vulnerability to attacks, or template\\nprotection.\\nTechnologies of biometric template protection are designed to\\nenable a secure and privacy-preserving deployment of biometrics.\\nIn the recent past, deep learning techniques have been frequently\\napplied in biometric template protection systems for various\\npurposes. This work provides an overview of how advances in\\ndeep learning take inﬂuence on the ﬁeld of biometric template\\nprotection. The interrelation between improved biometric per-\\nformance rates and security in biometric template protection\\nis elaborated. Further, the use of deep learning for obtaining\\nfeature representations that are suitable for biometric template\\nprotection is discussed. Novel methods that apply deep learning\\nto achieve various goals of biometric template protection are\\nsurveyed along with deep learning-based attacks.\\nIndex Terms—Biometrics, template protection, deep learning,\\nsurvey, overview\\nI. INTRODUCTION\\nDeep learning-based methods represent the current state-of-\\nthe-art for solving pattern recognition tasks [1], [2]. In recent\\nyears, advances in deep learning have led to remarkable perfor-\\nmance improvements in numerous areas of pattern recognition\\nincluding biometric recognition [3], [4]. These developments\\nhave further facilitated the incorporation of biometric tech-\\nnologies in personal, commercial, and governmental identity\\nmanagement applications as evidenced by various market\\nvalue studies [5], [6].\\nIn the international standard ISO/IEC 2382-37 [7], the\\nterm biometrics is deﬁned as: “automated recognition of\\nindividuals based on their biological and behavioural charac-\\nteristics”. There exist several highly distinctive biological and\\nbehavioural characteristics which are suitable for recognising\\nindividuals, examples of prominent physiological characteris-\\ntics are shown in ﬁgure 1.\\nIn an automated biometric system, a capture device (e.g.\\na camera) is used to acquire a biometric sample (e.g. facial\\nimage) during the enrolment process. Signal processing al-\\ngorithms are subsequently applied to the biometric sample,\\nFig. 1. Examples of physiological biometric characteristics. From left to right:\\nface, ﬁngerprint, and iris of a single subject.\\nwhich pre-process it (e.g. detection and normalisation of the\\nface), estimate the quality of the acquired sample, and extract\\ndiscriminative features from it. The resulting feature vector is\\nﬁnally stored as reference template. At the time of authen-\\ntication, another biometric sample is processed in the same\\nway resulting in a probe template. Comparison and decision\\nalgorithms enable ascertaining of similarity of a reference\\nand a probe template by comparing the corresponding feature\\nvectors and establish whether or not the two biometric samples\\nbelong to the same source.\\nFor a long period of time, biometric signal processing\\nalgorithms employed handcrafted features (e.g. texture de-\\nscriptors). Nowadays, the use of deep learning has become\\nincreasingly popular. Deep learning-based biometric technolo-\\ngies utilise massive training datasets to learn rich and compact\\nrepresentations of biometric characteristics. Until now, deep\\nlearning has been applied to the vast majority of biometric\\ncharacteristics including, e.g. face [8], ﬁngerprint [9], iris [10],\\nvein [11], online signatures [12], or gait [13]. For a review\\nof deep learning techniques applied within biometrics, the\\ninterested reader is referred to [4], [14], [15].\\nThe success and ubiquitous use of biometric technologies\\nhas raised different privacy concerns due to the potential\\nmisuse of biometric data. In response to this issue, current\\nprivacy regulations contain provisions on the storage and use\\nof biometric data. For instance, the General Data Protection\\nRegulation (GDPR) [16] generally classiﬁes biometric data\\nas sensitive data which require protection. It is important\\nto note that this applies to biometric samples as well as\\nbiometric templates. While the latter might not be human-\\ninterpretable, these can be misused to launch cross-matching\\narXiv:2303.02715v1  [cs.CV]  5 Mar 2023\\nattacks and approximations of biometric samples can be re-\\nconstructed from unprotected biometric templates [17]. It is\\nwell-known that traditional encryption methods are unsuitable\\nfor protecting biometric data, due to the natural intra-class\\nvariance of biometric characteristics. More precisely, biometric\\nvariance prevents the usage of symmetric cryptography and\\ntraditional hash functions with biometric input data since slight\\nchanges in the unprotected domain automatically leads to\\ndrastic changes in the protected domain. Consequently, the\\nuse of conventional cryptographic methods does not enable\\npermanent protection since it would require a decryption of\\nprotected biometric data prior to the comparison.\\nBiometric template protection [18], [19] refers to a class\\nof technologies which are capable of permanently protecting\\nbiometric reference data. In contrast to conventional biometric\\nrecognition methods, biometric template protection schemes\\ngenerate protected reference templates (while unprotected bio-\\nmetric data is discarded). Protected templates prevent from\\nreconstruction attacks, but nevertheless make it possible to\\nperform a biometric comparison in the protected domain.\\nMoreover, template protection scheme typically enable the\\nincorporation of random parameters in the generation process\\nof protected templates. Thereby, protected templates become\\nvariable and can be changed which prevents from cross-\\nmatching attacks. The operating principle of biometric tem-\\nplate protection is detailed in section II.\\nBiometric template protection has been an active ﬁeld of\\nresearch for more than two decades. Figure 2 shows the annual\\nnumber of scientiﬁc publications dealing with biometric tem-\\nplate protection. Clearly, a non-declining interest in biometric\\ntemplate protection is observable. It is reasonable to assume\\nthat biometric template protection approaches introduced in\\nthe last couple of years are incorporating deep learning-based\\nmethods. In many cases, the feature extraction step is based on\\ndeep learning techniques in order to achieve competitive bio-\\nmetric performance which in turn enhances privacy protection\\nas well as security (see section III). However, beyond the use\\nof deep learning for feature extraction, there exist numerous\\nfurther processing steps in biometric template protection that\\ncan beneﬁt from deep learning techniques, e.g. feature type\\ntransformation or feature alignment. This work is intended to\\nprovide an overview of different approaches of incorporating\\ndeep learning methods into various processing steps of bio-\\nmetric template protection schemes with the overall goal of\\nprivacy protection. In addition, deep learning-based attacks on\\nbiometric template protection schemes are summarised. Note\\nthat in contrast to other surveys in the ﬁeld, this work does\\nnot intend to completely survey existing template protection\\nschemes. In contrast, this work is meant to capture general\\ntrends of broad interest in the application of deep learning\\ntechniques in the ﬁeld of biometric template protection. It is\\nmeant to be a milepost along the research area of biometric\\ntemplate protection, offering guidance about future research\\ndirections.\\nThis work is organised as follows: fundamentals of bio-\\nmetric template protection schemes are brieﬂy introduced in\\nFig. 2.\\nAnnual amount of scientiﬁc publications containing the terms\\n“biometric template protection” or “biometric information protection”.1\\n1https://app.dimensions.ai/ (encyclopaedia entries have not been counted)\\nsection II. The impact of accuracy gains in deep learning-\\nbased biometric systems on biometric template protection are\\ndiscussed in section III. Section IV surveys the use of deep\\nlearning techniques for the purpose of feature type transfor-\\nmation. Deep learning-based methods which are employed to\\nachieve privacy protection are summarised in section V. Sub-\\nsequently, deep learning-based attacks on biometric template\\nprotection schemes are discussed in section VI. Section VII\\nlists further works which incorporate deep learning techniques\\nthat are relevant for biometric template protection. Finally, a\\nsummary is given in section VIII.\\nII. BIOMETRIC TEMPLATE PROTECTION\\nFor comprehensive surveys on pre-deep learning biometric\\ntemplate protection the reader is referred to [18]–[21]. Biomet-\\nric template protection schemes use auxiliary data to obtain\\npseudonymous identiﬁers from unprotected biometric data.\\nBiometric comparisons are then performed via pseudonymous\\nidentiﬁers while unprotected biometric data are discarded.\\nBiometric template protection methods are commonly cate-\\ngorised as cancelable biometrics, biometric cryptosystems, and\\nbiometrics in the encrypted domain. Cancelable biometrics\\nemploy transforms in signal or feature domain which enable a\\nbiometric comparison in the transformed (encrypted) domain\\n[22], [23]. In contrast, the majority of biometric cryptosystems\\nbinds a key to a biometric feature vector resulting in a\\nprotected template. Biometric comparison is then performed\\nindirectly by verifying the correctness of a retrieved key\\n[24]. That is, biometric cryptosystems further allow for the\\nderivation of digital keys from protected biometric templates,\\ne.g. fuzzy commitment [25] and fuzzy vault scheme [26].\\nAlternatively, homomorphic encryption has frequently been\\nsuggested for biometric template protection [27], [28]. Homo-\\nmorphic encryption makes it possible to compute operations\\nin the encrypted domain which are functionally equivalent to\\nthose in the plaintext domain and thus enables the estimation\\nof certain distances between protected biometric templates.\\nA general framework for biometric template protection\\nmethods is deﬁned in ISO/IEC 24745 [29]. At enrolment,\\nbiometric data M and some random secret k are fed to\\nFeature\\nextraction\\nTemplate\\nBiometric\\nsample\\nProtected\\ntemplate\\nPseudonymous\\nidentifier\\nencoder\\nKey\\n(a) feature level\\nFeature\\nextraction\\nBiometric\\nsample\\nProtected\\ntemplate\\nPseudonymous\\nidentifier\\nencoder\\nKey\\nProtected\\nsample\\n(b) signal level\\nFig. 3. Pseudonymous identiﬁer encoders applied at (a) feature level and (b) signal level.\\na pseudonymous identiﬁer encoder (PIE). The randomness\\nk can be a subject- or application-speciﬁc secret. PIE then\\ngenerates a pseudonymous identiﬁer PI and auxiliary data\\nAD, PIE(M, k) = [PI, AD]. That is, PI and AD constitute\\nthe protected template. In detail, PI represents a protected\\nidentity and AD is subject-speciﬁc auxiliary data which might\\nas well be protected. AD is used to reproduce PI in the\\nauthentication process. The unprotected biometric data M\\nis discarded at the end of the enrolment procedure. At the\\ntime of authentication, the pseudonymous identiﬁer recorder\\n(PIR) takes biometric data M ∗and AD as input and calcu-\\nlates another pseudonymous identiﬁer PI∗. This process may\\nalso require a repeated presentation of k, e.g. in the case\\nof cancelable biometrics. Subsequently, the pseudonymous\\nidentiﬁer comparator (PIC) compares PI with the stored\\nPI∗, PIC(PI, PI∗) = s. Depending on comparators, the\\ncomparison result s is either a binary decision (yes/no) or\\na similarity score which is then compared against a threshold\\nin order to obtain a binary decision.\\nNearly all biometric template protection schemes can be\\ndescribed in the aforementioned framework. It is important to\\nnote that the biometric data M can be of different represen-\\ntations. While the majority of biometric template protection\\nmethods are applied to biometric feature vectors (templates),\\nsome are applied at signal level, e.g. in the image domain. The\\ndifference in applying the PEI in feature and signal domain is\\nillustrated in ﬁgure 3. In such schemes, a subsequent feature\\nextraction process is necessary which can be seen as part of\\nthe PIE.\\nThe requirements on biometric template protection schemes\\nare deﬁned in ISO/IEC IS 24745 [29]:\\n• Unlinkability: the infeasibility of determining if two or\\nmore protected templates were derived from the same\\nbiometric instance, e.g. face. By fulﬁlling this property,\\ncross-matching across different databases is prevented. It\\ncan only be achieved by incorporating a random secret k\\nin PIE.\\n• Irreversibility: the infeasibility of reconstructing the\\noriginal biometric data given a protected identity PI and\\nits corresponding auxiliary data AD. In certain attack\\nmodels, the secret k may additionally be available2. To\\nachieve this goal the PIE needs to extract the protected\\ntemplate in an irreversible manner using a one-way func-\\ntion. With this property fulﬁlled, the privacy protection\\nis enhanced, and additionally the security of the system\\nis increased against reconstruction attacks.\\n• Renewability: the possibility of revoking old protected\\ntemplates and creating new ones from the same biometric\\ninstance and/or sample, e.g. face image, without the\\nsubject needing to re-enroll. With this property fulﬁlled,\\nit is possible to revoke and reissue the templates in case\\nthe database is compromised, thereby preventing misuse.\\nAgain, this goal is achieved through the incorporation of\\nrandom secrets into PIE.\\n• Performance preservation: the requirement of the bio-\\nmetric performance not being signiﬁcantly impaired by\\nthe protection scheme.\\nSome research efforts and standardisation activities have\\nbeen devoted to establishing metrics for evaluating the\\naforementioned properties of biometric template protection\\nschemes, e.g. in [30]–[34]. Nonetheless, additional speciﬁc\\ncryptanalytic methods may be necessary to precisely estimate\\nthe security/privacy protection achieved by a particular tem-\\nplate protection scheme. Different attack models for describing\\nscenarios and assumptions of attacks on biometric template\\nprotection schemes are standardised in ISO/IEC IS 30136 [34].\\n2In a scenario, where k is known to the attacker, irreversibility can not be\\nachieved by some template protection schemes, e.g. homomorphic encryption.\\nThe most restrictive model is referred to as na¨ıve model in\\nwhich an attacker has neither information of the biometric tem-\\nplate protection scheme, nor owns a large biometric database.\\nHowever, it is common practise to analysed template pro-\\ntection schemes under Kerckhoffs‘s principle. In this general\\nmodel, an adversary is assumed to possess full knowledge of\\nthe template protection algorithm. In addition, the adversary\\nmay have access to one or more protected templates from\\none or more databases. In the most challenging full disclosure\\nmodel, the general model is augmented by disclosing the\\nsecrets (if any) to the attacker.\\nIII. ACCURACY AND PRIVACY PROTECTION\\nAs mentioned before, deep learning-based pattern recog-\\nnition has the edge over previous handcrafted methods [35].\\nObviously, the availability of sufﬁcient training data is a\\nprerequisite deep learning-based approaches. In the ﬁeld of\\nbiometric recognition, the use of deep learning techniques is\\ncontinuously improving biometric performance, i.e. recogni-\\ntion accuracy, for various biometric characteristics [4]. Per-\\nformance gains have been achieved by incorporating deep\\nlearning-based methods at various processing stages of biomet-\\nric systems including detection, segmentation, quality control,\\nor feature extraction. The latter processing step has received\\nmost attention in the development of deep learning techniques.\\nIt should be noted that said performance gains are usually more\\npronounced when a large amount of training data is available\\nand vice versa.\\nThe performance of biometric algorithms is determined by\\nvarious metrics standardised in ISO/IEC IS 19795-1 [36]. Most\\nimportantly, the following metrics are reported to measure\\nrecognition accuracy of a biometric veriﬁcation system:\\n• False Non-Match Rate (FNMR): the FNMR is the\\nproportion of completed mated comparison trials that\\nresult in a comparison decision of “non-match”.\\n• False Match Rate (FMR): the FMR is the proportion of\\na speciﬁed set of completed non-mated comparison trials\\nthat result in a comparison decision of “match”.\\nIn template protection schemes, these performance metrics\\nare also applicable. However, for some classes of schemes, e.g.\\nbiometric cryptosystems, the conﬁguration of a desired opera-\\ntion point may turn out to be more complex. While the FNMR\\nis related to the usability of the biometric system, the FMR\\nis a measure of security. Both rates depend on the decison\\nthreshold of the biometric system. There is a relation between\\nthe FMR and the security of a template protection scheme.\\nThe so-called False Accept Security (FAS) is expressed in\\nbits and corresponds to an encryption using a key of that bit-\\nlength. The FAS provides a good approximation of the security\\nof a template protection scheme because the false accept\\nattack is typically one of the most efﬁcient attacks. In a false\\naccept attack, the adversary iteratively simulates non-mated\\nauthentication trails until a false match is reached. The FAS (in\\nbits) is often estimated as FAS = −log2(FMR). For instance,\\na biometric system operated at a FMR of 0.01% achieves a\\nsecurity level of approximately 13 bits. However, it should\\nbe noted that an attacker can deviate from the parameters\\ncorresponding to the operational FMR. Additionally, the time\\nrequired for performing a single authentication attempt should\\nbe taken into account when estimating the FAS. Alternatively\\nto the FAS, many published works report the Brute Force\\nSecurity (BFS) as a measure of security. Usually, the BFS is\\nderived from the size of an employed secret k (b= key space).\\nHowever, the BFS often overestimates the effective security\\nof a biometric template protection scheme.\\nThe aforementioned FAS directly relates to the privacy\\nprotection cappabilities of a biometric template protection\\nscheme. Assume that an attacker requires at most FMR−1\\nauthentication attempts to ﬁnd a biometric data M ′ which\\nresults in a false match for a target protected template. This\\nmeans, M ′ is sufﬁciently close to M that was used to generate\\nan attacked protected template [PI, AD]. Starting from M ′,\\nfor the majority of template protection schemes, it is then\\npossible for the attacker to reconstruct the original biometric\\ndata M. In case M is a binary feature vector, the attacker\\ncould iteratively ﬂip bits of M ′ until a false non-match is\\nreached resulting in ˆ\\nM. Hence, the Hamming distance of ˆ\\nM\\nand M is exactly one bit above the threshold for reaching a\\nfalse match. The attacker would then iteratively ﬂip bits of\\nˆ\\nM. If ﬂipping the i-th bit results in a false match, the bit at\\nthe i-th position of ˆ\\nM is equal to the ﬂipped bit. If not, the\\nbit at the i-th position of ˆ\\nM has already been correct. Similar\\nattacks can be applied for non-binary data representations.\\nMany template protection concepts, in particular biometric\\ncryptosystems, have been investigated several years ago, c.f.\\nﬁgure 2. Hence, such earlier approaches have been applied\\nto biometric systems utilising handcrafted feature extractors.\\nCompared to such handcrafted approaches, deep learning-\\nbased methods have been shown to improve biometric per-\\nformance for systems based on different biometric charac-\\nteristics [4]. This means that deep learning-based biometric\\nsystems can be potentially operated at lower FMRs and,\\nthus, provide a higher level of security. The same applies to\\nbiometric template protection schemes utilising deep learning-\\nbased techniques. For instance, recently proposed biometric\\ncryptosystems utilizing deep learning-based face recognition\\ncan be operated at FMRs signiﬁcantly below 0.1%, e.g. in [37],\\n[38], while comparable older approaches based on handcrafted\\nfeature extractors report unpractical FMRs above 10%, e.g.\\nin [39]. While many biometric performance benchmarks have\\nbeen published for various biometric characteristics, a com-\\nprehensive empirical comparison between earlier handcrafted\\nmethods and current deep learning-based methods is still\\nmissing.\\nThrough reduced error rates the level of privacy protection is\\nsigniﬁcantly increased in such systems. However, some studies\\nhave also reported an increased vulnerability of deep-learning-\\nbased biometric systems to certain attacks subjected to their\\nenhanced generalisation capabilities, e.g. in [40], [41].\\nIV. FEATURE TYPE TRANSFORMATIONS\\nBiometric feature vectors, i.e. templates, can be of different\\nrepresentations depending on the type of feature elements\\n(real, integer, or binary), their dimension and if they are ﬁxed-\\nlength or variable-length. Common feature representations\\nhave been established for templates of different biometric char-\\nacteristics, e.g. minutiae sets for ﬁngerprints or binary codes\\nfor iris. However, biometric template protection schemes, in\\nparticular biometric cryptosystems, may require templates in\\na distinct type of feature representation, e.g. ﬁxed-length\\nbinary strings for the fuzzy commitment scheme [25]. To\\nmake biometric templates compatible to a template protection\\nscheme, so-called feature type transformation processes may\\nbe required [42].\\nDeep learning-based biometric feature extraction methods\\nare commonly trained using differentiable loss functions, e.g.\\nEuclidean distance. Therefore, extracted templates are usually\\nrepresented as real-valued vector v of ﬁxed dimension n,\\nv ∈Rn. Compared to variable sized feature vectors, this\\nis favourable since it is expected to yield an equal level\\nof security across subjects. It is important to note that the\\nextraction of ﬁxed-length vectors is also possible for biometric\\ncharacteristics where templates are usually not of variable size,\\ne.g. ﬁngerprints. In [43], it was shown that deep networks can\\nlearn to extract ﬁxed-length ﬁngerprint representations incor-\\nporating domain knowledge, such as alignment and keypoint\\n(minutiae) detection3.\\nFixed-length real-valued vectors can be mapped to other\\nrepresentations using common procedures [42]. Based on\\nstatistical information about feature distributions, the feature\\nspace of each feature element can be divided in (equally\\nprobable) integer-labelled intervals. Subsequently, a quanti-\\nsation can be performed by mapping each feature element\\nto the integer value representing its interval. This results\\nin a quantised feature vector q of same length, q ∈Nn\\n0.\\nTo obtain a binary feature vector b, each feature element\\nof q is mapped to a binary string of length m. Different\\nbinary encoding methods have been suggested in the scientiﬁc\\nliterature [42]. The resulting binary vector consists consist of\\nnm bits, b ∈{0, 1}nm. These feature type transformation\\nsteps, which are illustrated in ﬁgure 4, may result in a loss of\\nbiometric performance due to information loss cause by coarse\\nquantisation. However, if parameters are chosen appropriately\\nbiometric performance may be maintained, e.g. in [45]. A\\nbinary vector b of length nm can be transformed back to an\\ninteger vector q of length n by mapping consecutive chunks\\nof m bits to their decimal representation. Further, b can be\\ntransformed to an integer set s of variable length. For instance,\\nthis feature set can consist of all indexes of 1s in the binary\\nvector, s = {i|bi = 1} with bi ∈b. This feature type\\ntransformation has been successfully applied to feature vectors\\nobtained by deep learning-based feature extractors, e.g. in [37].\\n3Note that a ﬁxed-length ﬁngerprint representation is different from ﬁxed-\\nlength minutiae (vicinity) representations, e.g. in [44], which usually result in\\na variable length ﬁngerprint representation.\\nDCNN-based feature\\nextraction\\nBiometric\\nsample\\nQuantisation\\nfloat vector (0.5, -0.8, . . . , 0.7)\\ninteger vector (2, 0, . . . , 3)\\nBinary encoding\\nbinary vector (10, 00, . . . , 11)\\nFig. 4. Feature type transformation: creating integer and binary vectors from\\nﬂoat vectors extracted by deep neural networks.\\nCompact, e.g. binary, representations (which additionally\\nturn out to be beneﬁcial for workload reduction in biometric\\nidentiﬁcation systems [46]) can also be extracted by deep\\nlearning techniques. The term deep hashing has been coined as\\nan umbrella term for methods which aim at extracting compact\\nand stable representations with deep learning techniques [47].\\nIf applied to biometric data, such methods would need to\\novercome intra-class variations. Consequently, a reliable ex-\\ntraction of stable feature vectors (deep hashes) would enable\\na subsequent application of conventional (and provable secure)\\ncryptographic algorithms for the purpose of template protec-\\ntion. In the recent past, deep hashes have been extracted from\\ndifferent biometric characteristics in various ways, e.g. in [48]–\\n[51], including multi-biometrics [51]. Most of the proposed\\nschemes, however, are focusing on facial images and have\\nbeen recently surveyed in [52].\\nIt is well-known that a single biometric characteristic, e.g.\\na single ﬁngerprint or face, contains an insuffcient amount of\\neffective entropy to resist attacks exploiting the feature dis-\\ntribution, speciﬁcally false accept attacks. Therefore, several\\nresearchers have proposed multi-biometric template protection\\nsystems [53], e.g. in [54], [55]. In multi-biometric template\\nprotection systems, the fusion process should be performed\\nat the feature level to achieve high security levels [56].\\nIn contrast, separate storage of multiple protected biometric\\nBiometric\\nsample\\nProtected\\ntemplate\\nPseudonymous\\nidentifier\\nencoder\\nKey\\nFig. 5. Application of deep neural network for biometric template protection.\\ntemplates would enable parallelized attacks [57] (analogous to\\nthe use of multiple short passwords compared to one long pass-\\nword). In multi-biometric template protection schemes, feature\\ntype transformation methods become increasingly important.\\nTo conduct a feature level fusion, it is vital that features of\\nmultiple biometric sources are of the same representation. As\\nmentioned before, this is generally feasible for the application\\nof deep learning-based feature extraction methods.\\nNote that in case a fusion of different biometric characteris-\\ntics is performed, it is likely that corresponding templates are\\nof different size. This means, in a concatenation of templates\\n(of the same representation) an implicit weighting is intro-\\nduced according to the template sizes, i.e. larger templates are\\nstronger weighted and vice versa. Such imbalance can in turn\\nhave a negative impact on the overall security of a biometric\\ntemplate protection system, since an attacker could merely\\nfocus on launching a false accept attack on the biometric\\ncharacteristic which constitutes the largest relative part of the\\nfused template. Further, it is important to note that templates\\nextracted from different biometric characteristics may exhibit\\nvarying biometric variances (intra- and inter-class).\\nV. TEMPLATE PROTECTION WITH DEEP NETWORKS\\nVery recently, ﬁrst attempts have been made to directly\\nincorporate biometric template protection into deep learning-\\nbased feature extraction. The key idea behind these approaches\\nis to embed some randomness into neural network-based\\nfeature extraction methods. That is, the neural network itself\\nserves as PIE taking a biometric sample M and a random key\\nk as input, see ﬁgure 5. This can be achieved by introducing\\na key-based random activation of neurons, i.e. a random sub-\\nnetwork selection, or random permutation, e.g. in [58]. Such\\na network can be applied subsequently to an existing network\\ntrained for biometric recognition. Alternatively, networks can\\nbe trained from scratch or a pre-trained model can be adapted\\nto achieve template protection properties [59]. Moreover, re-\\nsearchers have suggested special loss functions, e.g. in [59],\\n[60], that may even incorporate a comparison of keys as\\nsuggested in [59].\\nThe aforementioned concepts have mostly been applied to\\nfacial data [52]. However, similar schemes have already been\\nproposed for other biometric characteristics as well as multi-\\nbiometric systems, e.g. in [61], [62]. While the reported results\\nof these very recently proposed methods are promising, a\\nrigorous analysis of potential vulnerabilities is needed.\\nSo-called privacy-enhancing biometrics have been recently\\nproposed by various research groups. These methods do not di-\\nrectly fall under the category of biometric template protection.\\nIn contrast to traditional template protection schemes, these\\nmethods attempt to only remove (or conceal) soft-biometric\\ninformation, e.g. gender or age, from biometric data (while\\nleaving other useful information unchanged). In other words,\\nthese approaches could be seen as attempting to fulﬁll the\\nrequirement of irreversibility for soft biometric attributes while\\nunlinkability or renewability are not intended properties [63].\\nConsequently, privacy-enhancing biometrics do not require the\\nincorporation of a random secret. Like cancelable biometric\\ntransformations, privacy-enhancing functions can be applied at\\neither image level, representation level, or at inference level. At\\nimage level deep learning-based techniques, e.g. convolutional\\nautoencoder, can be applied to perturb images with the goal of\\nconcealing soft-biometric information [64]. At representation\\nlevel it is possible to (re-)train a biometric recognition system\\nwhile incorporating soft-biometric classiﬁers in the used loss\\nfunction. Thereby, the utility of the extracted features should\\nbe maximised while at the same time soft-biometric informa-\\ntion should be minimised.\\nThe majority of works on soft-biometric privacy enhance-\\nment focuses on face recognition [65], while the underly-\\ning concepts have been shown to be applicable to other\\nbiometric characteristics from which it is possible to derive\\nsoft-biometric information, e.g. gait [66]. Many works have\\nreported impressive results, e.g. in [67], [68], while the pri-\\nvacy protection capabilities of proposed methods are usually\\ntested using simple machine learning-based classiﬁers and\\nvisualisations of dimensionality reduction tools. It has been\\nargued that a more rigorous analysis is necessary to measure\\nthe actual privacy enhancement provided by such techniques\\n[69]. Moreover, it has already been shown that different soft-\\nbiometric privacy enhancement methods can be detectable [70]\\nand vulnerable to elaborated attacks [71].\\nVI. DEEP LEARNING-BASED ATTACKS\\nBiometric systems are challenged by numerous potential\\nattack vectors, see [72]. Some of the common attacks, e.g.\\npresentation attacks, require knowledge about the original\\nbiometric data M of the target subject to be attacked. In\\nbiometric template protection schemes, M is replaced by a\\nprotected biometric reference [PI, AD] such that said attacks\\nbecome less relevant. However, it was recently shown that\\ndeep learning techniques can be applied to create artiﬁcial\\nbiometric samples for which the probability of achieving a\\nfalse match when being compared against unknown biomet-\\nric references is signiﬁcantly higher than that of a random\\nimpostor. Inspired by the concept of master keys, different\\nresearchers proposed to train a Generative Adversarial Net-\\nwork (GAN) to create such biometric samples. These samples\\ncan then be applied to launch dictionary attacks on biometric\\nveriﬁcation systems that allow multiple authentication attempts\\non biometric identiﬁcation systems. The effectiveness of such\\nattacks has been demonstrated for ﬁngerprints [73] and face\\nimages [74]. Theoretically, such attacks can be applied to\\ntemplate protection systems, too.\\nAs mentioned earlier, a false match with [PI, AD] usu-\\nally makes it possible for an attacker to reconstruct a close\\napproximation of M. In most cases, M is a feature vector\\nof a certain type, e.g. binary. This means, an attacker may\\nadditionally want to reconstruct the corresponding biometric\\nsample, e.g. ﬁngerprint or face image. Such a sample could\\nthen be misused by the attacker to launch further attacks in\\nparticular presentation attacks [75].\\nDevelopments in deep learning have shown impressive\\nresults for reconstructing biometric samples from their corre-\\nsponding feature vectors. Particularly deconvolutional neural\\nnetworks can be employed to reconstruct biometric samples\\nfrom their corresponding embeddings in the latent space, e.g.\\nfor face in [76]. Further, deep learning-based methods can be\\nemployed to reconstruct biometric samples from conventional\\nfeature representations, e.g. minutiae sets in [77]. For different\\nbiometric characteristics, it has been shown that this is even\\npossible for binarised feature vectors. For instance, speciﬁc\\ndeep learning-based methods have been proposed to recon-\\nstruct face and vein images from binary feature vectors in [78]\\nand [79], respectively. This poses a severe security risk which\\nnecessitates strong protection of biometric reference data to\\nprevent from misuse, e.g. identity fraud. In biometric template\\nprotection schemes, said reconstruction attacks become a seri-\\nous threat, especially in a full disclosure attack model. In this\\nattack model, the attacker is in possession of the secret k such\\nthat the original biometric data M can likely be reconstructed\\n[80]. Directly reconstructing biometric samples from protected\\ntemplates with such approaches may be infeasible. However,\\nreconstruction methods can be applied after launching a false\\naccept attack [78].\\nThe aforementioned attacks can be applied as part of an\\nattack on biometric template protection. However, so far no\\ndeep learning-based methods that directly attack template pro-\\ntection schemes have been published in the scientiﬁc literature.\\nNevertheless, it is reasonable to assume that such attacks may\\nsoon be proposed.\\nFinally, it is worth noting that the improved generalisation\\ncapabilities of deep learning-based biometric systems can\\nlead to robustness against certain perturbations which have\\nbeen introduced for the purpose of template protection. Such\\nperturbations have frequently been proposed for cancelable\\nbiometrics [22]. For instance, it has been shown in [81] that\\ncurrent deep learning-based face recognition systems are to a\\ncertain degree robust against the application of image warping\\nwhich has previously been suggested to obscure and, hence,\\nprotect facial data [72].\\nVII. FURTHER WORKS\\nSome works in the area of deep learning are focusing\\non further processing steps that are vital in biometric tem-\\nplate protection schemes. One example is pre-alignment of\\nbiometric data. Since template protection schemes protect\\nand, therefore, conceal the original biometric data it is often\\nimpossible to perform an alignment during comparison. This\\nis particularly the case for biometric cryptosystems. Alignment\\nprocesses at comparison stage are for instance commonly\\nrequired for pairwise comparisons of ﬁngerprints [82] or irides\\n[83]. Since the original biometric data is “unseen” during the\\ncomparison of protected templates, pre-alignment methods are\\nnecessary.\\nIn [84], [85], different deep learning techniques have been\\ninvestigated for the purpose of ﬁngerprint alignment. In par-\\nticular, a ﬁngerprint pre-alignment method which can be\\nemployed as pre-processing step in ﬁngerprint-based template\\nprotection systems was proposed in [85]. Similar approaches\\ncould be applied to other biometric characteristics.\\nAs mentioned in the beginning, homomorphic encryption\\ncan be used to permanently protect biometric data. Homomor-\\nphic encryption makes it possible to compute basic operations\\nin the encrypted domain which are functionally equivalent to\\nthose in the plaintext domain and thus enables the estimation\\nof certain distances between protected biometric templates.\\nIn addition, it has been shown that deep Learning-based\\nclassiﬁcation tasks can be performed on homomorphically\\nencrypted data [86]. These so-called CryptoNets could also\\nbe applied to perform neural network-based classiﬁcations on\\nhomomorphically encrypted biometric data, e.g. prediction of\\nsoft-biometric attributes. Motivated by that fact that homomor-\\nphic encryption does not achieve irreversibility when the secret\\nk is known to an attacker, researchers have recently proposed\\nto apply homomorphic encryption on top of cancelable bio-\\nmetrics [87], [88]. Thereby, some protection is retained, even\\nin the full disclosure attack model.\\nVIII. SUMMARY\\nThe interest in biometric technologies has been steadily\\ngrowing in recent years. Systems incorporating biometric\\ntechnologies have become ubiquitous in personal, commercial,\\nand governmental identity management applications. Deep\\nneural networks currently represent the most popular method\\nfor pattern recognition and are applied in various biometric\\nsystems.\\nIn response to privacy concerns raised against the (mis-)use\\nof biometric data, template protection schemes that embody\\nfundamental data protection principles have been introduced.\\nThis work provides an overview of how recent developments\\nin deep learning affect biometric template protection. Several\\nresearch directions are discussed in which deep learning-\\nbased techniques can be employed to improve template pro-\\ntection systems. In addition, deep learning-based attacks are\\ndescribed. By focusing on recent trends of deep learning in the\\nresearch ﬁeld of template protection, this survey is intended\\nto complement existing works that review template protection\\nmethods published in the scientiﬁc literature.\\nACKNOLEDGEMENTS\\nThis research work has been funded by the German Federal\\nMinistry of Education and Research and the Hessian Ministry\\nof Higher Education, Research, Science and the Arts within\\ntheir joint support of the National Research Center for Applied\\nCybersecurity ATHENE.\\nREFERENCES\\n[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\\nno. 7553, pp. 436–444, May 2015.\\n[2] I. J. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.\\nCam-\\nbridge, MA, USA: MIT Press, 2016.\\n[3] A. K. Jain, P. Flynn, and A. Ross, Handbook of biometrics.\\nSpringer,\\n2007.\\n[4] K. Sundararajan and D. L. Woodard, “Deep learning for biometrics: A\\nsurvey,” Computing Surveys (CSUR), vol. 51, no. 3, pp. 1–34, July 2018.\\n[5] L. Pascu, “Global biometrics market to surpass $45B by 2024,\\nreports Frost & Sullivan,” https://www.biometricupdate.com/202003/\\nglobal-biometrics-market-to-surpass-45b-by-2024-reports-frost-sullivan,\\nMarch 2020.\\n[6] Markets and Markets, “Biometric system market by authentication type\\n- global forecast to 2023,” Markets and Markets, Tech. Rep. SE 3449,\\nJuly 2018.\\n[7] ISO/IEC JTC1 SC37 Biometrics, ISO/IEC 2382-37:2017. Information\\ntechnology – Vocabulary – Part 37: Biometrics, 2nd ed., Interna-\\ntional Organization for Standardization and International Electrotech-\\nnical Committee, February 2017.\\n[8] O. M. Parkhi, A. Vedaldi, A. Zisserman et al., “Deep face recognition,”\\nin British Machine Vision Conference (BMVC).\\nBMVA Press, Septem-\\nber 2015, pp. 1–6.\\n[9] Y. Tang, F. Gao, J. Feng, and Y. Liu, “FingerNet: An uniﬁed deep\\nnetwork for ﬁngerprint minutiae extraction,” in Intl. Joint Conf. on\\nBiometrics (IJCB).\\nIEEE, October 2017, pp. 108–116.\\n[10] N. Liu, M. Zhang, H. Li, Z. Sun, and T. Tan, “DeepIris: Learning pair-\\nwise ﬁlter bank for heterogeneous iris veriﬁcation,” Pattern Recognition\\nLetters, vol. 82, pp. 154–161, 2016.\\n[11] R. Das, E. Piciucco, E. Maiorana, and P. Campisi, “Convolutional\\nneural network for ﬁnger-vein-based biometric identiﬁcation,” IEEE\\nTransactions on Information Forensics and Security, vol. 14, no. 2, pp.\\n360–373, 2019.\\n[12] R. Tolosana, R. Vera-Rodriguez, J. Fierrez, and J. Ortega-Garcia,\\n“Exploring recurrent neural networks for on-line handwritten signature\\nbiometrics,” IEEE Access, vol. 6, pp. 5128–5138, 2018.\\n[13] K. Shiraga, Y. Makihara, D. Muramatsu, T. Echigo, and Y. Yagi,\\n“GEINet: View-invariant gait recognition using a convolutional neural\\nnetwork,” in Intl. Conf. on Biometrics (ICB), 2016, pp. 1–8.\\n[14] B. Bhanu and A. Kumar, Deep Learning for Biometrics, 1st ed. Springer\\nPublishing Company, Incorporated, 2017.\\n[15] R. Jiang, C.-T. Li, D. Crookes, W. Meng, and C. Rosenberger, Deep\\nBiometrics, 1st ed.\\nSpringer, 2020.\\n[16] European Parliament, “Regulation (EU) 2016/679,” Ofﬁcial Journal of\\nthe European Union, vol. L119, pp. 1–88, April 2016.\\n[17] M. Gomez-Barrero and J. Galbally, “Reversing the irreversible: A survey\\non inverse biometrics,” Computers & Security, vol. 90, March 2020.\\n[18] C. Rathgeb and A. Uhl, “A survey on biometric cryptosystems and\\ncancelable biometrics,” EURASIP Journal on Information Security, vol.\\n2011, no. 3, 2011.\\n[19] K. Nandakumar and A. K. Jain, “Biometric template protection: Bridg-\\ning the performance gap between theory and practice,” IEEE Signal\\nProcessing Magazine - Special Issue on Biometric Security and Privacy,\\npp. 1–12, 2015.\\n[20] M. Barni, G. Droandi, and R. Lazzeretti, “Privacy protection in\\nbiometric-based recognition systems: A marriage between cryptography\\nand signal processing,” IEEE Signal Processing Magazine, vol. 32, no. 5,\\npp. 66–76, 2015.\\n[21] M. Sandhya and M. V. N. K. Prasad, “Biometric template protection:\\nA systematic literature review of approaches and modalities,” Biometric\\nSecurity and Privacy, pp. 323–370, 2017.\\n[22] V. M. Patel, N. K. Ratha, and R. Chellappa, “Cancelable biometrics: A\\nreview,” IEEE Signal Processing Magazine, vol. 32, no. 5, pp. 54–65,\\n2015.\\n[23] N. Kumar, “Cancelable biometrics: A comprehensive survey,” Artiﬁcial\\nIntelligence Review, pp. 1–44, 2019.\\n[24] U. Uludag, S. Pankanti, S. Prabhakar, and A. K. Jain, “Biometric\\ncryptosystems: issues and challenges,” Proc. of the IEEE, vol. 92, no. 6,\\npp. 948–960, 2004.\\n[25] A. Juels and M. Wattenberg, “A fuzzy commitment scheme,” in 6th\\nACM Conf. on Computer and Communications Security (CCS), 1999,\\npp. 28–36.\\n[26] A. Juels and M. Sudan, “A fuzzy vault scheme,” in IEEE Intl. Symposium\\non Information Theory (ISIT), 2002, p. 408.\\n[27] C. Aguilar-Melchor, S. Fau, C. Fontaine, G. Gogniat, and R. Sirdey,\\n“Recent advances in homomorphic encryption: A possible future for\\nsignal processing in the encrypted domain,” IEEE Signal Processing\\nMagazine, vol. 30, no. 2, pp. 108–117, 2013.\\n[28] A. Acar, H. Aksu, A. S. Uluagac, and M. Conti, “A survey on\\nhomomorphic encryption schemes: Theory and implementation,” ACM\\nComputing Surveys (CSUR), vol. 51, no. 4, pp. 1–35, 2018.\\n[29] ISO/IEC JTC1 SC27 Security Techniques, ISO/IEC 24745:2022. In-\\nformation Technology - Security Techniques - Biometric Information\\nProtection, International Organization for Standardization, 2022.\\n[30] A. Nagar, K. Nandakumar, and A. K. Jain, “Biometric template trans-\\nformation: a security analysis,” in Media Forensics and Security II, vol.\\n7541.\\nSPIE, 2010, pp. 237 – 251.\\n[31] Y. Wang, S. Rane, S. C. Draper, and P. Ishwar, “A theoretical analysis of\\nauthentication, privacy, and reusability across secure biometric systems,”\\nIEEE Transactions on Information Forensics and Security, vol. 7, no. 6,\\npp. 1825–1840, 2012.\\n[32] K. Simoens, B. Yang, X. Zhou, F. Beato, C. Busch, E. Newton,\\nand B. Preneel, “Criteria towards metrics for benchmarking template\\nprotection algorithms,” in Intl. Conf. on Biometrics (ICB), 2012, pp.\\n498–505.\\n[33] M. Gomez-Barrero, J. Galbally, C. Rathgeb, and C. Busch, “General\\nframework to evaluate unlinkability in biometric template protection\\nsystems,” IEEE Transactions on Information Forensics and Security,\\nvol. 13, no. 6, pp. 1406–1420, 2018.\\n[34] ISO/IEC JTC1 SC37 Biometrics, ISO/IEC 30136:2018. Information\\ntechnology – Performance testing of biometric template protection,\\nInternational Organization for Standardization, 2018.\\n[35] L. Liu, J. Chen, P. Fieguth, G. Zhao, R. Chellappa, and M. Pietik¨ainen,\\n“From BoW to CNN: Two decades of texture representation for texture\\nclassiﬁcation,” International Journal of Computer Vision, vol. 127, no. 1,\\npp. 74–109, January 2019.\\n[36] ISO/IEC JTC1 SC37 Biometrics, ISO/IEC 19795-1:2021. Information\\nTechnology – Biometric Performance Testing and Reporting – Part 1:\\nPrinciples and Framework, International Organization for Standardiza-\\ntion, June 2021.\\n[37] C. Rathgeb, J. Merkle, J. Scholz, B. Tams, and V. Nesterowicz, “Deep\\nface fuzzy vault: Implementation and performance,” Computers &\\nSecurity, vol. 113, p. 102539, 2022.\\n[38] B. P. Gilkalaye, A. Rattani, and R. Derakhshani, “Euclidean-distance\\nbased fuzzy commitment scheme for biometric template security,” in\\nIntl. Workshop on Biometrics and Forensics (IWBF), 2019, pp. 1–6.\\n[39] L. Wu and S. Yuan, “A face based fuzzy vault scheme for secure online\\nauthentication,” in 2nd International Symposium on Data, Privacy, and\\nE-Commerce (ISDPE), 2010, pp. 45–49.\\n[40] A. Mohammadi, S. Bhattacharjee, and S. Marcel, “Deeply vulnerable: a\\nstudy of the robustness of face recognition to presentation attacks,” IET\\nBiometrics, vol. 7, no. 1, pp. 15–26, 2018.\\n[41] U. Scherhag, C. Rathgeb, J. Merkle, R. Breithaupt, and C. Busch, “Face\\nrecognition systems under morphing attacks: A survey,” IEEE Access,\\nvol. 7, pp. 23 012–23 026, 2019.\\n[42] M.-H. Lim, A. B. J. Teoh, and J. Kim, “Biometric feature-type trans-\\nformation: Making templates compatible for secret protection,” IEEE\\nSignal Processing Magazine, vol. 32, no. 5, pp. 77–87, 2015.\\n[43] J. J. Engelsma, K. Cao, and A. K. Jain, “Learning a ﬁxed-length\\nﬁngerprint representation,” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, vol. 43, no. 6, pp. 1981–1997, 2021.\\n[44] R. Cappelli, M. Ferrara, and D. Maltoni, “Minutia cylinder-code: A new\\nrepresentation and matching technique for ﬁngerprint recognition,” IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, vol. 32,\\nno. 12, pp. 2128–2141, 2010.\\n[45] P. Drozdowski, F. Struck, C. Rathgeb, and C. Busch, “Benchmarking\\nbinarisation schemes for deep face templates,” in Intl. Conf. on Image\\nProcessing (ICIP).\\nIEEE, 2018, pp. 1–5.\\n[46] P. Drozdowski, C. Rathgeb, and C. Busch, “Computational workload in\\nbiometric identiﬁcation systems: An overview,” IET Biometrics, vol. 8,\\nno. 6, pp. 351–368, 2019.\\n[47] Z. Cao, M. Long, J. Wang, and P. S. Yu, “HashNet: Deep learning to\\nhash by continuation,” in Intl. Conf. on Computer Vision (ICCV), 2017,\\npp. 5609–5618.\\n[48] A. K. Jindal, S. Chalamala, and S. K. Jami, “Face template protection\\nusing deep convolutional neural network,” in Conf. on Computer Vision\\nand Pattern Recognition Workshops (CVPRW), 2018, pp. 575–5758.\\n[49] Q. Zhang, X. Zhao, and Y. Hu, “A classiﬁcation retrieval method for\\nencrypted speech based on deep neural network and deep hashing,” IEEE\\nAccess, vol. 8, pp. 202 469–202 482, 2020.\\n[50] J. Cui and A. B. J. Teoh, “Deep index-of-maximum hashing for face\\ntemplate protection,” in Intl. Conf. on Computer and Communication\\nSystems (ICCCS), 2020, pp. 413–418.\\n[51] V. Talreja, M. C. Valenti, and N. M. Nasrabadi, “Deep hashing for secure\\nmultimodal biometrics,” IEEE Transactions on Information Forensics\\nand Security, vol. 16, pp. 1306–1321, 2021.\\n[52] V. K. Hahn and S. Marcel, “Biometric template protection for neural-\\nnetwork-based face recognition systems: A survey of methods and\\nevaluation techniques,” ArXiv preprint, vol. abs/2110.05044, 2021.\\n[53] C. Rathgeb and C. Busch, “Multibiometric template protection: Issues\\nand challenges,” in New Trends and Developments in Biometrics.\\nIn-\\nTech, 2012, pp. 173–190.\\n[54] A. Nagar, K. Nandakumar, and A. Jain, “Multibiometric cryptosystems\\nbased on feature-level fusion,” Transactions on Information Forensics\\nand Security, vol. 7, no. 1, pp. 255–268, 2012.\\n[55] M. Gomez-Barrero, E. Maiorana, J. Galbally, P. Campisi, and J. Fierrez,\\n“Multi-biometric template protection based on Homomorphic Encryp-\\ntion,” Pattern Recognition, vol. 67, pp. 149–163, July 2017.\\n[56] J. Merkle, T. Kevenaar, and U. Korte, “Multi-modal and multi-instance\\nfusion for biometric cryptosystems,” in Intl. Conf. of Biometrics Special\\nInterest Group (BIOSIG), 2012, pp. 1–6.\\n[57] C. Rathgeb and C. Busch, “Biometric template protection: state-of-the-\\nart, issues and challenges,” in User-Centric Privacy and Security in\\nBiometrics, ser. Security.\\nInstitution of Engineering and Technology,\\n2017, pp. 173–191.\\n[58] G. Mai, K. Cao, X. Lan, and P. C. Yuen, “SecureFace: Face template\\nprotection,” IEEE Transactions on Information Forensics and Security,\\nvol. 16, pp. 262–277, 2020.\\n[59] J. R. Pinto, M. V. Correia, and J. S. Cardoso, “Secure triplet loss:\\nAchieving cancelability and non-linkability in end-to-end deep biomet-\\nrics,” IEEE Transactions on Biometrics, Behavior, and Identity Science,\\nvol. 3, no. 2, pp. 180–189, 2021.\\n[60] H. Lee, C. Y. Low, and A. B. Jin Teoh, “Softmaxout transformation-\\npermutation network for facial template protection,” in 2020 25th\\nInternational Conference on Pattern Recognition (ICPR), 2021, pp.\\n7558–7565.\\n[61] J. Kim, Y. G. Jung, and A. B. J. Teoh, “Multimodal biometric template\\nprotection based on a cancelable softmaxout fusion network,” Applied\\nSciences, vol. 12, no. 4, 2022.\\n[62] E. Abdellatef, N. A. Ismail, S. E. S. E. Abd Elrahman, K. N. Ismail,\\nM. Rihan, and F. E. Abd El-Samie, “Cancelable multi-biometric recog-\\nnition system based on deep learning,” Vis. Comput., vol. 36, no. 6, p.\\n1097–1109, jun 2020.\\n[63] P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, and C. Busch,\\n“An overview of privacy-enhancing technologies in biometric recogni-\\ntion,” ArXiv preprint, vol. abs/2206.10465, 2022.\\n[64] V. Mirjalili, S. Raschka, A. Namboodiri, and A. Ross, “Semi-adversarial\\nnetworks: Convolutional autoencoders for imparting privacy to face\\nimages,” in Intl. Conf. on Biometrics (ICB), 2018, pp. 82–89.\\n[65] B. Meden, P. Rot, P. Terh¨orst, N. Damer, A. Kuijper, W. J. Scheirer,\\nA. Ross, P. Peer, and V. ˇStruc, “Privacy–enhancing face biometrics:\\nA comprehensive survey,” IEEE Transactions on Information Forensics\\nand Security, vol. 16, pp. 4147–4183, 2021.\\n[66] P. Delgado-Santos, R. Tolosana, R. M. Guest, R. Vera-Rodr´ıguez,\\nF. Deravi, and A. Morales, “GaitPrivacyON: Privacy-preserving mo-\\nbile gait biometrics using unsupervised learning,” ArXiv preprint, vol.\\nabs/2110.03967, 2021.\\n[67] A. Morales, J. Fierrez, R. Vera-Rodriguez, and R. Tolosana, “Sensi-\\ntiveNets: Learning agnostic representations with application to face im-\\nages,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\\npp. 1–8, 2020.\\n[68] B. Bortolato, M. Ivanovska, P. Rot, J. Kriˇzaj, P. Terh¨orst, N. Damer,\\nP. Peer, and V. ˇStruc, “Learning privacy-enhancing face representations\\nthrough feature disentanglement,” in 2020 15th IEEE Intl. Conf. on\\nAutomatic Face and Gesture Recognition (FG 2020).\\nIEEE, 2020,\\npp. 495–502.\\n[69] P. Terh¨orst, M. Huber, N. Damer, P. Rot, F. Kirchbuchner, V. ˇStruc,\\nand A. Kuijper, “Privacy evaluation protocols for the evaluation of\\nsoft-biometric privacy-enhancing technologies,” in Intl. Conf. of the\\nBiometrics Special Interest Group (BIOSIG), 2020, pp. 215–222.\\n[70] P. Rot, P. Peer, and V. ˇStruc, Detecting Soft-Biometric Privacy Enhance-\\nment.\\nSpringer, Cham, 2022, pp. 391–411.\\n[71] D. Osorio Roig, C. Rathgeb, P. Drozdowski, P. Terh¨orst, V. Struc, and\\nC. Busch, “An attack on facial soft-biometric privacy enhancement,”\\nIEEE Transactions on Biometrics, Behavior, and Identity Science, 2021.\\n[72] N. Ratha, J. Connell, and R. Bolle, “Enhancing security and privacy in\\nbiometrics-based authentication systems,” IBM Systems Journal, vol. 40,\\nno. 3, pp. 614–634, 2001.\\n[73] P. Bontrager, A. Roy, J. Togelius, N. Memon, and A. Ross, “Deep-\\nmasterprints: Generating masterprints for dictionary attacks via latent\\nvariable evolution,” in Intl. Conf. on Biometrics Theory, Applications\\nand Systems (BTAS’18), 2018, pp. 1–9.\\n[74] H. H. Nguyen, J. Yamagishi, I. Echizen, and S. Marcel, “Generating\\nmaster faces for use in performing wolf attacks on face recognition\\nsystems,” in Intl. Joint Conf. on Biometrics (IJCB’20), 2020, pp. 1–10.\\n[75] S. Marcel, M. S. Nixon, J. Fierrez, and N.Evans, Handbook of Biometric\\nAnti-spooﬁng: Presentation Attack Detection.\\nSpringer, 2019.\\n[76] G. Mai, K. Cao, P. C. Yuen, and A. K. Jain, “On the reconstruction of\\nface images from deep face templates,” IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, vol. 41, no. 5, pp. 1188–1202, 2019.\\n[77] H. Kim, X. Cui, M.-G. Kim, and T. H. B. Nguyen, “Reconstruction of\\nﬁngerprints from minutiae using conditional adversarial networks,” in\\nDigital Forensics and Watermarking, C. D. Yoo, Y.-Q. Shi, H. J. Kim,\\nA. Piva, and G. Kim, Eds., 2019, pp. 353–362.\\n[78] D. Keller, M. Osadchy, and O. Dunkelman, “Inverting binarizations of\\nfacial templates produced by deep learning (and its implications),” IEEE\\nTransactions on Information Forensics and Security, vol. 16, pp. 4184–\\n4196, 2021.\\n[79] C. Kauba, S. Kirchgasser, V. Mirjalili, A. Uhl, and A. Ross, “Inverse\\nbiometrics: Generating vascular images from binary templates,” IEEE\\nTransactions on Biometrics, Behavior, and Identity Science, vol. 3, no. 4,\\npp. 464–478, 2021.\\n[80] ISO/IEC JTC 1/SC 37 Biometrics, ISO/IEC 30136:2018 Information\\ntechnology – Performance testing of biometric template protection\\nschemes, International Organization for Standardization and Interna-\\ntional Electrotechnical Committee, 2018.\\n[81] S. Kirchgasser, Y. M. D´ıaz, H. Mendez-Vazquez, and A. Uhl, “Is\\nwarping-based cancellable biometrics (still) sensible for face recognition\\n?” in Intl. Joint Conf. on Biometrics (IJCB), 2020, pp. 1–8.\\n[82] B. Tams, “Absolute ﬁngerprint pre-alignment in minutiae-based cryp-\\ntosystems,” in Intl. Conf. of the Biometric Special Interest Group\\n(BIOSIG), 2013, pp. 1–12.\\n[83] P. Drozdowski, C. Rathgeb, H. Hofbauer, J. Wagner, A. Uhi, and\\nC. Busch, “Towards pre-alignment of near-infrared iris images,” in Intl.\\nJoint Conf. on Biometrics (IJCB), 2017, pp. 359–366.\\n[84] P. Schuch, J. M. May, and C. Busch, “Unsupervised learning of\\nﬁngerprint rotations,” in Intl. Conf. of the Biometrics Special Interest\\nGroup (BIOSIG), 2018, pp. 1–6.\\n[85] B. Dieckmann, J. Merkle, and C. Rathgeb, “Fingerprint pre-alignment\\nbased on deep learning,” in Intl. Conf. of the Biometrics Special Interest\\nGroup (BIOSIG).\\nGesellschaft f¨ur Informatik e.V., 2019, pp. 83–93.\\n[86] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and\\nJ. Wernsing, “CryptoNets: Applying neural networks to encrypted data\\nwith high throughput and accuracy,” in Intl. Conf. on Machine Learning,\\nvol. 48, 2016, pp. 201–210.\\n[87] H. Otroshi Shahreza, C. Rathgeb, D. Osorio-Roig, V. Krivoku´ca Hahn,\\nS. Marcel, and C. Busch, “Hybrid protection of biometric templates by\\ncombining homomorphic encryption and cancelable biometrics,” in Intl.\\nJoint Conf. on Biometrics (IJCB), 2022, pp. 1–9.\\n[88] A. Bassit, F. Hahn, R. Veldhuis, and A. Peter, “Hybrid biometric\\ntemplate protection: Resolving the agony of choice between bloom ﬁlters\\nand homomorphic encryption,” IET Biometrics, 2022.\\n',\n",
       " '2306.13586v1.pdf': 'NetBooster: Empowering Tiny Deep Learning By\\nStanding on the Shoulders of Deep Giants\\nZhongzhi Yu1, Yonggan Fu1, Jiayi Yuan2, Haoran You1, Yingyan (Celine) Lin1\\n1Georgia Institute of Technology, 2Rice University\\n{zyu401, yfu314, hyou37, celine.lin}@gatech.edu, jy101@rice.edu\\nAbstract—Tiny deep learning has attracted increasing attention\\ndriven by the substantial demand for deploying deep learning\\non numerous intelligent Internet-of-Things devices. However, it\\nis still challenging to unleash tiny deep learning’s full potential\\non both large-scale datasets and downstream tasks due to the\\nunder-ﬁtting issues caused by the limited model capacity of tiny\\nneural networks (TNNs). To this end, we propose a framework\\ncalled NetBooster to empower tiny deep learning by augmenting\\nthe architectures of TNNs via an expansion-then-contraction\\nstrategy. Extensive experiments show that NetBooster consistently\\noutperforms state-of-the-art tiny deep learning solutions.\\nIndex Terms—Tiny Neural Networks, Efﬁcient Deep Learning\\nI. INTRODUCTION\\nTiny deep learning, which aims to develop tiny neural\\nnetworks (TNNs) featuring much-reduced network sizes along\\nwith lower memory and computational costs, has emerged\\nas a promising direction to enable deep learning’s wider\\nreal-world applications in resource-constrained Internet-of-\\nThings (IoT) devices [17] and has attracted an increasingly\\ngrowing interest from both industry and academia [3], [17].\\nIn particular, existing tiny deep learning works strive to\\nimprove the achievable accuracy-efﬁciency trade-off of TNNs\\nby either designing novel efﬁcient network architectures [17],\\n[26] or compressing a large deep neural network (DNN) to\\nreduce their network redundancy [9]. However, the achieved\\naccuracy-efﬁciency trade-off of existing TNN works is still\\nfar from satisfactory for many IoT emerging applications [3].\\nSpeciﬁcally, we summarize that the constraints to the achievable\\naccuracy-efﬁciency trade-offs of TNNs stem from two factors:\\nConstraint 1: It is challenging for TNNs to learn complex but\\nrepresentative features [3] and achieve satisfactory accuracy\\non commonly used large-scale datasets (e.g., ImageNet), and\\nConstraint 2: TNNs’ limited accuracy on large-scale datasets\\nfurther hinders TNN-based solutions from leveraging the\\nwidely adopted pretrain-then-ﬁnetune paradigm for real-world\\ndownstream tasks.\\nIn parallel, it has recently been recognized that a dedicated\\ntraining recipe can boost the accuracy of TNNs [3], although\\nthis area remains still under-explored. Unlike DNN training,\\nwhich requires techniques like data augmentation [4] and/or\\nregularization [10], [27] to alleviate the over-ﬁtting issue, a\\nrecent study [3] has shown that the small network capacity\\nof TNNs makes them more prone to the under-ﬁtting issue.\\nThis work was supported by the NSF SCH program (Award number:\\n1838873) and NSF NIH program (Award number: R01HL144683).\\nMFLOPs\\nAccuracy (%)\\n+1.4%\\n+1.3%\\n+2.6%\\n-0.5%\\n-0.3%\\n-0.3%\\nVanilla, r=224\\n+1.3%\\n+0.2%\\nEpochs\\nVanilla, r=144\\nNetBooster, r=144\\nNetBooster\\nDropBlock\\nVanilla\\n(a)\\n(b)\\nFig. 1: (a) Constraint 1: TNN training suffers from under-\\nﬁtting issues. When training MobileNetV2 [26] on ImageNet,\\nregularization techniques (e.g., DropBlock [10]) even lead to\\ninferior accuracy compared with vanilla training. Our proposed\\nNetBooster can boost TNNs’ accuracy by increasing its capacity\\nduring training. (b) Constraint 2: Inadequately trained TNNs\\ncannot learn complex features and thus suffer from limited\\ndownstream task accuracy. Finetuning ImageNet pretrained\\nvanilla MobileNetV2-35 with a resolution of 224 × 224 and\\n144 × 144, respectively, on the CIFAR-100 dataset for even\\nfour times more epochs (i.e., 600 epochs) still cannot improve\\nthe achievable accuracy. Our proposed NetBooster can boost\\nTNNs’ accuracy by inheriting pretrained deep giants’ learned\\ncomplex features.\\nSpeciﬁcally, due to TNNs’ limited ability to learn complex\\nfeatures, extensively augmented training data or a heavily\\nregularized training process can hurt the achievable accuracy\\nof TNNs on large-scale datasets (i.e., Constraint 1), as shown\\nin Fig. 1 (a). The lack of learned complex and representative\\nfeatures in the pretrained TNNs further limits the achievable\\naccuracy of downstream tasks, which cannot be mitigated by\\nadditional training epochs (i.e., Constraint 2), as shown in\\nFig. 1 (b).\\nTo narrow the gap between the increasing demand for more\\npowerful TNNs in real-world applications and the lack of\\neffective TNN training schemes, we aim to develop a technique\\nthat can boost the achievable task accuracy of TNNs, while\\npreserving their appealing efﬁciency, by empowering TNNs’\\nlearned features. In particular, this work makes the following\\ncontributions:\\n• To the best of our knowledge, we are the ﬁrst to discover\\nand promote a new paradigm of training TNNs to boost\\ntheir achievable accuracy via constructing a competent\\ndeep giant using compound network augmentation (i.e.,\\naugmenting both width and depth dimensions of the\\ngiven TNNs), which is simple, effective, and generally\\napplicable.\\narXiv:2306.13586v1  [cs.LG]  23 Jun 2023\\n• By leveraging the above discovery, we propose a TNN\\ntraining framework, dubbed NetBooster, that alleviates\\nTNNs’ under-ﬁtting issue during training, boosting their\\nachievable accuracy while preserving their original net-\\nwork complexity and thus inference efﬁciency. Speciﬁ-\\ncally, NetBooster incorporates a two-step expansion-then-\\ncontraction training strategy: Step-1: Network Expan-\\nsion constructs an expanded deep giant by converting\\nsome layers of the original TNN into multi-layer blocks,\\nfacilitating the learning of more complex features by\\nleveraging the corresponding deep giant counterpart, which\\nequips the original TNN with an initial state already\\npossessing sufﬁcient knowledge, and Step-2: Progressive\\nLinearization Tuning (PLT) then reverts the deep giant\\nback to the original TNN’s structure by removing the\\nnon-linear layers from the expanded blocks and then\\ncontracting them.\\n• We make heuristic efforts to empirically investigate the\\noptimal setting for effectively boosting the achievable\\naccuracy of TNNs when implementing Network Expansion\\nin NetBooster. Speciﬁcally, we address the following\\nquestions: Q1. What kind of block to use for expansion,\\nQ2. Where to expand within a TNN, and Q3. How to\\ndetermine the expansion ratio.\\n• Extensive experiments and ablation studies on two tasks,\\nfour networks, and seven datasets demonstrate that\\nNetBooster consistently achieves a non-trivial accuracy\\nboost (e.g., 1.3% ∼2.6%) compared to state-of-the-art\\n(SOTA) TNNs on the ImageNet dataset and up to 4.7%\\nhigher accuracy on various downstream tasks, while still\\nmaintaining the original TNNs’ inference efﬁciency.\\nII. RELATED WORKS\\nA. Tiny Neural Network\\nTiny deep learning aims to develop TNNs with reduced\\nnetwork sizes, lower memory and computational costs, and\\nacceptable accuracy, enabling deep learning-powered solutions\\nin resource-constrained IoT devices. Existing techniques to-\\nwards fulﬁlling the goal of tiny deep learning can mostly be\\ncategorized into two trends. One trend is to design novel TNN\\narchitectures by leveraging either human expertise [26] or\\nautomated tools, e.g., neural architecture search [2]. The other\\ntrend is to make use of compression techniques, including\\npruning [19], quantization [9], dynamic inference [29], to\\nfurther reduce network complexity on top of existing TNN\\narchitectures.\\nIn this work, we propose to pursue a tiny deep learning\\nsolution that boosts TNNs’ accuracy-efﬁciency trade-off from\\nan underexplored and orthogonal direction: how to train TNNs\\nto unleash their achievable accuracy more effectively. To\\nthe best of our knowledge, the only pioneering work that\\nfocuses on a similar direction is NetAug [3], which proposes\\nto augment TNNs from the width dimension by introducing a\\nwider supernet to assist training and then directly remove the\\nsupernet during inference. In contrast, NetBooster proposes to\\nﬁrst expand a TNN from both the depth and width dimensions\\nto create a competent deep giant during TNN training, and\\nthen gradually contract it back to the original structure, instead\\nof directly removing expanded parts, to avoid unrecoverable\\ninformation loss that could result in nontrivial accuracy drops.\\nB. Data Augmentation and Regularization\\nData augmentation and regularization techniques have been\\nproposed to alleviate the over-ﬁtting issue associated with large-\\nscale DNNs in order to boost their network generalization\\nability and thus the achievable accuracy. Speciﬁcally, data\\naugmentation techniques focus on manipulating the input data\\nsamples [5], while regularization techniques focus on the\\nnetwork aspect and randomly drop different components from\\nthe network [27] during training.\\nHowever, a recent study [3] and Fig. 1 (a) have shown that\\nTNN training suffers from under-ﬁtting instead of over-ﬁtting.\\nAs a result, existing data augmentation and regularization\\ntechniques are unable to fully unleash the potential of TNNs.\\nC. Knowledge Distillation\\nKnowledge distillation (KD) aims to transfer the already\\nlearned knowledge from a larger teacher network to a smaller\\nstudent network [12]. Instead of relying on a teacher network\\nto provide guidance during training, NetBooster aims to inherit\\nthe learned features from the expanded deep giants to achieve\\nhigher accuracy than the original TNNs. As such, our proposed\\nNetBooster is orthogonal to KD, and when combined with KD,\\nit is expected to further enhance the performance of TNNs.\\nD. Transfer Learning\\nMotivated by DNNs’ strong feature extraction capability,\\ntransfer learning [7] has become a widely used paradigm for\\ntransferring knowledge across different domains [21]. Despite\\nthe extensive efforts to boost transferability, larger pretrained\\nnetworks are commonly believed to have better transferability\\ndue to their representative and generalizable features. Our\\nproposed NetBooster aims to empower TNNs with high-\\nquality features and thus better transferability by leveraging\\nTNNs’ corresponding deep giants through compound network\\naugmentation.\\nIII. THE PROPOSED NETBOOSTER FRAMEWORK\\nA. Motivations and Inspirations\\nThe key challenge for TNN training. Due to the lack of\\nsufﬁcient network capacity, TNNs tend to suffer from severe\\nunder-ﬁtting issues when being trained on large-scale datasets\\n(e.g., ImageNet), limiting their ability to learn complex but\\nrepresentative features [3] and further hindering their achievable\\naccuracy on downstream tasks.\\nInspirations for our works. Recent works have demon-\\nstrated that overparameterization during training can lead\\nto improved achievable accuracy, while network complexity\\nduring inference can be reduced without adversely affecting\\naccuracy [18]. Various techniques for compressing DNNs have\\nalso supported this observation. For example, the pruning\\nmethod [19] preserves the original dense network during\\nPointwise Conv\\nNetwork Expansion on Large Dataset\\nProgressive Linearization Tuning on the Target Dataset\\xa0\\nAct.\\nAct.\\nPointwise Conv\\nPointwise Conv\\nDepthwise Conv\\nPointwise Conv\\nPointwise Conv\\nAct.\\nPointwise Conv\\nPointwise Conv\\nDepthwise Conv\\nPointwise Conv\\nAct.\\nPointwise Conv\\nAct.\\nPointwise Conv\\nPointwise Conv\\nDepthwise Conv\\nPointwise Conv\\nAct.\\nContract\\nLayer i\\nLayer i-1\\nLayer i+1\\nTiny Neural\\nNetworks\\nLayer i\\nLayer i-1\\nLayer i+1\\nTiny Neural\\nNetworks\\nExpand\\nNon-Linear Func.\\nLinear Func.\\nNon-linearity Removal\\nLayer i+1\\nLayer i-1\\nLayer i+1\\nLayer i+1\\nLayer i-1\\nLayer i-1\\nFig. 2: An overview of the proposed NetBooster framework.\\nIn NetBooster, we augment the original TNN from both depth\\nand width dimensions. Speciﬁcally, we uniformly select layers\\nfrom the original TNN and expand them into inverted residual\\nblocks [26] to formulate the deep giant, helping to learn\\ncomplex features. Then in PLT, we progressively decay the\\nnon-linear activation functions within the expanded inverted\\nresidual blocks to an identity mapping function and contract the\\nexpanded blocks back to the corresponding layers to maintain\\nthe original TNN’s structure and inference efﬁciency.\\ntraining and then removes redundant neurons for inference.\\nNevertheless, the aforementioned methods only focus on\\noverparameterization from the width dimension. At the same\\ntime, existing work [22] shows that DNNs with varying depth\\nand width tend to learn different features, urging the need to\\nintroduce overparameterization into both dimensions. Thus,\\nif we can equip a given TNN (e.g., original TNN) with\\ncomprehensive overparameterization during training, and then\\nrestore the original TNN’s structure during inference, the\\nunder-ﬁtting issue can be effectively mitigated to achieve more\\naccurate yet efﬁcient TNN inference. This has inspired us to\\ndesign a principled expansion-then-contraction methodology by\\nﬁrst expanding the original TNN to a more overparameterized\\nnetwork (e.g., deep giant) for better feature learning and then\\ncontracting it back to the original structure to preserve its\\nefﬁciency.\\nB. Overview\\nImplementation of expansion-then-contraction. Given the\\nexpansion-then-contraction principle, there are several potential\\nimplementations. Inspired by the success of RepVGG [6],\\nwhich shows that parallel branches can be merged thanks\\nto their linearity, we hypothesize that if we can properly\\nremove the non-linear activation functions between layers, the\\nconsecutive layers can also be merged via a linear combination.\\nFortunately, recent works show that some of the activation\\nfunctions can be safely removed from the network without\\nhurting the task accuracy [13]. This motivates us to propose our\\nexpansion-then-contraction-based NetBooster training frame-\\nwork, which adopts two steps: Step-1: Network Expansion,\\nwhere we augment the original TNN from both depth and width\\ndimensions during training to construct the corresponding deep\\ngiant by replacing some layers in the original TNN with multi-\\nlayer blocks, aiming to increase the original TNN’s capacity\\nand alleviate its under-ﬁtting issue during training and thus\\nenabling a better feature learning on the large-scale training\\ndataset, and Step-2: Progressive Linearization Tuning (PLT),\\nwhere we progressively remove the non-linearity inside the\\nexpanded blocks on the target dataset. After the non-linear\\nlayers inside the blocks are removed, we contract the expanded\\ndeep giant back to the original TNN at the end of training to\\ninherit the learned features while ensuring the boosted accuracy\\ndoes not come with additional inference overhead.\\nTechnical challenges to achieve NetBooster. While the\\naforementioned principle sounds straightforward, implementing\\nsuch a training pipeline is non-trivial. In Step-1, naively\\nexpanding all layers with a high expansion ratio can lead\\nto an excessively large network which is difﬁcult to train. To\\nenable a practical and effective expansion strategy, at least\\nthe following three questions need to be addressed: Q1: what\\nblocks to insert? What kind of blocks we should use to expand\\nthe original TNN, Q2: where to expand? How to ﬁnd a set of\\ntarget layers to be expanded, and Q3: how to determine the\\nexpansion ratio? To what extent we should expand the target\\nlayers. In Step-2, how to contract the deep giant expanded\\nfrom both width and depth dimensions while preserving the\\nlearned knowledge of the deep giant is still an open question.\\nDespite the method proposed in [6] can merge the parallel\\nconvolution layers into one single layer via linear combination,\\nthe non-linear activation layers between sequentially connected\\nconvolution layers make it impossible to merge convolution\\nlayers along the depth dimension directly. We next elaborate\\non our proposed solutions to tackle the above challenges and\\nthe design of each step.\\nC. Step 1: Network Expansion\\nThe network expansion step aims to increase the capacity of\\nthe original TNN, transforming them into a more powerful deep\\ngiant. This boosts their ability to learn complex and representa-\\ntive features from large-scale training datasets, improving task\\naccuracy and transferability. To answer the questions raised in\\nSec. III-B, we propose the following criteria when expanding\\nthe network:\\na. Structural consistency: To guarantee that applying Net-\\nBooster does not change the original TNN’s structure\\nfor inference, each expansion block needs to be able\\nto be contracted back to the original single layer via\\nlinear combination in the PLT step. Thus, for the network\\nexpansion step, the receptive ﬁeld of each expansion block\\nshould be equal to that of the original layer.\\nb. Sufﬁcient capacity: Motivated by the ﬁndings in [3],\\nwe aim to alleviate the under-ﬁtting issue and ease the\\nlearning process by creating a deep giant with increased\\ncapacity. Thus, we should sufﬁciently expand the original\\nTNN from multiple positions and dimensions (i.e., expand\\nwidth with increased expansion ratios and depth by\\ninserting multiple layers).\\nc. Effective feature inheritance: In addition to having sufﬁ-\\ncient capacity, it is equally important to effectively inherit\\nthe learned features of the deep giant. As suggested in [20],\\nexcessively large networks tend to learn signiﬁcantly\\ndifferent feature distribution from that of small networks,\\nwhich can not only forbid small networks from inheriting\\nbut even hurt small networks’ task accuracy. Thus, (1)\\nthe complexity gap between the original TNN and its\\nexpanded deep giant should not be too large and (2) the\\nselected layers to be expanded should contain sufﬁcient\\nparameters to ensure an effective knowledge inheritance\\nfrom the deep giant.\\nBased on the above criteria, we answer the questions raised\\nin Sec. III-B below:\\nQ1. What kind of block to use? We select the type of inserted\\nblocks from a pool of well-established DNN building blocks\\n(e.g., the basic and bottleneck blocks in ResNet [11] and the\\ninverted residual block in MobileNetV2 [26]). To maintain\\nstructure consistency (criteria a.), we eliminate the basic block\\nas it stacks two layers with large convolution kernels, leading to\\na receptive ﬁeld larger than that of the original layer. To narrow\\ndown the complexity gap for effective feature inheritance\\n(criteria c.), we select the inverted residual block over the\\nbottleneck block.\\nQ2. Where to expand? The achievable accuracy of Net-\\nBooster is limited by a trade-off between increasing the network\\ncapacity by constructing a larger deep giant (criteria b.) and\\nimproving the feature inheritance effectiveness by narrowing\\ndown the capacity gap between the deep giant and original\\nTNN (criteria c.). A simple but effective way to push the\\naforementioned trade-off further is to consider the knowledge\\ninheritance effectiveness from a more ﬁne-grained granularity\\n(i.e., layer-wise instead of model-wise). Speciﬁcally, multiple\\nlayers can have a better representation ability than a single\\nlayer. Thus, the expanded block’s learned complex features can\\nbe more effectively inherited by distributing them to multiple\\nadjacent layers in the original TNN. To this end, we propose\\nto uniformly select layers to be expanded from the original\\nTNN, which can guarantee that there are sufﬁcient layers to\\ninherit learned features from each of the expanded blocks.\\nQ3. How to determine the expansion ratio? Similar to Q2,\\nthe selection of the adopted expansion ratio has to trade-off\\nbetween the network capacity (criteria b.) and the effectiveness\\nof knowledge inheritance (criteria c.). However, thanks to the\\nproposed uniform expansion strategy in Q2, we empirically\\nﬁnd that the commonly used expansion ratio, 6 [26] in the\\ninserted inverted residual blocks works well on balancing the\\naforementioned two criteria.\\nD. Step 2: Progressive Linearization Tuning (PLT)\\nThe next step is to recover the original TNN’s structure\\non the target dataset while inheriting the knowledge learned\\nby the deep giant. Inspired by [6], which proposes to merge\\nparallel convolution layers into one single layer, we ﬁnd\\nthat sequentially connected layers can also be merged via\\nlinear combinations by properly removing the non-linear\\noperations between them. To achieve this, we propose PLT to\\nprogressively remove the non-linearity from the expanded deep\\ngiant and then contract it back to the original TNN during\\nﬁnetuning on the target dataset.\\nMotivating observation. Non-linearity has been considered\\na key enabler for the promising performance of DNNs and\\nmost existing works use the combination of convolution and\\nnon-linear activation layers as a basic design unit. In parallel,\\nrecent works [13] have shown that non-linearity within DNNs\\ncan be highly redundant for inference, a large portion of\\nelement-wise non-linear activation functions can be removed\\nfrom a DNN, and the complex features learned from the original\\nTNN during training can be largely preserved. Inspired by the\\nrevolution from element-wise pruning to structure pruning,\\nwe aim to step further and remove the non-linearity in a\\nstructured manner (i.e., layer-wisely).\\nNon-linearity removal. We propose to transform the expanded\\ndeep giant back to the original TNN meanwhile preserve the\\nlearned features by slowly decaying the non-linear activation\\nfunctions.\\nWithout loss of generality, we take the ReLU activation\\nfunction as an example as it is the most commonly adopted\\nactivation function in TNNs, and the following discussion can\\nalso be extended to other activation functions like ReLU6. Here\\nthe ReLU function is deﬁned as:\\nYl = max(0, Xl),\\n(1)\\nwhere Xl and Yl are the input and output of layer l, respectively.\\nWe change the formulation of ReLU to the following format,\\nYl = max(αlXl, Xl),\\n(2)\\nwhere 0 < αl < 1 is the slope parameter to manipulate the non-\\nlinearity of the corresponding activation layer. When αl = 0,\\nit is exactly the ReLU function. When αl = 1, the activation\\nfunction is decayed to an identity mapping.\\nGiven a list L of non-linear activation layers to be removed,\\nwe gradually increase αl′ for l′ ∈L from 0 to 1 in Ed epochs,\\nthe value of αl′ is uniformly increased in each iteration. When\\nαl′ = 1, Eq. 2 is an identity mapping, and thus the non-linearity\\nis removed.\\nExpanded block contraction. With the non-linear activation\\nlayers removed, the remaining layers can be contracted into\\none layer via simple linear combinations.\\nFormulation: Without loss of generality, we take two con-\\nvolution layers as an example. Given the input to the ﬁrst\\nlayer X ∈Rh1×w1×c1, the output Y ∈Rh3×w3×c3, as well\\nas the kernels of two layers K1 ∈Rk1×k1×c1×c2 and K2 ∈\\nRk2×k2×c2×c3, where k1/2 are the kernel sizes, h1/2/3, w1/2/3\\nare the heights and widths of corresponding feature maps,\\nc1/2/3 are the channel numbers. The overall functionality of\\nthe two convolution layers can be formulated as\\nYp,q,o =\\nk−1\\n∑\\ni=0\\nk−1\\n∑\\nj=0\\nc1−1\\n∑\\nm=0\\nXp−i,q−j,mKi,j,m,o,\\n(3)\\nwhere Ki,j,m,o =\\nsh\\n∑\\ns=sl\\nth\\n∑\\nt=tl\\nc2−1\\n∑\\nn=0\\nK1\\ni−s,j−t,m,nK2\\ns,t,n,o,\\n(4)\\nwhere k = k1 +k2 −1, sl = max(0, i−k1 +1), sh = min(k2 −\\n1, i), tl = max(0, j −k2 + 1) and th = min(k2 −1, j).\\nRemark: It is worth noting that different expansion ratios of\\nthe inserted inverted residual block will result in the same\\nTABLE I: Benchmarking on ImageNet. ’r’ is the input\\nresolution.\\nNetwork\\nFLOPs\\nParams\\nTraining Method\\nAccuracy\\nMobileNetV2-Tiny\\n(r=144)\\n23.5M\\n0.75M\\nVanilla\\n51.2\\nRocketLaunch [31]\\n51.8\\ntf-KD [30]\\n51.9\\nRCO-KD [14]\\n52.6\\nNetAug [3]\\n53.0\\nNetBooster\\n53.7\\nMCUNet\\n(r=176)\\n81.8M\\n0.74M\\nVanilla\\n61.4\\nNetAug [3]\\n62.5\\nNetBooster\\n62.8\\nMobileNetV2-50\\n(r=160)\\n50.2M\\n1.95M\\nVanilla\\n61.4\\nNetAug [3]\\n62.5\\nNetBooster\\n62.7\\nMobileNetV2-100\\n(r=160)\\n154.1M\\n3.47M\\nVanilla\\n69.6\\nNetAug [3]\\n70.5\\nNetBooster\\n70.9\\ncomputational cost after contraction since the input and output\\nchannels after contraction are always equal to the input channel\\nof the ﬁrst layer and the output channel of the last layer,\\nrespectively, regardless of intermediate channel numbers (i.e.,\\nc2).\\nIV. EXPERIMENTS\\nA. Experiments Setup\\nTasks, datasets, and networks We consider two tasks,\\nincluding image classiﬁcation and object detection, with seven\\ndatasets to provide a thorough evaluation of NetBooster.\\nSpeciﬁcally, to evaluate NetBooster’s performance in alleviating\\nthe under-ﬁtting issue to achieve a higher accuracy on the\\nlarge-scale dataset, we consider the ImageNet dataset. To\\nevaluate how deep giant’s learned representation helps with\\ndownstream tasks, we consider image classiﬁcation tasks on\\nﬁve datasets, including CIFAR-100, Cars [15], Flowers102 [23],\\nFood101 [1], and Pets [24]. We also evaluate NetBooster on\\nthe downstream object detection task with the Pascal VOC\\ndataset [8]. We consider four networks, including MobileNetV2-\\n100/50/Tiny [26], and a neural architecture searched hardware\\nfriendly network MCUNet [17].\\nBaselines. We benchmark the proposed NetBooster over\\nﬁve baselines including networks trained with standard vanilla\\ntraining, a series of SOTA KD algorithms (i.e., tf-KD [30],\\nRCO-KD [14], and RocketLaunch [31]), and NetAug [3], which\\nis a pioneering work in boosting TNN training performance.\\nExpansion strategy. We uniformly expand 50% of blocks\\nin the original TNN. To expand each block, we replace the ﬁrst\\npointwise convolution layer with an inverted residual block\\nwith an expansion ratio of 6. The kernel size of the depthwise\\nconvolution layer in the inserted inverted residual block is set\\nto 1 to make the inserted block’s receptive ﬁeld the same as\\nthe pointwise convolution.\\nTraining settings. We develop our training settings based on\\nthe commonly adopted settings. Speciﬁcally, when evaluating\\nNetBooster’s performance in improving TNNs’ accuracy on\\nImageNet dataset, we follow [3] to train the deep giant for\\n160 epochs using SGD optimizer with a batch size of 1024,\\nan initial learning rate of 0.2 and cosine anneal learning rate\\nschedule. In PLT, we set Ed = 40, and further ﬁnetune for 110\\nepochs. For downstream tasks, we use the ImageNet pretrained\\nTABLE II: Benchmarking on downstream image classiﬁca-\\ntion datasets. ’r’ is the input resolution.\\nNetwork\\nTraining Method\\nCIFAR-100\\nCars\\nFlowers102\\nFood101\\nPets\\nMobileNetV2-Tiny\\n(r=144)\\nVanilla\\n74.07\\n76.18\\n90.01\\n75.43\\n78.30\\nNetBooster\\n75.46\\n80.93\\n90.53\\n75.96\\n78.90\\nMobileNetV2-35\\n(r=160)\\nVanilla\\n76.08\\n78.36\\n90.63\\n76.80\\n80.64\\nVanilla + KD\\n76.38\\n77.47\\n91.41\\n77.02\\n82.44\\nNetBooster\\n76.66\\n80.91\\n91.16\\n77.26\\n80.92\\nNetBooster + KD\\n77.15\\n83.36\\n92.68\\n77.81\\n83.37\\nTABLE III: Benchmarking on object detection tasks with\\nPascal VOC dataset with MobileNetV2-35 at 416 resolution.\\nMethod\\nVanilla\\nNetAug\\nNetBooster\\nAP50\\n60.8\\n62.4\\n62.6\\ndeep giant as the starting point and develop our training recipe\\non CIFAR-100 based on [28], on Cars, Flowers102, Food101,\\nand Pets based on [25], and on Pascal VOC based on [3]. In\\nall experiments on downstream tasks, we assign Ed to be 20%\\nof the total tuning epochs in PLT.\\nB. Easing Constraint 1: Benchmarking on Large-scale Dataset\\nTo evaluate whether the proposed NetBooster can help\\nTNNs to learn the complex features of the large-scale dataset\\nand thus improve accuracy, we benchmark NetBooster on the\\nImageNet dataset with vanilla training, NetAug, and various KD\\nalgorithms. As shown in Table I, NetBooster achieves 1.3% ∼\\n2.5% accuracy improvements over the vanilla training, showing\\nits strong ability in boosting the TNNs’ accuracy by standing\\non the shoulder of deep giants generated by NetBooster.\\nCompared with the KD baselines, our proposed NetBooster\\nachieves 0.9% ∼1.1% accuracy improvement without guidance\\nfrom the teacher network (Assemble-ResNet50 [16]), suggest-\\ning that Network Expansion in NetBooster can equip the deep\\ngiant with sufﬁcient capacity to learn complex features at\\nleast comparable with the large teacher DNN used in the KD\\nbaselines and the learned features can be effectively inherited\\nfrom the PLT step.\\nCompared with NetAug, which is a pioneering work fo-\\ncusing on a similar scenario as NetBooster, NetBooster also\\nachieves superior accuracy over NetAug, suggesting the multi-\\ndimensional Network Expansion and the PTL for features\\ninheritance is more effective than the network width expansion\\nand directly dropping augmented neuron adopted in NetAug.\\nC. Easing Constraint 2: Benchmarking on Downstream Tasks\\nTo evaluate whether the learned complex and representative\\nfeatures in the deep giant from the large-scale dataset can\\nfurther help the original TNN to achieve better accuracy on\\ndownstream tasks, we ﬁrst evaluate NetBooster’s performance\\nwhen transferring the ImageNet pretrained deep giant to ﬁve\\nrepresentative downstream image classiﬁcation datasets with\\nPLT. As shown in Table II, compared with vanilla training,\\nNetBooster achieves 0.46% ∼4.75% accuracy improvement,\\nshowing that the features learned by the deep giant are effec-\\ntively inherited after PLT. It is worth noting that NetBooster\\nis also orthogonal to KD, applying KD on top of NetBooster\\ncan lead to another 0.49% ∼2.45% accuracy boost over using\\nNetBooster alone.\\nTABLE IV: Ablation study on what kind of block to insert.\\nInserted Block Type\\nExpanded Acc.\\nFinal Acc.\\nVanilla\\n-\\n51.20\\nInverted Residual\\n54.90\\n53.70\\nBasic Block\\n54.52\\n53.41\\nBottleneck Block\\n55.23\\n53.62\\nTABLE V: Ablation study on which block to expand.\\nExpansion\\nExpanded\\nExpanded Acc.\\nFinal Acc.\\nFLOPs\\nParams\\nVanilla\\n29.4M\\n0.75M\\n-\\n51.20\\nExpand First 8\\n65.0M\\n0.83M\\n51.46\\n51.50\\nExpand Middle 8\\n49.6M\\n0.93M\\n52.98\\n52.62\\nExpand Last 8\\n51.2M\\n1.25M\\n53.90\\n52.47\\nUniform Expand 8\\n63.9M\\n0.99M\\n54.90\\n53.70\\nWe further evaluate NetBooster’s performance when trans-\\nferring to the Pascal VOC object detection task. As shown\\nin Table III, NetBooster achieves 1.8 and 0.2 higher AP50\\ncompared with vanilla training and NetAug, respectively. This\\nproves that NetBooster can be considered as a general method\\nto boost TNNs’ performance across various tasks.\\nD. Validating Expansion Strategy\\nWe validate our answer to each question in Sec. III-C by\\nvalidating the impact of replacing our proposed strategy with\\nalternatives when training MobileNet-Tiny on the ImageNet\\ndataset with an input resolution of 144.\\nQ1. What kind of block to use: We ablate the impact of\\nexpanding with different kinds of blocks and report the results\\nin Table IV. Expanding with inverted residual blocks leads\\nto slightly better results (0.08% ∼0.29%). It shows (1) the\\nNetBooster framework can robustly boost TNNs’ performance,\\nand (2) inserting inverted residual blocks is an effective choice.\\nQ2. Where to expand: We ablate different expansion loca-\\ntions’ impacts and report our ﬁndings in Table V. We observe\\nthat uniformly expanding the model achieves a 1.08% ∼2.20%\\nhigher accuracy compared with excessively expanding the\\nﬁrst/middle/last part of the network, proving the necessity\\nto expand the model uniformly.\\nQ3. How to determine expansion ratio: We ablate different\\nselections of expansion ratios in the inserted inverted residual\\nblocks and report the results in Table. VI. We observe that\\nNetBooster with commonly used expansion ratios (i.e., 4 ∼\\n6) consistently improves the TNNs’ accuracy, further proving\\nNetBooster’s robustness to hyperparameter selection.\\nV. CONCLUSION\\nIn this paper, we discover and promote a new paradigm\\nfor training TNNs to empower their achievable accuracy via\\naugmenting both dimensions of TNNs (i.e., depth and width)\\nduring training. Furthermore, we propose a framework dubbed\\nNetBooster, which is dedicated to boosting the accuracy of\\nSOTA TNNs by using an expand-then-contract training strategy\\nto alleviate TNNs’ under-ﬁtting issues. Finally, we make\\nheuristic efforts to empirically explore what/when/where to\\naugment when training TNNs with our proposed NetBooster.\\nTABLE VI: Ablation study on the expansion ratio.\\nExpansion ratio\\n2\\n4\\n6\\n8\\nFinal Acc.\\n52.94\\n53.52\\n53.70\\n52.56\\nExtensive experiments show that NetBooster consistently leads\\nto a nontrivial accuracy boost (e.g., 1.3% ∼2.5%) on top\\nof SOTA TNNs on ImageNet and as much as 4.7% higher\\naccuracy on various downstream tasks, while maintaining their\\ninference efﬁciency.\\nREFERENCES\\n[1] L. Bossard et al., “Food-101–mining discriminative components with\\nrandom forests,” in ECCV.\\nSpringer, 2014, pp. 446–461.\\n[2] H. Cai, “Proxylessnas: Direct neural architecture search on target task\\nand hardware,” arXiv preprint, 2018.\\n[3] H. Cai et al., “Network augmentation for tiny deep learning,” arXiv\\npreprint, 2021.\\n[4] E. D. Cubuk, “Randaugment: Practical automated data augmentation\\nwith a reduced search space,” in CVPR Workshops, 2020, pp. 702–703.\\n[5] E. D. Cubuk et al., “Autoaugment: Learning augmentation policies from\\ndata,” arXiv preprint, 2018.\\n[6] X. Ding et al., “Repvgg: Making vgg-style convnets great again,” in\\nCVPR, 2021, pp. 13 733–13 742.\\n[7] J. Donahue et al., “Decaf: A deep convolutional activation feature for\\ngeneric visual recognition,” in ICML.\\nPMLR, 2014, pp. 647–655.\\n[8] M. Everingham et al., “The pascal visual object classes (voc) challenge,”\\nIJCV, vol. 88, no. 2, pp. 303–338, 2010.\\n[9] Y. Fu et al., “Double-win quant: Aggressively winning robustness of\\nquantized deep neural networks via random precision training and\\ninference,” in ICML.\\nPMLR, 2021, pp. 3492–3504.\\n[10] G. Ghiasi et al., “Dropblock: A regularization method for convolutional\\nnetworks,” arXiv preprint, 2018.\\n[11] K. He et al., “Deep residual learning for image recognition,” in CVPR,\\n2016, pp. 770–778.\\n[12] G. Hinton et al., “Distilling the knowledge in a neural network,” arXiv\\npreprint, 2015.\\n[13] N. K. Jha et al., “Deepreduce: Relu reduction for fast private inference,”\\nin ICML.\\nPMLR, 2021, pp. 4839–4849.\\n[14] X. Jin et al., “Knowledge distillation via route constrained optimization,”\\nin ICCV, 2019, pp. 1345–1354.\\n[15] J. Krause et al., “3d object representations for ﬁne-grained categorization,”\\nin ICCV workshops, 2013, pp. 554–561.\\n[16] J. Lee et al., “Compounding the performance improvements of assembled\\ntechniques in a convolutional neural network,” arXiv preprint, 2020.\\n[17] J. Lin et al., “Mcunet: Tiny deep learning on iot devices,” arXiv preprint,\\n2020.\\n[18] S. Liu et al., “Do we actually need dense over-parameterization? in-time\\nover-parameterization in sparse training,” in ICML, 2021.\\n[19] Z. Liu et al., “Learning efﬁcient convolutional networks through network\\nslimming,” in ICCV, 2017, pp. 2736–2744.\\n[20] S. I. Mirzadeh et al., “Improved knowledge distillation via teacher\\nassistant,” in AAAI, vol. 34, no. 04, 2020, pp. 5191–5198.\\n[21] R. Mormont et al., “Comparison of deep transfer learning strategies for\\ndigital pathology,” in CVPR workshops, 2018, pp. 2262–2271.\\n[22] T. Nguyen et al., “Do wide and deep networks learn the same things?\\nuncovering how neural network representations vary with width and\\ndepth,” arXiv preprint, 2020.\\n[23] M.-E. Nilsback et al., “Automated ﬂower classiﬁcation over a large\\nnumber of classes,” in ICVGIP.\\nIEEE, 2008, pp. 722–729.\\n[24] O. M. Parkhi et al., “Cats and dogs,” in CVPR.\\nIEEE, 2012, pp.\\n3498–3505.\\n[25] H. Salman et al., “Do adversarially robust imagenet models transfer\\nbetter?” NeurIPS, vol. 33, pp. 3533–3545, 2020.\\n[26] M. Sandler et al., “Mobilenetv2: Inverted residuals and linear bottlenecks,”\\nin CVPR, 2018, pp. 4510–4520.\\n[27] N. Srivastava et al., “Dropout: a simple way to prevent neural networks\\nfrom overﬁtting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.\\n[28] Y. Tian et al., “Contrastive representation distillation,” arXiv preprint,\\n2019.\\n[29] Z. Yu et al., “Mia-former: Efﬁcient and robust vision transformers via\\nmulti-grained input-adaptation,” arXiv preprint, 2021.\\n[30] L. Yuan et al., “Revisiting knowledge distillation via label smoothing\\nregularization,” in CVPR, 2020, pp. 3903–3911.\\n[31] G. Zhou et al., “Rocket launching: A universal and efﬁcient framework\\nfor training well-performing light net,” in AAAI, 2018.\\n',\n",
       " '2306.16177v3.pdf': 'Defining data science: a new field of inquiry \\nMichael L. Brodie mlbrodie@seas.harvard.edu\\nData Systems Laboratory, School of Engineering and Applied Sciences\\nHarvard University, Cambridge, MA USA \\n=============DRAFT July 12, 2023 ======================\\nData science is not a science. It is a research \\nparadigm. Due its power, scope, and scale, it will \\nsurpass science – our most powerful research \\nparadigm – in enabling knowledge discovery, \\nwidely predicted to change our world[12]. We \\nhave yet to understand and define it.\\nModern data science is in its infancy. Emerging \\nslowly since 1962 and rapidly since 2000, data \\nscience is a fundamentally new field of inquiry, \\none of the most active , powerful, and rapidly \\n1\\nevolving innovations of the 21st century. Due to its \\nvalue, power, and scope of applicability, it is \\nemerging in over 40 disciplines, hundreds of \\nresearch areas, and tens of thousands of \\napplications. Yet we are just beginning to \\nunderstand and define it. 106 data science \\npublications contain myriad definitions of data \\nscience and data science problem solving. Due to \\nits infancy, many definitions are independent, \\napplication-specific, mutually incomplete, \\nredundant, or inconsistent, hence so is data \\nscience as a field of inquiry. This research \\naddresses this data science multiple definitions \\nchallenge by proposing the development of \\ncoherent, unified definition of data science based \\non a data science reference framework[25]-[31] \\nand a data science journal[31] for the data science \\ncommunity to achieve such a definition.\\nThis paper provides candidate definitions for \\nessential data science artifacts that are required \\nto discuss such a definition. They are based on the \\nclassical research paradigm concept[15] consisting \\nof a philosophy of data science, the data science \\nproblem solving paradigm, and the six component \\ndata science reference framework – axiology, \\nontology, epistemology, methodology, methods, \\nand technology that is a unifying framework that \\nis frequently called for[1][4][7][10][11][16][19] \\nwith which to define, unify, and evolve data \\nscience. It presents challenges of defining data \\nscience, solution approaches, i.e., means for \\ndefining data science, and their requirements and \\nbenefits – the basis of a comprehensive solution \\n[24]-[32].\\n1.\\nChallenges defining data science\\n1.1.\\nUnifying multiple definitions\\nThe emergence of a new field of inquiry and its \\nproblem solving techniques is rare. Science and \\nmodern scientific analyses emerged 400 years ago \\nand interpretivism and interpretivist analysis 200 \\nyears ago. While conventional data science is as \\nold as mathematics, AI-based data science is in its \\ninfancy. Tukey’s 1962 vision of exploratory data \\nanalysis[20][21] brought renewed attention to \\nstatistic’s learning and discovery power. After \\n2000, machine learning-based data science led to \\na fundamentally new, inscrutable field of inquiry \\nthat we are just beginning to understand. Valuable \\ncontributions – concepts, methods, solutions – \\nfrom hundreds of disciplines and thousands of \\napplications, while adequate for their source area \\nmust be unified to apply across data science and \\nin thousands of applications. This has led to calls \\nfor a unifying framework to guide unification. An \\nanalogous problem arose in science in the 1830s. \\nWhat is such a unifying framework? How do you \\ndefine a fundamentally new field of inquiry? For \\nthis we look to science, our currently most \\npowerful knowledge discovery paradigm.\\n1.2.\\n Classical paradigms to define science\\nAs described in Section 2, data science is a \\nresearch (knowledge discovery) paradigm distinct \\nfrom and independent of science – our most \\nmature, successful, and powerful research \\n 500,000 AI articles were published in 2021, more than in any other discipline, dominated by pattern \\n1\\nrecognition, machine learning, and computer vision[22]. ACM lists 200+ data science journals.\\nparadigm. 2,600 years ago science was defined \\nphilosophically by Thales of Miletus (624-560 BC) \\nand Aristotle (384-322 BC)) then in terms of \\nscientific models, theories, and the scientific \\nmethod by Francis Bacon [Novum Organum 1620] \\nand David Hume (1739-1779)[5] and only recently \\n(1962, 1970) was the philosophy of science \\ndefined by Thomas Kuhn[13]. Consider the \\nfollowing classical paradigms used to define and \\nunify science to be used to understand, define, \\nand unify data science. Knowledge of science \\ncontributes to understanding data science by \\ncontrast.\\nJust as data science is now enthusiastically \\nadopted and applied worldwide, so it was with \\nscience in the 19th century. In the 1830s, science \\nwas emerging in many disciplines and faced the \\nmultiple definitions challenge for science, \\nscientific problem solving, and scientific \\ndisciplines. Scientific research and analyses \\n(experiments) emerged rapidly, simultaneously, \\nand independently in many universities, \\nindustries, and countries, resulting in many \\npublications, each with its own definitions and \\nterminology. Different definitions emerged from \\ndifferent scientific experiments and disciplines, \\nfrom scientists with different backgrounds and \\nknowledge, and in different discipline-specific \\npublications.\\nIn 1837, Alexander Comte[15] recognized the \\nmultiple definitions challenge for science as \\nchaotic due to science and scientific disciplines \\nbeing inadequately understood and defined. \\nDefinitions were adequate for experiments, or \\nscientific problem solving in source scientific \\ndisciplines but were often inadequate for scientific \\nproblem solving across all scientific disciplines. \\nMyriad independent scientific definitions were \\nincomplete, mutually inconsistent and redundant \\nwith inadequate guidance to understand and \\nconduct science in as yet undefined scientific \\ndisciplines. Myriad definitions impeded scientific \\nunderstanding, communication, and collaboration, \\n– a persistent problem to this day[8]. A \\ncomprehensive solution to the multiple definitions \\nchallenge for science and scientific disciplines was \\nto deduce a unified definition of science and \\nscientific disciplines from myriad, independent \\ndefinitions. This required paradigms that were \\naccepted by scientists to guide the unification of \\nthe myriad definitions based on established \\nresults. Comte provided those paradigms. This \\nresearch addresses the analogous data science \\nmultiple definitions challenge and follows Comte’s \\nmodel used for science. The solution in science \\ntook 200 years. Defining and unifying data science \\nmay take decades due to its current inscrutability.\\n \\nComte introduced two paradigms and the concept \\nof a discipline that have been used ever since[15]. \\nHe defined the scientific research paradigm in \\nterms of the philosophy of science and a scientific \\nreference framework. The philosophy of science is \\na worldview that provides the philosophical \\nunderpinnings (i.e., objective, quantitative \\nreasoning) of empirical research for knowledge \\ndiscovery, with which to understand, reason \\nabout, discover, articulate, and validate the true \\nnature of the ultimate questions about natural \\nphenomena as new knowledge about the \\nphenomena. Scientific results are definitive, \\nconclusive, casual, robust, universal knowledge \\nabout the phenomena with verified explanations \\nand validated interpretations that are not \\nprovably complete.\\nThe scientific reference framework defines science \\nin terms of its axiology, ontology, epistemology, \\nmethodology, methods, and technology. \\nObserving that scientific discovery is conducted \\nonly on specific phenomena within a discipline, he \\ndefined a scientific discipline as the scientific \\nresearch paradigm applied to a specific class of \\nphenomena, e.g., biology is the application of the \\nscientific research paradigm to living organisms. \\nDisciplines can be structured into sub-disciplines \\nfor sub-categories of a discipline’s phenomenon, \\ne.g., microbiology is a sub-discipline of biology \\nthat deals with a sub-category of living organisms, \\ni.e., macromolecules that are essential to life. \\nSurprisingly, the concept of a scientific discipline \\ndid not exist until Comte’s definition. Observing \\nthat the central purpose of the scientific research \\nparadigm is scientific problem solving, Comte \\ndefined the scientific problem solving paradigm \\ngoverned by the scientific method. Comte’s \\n2\\ndefinitions of the scientific research paradigm, a \\nscientific reference framework, a scientific \\ndiscipline, the scientific problem solving paradigm, \\nand discipline-specific scientific problem solving \\nprovided a solution to the multiple definitions \\nchallenge in science. Comte’s 1830’s paradigms \\ndefined modern science and guided its conduct in \\nscientific disciplines ever since. The research \\nparadigm approach that applies the above \\nparadigms to define science is now used to define \\ndata science for which candidate definitions are \\nproposed for essential data science concepts. The \\nessential concepts are then used to address the \\nmultiple definitions challenge for data science.\\n1.3.\\nClassical paradigms to define data science\\nA definition of data science is of value to the \\nextent that it meets the needs of and is accepted \\nby the data science community. The data science \\ncommunity consists of data science researchers, \\ndevelopers, practitioners, educators, and students \\nthat research, develop, and apply data science in \\n40 contributing disciplines, hundreds of research \\nareas, and tens of thousands of applications in an \\never increasing number of data science \\ndisciplines. We need at least candidate data \\nscience definitions to discuss such an activity. This \\nsection provides informal, candidate definitions of \\nessential data science concepts and problem \\nsolving as a starting point for the data science \\ncommunity using an online data science \\njournal[32].\\nParadigm – a well-defined concept, model, or \\ntemplate – is used to understand and define data \\nscience. “In science and philosophy, a paradigm is \\na distinct set of concepts or thought patterns, \\nincluding theories, research methods, postulates, \\nand standards for what constitute legitimate \\ncontributions to a field” [Wikipedia] Levels of \\nabstraction are used extensively. A paradigm can \\nhave many distinct instances, i.e., paradigm \\ncategories . Comte was concerned exclusively \\n2\\nwith science; hence, his scientific research \\nparadigm was actually a research paradigm \\ncategory as is data science ; however, science and \\ndata science are two of over 30 such research \\nparadigms. Paradigm categories have many \\ninstances, e.g., the scientific research paradigm \\ncategory  has many scientific disciplines defined \\n3\\nas instances of applying the scientific research \\nparadigm category to a specific class of \\nphenomena. Levels of abstraction can be \\nconfusing as the precision required for definitions \\nleads to lengthy terms, e.g., the full term data \\nscience research paradigm category discipline is \\nrequired for precision; when the meaning is clear, \\nthe simpler term data science discipline is used. \\nSimilarly, when the meaning is clear, the simple \\nterm data science reference framework is used in \\nplace of longer data science research paradigm \\ncategory reference framework.\\nResearch paradigms used in this research were \\nintroduced indirectly and perhaps unwittingly by \\nComte. Comte tried to apply science, called \\npositivism at the time, to knowledge discovery in \\nsociology. “Although the positivist approach has \\nbeen a recurrent theme in the history of western \\nthought, modern positivism was first articulated in \\nthe early 19th century by Auguste Comte. His \\nschool of sociological positivism holds that society, \\nlike the physical world, operates according to \\ngeneral laws. After Comte, positivist schools arose \\nin logic, psychology, economics, historiography, \\nand other fields of thought.” [Wikipedia] As with \\ntoday’s enthusiasm for applying data science \\nextensively, so it was with science in the 18th and \\n19th centuries. Comte believed in the value of \\nscience and of scientific problem solving in \\nhumanistic topics. This led science to be applied \\ninappropriately to disciplines for which the \\nscientific method, objectivity, and quantitative \\nreasoning were impossible and subjective, \\nqualitative evaluations were essential. In the late \\n19th century, the positivist paradigm was replaced \\nfor such disciplines with the interpretivist \\nparadigm category – a fundamentally new \\nresearch paradigm independent of science with a \\nphilosophy of subjective, qualitative reasoning \\n The term category is used here to refer to a paradigm instance.\\n2\\n Scientific research paradigm is an imprecise term for the scientific research paradigm category.\\n3\\n3\\nbased on human knowledge, expertise, \\nexperience, biases, and opinions. Interpretivism \\ngoverns disciplines in the arts and humanities. \\nSimilar confusions now exist when data science is \\nconsidered as a science.\\nComte’s definitions with references to science \\nremoved define the generic research paradigm, \\nresearch paradigm discipline, and research \\nparadigm problem solving paradigm, as follows. \\nThe (generic) research paradigm is defined by its \\nphilosophy, and its reference framework[15][17]. \\nThe philosophy of a research paradigm is a \\nworldview that defines the purpose of work in the \\nresearch paradigm and its nature, i.e., how work is \\nconducted, e.g., Appendix I. The philosophy \\nprovides the philosophical underpinning for \\nresearch with which to discover, reason about, \\nunderstand, articulate, and validate the true \\nnature of the ultimate questions about \\nphenomena in a specific discipline as knowledge \\nabout those phenomena. The research paradigm \\nreference framework  consists of six components \\n4\\n– axiology, ontology, epistemology, methodology, \\nmethods, and technology. Full definitions of the \\ncomponents can fill entire books. They are defined \\nhere briefly to understand and define data \\nscience. Axiology defines the purpose, nature, \\nimportance, risks, and value of the research \\nparadigm for reasoning, problem solving, and the \\nproblem solving results, i.e., discovered \\nknowledge. An ontology and an epistemology are \\nto a research paradigm what footings are to a \\nhouse; they form the conceptual foundations of \\nthe edifice. For a research paradigm, the \\nfoundations are its artifacts (ontology) and means \\nfor reasoning with those artifacts (epistemology). \\nTogether they constitute a vocabulary and \\nlanguage of the research paradigm. Methodology \\ndefines the process, or methodology, referred to \\nas the method of the research paradigm, for \\ndefining, designing, and conducting analyses \\n(work), their expression in a workflow, and the \\ngoverning principles, e.g., the scientific method \\ngoverns the design and execution of scientific \\nanalyses expressed in scientific workflows \\ngoverned by scientific principles. Methods is the \\nheart of research paradigm analyses – analytical \\nmeans used to implement analyses in the \\nresearch paradigm defined by analytical methods, \\noperands, solutions (prepared models and \\noperands), results, and governing principles. \\nTechnology defines technical and engineering \\nconcepts, tools, languages, platforms, and systems \\ngoverned by engineering principles and practices \\nused in practice to implement artifacts and \\nanalyses in the research paradigm. The definition \\nof each component is complex involving many \\nartifacts (concepts, models, methods, analyses, \\nprinciples, and laws), reasoning, and technology \\nsystems.\\nThe definition of the research paradigm reference \\nframework is specialized to define specific \\nresearch paradigms, e.g., science and data \\nscience, that in turn are specialized to define its \\ndisciplines within the research paradigm. This \\nsimple yet powerful rule, the paradigm approach, \\ncentral in this research, is described below.\\nThe problem solving paradigm has been used for \\ncenturies to analyze (solve) complex domain \\nproblems that are expressed in terms of a specific \\nclass of phenomena. The problem solving \\nparadigm of a research paradigm is defined in its \\nreference framework. The methodology \\ncomponent defines the process that governs \\ndefining, designing, and executing a solution and \\nits expression in workflow steps. The methods \\ncomponent defines the analytical methods \\n(means), their operands, the solutions and results \\nthat are used in workflow steps to conduct \\nproblem solving. The following is a detailed \\ndescription of the (generic) problem solving \\nparadigm in a research paradigm, i.e., the process  \\n5\\nthat governs the definitions, design, and execution \\nof an analysis as a solution for a domain problem. \\nThis description does not address the frequent \\n For modern science and data science, methods and technology are added to Comte’s definition.\\n4\\n A research paradigm, e.g., science, has one methodology, e.g., the scientific method governs the design \\n5\\nand execution of experiments; analyses in data science are governed by the data science method.\\n4\\ncase of a domain problem that consists of multiple \\nsubproblems.\\nThe objective of generic problem solving paradigm \\n(Fig. P2) for a motivating domain problem is to \\ndesign a domain solution with an explanation that \\nverifies that it solves the intended problem and \\nproduces a domain result with an interpretation \\nthat validates the result is the intended domain \\nresult. Domain problem solving is done in terms of \\ninstances of a specific class of phenomena. To aid \\ndomain problem solving, the domain problem is \\ntranslated to an equivalent analytical problem \\nexpressed in terms of analytical models and \\nmethods used to design and execute an analytical \\nsolution to produce an analytical result with an \\nexplanation that verifies the analytical solution \\nand an interpretation that validates the analytical \\nresult. Analytical problem solving is done on data \\n– models of the phenomenon being analyzed and \\ndata analyses conducted over that data.\\nThe purpose of analytical problem solving is to use \\ndata and data analysis to provide insights into the \\ncorresponding domain problem, solution, and \\nresult. The benefits include providing problem \\nsolving techniques (models, methods) to augment \\nthose on the domain side and to overcome \\nlimitations of domain side models and methods. \\nParticle physics experiments are not possible \\nwithout analytical models and methods like the \\nstandard model of particle physics. Analytical \\nproblem solving requires demonstrating the \\nequivalence of the domain and analytical sides.\\nThe problem solving method for the generic \\nproblem solving paradigm involves several steps \\non both sides. On the domain side: Step 1 Use \\nestablished knowledge, or model, of the domain \\nphenomenon to define the domain problem in \\nterms of that model and methods. Step 2 Use \\ndomain-specific problem solving means (e.g., a \\nparticle physics cyclotron) to develop a domain \\nsolution. Step 3 Develop a solution explanation to \\nverify the solution. Step 4 Use the domain \\nsolution to produce a domain result. Step 5. \\nDevelop an interpretation of the domain result to \\nvalidate that it solves the domain problem. On the \\nanalytical side, for each analytical step attempt to \\nestablish the equivalence with its domain \\ncounterpart: Step 1 Use established analytical \\nknowledge, or model, of the domain phenomenon \\n(e.g., observed evidence of the Higgs boson) and \\nthe domain problem definition to define the \\nanalytical problem in terms of an analytical model. \\nStep 2 Use domain-specific analytical problem \\nsolving means to develop an analytical solution \\n(e.g., particle physics simulator). Step 3 Develop a \\nsolution explanation to verify the solution. Step 4 \\nUse the analytical solution to produce an \\nanalytical result. Step 5. Develop an interpretation \\nof the analytical result to validate that the \\nanalytical result solves the analytical problem. \\nWhile step 1 on both sides are done first, the \\norder of steps on each side and interleaving \\nbetween sides is driven by need. A step on one \\nside may assist a step on the other, e.g., domain \\nsteps provide insight into its analytical counterpart \\nand vice versa.\\nIn complex problem solving, domain and \\nanalytical problems are not solved in isolation. \\nMore context is required. For example, an \\nexperiment cannot detect a Higgs boson in \\nisolation. A 1.25GeV energy event required to \\ndetect a Higgs boson is not an isolated event. It \\nproduces an energy cascade in which a Higgs \\nboson exists momentarily before decaying into \\nleptons and other particles. For complex problem \\nsolving, the generic problem solving paradigm is \\naugmented to the generic problem solving \\nparadigm (Fig P3) with two additional levels, the \\nmodel and the standard solution levels. The \\ndomain and analytical problems (Fig P2) each exist \\nin a larger context called the domain and \\nanalytical models, respectively with additional \\nknowledge to assist problem definition and \\nsolving. For example, detecting the Higgs boson \\n5\\nFigure P2 Generic problem solving paradigm\\nAnalytical\\nsolution\\nDomain-analysis map\\nAnalytical\\nproblem\\nDomain\\nproblem\\n23\\nAnalysis model\\nAnalysis parameters\\nDomain\\nsolution\\nDomain\\nsolution\\nDomain model\\nDomain parameters\\nrequires detecting its signature energy cascade. \\nThe model of the atom and its behavior is the \\ndomain model to better define the domain \\nproblem. The standard model of particle physics is \\nthe analytical model to better define the analytical \\nproblem. A successful domain result becomes new \\nknowledge about the phenomenon that is curated \\ninto the domain model. For example, the \\nsignature Higgs boson cascade was detected, \\nhence the Higgs boson was detected. The \\nexistence of the Higgs boson and its signature \\nenergy cascade is curated into the domain model \\nof the atom and into the analytical model – the \\nstandard model of particle physics.\\nWhile there is considerable flexibility in the order \\nof the steps in a problem solution, the steps and \\ntheir order for a specific solution is expressed in a \\nproblem solving workflow that consisting of three \\nphases. The Discover phase defines a problem \\nthen designs, implements, producing a result \\n(steps 1, 2, 4), and analyzes the solution and \\nresult. The Interpret phase articulates, confirms, \\nauthorizes, and curates the solution and result \\n(steps 3, 5). The Deploy phase deploys the \\nsolution. Each step is implemented using one or \\nmore methods defined in the methods \\ncomponent of the research paradigm reference \\nframework. The workflow and the methods are \\ngoverned by principles of the research paradigm, \\ne.g., repeatability in the scientific method.\\nA research paradigm (category) is defined at two \\nlevels of abstraction. At the abstract (generic) \\nlevel, a research paradigm is defined by its \\nartifacts, defined in its ontology component, and \\nby its properties, capabilities, and limitations that \\nare, in part, deduced collectively from values of \\nartifact instances at the lower, applied level. A \\nparadigm has a value for each research paradigm \\nreference framework artifact. The two levels \\nuniquely define the research paradigm, \\ndistinguishing it from all other research \\nparadigms. For example, the scientific research \\nparadigm discipline biology is defined in terms of \\ngeneric artifacts of its phenomena, living \\norganisms (e.g., name, taxonomic ID (domain, \\nkingdom, phylum, class, order, family, genus, and \\nspecies), digestive system, form of mobility, etc.). \\nEach biology sub-discipline has a specific range of \\nvalues for each artifact instance. The unique \\nproperties, capabilities, and limitations of each \\nbiology sub-discipline are derived, in part, from its \\nartifacts’ values. For example, the living organism \\nsubcategory cheetah is the fastest land animal \\ndue, in part, to its form of mobility.\\nA discipline within a research paradigm is defined \\nby applying the research paradigm to a class of \\nphenomena. For example, a data science discipline \\nis the application of the data science research \\nparadigm to a class phenomenon, e.g., the data \\nscience NLP discipline is the data science research \\nparadigm applied to natural language problems. \\nDefining a research paradigm discipline is \\ncomplex. It is done by specializing each research \\nparadigm reference framework component and \\nproblem solving paradigm to the discipline’s class \\nof phenomena. The particle physics discipline is \\ndefined by applying the scientific research \\nparadigm to the class of elementary particle \\nphenomena. This requires a deep understanding \\nof the scientific research paradigm and of the \\nparticiple physics domain. This research provides \\nguidance for defining data science disciplines but \\ndoes not define any such disciplines that require \\nthe definition of all components of their data \\nscience discipline reference framework.\\nThe problem solving paradigm of a research \\nparadigm defines how work is conducted in a \\ndiscipline. Its definition is similarly complex. Work \\nincludes  reasoning (discover, analyze), \\n6\\narticulating (explain, interpret), confirming (verify, \\n Appendix I lists reasoning research paradigm actions (verbs).\\n6\\n6\\nAnalytical\\nsolution\\nDomain-analysis map\\nDomain\\nsolution\\nAnalytical\\nmodel\\nDomain\\nmodel\\nAnalytical\\nproblem\\nDomain\\nproblem\\nStandard\\nanalytical\\nsolution\\nStandard\\ndomain\\nsolution\\nAnalytical model\\nAnalytical parameters\\nDomain model\\nDomain parameters\\nExplanation\\n24\\nFigure P3 Generic problem-solving paradigm\\nDomain artifacts\\nDomain result\\nInterpretation \\nAnalytical artifacts\\nAnalytical result\\nInterpretation \\nExplanation\\nvalidate), authorizing, and curating (unify, \\norganize) knowledge about phenomena in that \\ndiscipline within a research paradigm. That work, \\nthe central purpose of a research paradigm, is \\ndefined by the methodology and methods \\ncomponents of the research paradigm’s reference \\nframework. The methodology component defines \\nthe process for defining, designing, and \\nconducting analyses, i.e., developing solutions, \\nthe workflow for expressing a solution, and their \\ngoverning principles. A problem solving paradigm \\nhas one methodology paradigm and one workflow \\nparadigm with a vast number of categories, e.g., \\nthe scientific method and the scientific workflow \\nin science, and the data science method  and data \\n7\\nscience workflow in data science. The methods \\ncomponent defines the analytical methods, their \\noperands, solutions, results, and governing \\nprinciples used in workflow activities. It defines \\nthe nature of solution explanations that explain \\nhow the problem was solved and is used to \\nunderstand and verify that the intended analysis \\nwas conducted successfully; and of analytical \\nresult interpretations that interpret the result in \\nterms of the analytical problem, and, more \\nimportantly, the motivating domain problem, and \\nis used to understand and validate those results.\\n1.4.\\nDefinition challenges\\nThe above classical paradigms, categories, and \\ndisciplines are used to address the data science \\nmultiple definitions challenge by focusing on the \\ntwo central concepts – data analysis artifacts and \\ndata analyses – each at three levels of abstraction, \\nresulting in six definitional challenges posed as:\\n1.\\nData science: What is it? What is data \\nscience as a research paradigm category, \\nor field of inquiry? How do you define a \\nfundamentally new field of inquiry?\\n2.\\nData science discipline: How is it used? \\nWhat is a data science (research \\nparadigm category) discipline? How do \\nyou a define a data science discipline \\nacross all data science disciplines?\\n3.\\nA distinct data science discipline: How is \\ndata science applied in a specific data \\nscience discipline? What constitutes a \\nspecific data science discipline, e.g., data \\nscience NLP? Is there value in unifying \\nmultiple definitions for data science NLP?\\n4.\\nData analyses: What are they? What are \\ndata analyses as a problem solving \\nparadigm category? How do you define a \\nfundamentally new problem solving \\nparadigm category?\\n5.\\nData analyses conducted in a data \\nscience discipline: How is it used? What \\nconstitutes the data science problem \\nsolving paradigm category as conducted \\nin a data science discipline? How do you \\ndefine data science problem solving \\nacross all data science disciplines?\\n6.\\nData science problem solving as \\nconducted in a specific data science \\ndiscipline: How is it applied in a specific \\ndata science discipline? What constitutes \\na data science problem solving paradigm \\ncategory in a specific data science \\ndiscipline? Is there value in unifying \\nmultiple definitions of data science NLP \\nproblem solving?\\nThe six cases are amenable to the same \\napproaches, processes, and means to achieve \\nsolutions – unified, coherent, evolving definitions. \\nUnderstanding and solving one aids understanding \\nand solving others. As data science is in its infancy \\nand is evolving rapidly, definitions are intended to \\ncapture the nature of artifacts, e.g., the nature of \\ndata science problem solving and of the resulting \\ndata science solutions, with flexibility to support \\ninexorable evolution.\\nThe myriad data science definitions, accepted \\nthrough proofs of utility in their respective \\ncommunities, may lead to confusion due to the \\nmultiple definitions challenge hence limit \\ncommunication and collaboration[8] across data \\nscience disciplines, hence across data science as a \\nfield of inquiry. Data science has reached a level of \\n The data science method is to data science as the scientific method is to science; it governs the expression and conduct of data \\n7\\nanalyses in workflows. A data science method is a computational analysis category that can be implemented by one or more \\nalgorithms. This dual use of the term “method” arises in conventional use in research paradigms and in computations.\\n7\\nmaturity as seen in its acceptance by the large \\ndata science community to initiate unifying data \\nscience to overcome the multiple definitions \\nchallenges guided by Comte’s paradigms.\\n1.5.\\nDefinition solution approaches\\nThis research proposes a solution for defining data \\nscience and addressing the multiple definitions \\nchallenge, answering the six problems, using the \\nresearch paradigm approach augmented by three \\nadditional approaches. Results of applying the \\nsolution are illustrated by 1,000 candidate \\ndefinitions in [24]-[32].\\nThe process for developing unified, coherent \\ndefinitions and means to express them applies to \\nall six cases. It is described only for the research \\nparadigm category (i.e., What is data science). \\nStep 1: Collect all relevant definitions of the data \\nscience research paradigm.  Step 2: Use the \\n8\\nresearch paradigm definition to align their \\nrespective artifacts. Step 3: For each artifact, \\nderive (coalesce) a unified understanding from the \\naligned artifact definitions. Step 4: For each \\nartifact, use its unified understanding together \\nwith its research paradigm definition, i.e., as a \\nunifying framework, to derive a unified, definition \\nconsistent with the research paradigm definition. \\nStep 5: Use a specification language to express a \\nunified, coherent definition. Step 6: For each \\nsource definition, examine the source to identify \\nand resolve challenges that arise from changing \\nthe definition. If possible, make appropriate \\nsource changes. Step 7: Use the unified definition \\nof the data science research paradigm to \\ndetermine if it is a new, independent research \\nparadigm category. Each of these steps can be \\ncomplex. Requirements and benefits are provided \\nfor each problem.\\nTo appreciate the process’s scale and scope, \\nconsider the corresponding case for science. The \\nscale of the process for science can be estimated \\nby the number of artifacts defined in a dictionary \\nof science, on average 20,000. The scope of such a \\nprocess can be seen in the number of scientific \\ndisciplines and sub-disciplines since science grows \\nby artifacts that emerge in existing and new \\nscientific disciplines that without unification can \\nlead to confusion[8]. There are ~16 major \\nscientific disciplines including: chemistry, biology, \\nphysics, mechanics, computer science, psychology, \\noptics, pharmacy, medicine, astronomy, \\narcheology, economics, sociology, anthropology, \\nlinguistics. Biology has ~135 sub-disciplines \\n[branches of biology UNC]. In practice, only the \\nmost accepted definition sources, e.g., journal \\npublications, are considered in Step 1. Other \\ncomplicating factors are problems that arise from \\ndifferences between a unified artifact definition \\nand source definitions. Changing a definition in a \\nsource context may impact related source \\ndefinitions or negate established results. In most \\ncases, changing source definitions is unlikely due \\nto Kuhn’s incommensurability thesis[13][20](§3.1). \\nAlternatively, the unified definition may require \\nmodification or consider the unified definition as \\nscientific progress and do not address consistency \\nwith source definitions.\\nTo address the data science multiple definitions \\nchallenge within the above process, use three \\nsolution approaches – the first to understand, the \\nsecond to define, and the third to manage \\ncoherence. The paradigm approach (steps 1-3) \\napplies the data science research paradigm, \\ndefined in terms of generic data science artifacts \\nand definitions of its categories defined by artifact \\ninstances. The research paradigm is a unifying \\nframework used to align the artifact instances at \\nthe category level with their generic artifacts at \\nthe paradigm level. The alignment can lead to a \\nunified understanding across the multiple \\ndefinitions of data science research paradigm \\ncategories thereby to a unified understanding of \\nthe data science research paradigm and its \\ncategories. The framework approach (steps 4-7) \\nuses the unified understanding of the data science \\nresearch paradigm categories guided by the \\nresearch paradigm definition as a unifying \\nframework, to guide and structure a single, \\nunified, coherent definition of emerging data \\n This example, posed in question 1, considers the data science research paradigm and its categories, I.e., data science disciplines. \\n8\\nThe process guides consistent definitions of the data analysis research paradigm and of data analysis disciplines.\\n8\\nscience research paradigm with generic artifacts \\nthat are consistent with those in each data science \\nresearch paradigm. The semantic approach is used \\nin all steps to express and manage semantic \\nrelationships, e.g., generalization and composition \\n(e.g., derived from, contains) amongst definitions \\nto ensure coherence. The approaches work in two \\ndirections – deduction and derivation. Multiple \\ndefinitions can be used to deduce a unified \\nunderstanding and definition. An accepted \\nparadigm definition can be used to derive multiple \\nincomplete or inconsistent category definitions. \\nThe approaches can be taken based on definitions \\nbeing mature, i.e., widely accepted within the \\ndata science community. They can be used to \\ncompare the resulting unified, coherent \\ndefinitions of the data science research paradigm \\nand its categories with definitions of the scientific \\nresearch paradigm and its categories to establish \\nthat data science is a fundamentally new field of \\ninquiry, independent of science as is done in §2.4.\\nMany research leaders [1][4][7][10][11][16][19] \\nhave called for a unifying framework with which \\nto structure, develop, unify, and evolve data \\nscience definitions. “Data science without a \\nunifying framework risk being a set of disparate \\ncomputational activities in various scientific \\ndomains, rather than a coherent field of inquiry \\np r o d u c i n g r e l i a b l e r e p r o d u c i b l e \\nknowledge. Without a flexible yet unified \\noverarching framework we risk missing \\nopportunities for discovering and addressing \\nresearch issues within data science and training \\nstudents in effective scientific methodologies for \\nreliable and transparent data-enabled \\ndiscovery\"[19].\\n1.6.\\nDefinition solutions\\nThe research paradigm, paradigm, framework, and \\nsemantic approaches are used to develop \\nsolutions to the data science multiple definitions \\nchallenge, i.e., to develop and express unified, \\ncoherent, evolving definitions for the six \\ndefinitional problems.\\n1.\\nWhat is data science? Data science \\n(research paradigm category) is a field of \\ninquiry, defined by applying the research \\nparadigm approach to computational \\nanalysis over data.\\n2.\\nHow is data science used? Data science \\napplies the data science problem solving \\nparadigm to a category of phenomena in \\na data science discipline. There are as \\nmany data science disciplines as \\ncategories of phenomena that can be \\nrepresented in data.\\n3.\\nHow is data science applied in a specific \\ndiscipline? The data science problem \\nsolving paradigm applies computational \\nmeans to analyze datasets that represent \\nthe discipline’s phenomena, e.g., data \\nscience NLP applies neural network \\nmodels for natural language problems.\\n4.\\nWhat is a data analysis? A data analysis \\napplies a trained, tuned computational \\nmethod to dataset prepared to represent \\nfeatures of a phenomenon that are \\ncritical to the analysis. Data science \\nproblem solving across all disciplines is \\ndefined in terms of computational \\nmethods trained and tuned to analyze \\ndatasets prepared to be analyzed by a \\nprepared method.\\n5.\\nHow are data analyses used? A data \\nanalysis selects, trains, and tunes a \\ncomputational method, adding \\nguardrails; acquires and prepares a \\ndataset to be analyzed by that method \\nresulting in a data analysis solution that \\ncan be applied to an unseen dataset to \\nproduce a data science result. The result \\nprovides insights to develop results for \\nthe motivating domain problem.\\n6.\\nHow is a data analysis applied in a \\nspecific discipline? A motivating domain \\nproblem in a discipline, e.g., protein \\nfolding, is expressed as one or more data \\nscience problems, e.g., 32 for AlphaFold. \\nEach data science problem is solved to \\nproduce a data science solution (4 and 5 \\nabove) that may be combined, e.g., an \\nensemble solution for AlphaFold. The \\nsolution is applied to an unseen dataset, \\ne.g., representing a specific protein, \\nproducing a result that provides insights \\n9\\nfor developing a domain solution, e.g., \\nthe 3D shape of the protein.\\nThe resulting definitions can be used to define the \\nemerging data science research paradigm and its \\ndisciplines as a fundamentally new, independent \\nparadigm and its disciplines in terms of data \\nscience artifacts and data science analyses.\\n2.\\nScience and data science are independent \\nfields of inquiry\\nThis section provides a definition of science as a \\nfield of inquiry based on Comte’s paradigms for \\ntwo reasons. First, it provides a detailed basis for \\ncomparing science and data science hence a \\ndeeper understanding of both. Second, the \\nparadigms used to define science are used to \\ndefine data science. The comparison shows that \\nscience and data science are distinct fields of \\ninquiry, and that data science violates the \\ndefinitive properties of science and vice versa.\\n2.1.\\nThe scientific field of inquiry \\nThe scientific research paradigm is defined by the \\nphilosophy of science and the scientific reference \\nframework (§1.2). The scientific ontology and \\nepistemology components consist of definitions of \\nscientific artifacts (concepts, models, methods, \\nanalyses, principles, and laws) and reasoning that \\ncomprise the language of science. The scientific \\nmethodology and methods components define \\nthe scientific problem solving paradigm, i.e., \\nscientific analyses (experiments). The scientific \\nmethodology component defines the scientific \\nmethod – how experiments are defined, designed, \\nand executed, the scientific workflow – how \\nexperiments are expressed, and scientific \\nprinciples that govern them, e.g., reproducibility, \\nfalsifiability, scientific induction. The scientific \\nmethods component defines scientific analytical \\nmethods (means), their operands (phenomena), \\nsolutions (experiments), and scientific results, e.g., \\na cyclotron is used to produce collisions of \\nelementary particles (operands) to achieve \\nparticle cascades (results) in which to find \\nevidence of the Higgs boson. The explanation of a \\nscientific solution is the experimental design and \\nits execution that explain how the analysis was \\ndesigned and conducted permitting verification \\nthat the intended solution was achieved. The \\ninterpretation of a scientific result is inherent in \\nthe results of the self-explanatory cause-effect \\nhypotheses thereby validating the scientific result.\\nIn summary, a scientific analysis, conducted \\ncorrectly following the scientific method and the \\nprinciples of science and of the relevant scientific \\ndiscipline, produces results that the hypotheses \\nare either true or false. Scientific results are \\ndefinitive, conclusive, casual, robust, and universal \\nscientific knowledge with verified explanations \\nand validated interpretations that are not \\nprovably complete. The power and elegance of \\nscience is that explanations and interpretations \\nare inherent in the scientific method.\\n2.2.\\nThe data science field of inquiry\\nThe data science research paradigm is defined by \\nthe philosophy of data science and the data \\nscience reference framework. The philosophy of \\ndata science is the worldview that provides the \\nphilosophical underpinnings (i.e., learning from \\ndata) for data science research for knowledge \\ndiscovery with which to reason about \\n(understand), discover, articulate, and validate  \\n9\\ninsights into the true nature of the ultimate \\nquestions about a phenomenon by computational \\nanalyses of a dataset that represents features of \\ninterest of some subset of the population of the \\nphenomenon. Data science results are \\nprobabilistic, correlational, possibly fragile or \\nspecific to the analysis method or dataset, cannot \\nbe proven complete or correct, and lack \\nexplanations and interpretations for the \\nmotivating domain problem.\\nThe data science reference framework consists of \\nthe data science axiology, ontology, epistemology, \\nmethodology, methods, and technology. A data \\nscience ontology consists of informal definitions of \\ndata science artifacts (concepts, models, methods, \\nanalyses, principles, and laws) and their \\n Validation in science is inherent in the scientific method. Validation in data science is not currently possible, posing \\n9\\nchallenges for explanations and interpretations.\\n10\\nrelationships that define what can be expressed \\n(represented) in data science. A data science \\nepistemology provides means for reasoning over \\nartifacts. Together they constitute the language of \\nthe data science. A data science problem is \\nspecified by the nature of analysis to be \\nconducted over a dataset that represents the \\nfeatures of interest of the phenomenon to be \\nanalyzed. An ideal dataset represents the features \\nthat are critical to the analysis, of the entire \\npopulation of the phenomenon, and has the \\nhighest entropy relative to the intended analysis. \\nSuch a dataset is impossible in data science since \\ndatasets are observational, i.e., not subject to \\nspecific constraints. A data science solution \\nconsists of a data science model trained, modified, \\nand tuned to meet specific requirements to \\nconduct the intended analysis on a data science \\ndataset prepared for that analysis. It is considered \\na solution only after it has been modified, possibly \\nwith guardrails, to meet its requirements. A data \\nscience result is the computational result of \\nexecuting the solution over a prepared dataset. \\nThe data science problem solving paradigm is \\ndefined by the methodology and methods \\ncomponents. The data science methodology \\ncomponent defines the data science method, the \\ndata science workflow, and principles that govern \\nthem. The data science methods component \\ndefines the constituents of data analyses – the \\ncomputational, analytical methods, their operands \\n(datasets), solutions (trained models and prepared \\ndatasets), (computational) results, and the \\ngoverning principles. It also defines the nature of \\ndata science solution explanations and data \\nscience result interpretation. Due to the \\ninscrutability of AI-based methods, the data \\nscience methods component cannot define \\nexplanations or interpretations but may provide \\ntechniques to address inscrutable solutions and \\nresults, e.g., demonstrable, standard data science \\nsolutions of which foundation models[3] are \\nexamples.\\nThere are three categories of data science defined \\nby computational methods. First, conventional \\ndata science methods include mathematics, \\nsimulation, databases, data mining, statistics \\nincluding probability theory and approximation \\ntheory, and some AI techniques like decision trees \\nand linear SVMs. Such methods fall into well-\\nknown categories, e.g., clustering, outlier \\ndetection, association, classification, regression, \\nsummarization, factor, PCA, cohort, cluster, time \\nseries, sentiment, Monte Carlo simulation. While \\nconventional data science methods have been \\nused for centuries, only now are they recognized \\nas the only transparent – scrutable – methods \\ncategory, and the least powerful. In this well-\\nunderstood category, solution explanations and \\nresults interpretations, while not inherent, are \\neasier to construct than for AI-based methods, \\ne.g., weather prediction models are designed, \\nexplained, and interpreted by experts using \\ncomplex mathematics and simulations. The key \\nfactors here are that conventional methods and \\nmodels are designed by humans to meet specific \\nrequirements hence humans are the agents of \\nlearning. The second and third categories are AI-\\nbased methods that emerged in the 1990s and \\nnow dominate data science. They are designed \\nand conducted computationally by AI algorithms \\nsuch as machine learning (ML), evolutionary, \\nheuristic, and generative algorithms. Hence, the \\nAI-based methods category is inscrutable lacking \\nsolution explanations and results interpretations. \\nA key factor here is that the algorithm is the \\nlearning agent. That difference alone – human \\nversus algorithmic design and learning – \\ndistinguishes AI-based data science from science. \\nAI-based data methods has two subcategories \\nmachine learning-based methods and deep \\nlearning-based methods. DL-based methods, a \\nsub-category ML-based methods, is a separate \\ncategory as it is significantly more challenging to \\naddress. The three computational data science \\nmethod categories are described further in[30].\\nA data science discipline is defined by applying the \\ndata science reference framework to a specific \\nclass of phenomena for which there is adequate \\ndata for data science analyses, e.g., data science \\nNLP is a data science discipline that addresses NLP \\nusing large language models.\\nThe data science method that governs the data \\nscience problem solving paradigm, illustrated in \\nFig. P4, involves four specializations of the generic \\n11\\nproblem solving paradigm (Fig P3). First, data \\nanalyses take place entirely in data on the \\nanalytical side. Second, data science problem \\nsolving aids domain problem solving by providing \\ninsights into the motivating domain problem, \\nsolution, and result. Third, a domain problem is \\nused to define corresponding data science \\nproblems but is rarely used to develop an often \\nunsolvable domain solution. Data science \\nsolutions may provide insights into the domain \\nsolution for conventional data analyses but rarely \\ndo for AI-based data analyses. Fourth, increasingly \\nclasses of domain problems have been solved \\nusing data science. In such cases generic data \\nscience models, problem solutions, and results \\nexist, e.g., in foundation models[3], and can be \\nreused with techniques, e.g., transfer learning, to \\nspecialize them to specific domain problems. \\nSimilarly, foundation datasets can be used for \\nspecific features of specific phenomena.\\nConsider the data science problem solving \\nparadigm for a domain model and problem \\nconcerning a specific phenomenon with no \\ncurrent data science solution or corresponding \\ndata science model and problem. The domain \\nmodel and problem are developed as in the \\ngeneric problem solving paradigm. Analytical \\nproblem solving are specialized for data science as \\nfollows.\\nFirst, the domain problem must be translated to a \\ndata science problem with their equivalence \\ncaptured in a domain-analysis map to be \\nmaintained to assist later insights. Indirect \\napproaches are common using one or more data \\nscience models and problems that map to parts of \\nthe domain model and problem. This challenging \\nstep requires deep knowledge on the domain and \\nanalytical sides. The DeepMind AlphaFold data \\nscience protein folding solution is a celebrated \\ncase. The protein folding problem was translated \\nto 32 data science problems each with its own \\nsolution and results that were combined to form \\nan ensemble analytical solution that can be \\napplied to proteins to gain insights into the 3D \\nstructure of the protein. The AlphaFold team of \\nover 40 of the world’s leading experts in data \\nscience, genomics, and bioinformatics built on \\nmultiple existing data science models and \\nstandard data science solutions over several years \\nand several previous models.\\nSecond, a data science solution must be \\ndeveloped for the data science problem. Based on \\nthe nature of the analysis to be conducted, e.g., \\nthe analytical category and the nature of the \\navailable observational datasets, select the \\ncomputational method and corresponding dataset \\nthat best represents the features of the \\nphenomenon critical to the intended analysis. This \\nstep involves selecting, training, tuning, and \\nevaluating multiple computational methods, \\nacquiring and preparing multiple candidate \\ndatasets, and experimenting to select the optimal \\npair. A dataset that meets requirements can drive \\nthe selection of the data science method and vice \\nversa. A computational method requires training \\nand tuning for which there are many techniques, \\ne.g., supervised, semi-supervised, unsupervised, \\nand self-supervised, each with specific dataset \\nrequirements. Dataset preparation can involve \\npartitioning into training, inference, validation, \\nand verification datasets. As with the model, the \\ndataset may require modification. Correcting \\nerrors in the dataset can improve the quality of \\nthe dataset and results. However, adding or \\neliminating data representing features of the \\nphenomenon can have negative effects. By the \\ndata-driven principle of data analyses, eliminating \\ndata from the dataset eliminates any associated \\npatterns from the results thus eliminates the \\ncorresponding insights into the phenomenon that \\nthey might have provided. The data science model \\nis trained by computation over the dataset \\ninvolving inference going forward from the initial \\nstate of the analytical model to the final state, \\ncalled forward chaining, and backward chaining. \\n12\\nFigure P4 Data analysis problem solving paradigm\\nData \\nanalysis\\nsolution\\nDomain-analysis map\\nDomain\\nsolution\\nData \\nanalysis\\nmodel\\nDomain\\nmodel\\nData \\nanalysis\\nproblem\\nDomain\\nproblem\\nStandard\\ndata \\nanalysis\\nsolution\\nStandard\\ndomain\\nsolution\\nData analysis result\\n27\\nExplanation\\nInsights\\nExplanation\\nInsights\\nInsights\\nData analysis model\\nData analysis dataset (training)\\nInterpretation \\nDomain artifacts\\nDomain result\\nInterpretation \\nInterpretation\\nInsights \\nDataset\\nForward and backward chaining can be repeated \\nmany times, e.g., thousands or millions, while \\nbeing monitored and tuned or otherwise modified \\nuntil the data science result meets specific \\nrequirements. This step creates a data science \\nsolution, i.e., a trained, tuned computational \\nmethod for a prepared data science dataset that \\ncan be applied to a similar, unseen dataset to \\nproduce a data science result. An significant open \\nchallenge in developing a trained data science \\nmodel is to constrain, using so-called guardrails, \\nresults to meet requirements, e.g., safety, \\ntrustworthiness, and prevent hallucinations. \\nThird, the data science solution and data science \\nresult are analyzed to develop an explanation of \\nhow it solves the intended data science problem \\nand how to interpret the data science result in \\nterms of the motivating domain problem. These \\nare done to increase confidence of obtaining the \\ncorrect solution and result and to be used as \\ngeneric explanations and interpretations to be \\napplied when the data science solution is \\ndeployed on an unseen dataset. This is easy for \\nconventional data science methods but not for AI-\\nbased methods for which techniques such as \\ndemonstrable, standard data science solutions are \\nused to aid in evaluating the uncertainty of the \\nresult and the risk of using the result to infer \\ndomain results to be used in practice. AI-based \\ndata science methods provide no technical \\nsolutions for these steps for which engineering \\ntechniques have been developed. Domain \\nproblem solutions  are seldom developed in such \\n10\\ncases (shown as feint in Fig P4) as they are often \\nbeyond human capacity to understand in scope, \\nscale, and complexity. This shows the strength of \\ndata science to develop data science results for \\nsuch problems and its inability to develop domain \\nsolutions.\\nFourth, a data science solution can be applied to \\nan unseen dataset prepared for the analysis to \\nproduce a data science result. The fifth and final \\nstep is the most critical. The data science solution \\nand result are used to develop insights into the \\ndomain problem, solution, and result to develop \\nresults for the motivating domain problem to be \\napplied in practice. \\nThe greatest risks posed by data science problem \\nsolving is the lack of technical solutions to control, \\nunderstand, explain, or interpret data science \\nsolutions and results. Human designers, \\ndevelopers, and users are exclusively responsible \\nfor demonstrating that results are safe, reliable, \\ntransparent, explainable, interpretable, privacy-\\npreserving, accountable, fair, trustworthy, reliable, \\nand robust, often post-facto. Data science \\nprovides no technical means for this step.\\nThe uncertainty of data science results poses risks \\nin their use to develop results for motivating \\ndomain problems to be applied in practice. Data \\nscience solutions and results are inscrutable for \\nseveral reasons. AI-based data science methods \\nand their results are inscrutable. We do not \\nunderstand what analysis is conducted, the \\ncertainty of the results, i.e., neither robust nor \\nreliable, nor how to interpret them. The prepared \\ndatasets cannot be proven to accurately represent \\nthe features of the phenomenon most critical to \\nthe analysis nor the extent to which a dataset \\nrepresents the entire population. Critical features \\ncould be missing or a feature pattern may not \\naccurately represent its occurrence in the \\npopulation. To be applied in practice, the result \\nintuited for the motivating domain problem must \\nbe demonstrated to be within acceptable bounds.\\nWhile data science solutions and results are \\ninscrutable, many techniques have been and are \\nbeing developed, often integrated into the \\nproblem solving process, to assist with developing \\ninterpretations with acceptable risks. For example, \\na well-defined and maintained domain-analysis \\nmap can assist in mapping data science results to \\ndomain results, as is done in data science model \\ntraining techniques, e.g., supervised and self-\\nsupervised. Demonstrable, standard data science \\nsolutions, like foundation models, assist by \\ndeveloping and demonstrating probabilistic \\nestimates of uncertainty and risks of applying a \\n For most domain problems addressed with data science, the objective is to gain insights into the domain result with little or no \\n10\\ninterest in a corresponding domain solution. \\n13\\ndomain result in practice. More convincingly, \\nmeans outside data science, e.g., empirical \\nanalysis of the behavior of a solution , are used. \\n11\\nThis illustrates the power and limits of data \\nscience to develop insights into motivating \\ndomain problems, solutions, and results.\\nDeveloping a demonstrable, standard data science \\nsolution addresses the most critical step five in \\ndata science problem solving – developing \\nexplanations and interpretations for the \\nmotivating domain problem within demonstrably \\nacceptable bounds, as outlined below.\\nDomain problems concerning a phenomenon are \\nposed in a model of the phenomenon based on \\nestablished knowledge, called a standard model of \\nthe phenomenon, e.g., the standard model of \\nparticle physics for the Higgs boson problem and \\nthe central dogma of microbiology for the protein \\nfolding problem. Standard models provide a frame \\nof reference within which domain problems are \\nposed and solutions developed. The Higgs boson \\nexperiment was not discovered from scratch. It \\nwas discovered as the 61st elementary particle \\nwithin a model consisting of 60 known elementary \\nparticles. The generic problem solving paradigm \\nhas standard models on the domain and analytical \\nsides. The domain standard model is often used to \\nguide the development of the analytical standard \\nmodel and to establish a domain-analysis map.\\nThis classical technique applies to data science \\nproblem solving. Apply the first step in data \\nscience problem solving using the domain \\nstandard model and problem to specify or select a \\ncorresponding data science standard model and \\nproblem and a domain-analysis map. As data \\nscience matures, there are more standard data \\nscience models, i.e., trained, tuned data science \\nmodels for specific types of domain analyses on \\nprepared datasets, e.g., foundation models and \\nfoundation datasets. While these uses of the \\ndomain-analysis map cannot be proven to be \\ncorrect, they can be used to demonstrate that the \\nselected data science model and dataset can be \\napplied within acceptable bounds. Due to the \\ninscrutability of AI-based data science methods, \\nthe map rarely aids in developing an explanation \\nfor the data science solution nor for its domain \\ncounterpart. \\nIf there is an acceptable standard data science \\nmodel and dataset and domain-analysis map, it \\nmay require modification for the specific \\napplication by applying steps two through four for \\nadditional model training (transfer learning), \\ntuning, and refinement or addition of guardrails, \\nand further dataset acquisition and preparation. If \\nthe results of step four are within bounds, then \\napply step five by modifying the generic \\nexplanations and interpretations of standard data \\nscience model and dataset to meet the needs of \\nthe specific domain problem and solution. If no \\nacceptable standard data science solution and \\ndataset can be found then previously mentioned \\nengineering techniques may apply.\\n2.3.\\nData science and science are independent, \\ncomplementary fields of inquiry\\nThe unique properties, capabilities, and \\nlimitations of science are to seek by objective, \\nquantitative means definitive outcomes for \\nobservable, i.e., measurable, bounded, \\nphenomena versus data science that seeks by \\ninscrutable means uncertain insights into \\nphenomena beyond human capacity to \\nunderstand in scope, scale, and complexity. As \\ndescribed in §2.2, data science results violate the \\nprinciples of scientific results, and vice versa. \\nHence, data science and science are independent \\nresearch paradigms. The contradictions between \\nscience and data science hold for the questions \\nposed in §1.4 and solutions proposed in §1.6 .\\nThese differences make science and data science \\npowerful, complementary research paradigms. \\nOne of the most important 21st century \\nexperiments, the Higgs Boson experiment (ATLAS, \\nCMS), used data science to discover the definitive \\nsignature Higgs boson energy cascade, that was \\nhypothesized by science, in the largest particle \\nphysics dataset in history. As in such experiments, \\nfoundation models[3], the currently most \\n Most easily done with task-based problems since results expressed in tasks can be readily evaluated. \\n11\\n14\\npowerful data science models, are based on well-\\ndefined scientific domain models of natural \\nlanguage and vision.\\n3.\\nHow data science got its name\\nThe history of the emergence of data science \\nprovides insights as to why data science is referred \\nto, incorrectly, as a science – a misnomer with \\nsignificant negative consequences.\\n3.1.\\nTukey named his vision data analysis\\nIn 1962[21], John Tukey, a prominent statistician, \\npresented a prescient, laudable, vision to extend \\nstatistics from its confirmatory role that he saw as \\nboring, formulaic, and rigid, confirming ever more \\nprecise answers to known quantities, e.g., \\ninflation rate. He envisioned exploratory data \\nanalysis[22] with which to discover insights, \\nversus answers, into “what lies beneath the data”, \\nignoring obvious, recognizable patterns often \\nsought in statistics, to seek insights that you never \\nexpected, always doubting your analysis and \\nresults. As a counterpoint to mathematics, \\nmathematical statistics, and statistics that use \\ntheory rigidly to determine and validate what to \\nderive from analyses, he saw data analysis, like \\nscience, requiring “art, flexibility, and judgement” \\nto select analysis methods and interpret what is \\nderived, based on “experience” (e.g., empirical \\nresults) guided, not determined, by theory. Based \\non mirroring the exploratory nature of scientific \\ndiscovery, Tukey claimed what he called data \\nanalysis to be an experimental “science”. Tukey’s \\n1960’s vision has expanded significantly by \\ncontributors from 40 disciplines, predominantly \\nstatistics, AI, and data systems with modeling, \\ninference, workflows, visualization, and data \\nmining. While AI algorithms (decision trees, \\nlearning algorithms using neural networks – \\nMachine Learning, evolutionary, heuristic, and \\ngenerative algorithms) had origins in the 1960s, \\nthey were not part of data science until the 1990s. \\nNow, AI-based methods that qualitatively changed \\nand now dominate data science in ways we are \\njust beginning to understand.\\nBy 2000, the power and applicability of the \\nmodern data science began to emerge far beyond \\nTukey’s vision. In 2007[9], the brilliant Turing \\nAward winner, Jim Gray, recognized the potential \\nof what Gray called the “data-intensive science \\nparadigm”, and its primary application in eScience \\nin which discoveries were made by computational \\nmeans over Big Data. Gray gave a comprehensive \\nview of computational data science as a new \\ndiscovery paradigm, distinct from science, along \\nthe following evolution of research (knowledge \\ndiscovery) paradigms.\\n•\\n1st paradigm: Empiricism (11th C BCE) \\nEmpirical evidence is discovered as \\nobservations from repeated experimentation \\nas in agriculture to discover better ways to \\nproduce food in China and India. \\n•\\n2nd paradigm: Science (12th-19th C CE) \\nKnowledge of the natural world is discovered \\nempirically following the scientific method \\nintroduced by Roger Bacon (1219-1292), a \\npolymath English monk, practiced intuitive \\nversions of the scientific method in many \\ndomains - biology, optics, alchemy, and \\nastronomy. The philosophical basis of science \\nwas defined by Francis Bacon (17th C CE) and \\nHume (18th C CE)[5]. Science was defined \\npragmatically in the 1830’s by Comte[15].\\n•\\n3rd paradigm: Simulation (aka computational \\nscience, scientific computing, scientific \\ncomputation) makes discoveries of natural \\nand manmade systems by developing and \\nexperimenting with computational models \\nthat simulate those systems.\\n•\\n4th paradigm: Data-intensive science (aka Big \\nData analytics) makes discoveries by evidence \\ngained from complex analytical methods, \\nincluding AI methods, to extract insights from \\nm a s s i v e \\nu n s t r u c t u r e d \\na n d \\nstructured datasets. Gray’s insights are \\nrecognized in the history of science[18].\\nGray distinguished the data-intensive, or data-\\ndriven, paradigm from its predecessors by analysis \\nbeing the computational analysis of data. He \\nidentified scientific discovery to be the dominant \\napplication and predicted the emergence of \\neScience and cited as a leading example, particle \\nphysics research conducted using the Large \\nHadron Collider. Gray saw Big Data analytics as \\nscientific not due to the amazing power and \\nlimitations, i.e., inscrutability, of data science \\n15\\nmethods, largely unknown in 2007; but due to his \\nprediction of its dominate application in science \\nthat he called eScience. Science makes extensive \\nuse of data science and vice versa. Gray’s \\ncontributions were to recognize “data-intensive” \\n(Big Data) analysis, first, as fundamentally new, \\nemerging research paradigm with massive \\npotential worthy of recognition by the National \\nAcademies of Science[16] and second, as a means \\nfor realizing Tukey’s exploratory data analysis \\nvision.\\n3.2.\\nWhy data analysis was called data science\\nTukey’s compelling vision of exploratory data \\nanalysis was widely adopted, especially his \\ninsightful, exploratory objective. Tukey correctly \\nobserved that scientific discoveries are made \\nthrough “exploratory analyses”, “without its strict \\nrules, guided, not determined by theory” using \\n“art, flexibility, and judgement”. These \\ndescriptions mirrored those made by Richard \\nFeynman, the Nobel prize winning physicist and \\nTukey contemporary. For example, the Higgs \\nboson was hypothesized in the mid-1960s. It was \\ndiscovered empirically and collaboratively by \\nthousands of physicists in two phases, the 45 year \\nphase (1965-2010) that Tukey called exploratory \\nscience, and the 2 year (2010-2012) experimental \\nphase when the scientific method and principles \\nwere followed rigorously. As a researcher, Tukey \\nfocused on the potential of exploration to discover \\n“what lies beneath the data”. As he did not apply \\ndata science in practice, he may have focused less \\non the experimental, confirmatory phase, i.e., \\nconducting an analysis, where science and data \\nscience differ fundamentally. This inherent \\nlimitation of data science became more evident in \\nthe 2000s due to inscrutable AI-based data \\nscience entirely unforeseen in the 1960s. Tukey’s \\nvision of data analysis as an exploratory science \\nmay have led to the misnomer data science.\\nIn 2015, Donoho, an influential statistician and \\nstudent of Tukey, supported Tukey’s claim that \\ndata analysis was a scientific endeavor[6][7] \\nfurther confirming the data science misnomer. \\nTukey and Donoho based their claim of data \\nanalysis being a science on what they defined as \\nessential characteristics of science.\\n•\\nIntellectual content\\n•\\nOrganization into an understandable form\\n•\\n“reliance upon the test of experience as the \\nultimate standard of validity” Tukey [21]\\n•\\n“… to be entitled to use the word ‘science’ we \\nmust have a continually evolving, evidence-\\nbased approach.” Donoho [6][7]\\n“Evidence-based approach” alludes to, but does \\nnot mention, empirical evidence obtained using \\nthe scientific method. The “test of experience” \\nand “evidence-based approach” applies not just to \\nscience, but to interpretivism, e.g., history, \\narcheology, economics, politics, and data science. \\nThe scientific version of the “test of experience” \\nand “evidence-based approach” relate to the strict \\napplication of the scientific method in the \\nempirical phase, including the evaluation under \\nempirical conditions of causal hypotheses that \\ncannot be done in data science or statistics both \\nof which are on the analytical versus the domain \\nside of problem solving. Finally, data science \\nviolates the definition of science and vice versa as \\ndescribed in the previous section, §2.\\nDonoho[6][7] makes the case for developing of a \\ntheory for “data science”. “Fortunately, there is a \\nsolid case for some entity called ‘Data Science’ to \\nbe created, which would be a true science: facing \\nessential questions of a lasting nature and using \\nscientifically rigorous techniques to attack those \\nquestions”. Donoho concludes “As data analysis \\nand predictive modeling becomes an ever more \\nwidely distributed global enterprise, ‘Science \\nabout Data Science’ will grow dramatically in \\nsignificance.” The need to understand currently \\ninscrutable AI-based data science methods is \\nwidely accepted. Unfortunately, no such theory, \\nscientific or otherwise, has been proposed \\nrequiring, as an alternative, the development of \\ndemonstrable, standard AI solutions that do not \\nprove but demonstrate plausible explanations and \\ninterpretations as in foundation models[3] that \\nare predicted to be the future of applied data \\nscience. A theory of data science is an open \\nresearch challenge. This research shares Donoho’s \\nresearch objectives quoted below from[6][7].\\n16\\n“GDS6: Science about Data Science. Tukey \\nproposed that a ‘science of data analysis’ exists \\nand should be recognized as among the most \\ncomplicated of all sciences. He advocated the \\nstudy of what data analysts ‘in the wild’ are doing \\nand reminded us that the true effectiveness of a \\ntool is related to the probability of deployment \\ntimes the probability of effective results once \\ndeployed. \\nData scientists are doing science about data \\nscience when they identify commonly occurring \\nanalysis/processing workflows, for example, using \\ndata about their frequency of occurrence in some \\nscholarly or business domain; when they measure \\nthe effectiveness of standard workflows in terms \\nof the human time, the computing resource, the \\nanalysis validity, or other performance metric, and \\nwhen they uncover emergent phenomena in data \\nscience, for example, new patterns arising in data \\nscience workflows, or disturbing artifacts in \\npublished analysis results. \\nThe scope here also includes foundational work to \\nmake future such science possible—such as \\nencoding documentation of individual analyses \\nand conclusions in a standard digital format for \\nfuture harvesting and meta-analysis. \\nAs data analysis and predictive modeling becomes \\nan ever more widely distributed global enterprise, \\n“science about data science” will grow \\ndramatically in significance.”\\n3.3.\\nWhy it matters\\nThe “data science” misnomer reflects similar \\nmisnomers in the 18th and 19th centuries when \\nComte’s belief in the value of science led to the \\nscientific research paradigm being applied \\ninappropriately to interpretivist disciplines. As in \\nthose times, the popularity of “data science” may \\nhave inappropriately led to its use in disciplines \\nwhere it doesn’t apply, where results must be \\nscientific, possibly leading to the false belief that \\ndata science results are scientific, i.e., definitive. \\nData science can accelerate scientific discovery, \\nhowever, the two employ distinct discovery \\nmethods from distinct paradigms.\\nMislabeling a research discipline as a science is \\ncommon possibly to confer rigor on those \\ndisciplines like social science and political science \\nwhere few discoveries can made empirically since \\nrandomized control trials (RCTs) were infeasible \\nuntil the advent of natural experiments that \\nrequire naturally arising empirical conditions. The \\n2019 Prize in Economic Sciences was awarded for \\n“the application of Natural Experiments applied to \\nmeasure the effect of programmes and policies \\nsuch as rural electrification and iron \\nsupplementation, and of uncontrolled events such \\nas famines”. In these cases, the Rwandan \\ngovernment applied different programs and \\npolicies in different regions of the country giving \\nrise to natural experiments conducted across \\notherwise similar regions. Natural experiments are \\napplied in social sciences[14] as data science.\\nThe term data science implies a fundamental \\nmisunderstanding that data science’ results are \\ndefinitive thus limiting data science understanding \\nand application. Such confusions can have serious \\nconsequences. First, understanding the true \\nnature of data science facilitates identifying open \\nchallenges (e.g., defining an epistemology, \\nrecognizing its limitations and risks) and \\nunfathomed opportunities. The power and \\nwidespread use of AI-based data science has led \\nto world-wide calls to action to address its \\npotential negative consequences and risks. This \\nrequires a deep understanding of data science, \\nespecially the currently inscrutable nature of AI-\\nbased data science. Second, defining the data \\nscience research paradigm, distinct from science, \\nprovides clear definitions of data science and data \\nscience problem solving to guide the development \\nof data science disciplines and associated practical \\napplications, just as in the 19th century the \\nscientific paradigm guided the development of \\nhundreds of scientific disciplines and myriad \\nscience-based industries. Third, as the data \\nscience research paradigm is distinct from science, \\nit can be used within the scientific research \\nparadigm, and vice versa, to address grand \\nchallenges, e.g., protein folding, to accelerate \\nscientific discoveries that humans could not \\notherwise achieve. DeepMind has delivered \\nspectacularly on its mission to accelerate scientific \\n17\\ndiscovery for “solving intelligence to advance \\nscience and benefit humanity”. Fourth, achieving \\ndiscipline-specific data science solutions in one \\ndiscipline can lead to expanding the scope of data \\nscience by generalizing the solutions to augment \\nthe data science paradigm (e.g., methods, models, \\nprinciples, laws) that can be applied in other data \\nscience disciplines. Fifth, formal definitions of data \\nscience as a field of inquiry provides a coherent \\nbasis for the data science community in 40+ \\ncontributing disciplines, data science sub-areas, \\n100s of application domains, tens of thousands of \\napplications, and industries to understand, use \\nand evolve data science. Sixth, understanding data \\nscience as a science prevents understanding the \\npotential of data science for addressing challenges \\nand producing results that are beyond human \\ncapacity in scope, scale, and complexity using \\nnon-human, but inscrutable concepts, logic, and \\nreasoning.\\n4.\\nData science multiple definitions challenge\\nThis section addresses challenges within the data \\nscience multiple definitions challenge (§1.4) and \\nintended solutions (§1.6) – the approaches taken \\nto address them, the solution requirements and \\nbenefits, and the solutions themselves. As each \\ncase is analogous, a detailed description is given \\nfor the data science research paradigm case with \\nsubsequent cases described by their unique \\ndifferences. In each case, a unified, coherent \\ndefinition of a concept is developed from multiple \\naccepted definitions for the concept using the \\nparadigm, unifying framework, and semantic \\napproaches and the unified definition of the data \\nscience research paradigm.\\n4.1.\\n... for the data science research paradigm\\nWhat is the data science research paradigm? How \\ndo you define a fundamentally new field of \\ninquiry? This is done applying Comte’s paradigms \\nto understand the philosophy of data science – its \\nnature, purpose, and methods – and to guide its \\ndefinition using the data science reference \\nframework, predominantly the ontology. First, \\napply the unifying framework approach, to \\nidentify the artifacts that constitute each \\ncomponent of the data science reference \\nframework. Second, apply the paradigm approach \\nto the myriad definitions of the artifacts to \\ndevelop a unified understanding of each artifact. \\nThird, from the unified understandings develop a \\nunified, coherent definition of each artifact. \\nFourth, use the definitions of the constituent \\nartifacts collectively to derive the properties, \\ncapabilities, and limitations of the data science \\nparadigm, thereby defining the data science field \\nof inquiry. Throughout, apply the semantic \\napproach to ensure coherence and consistence \\nwith related definitions.\\n4.1.1.\\nComplicating factors\\nConsider the nature of the challenge in more \\ndetail. Data science has been emerging slowly \\nsince 1962 and rapidly since 2000, in over 40 \\ndisciplines and tens of thousands of applications, \\neach resulting in largely independent definitions \\nand terminology for data science artifacts and \\nthereby for the data science research paradigm. \\nEach of the 106 data science publications – papers, \\nreports, text books, course notes – define relevant \\ndata science artifacts. As data science is in its \\ninfancy, it is unlikely that they were developed \\nusing reference definitions. Despite application-\\nspecific inconsistencies or incompleteness, each \\ndefinition is valuable for understanding and \\ncollectively defining the data science field of \\ninquiry. While the multiple definitions challenge \\narises with as few as two data science artifact \\ndefinitions, the scale of the challenge is enormous \\nwith many complicating factors of which a few are \\nconsidered below.\\nFirst, consider the basic challenges of analyzing \\nmyriad definitions of data science artifacts to \\ndevelop unified understandings and standard \\nterminology. Precision required in this work as \\ndescribed in §1.3. Imprecise or complex \\nterminology can impede unification. There is little \\nchallenge in understanding and unifying similar or \\nidentical definitions. Challenges arise for \\nseemingly distinct definitions that reflect different \\naspects or perspectives of an artifact. If they \\naccurately define distinct aspects of the same \\nartifact, the definitions enrich its definition but \\nrequire unification. For example, this research has \\ndiscovered more than fifteen distinct, enriching \\ndefinitions of data science workflow. More \\n18\\nchallenging is resolving inconsistent or conflicting \\ndefinitions. Are they intended to define the same \\ndata science artifact? Are some incorrect to be \\nignored? Also challenging is to maintain the \\ncoherence and consistency of definitions. Unifying \\nsource definitions requires maintaining the \\nsemantic coherence of related artifact definitions. \\nMore complex again is to detect and maintain the \\ncoherence of artifacts beyond semantic \\nrelationships across a data science ontology and \\nepistemology, e.g., the meaning of a system of \\ncomputations. A full definition of data science \\nrequires defining the complete reference \\nframework, i.e., axiology, ontology, epistemology, \\nmethodology, methods, and technology. This \\npaper addresses artifacts in the ontology and \\nsome in the epistemology.\\nSecond, due to data science being a very active in \\nresearch and applications, data science artifact \\ndefinitions are emerging and evolving rapidly. \\nSuch evolution is inherent in the infancy of an \\nemerging field. The approaches and means are \\nproposed to aid managing this evolution, e.g., \\ndetect and maintain semantic relationships with \\nartifacts from source disciplines.\\nThird, AI-based data science is inscrutable, lacking \\nexplanations of data analyses solutions and \\ninterpretations of data science results that are \\nneither reliable nor robust. Despite those \\nlimitations, it has been applied successfully to \\nmake major knowledge discoveries in many \\ndisciplines and applications. Inscrutability yields \\nunanswered questions like what do deep learning \\nmodels learn and how do they reason? Such \\nknowledge would improve the understanding \\nhence quality of data science designs, solutions \\nand their explanations, and results and their \\ninterpretations. While these questions are \\nunanswered, guidance for safe, trustworthy \\napplication is provided for foundation models[3] \\nfor important categories of data science problems \\nand solutions. Demonstrable, standard data \\nscience solutions (§2.2) should be developed to \\ndemonstrate interpretations, and optionally \\nexplanations, meet acceptable requirements that \\npose acceptable risks when applied in practice. \\nUnified, coherent, well-understood definitions of \\nthe data science artifacts contribute to addressing \\nsuch challenges despite inscrutability. This \\nemphasizes the importance, complexity, and \\ndepth of understanding required to develop \\nunified, coherent definitions.\\nFourth, Kuhn’s incommensurability thesis[13][20], \\nthat concerns multiple definitions of science, \\napplies to all research paradigms including data \\nscience. Multiple definitions are a natural part of \\nan emerging, evolving field of inquiry reflecting \\nadvances in the field as it evolves and matures. \\nThe thesis suggests that data science, data science \\ndisciplines, and data science results obtained \\nunder different definitions of data science may be \\nincomparable without a means of translating \\nbetween different definitions. In the worst case, \\none version of data science definitions may \\ninvalidate not only another version, but also \\ninvalidate its results. This may seem extreme, \\nhowever, the challenge of determining the \\ncoherence of two versions of definitions of data \\nscience is complex. The proposed approaches are \\nintended to identify and possibly resolve such \\ninconsistencies including incommensurability.\\nFifth, essential aspects of data science can be \\nmisunderstood. In addition to its inherent \\ninscrutability, common misunderstandings of data \\nscience make it difficult to understand, define, and \\napply. Data science model parameters and \\ndatasets volumes can be beyond human capacity \\nto understand in scope, scale, and complexity. \\nHumans accustomed to reasoning in less than ten \\ndimensions are incapable of reasoning in billions \\nor trillions of dimensions or parameters. Intuition \\nfor such reasoning may come as data science \\nmatures over decades. Most human reasoning in \\nlogic, mathematics, science, and the humanities \\nand in their corresponding education are directed \\nat achieving certain outcomes. Consequently, \\nmost people, including Einstein by his own \\nadmission, are not comfortable with, educated or \\npracticed in reasoning about uncertainty, \\ncounterfactual reasoning and uncertain, \\nprobabilistic or counterfactual outcomes. For over \\n50 years, Einstein considered the probabilistic \\nnature of quantum mechanics to be \\ncounterintuitive, saying that “God does not play \\n19\\ndice with the universe”. Data science is exclusively \\nabout reasoning in uncertainty. The lack of \\ncomfort, education, and practice with uncertainty \\ncan lead to misunderstanding data science – its \\npurpose, the nature of analyses, solutions, and \\nresults. A unified, coherent definition of data \\nscience must overcome these misunderstandings \\nand may lead to a deeper understanding of data \\nscience, and perhaps of the world. \\nHow do we understand and define the inscrutable \\n– phenomena beyond human understanding? \\nConsider data science as a field of inquiry and \\ndark energy – the hypothesized dominant energy \\nof the universe. Before these hypotheses, our \\nworld knowledge was adequate for life on the \\nplanet – AI-based data science was unknown \\nbefore machine learning’s rise in the 1990s, yet to \\nbe explained; dark energy was unknown before \\nHubble’s 1929 Law, explained only in 1998. We \\nthink of such phenomena being defined by their \\nconstituents (data science artifacts; dark energy \\nparticles, fluids, and forces) and collectively by \\ntheir unique properties, capabilities, and \\nlimitations. While inscrutable, they offer insights \\npreviously impossible and not otherwise possible \\n– data science insights beyond human \\nunderstanding in scope, scale, and complexity into \\nphenomena; dark energy analogously into our \\nuniverse. As with previous such challenges in \\ncomputation, e.g., P=NP, and astrophysics, e.g., \\nthe nature and extent of the universe, \\ninscrutability was reduced by persistent \\nknowledge discovery. The proposed processes to \\nunderstand and define data science may reduce \\nthe inscrutability of data science, or not!\\n4.1.2.\\nRequirements and benefits of unified \\ndefinitions\\nA unified, coherent definition of data science to be \\nachieved with the above methods has several \\nrequirements. First, such definitions and \\nterminology must be accepted by the data science \\ncommunity requiring that they reflect the \\nmultidisciplinarity of the community and scope of \\napplicability of data science. Synonyms can be \\nused for terminology required by sub-\\ncommunities. Such acceptance is to be achieved \\nby a refereed publication process of the proposed \\ndata science journal. Second, definitions should \\nsatisfy critical properties such as consistency and \\ncoherence with related definitions and desirable \\nproperties such as robustness and reliability. \\nThird, specification means, i.e., graphs, schema, \\nand language, must be able to represent essential \\nproperties and support definition, modification, \\nand unification operations that aid in maintaining \\ncritical properties, as proposed for the online \\njournal system[32]. Fourth, source definitions \\nshould be from the many (106) publications that \\nmeet requirements, e.g., acceptance using \\nPaperRank, i.e., PageRank modified for this case. \\nThey should be curated for use, e.g., ensure that \\nthey are at the right level of abstraction, e.g., \\nparadigm, category, discipline, or instance. Finally, \\na unified definition should subsume the essence \\nof the candidate definitions and selectively \\ninclude properties that may arise in individual \\nsource definitions but are applicable to all. To \\nassist with the definition process and to illustrate \\ntheir representation, 1,000 proof of concept, \\ncandidate definitions are provided [24]-[32]. A \\ncomprehensive definition of data science may \\ntake a decade or more. Consider the emergence \\nand evolution of data science and the \\ncorresponding 400 year history of defining the \\nscientific research paradigm made more complex \\nby the inscrutability of AI-based data science and \\ninevitable exploration, Tukey’s passion, to discover \\nwhat lies beneath.\\nThere are many benefits of a unified, coherent \\ndefinition of data science. First, a unified \\ndefinition can be used to recognize data science as \\na fundamental, independent research paradigm \\ndistinct from other research paradigms including \\nscience. Second, such a definition is easier to \\nunderstand and apply since it is simplified, less \\nredundant, and more coherent than myriad \\ndefinitions. A unified data science definition \\ntogether with the philosophy of data science and \\ndata science reference framework provide a \\ndeeper understanding to guide its use and \\ndevelopment. Third, standard definitions and \\nterminology support understanding, collaboration, \\nand cooperation across data science disciplines \\nand those that employ data science – a persistent \\nproblem in science[8] and may help to identify \\n20\\nand address incommensurability[20]. Data \\nscience’s scope of applicability is inherent multi-\\ndisciplinary  requiring that it be defined to be \\n12\\namenable to all relevant domains, as observed by \\ndata science leaders, e.g., “A common intellectual \\nframework can facilitate knowledge sharing about \\ndata science as a discipline across different the \\nfields and domains using data science methods in \\ntheir research”[19]. Fourth, this in turn provides a \\nbasis for the data science community to \\ncollaboratively evolve data science thus revealing \\ndata science’s true scope of applicability. For \\nexample, the unification process enables concepts \\nand techniques currently restricted to one data \\nscience discipline to be generalized up to the data \\nscience paradigm level then applied (down) to all \\nrelevant data science disciplines. This too has \\nbeen recognized by data science research leaders \\n(§1.5). Fifth, understanding what is understood \\nhelps to identify what has yet to be understood, \\ne.g., a theory of data science, including a data \\nscience epistemology due to its inscrutable AI-\\nbased data science methods, solutions, results, \\nand other open challenges. This benefit is based \\non the ancient maxim  that progress is made less \\n13\\nby accumulating facts than by knowing what you \\ndo not know. Sixth, as data science becomes \\nbetter understood, defined, taught, and practiced \\nat all education levels, people will become more \\ncomfortable with it and better able to reason in \\nuncertain and counterfactual worlds, hence better \\nable to use data science to solve real world \\nproblems. Finally, defining the data science \\nresearch paradigm (the first of six definitional \\nchallenges) can contribute to the addressing the \\nremaining five challenges and vice versa. \\nThe above requirements and benefits of a unified \\ndefinition of the data science research paradigm \\napply to all six challenges. Only differences are \\ndescribed for the remaining five cases.\\n4.2.\\n... for data science disciplines\\nData science artifacts have instances in hundreds \\nof data science disciplines and tens of thousands \\nof data science applications, hence there can be \\nfour orders of magnitude (104) more candidate \\ndefinitions for cases that concern disciplines and \\ndata analyses, i.e., all but case 1. The vastly larger \\nnumber of candidate definitions increases the \\nmultiple definitions challenge scope (disciplines) \\nand scale (applications). In turn, the diversity of \\nthe increased scope and scale provide greater \\nopportunities to enrich the resulting unified \\ndefinitions.\\nWhat is a data science discipline? How is the data \\nscience research paradigm applied in all \\ndisciplines. How do you define the concept of a \\ndata science discipline? The goal here is to derive \\nfrom accepted data science application \\ndefinitions, a unified, coherent definition of a data \\nscience discipline using the paradigm and unifying \\nframework approaches.\\nThis case concerns the myriad definitions of data \\nscience applications across their vast scope and \\nscale. The nature of this challenge differs from the \\nabove challenge for the data science research \\nparadigm. A data science discipline is defined by \\napplying the data science research paradigm to a \\nspecific category of phenomena. The definition of \\nthe data science discipline, i.e., concepts that \\ndefine all data science disciplines, requires \\ndefining the generic nature of the datasets and \\ntheir corresponding computational analytical \\nmethods and vice versa for all data science \\ndisciplines. These concepts are defined in the data \\nscience reference framework. Their definition in \\nall data science disciplines involves details, such as \\nrequirements for techniques used to realize them. \\nThis uses the unifying framework approach to \\ndeduce from myriad versions of definitions, the \\ngeneric properties for all data science disciplines.\\n Data science is devoid of human concepts, including disciplines. Multidisciplinarity is a human concept concerning \\n12\\nits scope of applicability. \\n Posed by the fathers of natural philosophy and science – Thales, Anaximander and Anaximenes in 6th century BC.\\n13\\n21\\nSeveral requirements are unique to this case. First, \\nthe unified definition of data science must be \\nadequate to guide the solution to this challenge. A \\nsecond is to acquire and select those definitions \\nthat adequately represent all data science \\ndisciplines. This is challenging due to data \\nscience’s currently unfathomed scope of \\napplicability as seen in the continued rapid \\nemergence of applications and disciplines. This \\nrequirement can be relaxed as data science is \\nassumed to evolve perhaps for decades which in \\nturn reinforces a third requirement that the \\nunified definition be amenable to evolution \\nmaintaining coherence and consistency. Finally, \\nthe solution requires understanding the nature of \\ndomain knowledge and its role in defining all \\ndisciplines. A benefit of a unified definition of the \\nconcept of a data science discipline is its use to \\ndiscover and define new data science disciplines. \\nThese requirements and benefits apply in all \\nsubsequent cases.\\n4.3.\\n... for a specific data science discipline\\nHow is data science applied in a specific \\ndiscipline? How do you define a specific data \\nscience discipline? How is the data science \\nresearch paradigm applied to a specific category \\nof phenomena in a specific discipline? What \\nconsiderations are required to specialize data \\nscience to a specific discipline, i.e., category of \\nphenomena. How do data science disciplines \\ndiffer? What can be learned addressing this \\nchallenge? The goal here is to derive from \\naccepted data science application definitions, a \\nunified, coherent definition of a specific data \\nscience discipline, e.g., are GPT-4 and Beijing \\nAcademy of AI Wu Dao 2.0 based on a common \\ndefinition of the data science NLP discipline? If \\nnot, would such a definition be valuable? The \\nparadigm approach applies in these cases.\\nThese cases concern instances of multiple \\ndefinitions of a specific data science discipline. \\nAddressing this challenge has value if there are \\nmultiple definitions that are inconsistent or that \\naddress phenomena or analyses of different \\nnature, scope, and scale, e.g., address different \\nnatural language problems or analyses, and if \\neach definition contributes techniques to the \\ndiscipline. The nature of this challenge is to apply \\nthe above factors, requirements, and benefits to \\nspecific data science discipline definitions using \\ndomain knowledge of the discipline. This involves \\napplying the unified definition of data science \\ndisciplines to the specific discipline definitions \\nconsidering the phenomena, datasets, and types \\nof analysis specialized to those discipline \\ndefinitions using domain knowledge of the \\ndiscipline. In all such cases, the unified definition \\nmust be a specialization of the generic reference \\nframework for the specific discipline. This case \\ntypically involves unifying small number of \\ndefinitions of a specific data science discipline, \\ne.g., in data science NLP publications. A benefit of \\nsuch a unified definition is its use to refine and \\nexpand a specific data science discipline, e.g., \\nwould data science NLP be improved by unifying \\ndefinitions from two distinct instances, e.g., \\nstatistical (finding statistical patterns), behavioral \\n(finding behaviors in speech acts), and \\nconnectionist (combing both), or by unifying \\n(determining inherent similarities) from NLP \\ntechniques, e.g., sentiment analysis, named entity \\nrecognition, summarization, topic modeling, text \\nclassification, keyword extraction, lemmatization \\nand stemming?\\n4.4.\\n... for the concept of a data analysis\\nWhat are data analyses? What is the data science \\nproblem solving paradigm category? How do you \\nunderstand and define a fundamentally new \\nproblem solving paradigm category? The goal here \\nis to derive from accepted definitions of data \\nanalyses in all data science applications, a unified, \\ncoherent definition of data analyses using two \\ndata science reference framework components – \\ndata science methodology (the data science \\nmethod, the data science workflow, governing \\nprinciples) and data science methods \\n(computational methods, their operands \\n(datasets), data science solutions (trained \\nmethods for prepared datasets), results, governing \\nprinciples). This uses the paradigm, unified \\nframework, and semantic approaches.\\nThis case concerns computational, algorithmic \\nmeans for conducting data science problem \\nsolving to achieve the central purpose of data \\n22\\nscience – data analyses. While the previous three \\ncases involve concepts concerning states, this and \\nsubsequent cases involve computations. The \\nchallenges, approaches, requirements, and \\nbenefits of the following three cases are \\nanalogous to the previous cases modified by \\napplying them to computations leading to \\nsignificant complicating factors. This requires \\nunderstanding computational and inferential \\nthinking[1].\\nFirst, the data science methodology and methods \\ncomponents are seldom defined or specified for \\ndata analyses that are expressed as algorithms. \\nUnification requires that the methodology and \\nmethods component definitions be defined, \\npotentially deduced from applications using the \\nparadigm, unifying framework, and semantic \\napproaches. Fundamental differences between \\napplications, e.g., image versus language \\nrecognition, are reflected in distinct specializations \\nof the generic methodology (the data science \\nmethod and workflow) and methods. The nature \\nof computational methods are typically \\nconsidered by analytical categories (e.g., \\nregression, classification, factor, dispersion, \\ndiscriminant, time series, decision trees) each of \\nwhich can have multiple algorithmic \\nimplementations. Unification requires identifying \\nthe same, similar, or inconsistent computations of \\nmultiple data analyses and unifying similar \\ncomputations. The process is challenging requiring \\nextensive knowledge of computation and \\nalgorithms. Evidence of this difficulty can be seen \\nin the many, often inconsistent, taxonomies of \\ndata science computational methods. Defining the \\nnature of computations requires defining the \\nnature of the datasets that they analyze and vice \\nversa. Second, unification requires a common \\nterminology. This is easy for data science artifacts \\nsince differences can be handled with synonyms. \\nFinding common terminology for computational \\nanalyses is deeper than synonyms. Determining \\nthat multiple specifications define the same \\ncomputation is as challenging as determining that \\ntwo algorithms implement the same analysis. \\nThird, defining the nature of data analyses \\nrequires defining the nature of the solutions \\nproduced and their explanations and of the data \\nscience results and their interpretations. The \\nobjective is to define their nature including the \\nfact that AI-based data science solutions and \\nresults are inscrutable with techniques to achieve \\nacceptable explanations and interpretations \\nincluding their weaknesses and strengths, the \\nrisks of applying them in practice, and means for \\nameliorating the risks. These issues apply to the \\nthree cases for data analyses. Fourth, data science \\nanalyses, as defined in the methods component, \\nare computations with computational properties, \\ne.g., computability, reliability, and robustness. \\nRequirements for such properties should be \\ndefined, e.g., AI-based data science methods are \\ncomputable but are neither reliable nor robust.\\n4.5.\\n... for data analyses in all data science \\ndisciplines\\nWhat is data science problem solving as \\nconducted in all data science disciplines? What is \\nthe generic data science problem solving \\nparadigm category as conducted in all data \\nscience disciplines, i.e., specializing the \\ncomputational problem solving category to all \\ndata science disciplines? The goal here is to derive \\nfrom accepted definitions of data analyses in data \\nscience applications, a unified, coherent definition \\nof the data science problem solving paradigm as \\nconducted in all data science disciplines using the \\nparadigm approach.\\nThis case concerns how data analyses are \\nconducted in all data science disciplines. It faces \\nthe same challenges as the above data analyses \\ncase, i.e., definitive properties are seldom defined \\nabove the algorithm level, hence, the nature of \\nthe data analyses conducted in all disciplines must \\nbe deduced from applications. For example, are \\nthere rules that define what computational \\nanalysis categories apply in all data science \\ndisciplines versus in specific data science \\ndisciplines? The unified definition of data analyses \\ncan be used to derive the nature of data analyses \\nin all data science disciplines, e.g., properties of \\nclassifications that apply to all disciplines, i.e., all \\ncategories of phenomena.\\n23\\n4.6.\\n... for data analyses in a specific discipline\\nWhat is data science problem solving in a specific \\ndata science discipline? How is the data science \\nproblem solving paradigm category conducted in a \\nspecific data science discipline, i.e., applied to a \\nspecific category of phenomena? What \\nconsiderations are required to specialize a data \\nscience discipline to a specific category of \\nphenomena to define a specific data science \\ndiscipline? How is the data science problem \\nsolving paradigm used to define (specialize) the \\ndata analysis concept to a specific data science \\ndiscipline? How does the data science problem \\nsolving paradigm differ between data science \\ndisciplines? What can be learned by addressing \\nthis challenge? The goal here is to derive from \\naccepted definitions of data analyses in a specific \\ndata science application category, i.e., applied to a \\nspecific class of phenomena, a unified, coherent \\ndefinition of that data science discipline, e.g., data \\nscience NLP, using the paradigm approach.\\nThis case concerns how data analyses are \\nconducted in a specific data science discipline. It \\nfaces problems shared with other cases, i.e., the \\nscope and scale problem, and the need to deduce \\ndefinitive properties that are seldom defined. It \\ninvolves specializing the definition of data \\nanalyses applied in all disciplines (§4.5) to a \\nspecific discipline. The benefit of this analysis is to \\nimprove or validate the definition of data analyses \\nin a specific discipline by identifying and resolving \\ndifferences across definitions with the benefit of \\ncorrecting or extending the methodology and \\nmethods components for the specific data science \\ndiscipline. For example, how do you define data \\nscience problem solving in NLP deduced from \\nmany relevant applications and their algorithms, \\ne.g., support vector machines, Bayesian networks, \\nmaximum entropy, conditional random field, \\nneural networks/deep learning? How are they \\nsimilar or do they differ? How do you select an \\nalgorithm for a specific NLP problem? Another \\nbenefit is to use the unified definition of data \\nanalyses in a specific discipline to similarly expand \\nthe definition of data analyses as conducted in all \\ndisciplines (§4.5).\\n5.\\nThe data science reference framework\\nThis work provides an understanding of data \\nscience as a field of inquiry based on classical \\nparadigms. It defines challenges defining data \\nscience, approaches to develop solutions, means \\nto define data science, and solution requirements \\nand benefits. A classical definition of data science \\nhas six components – axiology, ontology, \\nepistemology, methodology, methods, and \\ntechnology. Such a comprehensive definition may \\ntake decades. The definitions pursued in this \\nresearch aim for a minimal (necessary and \\nsufficient) definition for understanding data \\nscience. The epistemology component, i.e., \\nreasoning or theory, is an open research challenge \\nand may remain so for some time.\\nThe above results provide the basis developing a \\nunified, coherent definition of data science based \\non the data science reference framework. \\nCandidate definitions [24]-[31] provide proofs of \\nconcept and value of being implemented in an \\nonline system as a data science journal[32] for the \\ndata science community to review, define, evolve, \\nauthorize, and apply data science, possibly over a \\ndecade.\\nAcknowledgement\\nI thank Prof. John Mylopoulos, University of \\nToronto, for thoughtful comments on this paper.\\nReferences\\n1.\\nAdhikari, Ani; DeNero, John, Computational \\nand inferential thinking: The foundations of \\ndata science. 2nd edition (2021). \\n2.\\nBaeza-Yates, Ricardo, 10 Key Questions Every \\nCompany Should Ask Before Using AI, Fortune \\nTechnology Council, August 23, 2021\\n3.\\nBommasani, Rishi, Drew A. Hudson, Ehsan \\nAdeli, Russ Altman, Simran Arora, Sydney von \\nArx, Michael S. Bernstein et al. \"On the \\nopportunities and risks of foundation \\nmodels.\" arXiv preprint arXiv:2108.07258 \\n(2021).\\n24\\n4.\\nCleveland, W. S. (2001). Data Science: An \\nAction Plan for Expanding the Technical Areas \\nof the Field of Statistics. International \\nStatistical Review, 69(1), 21–26. http://\\ndoi.org/10.1111/j.1751-5823.2001.tb00477.x \\n5.\\nDavid Hume, Stanford Encyclopedia of \\nPhilosophy, April 2019.\\n6.\\nDonoho D. 50 Years of Data Science. \\nPresented at Tukey Centennial Workshop, \\nPrinceton, NJ, Sept. 18, 2015 (original)\\n7.\\nDonoho, David (2017) 50 Years of Data \\nScience, Journal of Computational and \\nGraphicalStatistics, 26:4, 745-766, DOI: 10.10\\n80/10618600.2017.1384734 (republished \\nwith comments)\\n8.\\nHesketh EE, Sayir J and Goldman N. Improving \\ncommunication for interdisciplinary teams \\nworking on storage of digital information in \\nDNA. F1000Research 2018, https://doi.org/\\n10.12688/f1000research.13482.1 \\n9.\\nHey, A. J. G., Tansley, S., & Tolle, K. M.(eds) \\n(2009). Jim Gray on eScience: a transformed \\nscientific method. The Fourth Paradigm. \\n(Chapter based on the transcript of the talk \\ngiven to the NRC-CBT in Mountain View, CA \\non January 11, 2007)\\n10. Jordan, M. Artificial Intelligence\\u200a —\\u200a The \\nRevolution Hasn’t Happened Yet; UC Berkeley, \\nApril 18, 2018\\n11. Jordan, M. I. (2019). Dr. AI or: How I Learned \\nto Stop Worrying and Love Economics. \\nHarvard Data Science Review. \\n12. Kissinger, Henry A., Schmidt, Eric, \\nHuttenlocher, Daniel. The Age of AI: And Our \\nHuman Future. United Kingdom: John Murray \\nPress, 2021. \\n13. Kuhn, Thomas S. The Structure of Scientific \\nRevolutions. Chicago: University of Chicago \\nPress, 1962\\n14.  Lazer, D.M.J., A. Pentland, D.J. Watts, S. Aral, \\nS. Athey, N. Contractor, D. Freelon, S. Gonzalez-\\nBailon, G. King, H. Margetts, A. Nelson, M.J. \\nSalganik, M. Strohmaier, A. Vespignani, and C. \\nWagner. 2020. Computational social science: \\nObstacles and opportunities. Science 369 \\n(2020), 1060ś1062. \\n15. Mill, John Stuart, Auguste Comte and \\nPositivism, Westminster Review 1865, \\nreproduced Project Gutenberg, Oct. 2005.\\n16. National Academies of Sciences, Engineering, \\nand Medicine. 2018. Data Science for \\nUndergraduates: Opportunities and Options. \\nWashington, DC: The National Academies \\nPress. https://doi.org/10.17226/25104. 17 \\nmembers of the committee 170 expert \\ncontributors.\\n17. Rehman, Abdul, Adil & Alharthi, Khalid. \\n(2016). An introduction to research \\nparadigms. Int’l Journal of Educational \\nInvestigations, Vol.3, No.8, 2016 (October)\\n18. Spinney, L., History as a giant data set: how \\nanalyzing the past could help save the future, \\nThe Guardian, Nov. 12, 2019.\\n19. Stodden, Victoria. The Data Science Life Cycle: \\nA Disciplined Approach to Advancing Data \\nScience as a Science. Commun. ACM 63, no. 7 \\n(2020): 58-66.\\n20. The Incommensurability of Scientific Theories, \\nStanford Encyclopedia of Philosophy\\n21. Tukey, John W. “The Future of Data \\nAnalysis.” The Annals of Mathematical \\nStatistics 33, no. 1 (1962): 1–67. http://\\nwww.jstor.org/stable/2237638.\\n22. Tukey, John Wilder (1977). Exploratory Data \\nA n a l y s i s \\nA d d i s o n -\\nWesley. ISBN 978-0-201-07616-5.\\n23. Zhang, D., Saurabh M., Brynjolfsson, E., John \\nEtchemendy, Deep Ganguli, Barbara Grosz, \\nTerah Lyons, James Manyika, Juan Carlos \\nNiebles, Michael Sellitto, Yoav Shoham, Jack \\nClark, and Raymond Perrault, “The AI Index \\n2021 Annual Report,” AI Index Steering \\nCommittee, Human-Centered AI Institute, \\nStanford University, Stanford, CA, March \\n2021. And Nestor Maslej, Loredana Fattorini, \\nErik Brynjolfsson, et. al, “The AI Index 2023 \\nAnnual Report,” AI Index Steering Committee, \\nInstitute for Human-Centered AI, Stanford \\nUniversity, Stanford, CA, April 2023.\\n25\\nData science reference framework\\n24. Brodie, M.L., Defining data science: a new \\nfield of inquiry, Harvard University, Spring \\n2023.\\n25. Brodie, M.L., Data science reference \\nframework Part 0: introduction and \\ndevelopment guidelines, Harvard University, \\nSummer 2023.\\n26. Brodie, M.L., A data science axiology: the \\nnature, value, and risks of data science (Data \\nscience reference framework Part I), Harvard \\nUniversity, Spring 2023.\\n27. Brodie, M.L., Data science reference \\nframework Part II: Data science ontology: \\ninformal definitions, Harvard University, \\nSummer 2023.\\n28. Brodie, M.L., Data science reference \\nf ra m ewo r k Pa r t I I I : D ata s c i e n c e \\nepistemology: data science reasoning, \\nHarvard University, Summer 2023.\\n29. Brodie, M.L., Data science reference \\nf ra m e wo r k Pa r t I V: D ata s c i e n c e \\nmethodology: the data science method and \\nworkflow, Harvard University, Summer 2023.\\n30. Brodie, M.L., Data science reference \\nframework Part V: Data science methods: \\ncomputational methods, solutions and \\nresults, Harvard University, Summer 2023.\\n31. Brodie, M.L., Data science reference \\nframework Part VI: Data science technology: \\ndata science engineering, Harvard University, \\nSummer 2023.\\n32. Brodie, M.L., Data science reference \\nframework Part VII: Data science Journal: a \\nproof of concept system, Harvard University, \\nSummer 2023.\\nAppendix I: Reasoning in data science\\nMany forms of reasoning are required to \\nunderstand, define, and apply data science. The \\nterm reasoning is can include analyze, apply, \\ndefine, deploy, design, determine, develop, \\ndiscuss, ensure, evaluate, evolve, execute, govern, \\nguide, identify, implement, interpret, operate, \\nprove, refine, research, specify, teach, test, \\nunderstand, verify, and more.\\nAppendix II:Data science and science philosophies\\n“Philosophy of science is a branch of philosophy \\nconcerned with the foundations, methods, and \\nimplications of science. The central questions of \\nthis study concern what qualifies as science (i.e., \\ndistinguish science from non-science) the \\nreliability of scientific theories, and the ultimate \\npurpose of science. This discipline overlaps \\nwith metaphysics, ontology, and epistemology, for \\nexample, when it explores the relationship \\nbetween science and truth. Philosophy of science \\nfocuses on metaphysical, epistemic and semantic \\naspects of science. Ethical issues such \\nas bioethics and scientific misconduct are often \\nconsidered ethics or science studies rather than \\nthe philosophy of science.” [Wikipedia]\\nReplacing science with data science gives a \\ndefinition of the philosophy of data science that is \\ndiscussed in this paper and is widely studied in the \\ndata science community.\\n“Philosophy of data science is a branch of \\nphilosophy concerned with the foundations, \\nmethods, and implications of data science. The \\ncentral questions of this study concern what \\nqualifies as data science (i.e., distinguish data \\nscience from non-data science) the reliability of \\ndata science theories, and the ultimate purpose of \\ndata science. This discipline overlaps \\nwith metaphysics, ontology, and epistemology, for \\nexample, when it explores the relationship \\nbetween data science and truth. Philosophy of \\ndata science focuses on metaphysical, epistemic \\nand semantic aspects of data science. Ethical \\nissues such as bioethics and data science \\nmisconduct are often considered ethics or data \\nscience studies rather than the philosophy of data \\nscience.”\\n26\\n',\n",
       " '2308.04896v1.pdf': 'Why Data Science Projects Fail\\nUnderstanding of Data Science Project failure and reasoning\\nBalaram Panda\\nSoftware Engineering\\nUniversity of Auckland\\nAuckland, New Zealand\\nbpan575@aucklanduni.ac.nz\\nAbstract—Data Science is a modern Data Intelligence practice,\\nwhich is the core of many businesses and helps businesses build\\nsmart strategies around to deal with businesses challenges more\\nefficiently. Data Science practice also helps in automating business\\nprocesses using the algorithm, and it has several other benefits,\\nwhich also deliver in a non-profitable framework. In regards\\nto data science, three key components primarily influence the\\neffective outcome of a data science project. Those are\\n1) Availability of Data\\n2) Algorithm\\n3) Processing power / infrastructure\\nIn today’s technology world, there is no limitation on data as\\nwell as processing power, and we have a much more efficient\\nalgorithm to produce the desired output. In spite of the success\\nof Data Science projects, many Data Science projects still fail\\nand are unable to produce the desired outcome. In this paper,\\nwe have explored the bottleneck of Data Science projects and\\nprovided some recommendations to make data science projects\\nmore successful. Standard Data Science project development life-\\ncycle CRISP-DM [1] is old in this agile development [31] world,\\nand but most Data Science practices still follow CRISP-DM.\\nIn general, Data Scientist analyses scenarios where a predictive\\nmodel or machine learning model might fail. But this study is to\\nanalyze when and why the Data Science project fails despite\\nan excellent model. Data Science is a diverse field. It needs\\ntechnical as well as business knowledge to deliver a project.\\nHence, to understand why the Data Science project fails, we need\\nto understand challenges from the business side and technical\\nside.\\n1) Technical perspective\\n2) Business leader perspective or stakeholder perspective\\nAlso, it has been observed that the success of the Data science\\nproject depends on business domain. Example market propensity\\nmodel, a strategic market campaign method, is more successful\\nin any retail use case than a fraud analytic model in the banking\\nfraud domain. So domain agnostic framework was implemented\\nin this research to make this research independent of the business\\ndomain.\\nIndex Terms—Software Engineering, Data Science, Machine\\nLearning, CRISP-DM\\nI. INTRODUCTION\\nData Science is comparable to applied research. Often in\\nresearch, there are lots of repetitive processes to be followed\\nuntil it generates the desired outcome. Similarly, data science\\nprocesses are often repetitive until they meet the business\\nobjectives. Those business objectives are set during the re-\\npeated Business Understanding and Data Understanding phase\\nof CRISP-DM. CRISP-DM developed during 90’s, and since\\nthen, components of Data Science have evolved a lot in regards\\nto the way data is collected, volume, velocity, and variety\\nof the information being analyzed(big data [24]), processing\\nmethodology, algorithms, and outcome delivery. Hence it is\\ncrucial to understand in detail the current day’s Data Science\\nchallenges, deep dive into the reasoning of failures, and find\\nout the best practice. Data Science is an evolving field, and\\nlots have been changed since last decade in regards to various\\nkeys areas, such as data collection, big data processing on\\ncloud and/or parallel or distributed processing framework,\\nevolution of new algorithm, data visualization, deployment and\\nproductionizing Data Science solutions as a digital product etc.\\nThere are several challenges in those key areas; as an example,\\nbecause of big data, the Data Science project collects lots\\nof irrelevant and uncleaned data. Sometimes the intermediate\\ndata source doesn’t have the correct business logic, which is\\nmapped to the business objective of the data science project.\\n“big data brings lots of “big errors” in data quality and data\\nusage, which cannot be used as a substitute for sound research\\ndesign and solid theories.” [4] Data quality is one of the com-\\nmon challenges in the Data Science Project. Another major\\nchallenge is domain understanding, and many times domain\\nunderstanding is a bottleneck for Data Science project success.\\nAlso, in many cases, data science issues are seen differently\\nand vary from one domain to another, stakeholder view and\\ndata scientist view. As mentioned by Dr.Om Deshmukh [9] ”\\nData Science project success also depends on what stakeholder\\nknows about the expected outcome of the data science project\\nand how effectively stakeholder expectations being managed\\nthough effectivthroughunication.” Data Science projects strug-\\ngles with these types of challenges and more other challenges\\nin industry settings, but not enough research has been done\\nin this space to highlight those problems and their possible\\nsolutions. For better quality research in this domain, years of\\nexperience in the Data Science field and role/designation level\\ndiversity in data science projects such as key stakeholders and\\ndata scientists have been included.\\nWe prepared a list of questions and interviewed experienced\\nData Scientists, Senior Leader who actively manage data\\nscience and data analytic team, and stakeholders part of data\\nscience projects, in a motivation to understand the perception\\narXiv:2308.04896v1  [cs.LG]  8 Aug 2023\\non data science project failure. Then did the thematic analysis\\nto analyze and draw conclusions from data. Then came up\\nwith the most popular theme by thoroughly analyzing the\\ndiscussion.\\nIn this research, we have discovered and highlighted the\\nkey challenging areas where Data Science practice and Data\\nscientists should focus. Also, some suggestions on tackling\\nthose challenges. Also, we came up with a state of art\\nframework to tackle those challenges in efficient ways.\\nII. RESEARCH METHODOLOGY\\nA. Research methodology\\nIn this research Interview based semi-structured Qualitative\\nresearch methodology [8] have been used.\\nB. Interview Design\\nIn this research the objective is also to capture the diversity\\nin regards to the people involved in a data science project\\nsuch as Data Scientist and business owner and/or data science\\ncapability leader and/or stakeholder. Hence for better outcome,\\ntwo types of interview questions have prepared\\nGroup A: Data Scientist with some years of experience\\nworking in Data Science project\\nGroup B: Business owner / leader / stakeholder\\nThe interviews had done via zoom software. During the in-\\nterview, answers and key points were captured. Also, interview\\nvoice recording has been stored in google drive for reference.\\nC. Candidate selection for interview and Sample Diversity\\nInterview\\ncandidates\\ncontacted\\nvia\\nreference\\nand\\nLinkedin(professional\\nnetworking\\nwebsite).\\nManually\\nreviewed their background and put them into the Group\\nA or Group B Category based on their experience. Some\\ncandidates were also selected through reference. While\\nselecting a reference candidate, Snowball technique [5] had\\nbeen used to select the reference candidate. Snowball helped\\nto capture unbiased interviews candidate samples. During the\\nselection process, candidates domain experience deiversity\\nsuch as Banking and finance, Retail, Software Product, IT\\nServices, Telecommunication etc. had been considered. In this\\nresearch the objective also is to add domain diversity so that\\nData Science problems can be viewed from a normalized and\\ngeneric approach. Also to capture more diversity candidate\\nselected from various geography such as India, USA, Canada\\nin addition to New Zeland.\\nD. Preparation of research Questions\\nIn first iteration a list of questions has been prepared from\\nprior knowledge and after going through literature review\\npapers [10] [11] [12] Then questions are discussed with the\\nresearch supervisor. Based on the discussion few questions are\\nadded, and a few are modified. A final list of questions has\\nbeen prepared and shared with the supervisor in the second\\niteration. Then after 2nd interview, 2 more questions were\\nadded to make the interview more target-oriented.\\nTABLE I\\nLIST OF INTERVIEW CANDIDATES\\nID\\nYear Of\\nExperience\\nGroup A or\\nGroup B\\nBusiness\\nDomain\\nWorking\\nGeography\\nC1\\n5 +\\nGroup A\\nIT and\\nConsulting\\nUSA\\nC2\\n10 +\\nGroup B\\nBanking and\\nFinance\\nNZ\\nC3\\n10 +\\nGroup A\\nSoftware\\nProduct\\nIndia\\nC4\\n1 +\\nGroup A\\nResearch\\nNZ\\nC5\\n5 +\\nGroup B\\nSoftware\\nProduct\\nCanada\\nC6\\n10 +\\nGroup B\\nIT Service and\\nConsulting\\nIndia\\nC7\\n10 +\\nGroup B\\nBanking and\\nFinance\\nNZ\\nC8\\n5 +\\nGroup A\\nSoftware\\nProduct\\nUSA\\nC9\\n10 +\\nGroup B\\nConsulting\\nNZ\\nGroup A: Data Scientist with some years of experience\\nworking in Data Science project\\nGroup B: Business owner / leader / stakeholder\\nE. Research Questions\\nGroup A Questions\\n1) What are the key reasons for a Data Science project\\nsuccess as per your experience\\n2) What are key reasons for Data Science project failure as\\nper your experience\\n3) What steps Data Science should follow to match user\\nexpectations? Can you refer to any specific project and\\nexplain it in detail?\\n4) How stakeholder can contribute towards Data Science\\nproject success, and how important is it?\\n5) How value measured in the Data Science project and\\nwho are involved and what are the challenges? Any\\nexample based on a specific project?\\n6) Do you think infrastructure is a challenge for any data\\nscience project?\\n7) How important Data Quality is for Data Science\\nprojects?\\n8) What are the hindrances to maintaining data quality?\\n9) What are the solutions to overcome those hindrances?\\n10) What testing approaches do you take to test the model\\nbased on business user expectations?\\n11) Some detailed discussion on failure mentioned in open\\nended. Like how to solve model explainability? Is there\\nany approach you followed, or can you suggest from\\nyour experience how to maintain model explainability\\nand without compromising the model accuracy?\\n12) Does the level of competencies affect project outcome\\nand how to solve that?\\n13) How often do Data Science projects productionize and\\nthe pitfalls you experienced during productionization ?\\n14) Many Data Science projects help automate the existing\\nprocess; what are the challenges you face during knowl-\\nedge elicitation from business users in those cases?\\n15) How challenging is change management in Data Sci-\\nence?\\n16) There are limitations of Data usability due to governance\\nrestriction? How often do you see this as a limitation,\\nand how do you solve these?\\nIn Group A, questions from 1 to 6 are open ended questions\\nand 6 to 16 are focused questions.\\nGroup B Questions\\n1) What are the essential things Data Scientists must do to\\nmake a Data Science project successful?\\n2) What is your view on Stakeholder’s contribution to a\\nData Science project?\\n3) What\\nare\\nthe\\ncritical\\ngaps\\nbetween\\na\\nsuccessful\\nand unsuccessful Data Science Project from stake-\\nholder/business owner’s perspective?\\n4) What improvement do you want to make towards the\\ntraditional Data Science approach to make it more\\nvaluable for business?\\n5) How important change management is for a Data Sci-\\nence Project? Any suggestions to make change manage-\\nment successful?\\n6) Data Science projects have lots of intangible benefits;\\nhow do you create value metrics for Data Science\\nprojects? How often are you able to achieve that?\\n7) Do you follow any ROI metrics for Data Science\\nprojects? (question 6 extension)\\n8) How do you define a Data Science project different from\\na Business Intelligence project? (Optional question)\\nF. Interview Process and Data Collection\\nInterviewer contacted via Linked-in and via emails. Once\\nthey confirmed their participation, the consent document was\\nshared, and formal consent approval was taken before the\\ninterview. Interviews are done via Zoom and in-person meet-\\nings. During the interview, answers and critical points has\\nbeen captured. Also, interview voice recording has been stored\\nin google drive for reference. Only the question-answer part\\nof the interview has been recorded. In an average interview,\\nonly the question-answer part lasted for 30 minutes. For the\\nintroduction, including research motivation, we spent around\\n8 to 10 minutes in each interview.\\nG. Data Understanding and Analysis\\nThematic analysis [6] has been used to analysis the data.\\nThematic analytic is a popular method for qualitative research.\\nAs suggested by Braun and Clark [30] there are 6 steps of\\nthematic analyses as follows\\nStep 1: Become familiar with the data\\nStep 2: Generate initial codes\\nStep 3: Search for themes\\nStep 4: Review themes\\nStep 5: Define themes\\nStep 6: Write-up\\nAs the questions were designed before the interviews and\\ninterview, we familiarised ourselves with the data during the\\ninterview. Also, note has been taken during the interview.\\nThe code generation to theme generation is described in the\\nfollowing Theme Generation section and the write-up step\\nmentioned in the result section.\\nH. Theme Generation\\nThere are various orientations in thematic analytic men-\\ntioned in [26], [27]. In this research, we have used a com-\\nbination of the Inductive and Deductive approaches. We have\\nprior knowledge from experience and literature review; based\\non that, questions have been prepared on the most common\\nissues that have been known so far. After the interview, get\\nto know more new issues that the Data Science project is\\nexperiencing and solutions around how to tackle that. After\\nthe interview, we had iterate over the recordings, list down\\nall the key issues that experts mentioned in the interview,\\nand mapped that to the questions. Then cluster those issues\\ntogether to identify the common themes in those issues. In\\naddition to the theme, we also got solutions and workaround\\nto tackle those issues.\\nIII. RESULTS\\nTABLE II\\nRESULTED THEMES\\nThemes\\nQuestions\\nwhere themes\\nwere found\\nKey concerns\\nfor\\nData Sciecne\\nProject Success\\nEffective Stakeholder\\nManagment\\nAQ4,AQ10,BQ1,BQ2\\nYes\\nClarity in Business\\nproblem understanding\\nAQ1,BQ6,AQ2\\nYes\\nData quality Issues\\nAQ7, AQ2, AQ9\\nYes\\nModel deployment\\nand productionzaiton\\nAQ3,AQ6,AQ13,BQ3\\nYes\\nDepartment level\\nintigration while\\nproductionizing the model\\nBQ3,BQ5\\nYes\\nChange Managment\\nAQ15, BQ5\\nNo\\nData governance road blocker\\nAQ17\\nNo\\nA. Effective Stakeholder Management:\\nStakeholders are the people representing a department or\\nowning a business product or a group of people who benefit\\nfrom a project’s success. In Data Science, Stakeholder can\\nbe business process owner or product owner or end-user who\\nis going to use the Data science outcome or product. Many\\nparticipants mentioned how crucial it is to communicate and\\nmanage Stakeholder effectively. C1 also mentioned that data\\nscientists need to communicate well across different project\\nstakeholders like project managers, business stakeholders, and\\ndata engineers. ”Stakeholders should share equal responsibility\\nas data scientist to make the project successful” - C1. But\\noften, stakeholders may not be available all the time as they\\nhave other priorities. Also its often found the there is a gap\\nwhat Stakeholder thinks about Data Science can solve vs what\\ndata science can actually solve for business.\\nC1 mentioned that from her experience, she found that\\ndata science projects are more likely to successful where data\\nscientist and stake holder work together towards a common\\nbusiness goal. C2 mentioned that Stakeholder should take\\nequal responsibilities as data scientist to make the Data Sci-\\nence project successful.\\n[13] Data science teams often struggle to use their model\\ninto business processes. Also stakeholders often can’t artic-\\nulate the problems they are aiming to solve. [13] there is a\\nsignificant gap between stakeholder and data science teams\\nthat needs to be recognized and addressed. [13] suggest data\\nscience need to bridge the gap between an organizational\\nstructure and leadership commitment to develop better com-\\nmunication, processes, and gain trust among all stakeholders.\\nB. Business Problem Understanding:\\nAll the participants mentioned that Business problem un-\\nderstanding is the most critical step in a data science project.\\nBusiness problem understanding with effective stakeholder\\ncommunication improves the success criteria of a data science\\nproject. During the business problem understanding process,\\nthe data science team gets an overview of the business process\\nand tries to articulate the problem that the business is trying\\nto solve. It helps the data science team get more clarity\\naround the business objective, business process, and some\\nunderstanding of data. Then Data Science team prepares the\\nplan to check for data availability. This also helps define the\\ntarget variable or dependent variable in the case of a super-\\nvised modeling approach. In this process Data Science team\\nunderstands the business problem well; more accurately and\\ndefine the business objective and target/dependent/outcome\\nvariable for the predictive model or another type of modeling\\ntask. More accurately, the outcome variable is defined, more\\nchances of success of the data science project.\\nC. Data Quality issues:\\nData quality issue another common issue which came out\\nas a theme. Most participants from group a mentioned about\\nData Quality issue. Also they mentioned most teams don’t\\nwant to spend much time on fixing data quality issues and\\nfocusing more on deliverable, which can be measured. Because\\nthe time spend is not mapped to any tangible benefit at this\\nstage. So the idea is to create something which can be used\\nfor multiple projects. Example the DW(Data Warehousing)\\nand Cube concept got more attention when BI (Business\\nIntelligence) generate value for organisation and BI/ DW team\\nstarted creating something which can be reused for many\\ndifferent projects.\\nD. Model Deployment and Production:\\nSeveral participants from group A and group B, described\\nthat many Data Science project is not deployable or failed due\\nto complexity in productionzing the model or the outcome.\\nExample an antivirus software company built a predictive\\nmodel build with 99.1% accuracy on test data. No doubt that\\nis a that is one of the best model, but if the expectation is\\nmodel should be integrated into a real-time environment, that\\nmight become tricky. That requite a software engineering skill,\\nalso the key thing is to how a model file can be integrated\\nin the existing programming environment. These things need\\nto be plan ahead before the project starts during the business\\nunderstanding and planning phase. Based on the complexity of\\nimplementation project kick-off. If its is feasible to implement,\\nthen the production implementation plan need to be discussed\\nand agreed with the stakeholder at the beginning of the project.\\nE. Department level Collaboration:\\nSeveral group B candidates highlighted that cross depart-\\nment collaboration is one of the major challenges while\\ndelivering the Data Science outcome. Many times model\\noutperform and implementation done but due to lack in\\ndepartment level integration project may not be useful for the\\norganization. Example a market propensity model can produce\\nmost accurate target customer, but that need to integrated in\\nthe marketing channel. Even though its integrated, the forntline\\nchannel might de-prioritize the request due to some other\\npriority they are having. These situation are out of control\\nof a Data Science and Stakeholder.\\nF. Change Management:\\nMost of the data science project are either used for strategy\\nor time saving or cost saving using automation to reduce\\nmanual effort. Sometimes this creates resistance from the\\npeople affected. Several candidates from group B mentioned\\nthat Change Management is one of the challenge in any data\\nscience implementation. But this is not the key challenge as\\nthere are strategy to tackle it. Also few participants mentioned\\nthat the solution for the change management sometimes out of\\ncontrol of Data Science team. As suggested by C2 ”Change\\nmanagement specialized area and need specialized people to\\nhandle such things.”\\n”Change management is not a key challenge” -C3. May\\nbe because he is working in google where automation using\\nalgorithm is always promoted.\\nTo handle change management, ”DS team need to engage\\nwith the team going to affect very early and try to gain\\nconfidence from the team affected by illustrating an explaining\\nhow this is going to improve their work” - C1.\\nG. Data Governance:\\nData governance processes, roles, policies and standard are\\ngetting more stronger after GDPR(General Data Protection\\nRegulation) [21] law. ”Around 30% of data science projects\\nunable to go to production due to Data Governance rules”\\nC3. Same issues due to data governance also mentioned by\\nvarious other participants from group A and Group B. Data\\ngovernance is a must have standard for any organization using\\ndata as a asset. But to avoid data science project late failure,\\ndata governance team engagement need to be discussed at the\\nearly stage of the project. So that Data governance team can\\nadvise on alternate solution to get the data, or they can advise\\nnot to opt for this project due to the data used in Data Science\\nproject are not meeting the governance standard. In addition\\nto that organisation need to adopt PII (personal identifiable\\ninformation) masking approaches to mask the data.\\nIV. DISCUSSION AND RECOMMENDATIONS\\nThere are the top 7 themes generated [Table II] in this\\nresearch. More emphasis was given to Effective Stakeholder\\nmanagement, Most of the interview participants mentioned\\nthe challenges during stakeholder management, and few men-\\ntioned that stakeholder management is not that challenging.\\nAlso, business leaders who sometimes act as stakeholders\\n(in our interview panel) also agreed that there is a gap in\\nunderstanding the data science process from a stakeholder lens.\\nUsually, Data science projects have more than one stake-\\nholder. In this multi-stakeholder environment, it’s always\\ngood to have a strategy around stakeholder management. The\\nstakeholder can have different types of posture [15] based on\\nthe RDAP(reactive, defensive, accommodative, or proactive)\\nstrategy (table Fig 2). The same can be applied to Data Science\\nprojects. As C2 mentioned, stakeholders should share equal\\nresponsibility. Hence for a successful data science project,\\nthe strategy should be Accommodative or Proactive where\\nstakeholder postures are Accept responsibility and Anticipate\\nresponsibility, respectively, as described in fig1. But to do that\\nas suggested by few candidated, every Data Science project\\nshould have top management buying to make the DS project\\nsuccessful.\\nFig. 1. Stakeholder Management strategy [15]\\nSometimes these postures of stakeholders due to the nature\\nof the organization of the nature of data science application.\\nSometimes the posture varies at the department level, even\\nin the same organization. Example in a Banking and finance\\nbusiness posture of the same stakeholder may vary from a\\nCredit Risk project to a pure customer marketing project.\\nHence stakeholder communication and management should be\\nstrategic, and one strategy can’t be generalized for all data\\nscience projects.\\nAnother key challenge most data scientists mention is that\\nmost projects struggle to articulate the business problem well\\nenough. Business problem understanding is not easy; it usually\\nhappens with stakeholders, business subject matter experts,\\nand the data science team. This isn’t easy because it requires a\\nsubstantial amount of prior business knowledge to understand\\nthe business objective clearly. Another challenge sometimes it\\nis hard to define a uniform business process or rule even in\\nthe same department. For example, in a credit risk modeling\\nscenario, the objective is to identify the risky customer based\\non historical default customer data. But defining a default\\ncustomer sometimes varies from one business SME to another\\nbusiness SME. Some SMEs can define a customer as a default\\ncustomer if someone has not made the payment 60 days after\\nthe due date. In some cases, SME may consider 90 days or\\nsometimes 1st-time defaulter is not true defaulter as 1st time\\ncan be due to negligence or due to some mistake. So it’s\\ncritical to understand the business problem definition clearly\\nand once you get some level of understanding, validate that\\nunderstanding in a meeting with business SME. Sometimes\\nstakeholder comes from a mindset predictive model or ma-\\nchine learning a solution to any type of business problem.\\nIn some cases, it is true, but the definition of the business\\nrule should be clear enough to make any Data Science project\\nsuccessful. [13] stakeholders are sometimes not sure what they\\nwant. This usually happens in business strategy projects. In\\nthis business understanding process, Data scientists should also\\ntake the opportunity to help refine the stakeholder objective\\nin several meetings. But often, business understanding is not\\ngiven priority because of the following reason.\\n1) Stakeholder and Data scientist are both very excited to\\nstart working on the project\\n2) Data scientists cannot ask the right questions initially\\ndue to a lack of domain knowledge. So to solve this\\nissue, business SME and Data Scientists should spend a\\ngood amount of time articulating the business problem\\nclearly.\\nAnother challenge in Data Science we found from this research\\nis that the data quality. Data quality has always been an issue.\\nSpending a good amount of time on fixing Data Quality is\\nalways difficult because effort/time to value mapping in data\\nquality framework is always missing. Hence fixing data quality\\nis never get prioritized. To understand data quality, we need\\nto understand what data quality means. Data quality has three\\ndimensions as mentioned in [20].\\n1) Accuracy : Are the data free of error ?\\n2) Consistency in Timeliness : Are the data up-to-date ?\\n3) Completeness : Are necessary data present ?\\nReasons: Data Quality issues arise due to three main rea-\\nsons.\\n1) Standardized Data collection process across organiza-\\ntion margin someone’s in saying about group Simpson\\nsomeone’s in a number of energy\\n2) Error due to human or software\\n3) Error in business logic to process the data\\nSolution we proposed to fix data quality :\\n1) Better data collection standard from application level\\ndata generation\\n2) Centralized data store : An organization needs to spend\\ntime building a feature store with the help of a robust\\ndata pipeline. The objective of the feature store is to\\ncreate a dataset which can be of high quality and useful\\nfor various DS projects. Ideally the team should list\\ndown all the possible DS projects and start mapping\\nthe possible features/ data which need to be created.\\nAnother key issue identified from this research is the lack of\\ndepartment-level collaboration, leading to low-value outcomes.\\nBecause the department which should use the product doesn’t\\nuse the model as they might have other priorities to attend.\\nThe solution is to list down all the departments going to be\\naffected by this outcome and department who are going to use\\nthe outcome and which department is going to be involved in\\nhosting this solution. This need to be discussed keeping stake-\\nholder in loop. After this department level engagement Data\\nScience team need to share the deployment plan mention how\\nthe outcome can be deployed or reusable. Other issues such\\nas change management and data governance came out of the\\ntheme are not the key bottleneck for the data science project\\nsuccess. However one participant mentioned that ”around 30%\\nof the time they loose the project due to data governance\\nand privacy limitation” - C3. The earlier Data Science assess\\nthe data privacy and governance issues, that is better for the\\nproject. So that Data Science can have the opportunity to\\nnegotiate or can find other source which can be approved by\\ngovernance.\\nWe discussed the problems and the solution in the discus-\\nsion section. In this section, we have summarized the solutions\\nbased on interviews, my knowledge, and my experience in\\nData Science. Based on this research work, we would recom-\\nmend there are three key areas where improvement needed\\n1) Stakeholder Management\\n2) Data Quality\\n3) Durable and deployable outcome\\nA potential data science project might fail due to the above\\nreasons what we discussed so far. The Data Science team\\nneeds to have good stakeholder management skills to set the\\nexpectation of the stakeholder at an early stage based on the\\nbusiness problem understanding. Also, a data scientist should\\nhave the ability to highlight the potential risk ahead of time\\nthat might occur due to data privacy and/or data security and/or\\ndata governance restrictions. Also, Data scientists should have\\nvisibility of the risk of failure due to factors such as data qual-\\nity issues and/or deployment limitations due to the complexity\\nof the software platform and/or IT infrastructure. To create\\na fail-proof data quality management for the Data Science\\nproject, the Data Science team needs to spend dedicated time\\nchecking the data quality and plan to engage the data engineer\\nto improve the data quality standard. Also, to validate the\\nrequirement with stakeholder Data Scientist need to build a\\ndeployment plan ahead and share it with the stakeholder in\\nthe early phase of the project. By combining all, we are\\nproposing a new Data Science methodology called HYBRID\\nCRISP DS(Hybrid Cross Industry Standard Process for Data\\nScience), which will take care of all the above hindrances and\\nmake Data Science project less prone to failure.\\nFollowing figure-II represents HYBRID-CRISP-DS.\\nFig. 2. HYBRID-CRISP-DS\\nIn this HYBRID-CRISP-DS methodology stakeholder and\\nbusiness SME always be in the loop during the various phases\\nof the Data Science project. As mentioned in the figure-II,\\nemphasis has been given on variable selection after consulting\\na business SME(subject matter expert), which is an iterative\\nprocess. Soon after, that needs to be validated through DGP\\n(Data Governance and Privacy) in collaboration with DG(Data\\nGovernance) team. Based on the outcome from DGP, proceed\\nto the next step, DDP(deployment and delivery plan), or\\nnegotiate on variable selection for those variables rejected\\nby DG. Based on the complexity of requirements in DDP\\nSE(software engineer) or Data engineer, dependencies will\\nbe created. Once the deployment plan is ready, that needs to\\nbe validated by stakeholders. Then, the project flows through\\nDCQ(Data collection and quality) to find the right data source\\nand fix the quality issues where needed. After DCQ, next is to\\nprepare the data, will follow through with model building and\\nvalidation. Then the emphasis is given to outcome validation\\nby the business user. The accuracy of a predictive model or\\nany unsupervised model may not make sense to the business.\\nBusinesses should always test the model with live data or any\\nvalidation data on which businesses can rely upon. Once the\\nbusiness is satisfied with the outcome, then the project moves\\nto deployment. Once the project is deployed, a proper drift\\nmonitoring technique is implemented to monitor the drift in\\nan automated fashion.\\nV. FUTURE WORK\\nAlthough several industry experts and researchers suggested\\nalternate approaches to CRISP-DM such as TDSP(team data\\nscience process) by Microsoft [25] SEMMA(Sampling, Ex-\\nploring, Modifying, Modeling, and Assessing) by SAS [28],\\neach comes with the limitation. HYBRID-CRISP-DS is a new\\napproach that came out of this research. In the future, this\\napproach needs to be applied and can be observed where\\nissues might occur while applying HYBRID-CRISP-DS in\\nreal-world data science projects. In this process, the Data\\nScience project outcome never looked from the Data Science\\nproject as a software product lens. So the testing ends in step\\n8 of this proposed methodology after the business validates\\nthe modeling outcome. So the HYBRID-CRISP-DS approach\\ncan be researched and analyzed further to the Data Science\\nas a product scenario, where the methodology can be slightly\\ndifferent with respect to business requirement, deployment,\\nand software product testing. In addition to that, the proposed\\nHYBRID-CRISP-DS is a series of processes. Hence this\\nresearch can be further extended to understand how HYBRID-\\nCRISP-DS process can be paralleled to make the Data Science\\nprocess more efficient.\\nData Science experts participated in this interview process,\\nnot emphasizing model explainability. However, model ex-\\nplainability can be a big roadblock in other domains such\\nas healthcare, critical safety environment. Even though this\\nstudy has been conducted by interviewing experts from various\\ndomains and geography, this study can be further extended by\\nincluding more professionals from a data science background.\\nACKNOWLEDGMENT\\nSpecial thanks to my supervisor Dr Kelly Blincoe (Senior\\nLecturer in Software Engineering, University of Auckland) for\\nher expert advise, guidance and excellent support during this\\nresearch. We acknowledged all the candidates ((Pragyansmita\\nNayak (C1), Confidential (C2), Mani Kanteswara Rao Gar-\\nlapati (C3), Peter Devine (C4), Tural Gulmammadov (C5),\\nSuresh Bommu (C6), Confidential (C7), Siddhant Gawsane\\n(C8), Confidential (C9)) for their participation in this research\\ninterview.\\nREFERENCES\\n[1] R Wirth, J Hipp, “CRISP-DM: Towards a standard process model for\\ndata mining” Proceedings of the 4th international conference on the\\npractical applications of knowledge discovery and data mining, 2000.\\n[2] https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-\\nnever-make-it-into-production/\\n[3] https://blogs.gartner.com/andrew white/2019/01/03/our-top-data-and-\\nanalytics-predicts-for-2019/\\n[4] Jianzheng Liu, Jie Li, Weifeng Li, Jiansheng Wu, Rethinking big data:\\nA review on the data quality and usage issues, ISPRS Journal of\\nPhotogrammetry and Remote Sensing, Volume 115, 2016, Pages 134-\\n142, ISSN 0924-2716, https://doi.org/10.1016/j.isprsjprs.2015.11.006.\\n(https://www.sciencedirect.com/science/article/pii/S0924271615002567)\\n[5] @article10.1214/aoms/1177705148, Leo A. Goodman, Snowball Sam-\\npling, volume = 32, journal = The Annals of Mathematical Statis-\\ntics, number = 1, publisher = Institute of Mathematical Statis-\\ntics, pages = 148 – 170, year = 1961, 10.1214/aoms/1177705148\\nhttps://doi.org/10.1214/aoms/1177705148\\n[6] @articlebraun2012thematic, title=Thematic analysis., author=Braun,\\nVirginia and Clarke, Victoria, year=2012, publisher=American Psycho-\\nlogical Association\\n[7] M. Young, The Technical Writer’s Handbook. Mill Valley, CA: Univer-\\nsity Science, 1989.\\n[8] @articledicicco2006qualitative, title=The qualitative research inter-\\nview, author=DiCicco-Bloom, Barbara and Crabtree, Benjamin F,\\njournal=Medical education, volume=40, number=4, pages=314–321,\\nyear=2006, publisher=Wiley Online Library\\n[9] https://www.analyticsvidhya.com/blog/2019/08/data-science-leader-\\nguide-managing-stakeholders/\\n[10] Longbing Cao. 2017. Data Science: A Comprehensive Overview.\\nACM Comput. Surv. 50, 3, Article 43 (October 2017), 42 pages.\\nDOI:https://doi-org.ezproxy.auckland.ac.nz/10.1145/3076253\\n[11] Jordan, M. I., and T. M. Mitchell. “Machine Learning: Trends, Per-\\nspectives, and Prospects.” Science, vol. 349, no. 6245, American\\nAssociation for the Advancement of Science, 2015, pp. 255–60,\\nhttp://www.jstor.org/stable/24748571\\n[12] @inproceedingsvogelsang2019requirements, title=Requirements engi-\\nneering for machine learning: Perspectives from data scientists, au-\\nthor=Vogelsang, Andreas and Borg, Markus, booktitle=2019 IEEE 27th\\nInternational Requirements Engineering Conference Workshops (REW),\\npages=245–251, year=2019, organization=IEEE\\n[13] https://sloanreview.mit.edu/article/to-succeed-with-data-science-first-\\nbuild-the-bridge/\\n[14] https://sloanreview.mit.edu/article/why-so-many-data-science-projects-\\nfail-to-deliver/\\n[15] Geunchan Lim, Hyunchul Ahn, Heeseok Lee, Formulating strategies\\nfor stakeholder management: a case-based reasoning approach, Expert\\nSystems with Applications, Volume 28, Issue 4, 2005, Pages 831-840,\\nutanISSN 0957-4174, https://doi.org/10.1016/j.eswa.2004.12.038.\\n[16] Malone, K. (2020). When Translation Problems Arise Between Data\\nScientists and Business Stakeholders, Revisit Your Metrics. Harvard\\nData Science Review, 2(1). https://doi.org/10.1162/99608f92.c2fc310d\\n[17] C. Cichy and S. Rass, ”An Overview of Data Quality Frameworks,”\\nin IEEE Access, vol. 7, pp. 24634-24648, 2019, doi: 10.1109/AC-\\nCESS.2019.2899751.\\n[18] https://hbr.org/2019/07/building-the-ai-powered-organization\\n[19] https://journals.sagepub.com/doi/full/10.1177/1096348017753521\\n[20] Benjamin T. Hazen, Christopher A. Boone, Jeremy D. Ezell, L.\\nAllison Jones-Farmer, Data quality for data science, predictive ana-\\nlytics, and big data in supply chain management: An introduction\\nto the problem and suggestions for research and applications, Inter-\\nnational Journal of Production Economics,Volume 154, 2014, Pages\\n72-80,\\nISSN\\n0925-5273,\\nhttps://doi.org/10.1016/j.ijpe.2014.04.018.\\n(https://www.sciencedirect.com/science/article/pii/S0925527314001339)\\n[21] https://en.wikipedia.org/wiki/General Data Protection Regulation\\n[22] https://en.wikipedia.org/wiki/Cross-industry standard process for data mining\\n[23] ftp://public.dhe.ibm.com/software/analytics/spss/documentation/modeler/18.0/en/Modele\\n[24] S. Sagiroglu and D. Sinanc, ”Big data: A review,” 2013 International\\nConference on Collaboration Technologies and Systems (CTS), 2013,\\npp. 42-47, doi: 10.1109/CTS.2013.6567202.\\n[25] https://docs.microsoft.com/en-us/azure/architecture/data-science-\\nprocess/overview\\n[26] https://www.psych.auckland.ac.nz/en/about/thematic-analysis.html\\n[27] Braun,\\nV.,\\n&\\nClarke,\\nV.\\n(2006).\\nUsing\\nthematic\\nanalysis\\nin\\npsychology.\\nQualitative\\nResearch\\nin\\nPsychology,\\n3(2),\\n77-101.\\nhttps://doi.org/10.1191/1478088706qp063oa\\n[28] http://documentation.sas.com/doc/en/emref/14.3/n061bzurmej4j3n1jnj8bbjjm1a2.htm\\n[29] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.9085&rep=rep1&type=pdf\\n[30] @articlebraun2006using, title=Using thematic analysis in psychology,\\nauthor=Braun, Virginia and Clarke, Victoria, journal=Qualitative re-\\nsearch in psychology, volume=3, number=2, pages=77–101, year=2006,\\npublisher=Taylor & Francis\\n[31] https://agilemanifesto.org/\\n',\n",
       " '2403.00776v1.pdf': 'A framework for understanding data science \\nMichael L. Brodie mlbrodie@seas.harvard.edu  \\nData Systems Laboratory, School of Engineering and Applied Sciences \\nHarvard University, Cambridge, MA USA \\nDRAFT February 16, 2024 \\nAbstract \\nThe objective of this research is to provide \\na framework with which the data science \\ncommunity can understand, define, and \\ndevelop data science as a field of inquiry. \\nThe framework is based on the classical \\nreference framework  (axiology, ontology, \\n1\\nepistemology, methodology) used for 200 \\nyears to define knowledge discovery \\nparadigms and disciplines in the \\nhumanities, sciences, algorithms [28], and \\nnow data science. I augmented it for \\nautomated problem-solving with (methods, \\ntechnology, community) [3][4]. The \\nresulting data science reference framework \\nis used to define the data science \\nknowledge discovery paradigm in terms of \\nthe philosophy of data science addressed in \\n[3] and the data science problem-solving \\nparadigm, i.e., the data science method , \\n2\\nand the data science problem-solving \\nworkflow, both addressed in this paper. The \\nframework is a much called for [8][33] \\nunifying framework for data science as it \\ncontains the components required to \\ndefine data science. For insights to better \\nunderstand data science, this paper uses \\nthe framework to define the emerging, \\noften enigmatic, data science problem-\\nsolving paradigm and workflow, and to \\ncompare them with their well-understood \\nscientific counterparts – scientific problem-\\nsolving paradigm and workflow. \\n1.\\nNeed to better understand inscrutable \\ndata science. \\nOur 21st C world is transforming faster than \\ne v e r b e f o r e i n h u m a n h i s t o r y. \\nUnderstanding and utilizing knowledge in \\nour knowledge-based, digital world is \\nincreasingly challenging due to its \\nexpanding scope (e.g., bioinformatics, \\nneurology, astrophysics), scale (e.g., \\nmedical knowledge doubles every 73 days \\n[7]), and complexity (neurology studies the \\nmost complex phenomenon known to \\nman). Technology not only contributes to \\nthis growth with automation, but it also \\nprovides data science knowledge \\nrepresentation and discovery capabilities \\nthat are transforming every digitally \\nexpressible human endeavor for which \\nthere is adequate data. Data science is used \\nto understand (reason over) existing \\nknowledge and to discover new knowledge \\n(i.e., solve problems, generate innovations) \\nat previously impossible scopes, scales, \\ncomplexity, and power, often beyond \\nhuman capacity to understand. We do not \\nyet know how or what AI-based methods  \\n3\\nlearn in training nor infer in analysis yet \\noffer the potential of solving otherwise \\ninsoluble problems including existential \\nproblems like climate change and cancer, \\nwith equal potential of causing harm. \\nWe offer a framework with which to better \\nunderstand data science to achieve at least \\n The classical reference framework has evolved over 200 years and is not attributable to any one individual.\\n1\\n We use one of many useful definitions of science and of data science, as explained in Appendix §8.1.\\n2\\n Appendix §8.2 defines conventional and AI-based data science.\\n3\\n 1\\ntwo things. First, that it enables more \\nefficient, effective, and beneficial \\nknowledge discovery and innovation with \\nproperties such as accuracy, integrity, and \\nrobustness. Second, that it aligns with \\npositive human values and life in two risk \\nscenarios: the bad actor problem of AI used \\nto cause harm; and the alignment problem \\nto ensure that the goals of an AI align with \\npositive human values. \\nAI-based data science is currently based on \\nAI-based methods such as Machine \\nLearning (ML), Deep Learning (DL), neural \\nnetworks (NN), dynamic programming (DP), \\nGenerative AI, and Large Language Models \\n(LLMs). While essential data science \\nreference framework components, e.g., \\nepistemology of AI-based methods, are \\ninscrutable, the framework aids in \\ndeveloping a comprehensive a definition of \\ndata science to contribute to identifying, \\ndefining, and addressing open research \\nchallenges. \\nTo better understand AI-based data science, \\nthis paper uses the framework to gain \\nvaluable insights into data science by \\ncomparing data science with science by \\nelaborating their respective problem-\\nsolving paradigms and corresponding \\nworkflows. Two types of insight emerge. \\nThe first arises from comparing the \\nemerging, inscrutable data science \\nknowledge discovery paradigm with the \\nwell-known and well-defined scientific \\nknowledge discovery paradigm. The second \\nderives from identifying fundamental \\ndifferences between data science and \\nscience, our previously most powerful and \\nbest understood knowledge discovery \\nparadigm. For example, science provides \\ncertainty with provable, incomplete \\nknowledge of the natural world, at human \\nscale; in contrast, data science provides \\nuncertain, probabilistic knowledge of any \\nphenomenon for which there is adequate \\ndata, often at scales beyond human \\nunderstanding. The uncertainty of data \\nscience may better reflect reality than the \\nperceived certainty of science. \\nWhile what is data science? may seem \\nphilosophical and far from urgent, practical \\nconcerns, it must be understood to \\nmaximize potential benefits, to identify and \\nminimize risks, and to anticipate our 21st C \\nworld. Such data science thinking [13] is \\nrequired to gain insights from data science \\nproblem-solving. Our objectives are \\nmotivated by applications and research \\nchallenges exemplified below. \\n1.1.\\nMotivating data science applications \\nComparing data science with science \\nstrongly suggests that due to its scope, \\nscale, complexity, and power, the already \\nwidely deployed AI-based data science will \\nreplace science as our dominant knowledge \\ndiscovery paradigm. Consider example \\napplications with positive and negative \\npotential that motivate the need to better \\nunderstand AI-based data science. \\n1.1.1. Economics motivates widespread \\nadoption \\nMatt Welsh, CEO of Fixie.ai [38] estimated \\nthe cost of a human software engineering \\nday in Silicon Valley in 2023 to be $1,200. \\nWelsh estimated the cost of an equivalent \\namount of work produced by an AI agent, \\ne.g., Fixie or Copilot, to be $0.12. The \\neconomics will lead all programming \\nachievable with AI to done by AI, replacing \\nmillions of human programmers. However, \\nlike all such AI applications, this is just the \\nbeginning. While AI programming works \\nwell for modules, it has yet to be applied to \\nlarge scale programming, e.g., query \\noptimizers, DBMSs, and enterprise \\napplications, but that too seems to be \\ninevitable. Consider another benefit of AI \\n 2\\nprogramming. A human programmer may \\ntake weeks to discover all APIs relevant to a \\nprogramming task, to determine which is \\nbest, and how to deploy it. An AI agent can \\nlearn every API and its uses then select and \\ndeploy the best one in seconds. Methods \\nare being developed to keep such an AI API \\nagent current automatically and cheaply \\ncompared with humans having to repeat \\nthe entire task for each API choice. \\n1.1.2. AI will dramatically improve \\nmedical care \\nCenturies of research and practice have \\nproduced a vast amount of medical \\nknowledge that is the basis of medical \\neducation. Doctors are trained with this \\nknowledge then by practical experience. \\nFirst, “It is estimated that the doubling time \\nof medical knowledge in 1950 was 50 \\nyears; in 1980, 7 years; and in 2010, 3.5 \\nyears. In 2020 it is projected to be 0.2 years\\n— just 73 days” [7]. Doctors cannot stay \\ncurrent, but an AI medical agent can, \\nautomatically. Second, doctors practical \\nexperience is limited to the patients that \\nthey treat while an AI agent can learn from \\nevery available patient medical history. \\nThese advantages have been realized in AI-\\nbased medical care agents, e.g., to improve \\nmaternal health, and are being applied in \\nmany medical care and mental health \\napplications. Due to the critical nature of \\nsuch knowledge, AI agents are being \\ndeployed as doctor’s assistants that offer \\nthe most relevant knowledge, assessments, \\nand treatment plans that leverage all \\nrelevant and recent knowledge. \\nDelfina care is an example of data-driven \\ndigital care systems closing existing gaps in \\npregnancy care by enabling earlier \\nintervention and monitoring. Isabel Fulcher, \\nDelfina Chief Scientific Officer, describes in \\na Delfina Care lecture how the Delfina team \\nleverages data to design data visualizations \\nto improve remote patient monitoring, \\ndeploy predictive algorithms to enable \\npersonalized interventions, and implement \\nquasi-experimental designs to optimize \\npatient care experiences and outcomes \\nusing causal reasoning. \\n1.1.3. The world will accept AI as it did \\nphotography \\nPhotography emerged in the early 19th C \\ntransforming the creation of images from \\nskilled, knowledge- and innovation-based, \\ncostly human endeavors that produced \\nhigh-value images, to an automated activity \\nrequiring little skill, knowledge, or time, \\nsignificantly impacting the value of images. \\nOver time, that fundamental change was \\naccepted and integrated into modern life. \\nIn the 21st C, knowledge and innovation can \\nbe automated based on existing data, \\nknowledge, and data science, making \\nknowledge innovation and discovery fast, \\ncheap, and indistinguishable from human \\ncounterparts. Previously, the resulting \\nknowledge was practically and legally \\nconsidered an asset or property, i.e., IP. \\nHow will the automation of knowledge and \\ninnovation change our world? \\n1.1.4. A I v s . h u m a n p r o d u c t s : \\nindistinguishable, more flexible \\nSince its launch in London in 2022, sold-out \\nperformances of ABBA Voyage earns $2M \\nper week. This performance of ABBA, the \\n1970s, four member Swedish rock group, \\nhas been seen by over 1M people. The \\nperformers are AI avatars of the \\nseptuagenarian musicians. Soon, the \\nperformances may be in cities around the \\nworld with the avatars speaking in the local \\nlanguages with the correct lip and body \\nmovements. Another example is that AI can \\nproduce books so rapidly that “Amazon \\nbanned authors from self-publishing more \\nthan three e-books a day on its Kindle \\nplatform and required publishers to clearly \\n 3\\nlabel books by robot writers.” [The \\nGuardian, Sept. 20, 2023] “Now and Then”, \\na song written and recorded in the late \\n1970’s by John Lennon, was re-produced \\nusing AI to improve the original audio and \\nto mix in Paul McCartney and Ringo Star \\nvoices, despite Lennon having passed in \\n1980. These are examples of human \\nendeavors that are being automated with \\nAI, often indistinguishable from their \\nhuman counterparts.  \\n1.2.\\nMotivating data science challenges \\nWhile the essential aspects of AI-based \\ndata science learning and discovery are \\ninscrutable, framing data science and its \\nresearch challenges may lead to better \\nunderstand specific features of AI-based \\nknowledge discovery. Consider three such \\nexamples. \\n1.2.1. Power versus safety \\nThere appears to be a fundamental tradeof \\nbetween 1) maximizing the scope, scale, \\ncomplexity, and power of unconstrained, \\nuntrained AI-based knowledge discovery \\nmethods, (trained) models,  solutions, and \\nresults, and 2) limiting those properties by \\nconstraining models, problems, solutions, \\nand results with guardrails to bound their \\nbehavior for safety and accuracy. This poses \\nsignificant research challenges for achieving \\nan optimal tradeoff between maximizing \\nthose properties and operating within \\nrequired bounds. We need to understand \\nthe tradeoff, related challenges, and \\nresearch to address them. \\n1.2.2. Data quality \\nData quality is a measure of the extent to \\nwhich training or inference data is an \\naccurate, true characterization of a \\nphenomenon, often for an intended \\npurpose. More precisely, data quality is a \\nmeasure of the extent to which a data \\nvalue for a property of a phenomenon \\nsatisfies its specification. While verifying \\nthe correspondence between a value and \\nits specification can be automated, the \\ncritical correspondence is between the \\nvalue and the reality that it represents, \\nverifiable only by humans. This must be \\ndone at scale. \\n1.2.3. Data scale \\nHow much data is required to address the \\nabove two challenges – to maximize the \\nscope, scale, and complexity of AI-based \\ndata science, and to achieve human-\\nevaluated veracity of training and inference \\ndata, trained models, solutions, and results. \\nCan there be too much or too little data? \\nCan we measure whether a vast dataset \\nhas a distribution that accurately and \\nadequately reflects reality? If not, how do \\nwe deal with its inherent biases? Adequate \\ntraining data challenges involve, at least 1) \\ndata at scale, and 2) biased data. \\n•\\nData at scale: Scale is currently a \\nchallenge as the largest datasets, e.g., \\nthe Internet, have been exhausted. This \\nwill change as the Internet grows \\nexpanding beyond its predominantly \\nwhite, western origins and as massive, \\nyet unused private and public data \\nstores become available, e.g., from \\nprivate and public institutions, \\ngovernments, businesses, and \\nindividuals. This will result in a data \\nmarketplace with complex ownership, \\nlegal, and governance issues yet to be \\naddressed. Synthetic data is being used, \\nbut it may not represent reality and the \\nextent to which it does represent \\nreality, it may not reflect the natural \\noccurrences, i.e., distributions. \\n•\\nBiased data True, accurate data that \\nreflects the values and knowledge of \\nthe data source e.g., culture, enterprise, \\nindividuals, reflects their inherent \\nbiases. Models trained on such data \\nreflect those biases. For example, \\n 4\\nmedical models trained on white, \\nwestern patients may adequately \\nreflect the medical issues of the source \\npatients but may not adequately reflect \\nthose of non-white, non-western \\npatients. Medical applications may \\nrequire that data quality requirements \\ninclude accurately reflecting the target \\npatient population. \\n•\\nEliminate societal biases to improve AI \\nmodels There is considerable research \\non improving the aspirational quality of \\ndata used for training AI-based models, \\ne.g., eliminating systemic racial bias \\nfrom AI-based models used in medicine \\n– medical diagnosis and treatment that \\ncurrently discriminate against people \\nthat are not white, English speaking, or \\nlive in the Western world. This means \\nthat real data cannot be used as is. \\nWould the ideal training dataset for a \\nspecific medical diagnosis represent all \\npeople equitably relative to their \\nexistence in the population? Should real \\ndata be curated so to remove or correct \\ndata that is erroneous or incorrectly \\nrepresents the intended phenomena? \\nIn summary, core data science data \\nchallenges are 1) data representation – \\naccurately representing features of \\nphenomena, 2) data purpose – accurately \\nand adequately representing data for \\nspecific analytical purposes, 3) data \\nadequacy and scale, and 4) verifying truth – \\nmeans with which the relevant community \\ncan evaluate data quality, hence, the \\nquality of data science results [5]. \\n1.3.\\nData science and science differ \\nfundamentally \\nTo better understand data science, we use \\nthe conventional problem-solving paradigm \\nas a framework with which to compare the \\nwell-understood scientific problem-solving \\nparadigm – the scientific method – with the \\nemerging data science problem-solving \\nparadigm – the data science method. As a \\ncontext for the comparison, consider \\ncritical fundamental similarities and \\ndifferences. Science and data science are \\nknowledge discovery paradigms that \\nemploy the conventional problem-solving \\nparadigm and workflow in fundamentally \\ndifferent ways reflecting their respective \\nnatures, introduced here, elaborated in §5. \\nOne of the greatest innovations and \\nresulting discoveries of the 21st C is AI-\\nbased data science that enables knowledge \\ndiscovery, problem solutions, and \\ninnovation at scopes (applications), scales, \\ncomplexity, and power beyond those of \\ns c i e n c e , o ft e n b e y o n d h u m a n \\nunderstanding [4]. Scientific analysis is \\nrestricted to the physical realm – \\nmeasurable phenomena – and to problems \\nat human scale – that humans can \\nunderstand - at the cost of physical labor \\nand time. In contrast, AI-based data science \\nanalysis operates in the digital realm \\nautomating and accelerating discovery for \\nproblems potentially at unfathomed scale \\nwith increasingly powerful, fast, and \\ninexpensive computing and data resources. \\nAI-based data science introduces a \\nfundamentally new knowledge discovery, \\ngeneration, and problem-solving paradigm \\nwith methods that enable solutions not \\notherwise possible, not only at scopes, \\nscales, and complexities never before \\npossible, but for any problem for which \\nthere is adequate data. Central to data \\nscience is data at scale from which it learns \\n(trains) and infers (discovers) patterns at \\nscale and complexity. It knows nothing of \\nhuman concepts, despite humans \\ninterpreting results as such. The results – \\ndiscovered patterns – can be seen as \\nphenomena that humans may have never \\nimagined, hence potentially providing \\n 5\\nfundamentally novel conceptions of reality. \\nHuman concepts are artificial, i.e., \\nconceived of by humans, as are data \\nscience-discovered innovations, created  by \\nAI-programmed computations. Data-driven, \\nAI-based data science will discover \\nknowledge not previously discovered by \\nscience or conceived of by humans. \\nScientific experiments are governed by the \\nscientific method that requires that \\nexperiments be validated, i.e., the \\nexperimental design is correct; the \\nexperiment was conducted according to \\nthe design; and the results verified, i.e., \\nwithin acceptable tolerances of human \\nhypothesized values. Validation and \\nverification is done first by a scientific team \\nand then authorized by the relevant \\ncommunity, e.g., journal, as required by the \\nscientific reference framework. Hence, \\nscientific problems, experiments, and \\nresults are well understood. In contrast, the \\nnature, scope, scale, complexity, and power \\nof AI-based data science enable previously \\nunimagined benefits. However, these \\nbenefits render inscrutable AI-based data \\nscience (untrained) methods, (trained) \\nmodels, and results (discovered knowledge, \\ninnovations) often posing major challenges. \\nThere are no theoretical or practical means \\nfor explanation – to explain and verify what \\nor how methods learn or what or how \\nmodels infer. Similarly, there are no \\ntheoretical or practical means for \\ninterpretation – to interpret and validate \\nresults in terms of the motivating domain \\nproblem. This poses two major risks. First, \\nmodels and their results could be \\nerroneous. A model may conduct the \\nwrong analysis or conduct the right analysis \\nincorrectly. This risk can be addressed to a \\nlimited degree within data science using \\nempirically developed guardrails. Second, \\nsolutions and their results may be \\nindistinguishable from their human \\nproduced counterparts. Whether correct or \\nnot, when applied in practice they may be \\ndestructive or cause harm, potentially at \\nscale. These risks cannot be addressed \\nwithin data science, prompting hyperbolic \\nclaims of risks of ending civil society or the \\nworld. \\n1.4.\\nAI-based data science challenges \\nIn ~500,000 papers [40], hundreds of \\nthousands of researchers attempt to \\nunderstand AI-based data science \\nchallenges such as 1) how to constrain AI \\nwith empirically developed guardrails, with \\nconsiderable success; 2) how to train \\nmodels to be more precise and less error \\nprone, also with success; and 3) how AI \\nsolutions learn and infer, e.g., comparing \\nAI-based learning versus human learning as \\na guide to alter AI-based algorithms, with \\nlimited success. These challenges fall into \\ntwo areas – 1) understanding its reasoning \\n(problem-solving paradigm), and 2) \\nunderstanding the mappings between two \\nsides of the conventional problem-solving \\nparadigm (Fig. 1) – a) the motivating \\ndomain side – model, problem, solution, \\nand result, and b) the analytical side – (AI-\\nbased) model, problem, solution, and \\nresult. The first challenge concerns the \\nnature of AI-based reasoning. The second \\nconcerns the application of AI-based \\nreasoning in problem-solving or knowledge \\ndiscovery. This paper attempts to \\ncontribute to frame and understand these \\nchallenges. \\n3.\\nProblem-solving paradigms and \\nworkflows \\n3.1.\\nConventional problem-solving \\nparadigm \\nThe conventional problem-solving \\nparadigm (Fig. 1) is used to solve a problem \\nconcerning a phenomenon in a discipline, \\ni.e., domain of discourse. Domain problems \\n 6\\nare normally solved directly on real world \\nphenomena, by evaluating hypothesized \\nsolutions directly on those phenomena (left \\nside of Fig. 1). Consider domain problem-\\nsolving on its own. A domain problem is \\nexpressed in the context of a known \\ndomain model as a hypothesized extension, \\nparameterized by domain problem \\nparameters over which the domain \\nproblem is evaluated. A result specification \\naids all problem-solving steps, especially \\nthe domain problem result that it specifies. \\nA domain solution is created by augmenting \\nthe domain model with the hypothesized, \\nparameterized domain problem. The \\ndomain solution is evaluated over the \\nhypothesized domain problem parameter \\nvalues against real phenomena producing a \\ndomain solution result that is then used to \\nproduce a domain problem result, i.e., the \\nhypothesized solution is true or false. \\nConventional problem-solving solves a \\ndomain problem aided by solving a \\ncorresponding, equivalent analytical \\nproblem that provides guidance for solving \\nthe domain problem and vice versa. \\nAnalytical problem-solving (right side of Fig. \\n1) uses an analytical model – typically a \\nmathematical model of the phenomenon, \\npossibly parameterized by model \\nparameters, that has been demonstrated to \\ncorrespond to the domain model via a \\ndomain-analysis map. A domain-analysis \\nmap aids expressing the domain problem \\nas an analytical problem within an \\nanalytical model, that together with \\nanalytical problem parameters defines an \\nanalytical solution. A result specification is \\nused to specify or bound the intended \\nanalytical problem result. The analytical \\nsolution can be applied by instantiating it \\nwith analytical problem parameter values \\nand evaluating it over the analytical data \\nthat represents phenomenon instances \\nthus solving the analytical problem and \\nproducing an analytical solution result. That \\nresult is used to evaluate whether the \\nhypothesized analytical problem was true \\nor false thus producing an analytical \\nproblem result that is verified against the \\nresult specification. The domain problem is \\nsolved only when the analytical problem \\nresult is interpreted in terms of the \\nmotivating domain problem producing the \\ndesired domain problem result. This is done \\nusing a domain-analysis map. \\nIn summary, the conventional problem-\\nsolving paradigm consists of the domain \\nproblem-solving paradigm that is aided by \\nthe analytical problem-solving paradigm, \\nand vice versa. \\n \\nFigure 1: Conventional problem-solving \\nparadigm. \\nIn all problem-solving paradigms, domain-\\nanalysis maps establish correspondences \\nbetween domain and analytical problem \\nsolving components at the same level. At a \\nminimum, the correspondences aid in \\nexplaining, interpreting, and even deriving \\na domain component in terms of its \\nmapped analytical component, or vice \\nversa. Hence, they aid in developing and \\nexplaining problem-solving components at \\nthe same level, e.g., domain model to \\nanalytical model, as well as domain-analysis \\nat different levels, e.g., a domain-analysis \\n 7\\nmap between models aids developing a \\ncorresponding maps between problems. As \\ndescribed below, domain-analysis maps in \\nscience always establish equivalence; in \\ndata science they may provide insights. \\nDerivation relationships (downward arrows \\nfrom upper to lower components in) are \\nalso valuable in all problem-solving \\nparadigms. They represent theoretical and \\npractical means for deriving and verifying \\nlower from upper problem-solving \\ncomponents. Unlike a domain-analysis map \\nused to establish a direct correspondence, \\neach derivation relationship is specific to \\nthe problem and the discipline. For \\nexample, in the Higgs boson experiment \\n(§3.2.2) the scientific model is the standard \\nmodel or particle physics (SMPP) minus the \\nHiggs boson. The scientific problem is the \\nhypothesized behavior of the Higgs boson \\nexpressed as an SMPP extension. \\n3.2.\\nScientific problem-solving paradigm \\nThis section describes the components of \\nthe scientific problem-solving paradigm  \\n4\\n(Fig. 2), i.e., the conventional problem-\\nsolving paradigm applied in science. \\nScientific problem-solving is conducted on \\nthe domain side against real phenomena, \\ni.e., by a scientific experiment, aided by \\nanalytical problem-solving on the analytical \\nside, against data representing an \\nequivalent problem in an independent, \\noften mathematical or simulation, \\nanalytical model. The following sub-\\nsections describe how the paradigm is \\napplied in a scientific workflow. \\n \\nFigure 2: Scientific problem-solving \\nparadigm. \\nScientific problem-solving follows the \\nscientific method to discover novel \\nproperties of a physical phenomenon for \\nwhich there is a scientific model, expressed \\nas a conceptual model, that represents the \\nscientific knowledge of the phenomenon \\nthat has been proven theoretically and \\nvalidated empirically. The purpose of \\nscientific problem-solving is to solve a \\nscientific problem defined as a hypothetical \\nextension of the scientific model over \\nscientific problem parameters against the \\nreal phenomena. A result specification \\ncharacterizes a hypothesized scientific \\nproblem result including required precision \\nand aids future problem-solving steps. The \\nscientific model is augmented with the \\nscientific problem and its scientific problem \\nparameters to form, i.e., to derive and \\nverify, a scientific solution (experiment). \\nThe experiment evaluates, i.e., measures, \\nthe phenomena  directly over the scientific \\n5\\nproblem parameter values producing a \\nscientific solution (experimental) result, i.e., \\nobservations of the hypothesized values of \\nproperties of the phenomenon. The \\nexperimental result must be interpreted in \\n There are many accepted definitions of the scientific problem-solving paradigm. The definition used here was \\n4\\nchosen as a basis of comparison with the emerging data science problem-solving paradigm.\\n Physical empiricism, or by analogy with data science terminology, learning from physical phenomena.\\n5\\n 8\\nterms of the motivating scientific problem \\nto produce a scientific problem result. If the \\nobserved values satisfy the result \\nspecification, then the scientific solution, \\ni.e., hypothesized behavior, is true; \\notherwise, it is false. Scientific truth is \\ndetermined first by a successful \\nexperiment, supported by a corresponding \\nanalytical solution and ultimately by \\nconcurrence of the relevant scientific \\ncommunity. The community evaluates the \\nmodel, problem, experiment, and results to \\nauthorize it by acceptance in a journal or a \\nconference. Subsequently, the new \\nscientific knowledge is curated, i.e., \\nincorporated, into the scientific knowledge \\nof the relevant discipline. \\nAnalytical problem-solving is used to \\ndevelop and guide scientific problem-\\nsolving. Consider the analytical problem \\nsolving components. Such an analytical \\nsolution provides means for designing, \\nexploring (discovering), and evaluating \\nanalytical problems, solutions, and results. \\nAn analytical problem parameterized by \\na n a l y ti c a l p r o b l e m p a r a m e t e r s , \\nhypothesizes the properties of the behavior \\nof a phenomenon to be proven empirically \\non the scientific side. They are expressed as \\nextensions of an analytical model, possibly \\nparameterized by model parameters, that \\nrepresents the scientific knowledge of the \\nphenomenon that has been proven \\ntheoretically and validated empirically. An \\nanalytical model is an independent \\nrepresentation, often in mathematics or a \\nsimulation. Their equivalence is established \\nwith a verified domain-analysis map. The \\nanalytical side analyzes a model of the \\nscientific side phenomenon. \\nAn analytical solution is created, i.e., \\nderived, by augmenting the analytical \\nmodel with the hypothesized analytical \\nproblem parameterized over the conditions \\nbeing explored by the analytical problem \\nparameters that represents the \\nhypothesized behavior of instances of the \\nphenomenon. A result specification is used \\nto specify the hypothesized analytical \\nproblem result and aids in all problem-\\nsolving steps. A verified domain-analysis \\nmap is used to map the analytical \\nproblems, solutions, and results to their \\nscientific counterparts. \\nAn analytical problem is solved (explored) \\nby executing the analytical solution over \\nspecific analytical problem parameter \\nvalues and analytical data, representing \\ninstances, and producing an analytical \\nsolution result, i.e., computational results \\nof the hypothesized behavior of the \\nphenomena. The analytical solution result \\nmust be interpreted in terms of the \\nanalytical problem to produce, i.e., \\ndiscover, an analytical problem result. If \\nthat result satisfies the result specification, \\ne.g., within the required bounds of the \\nhypothesized properties of the behavior of \\nthe phenomenon, then the hypothesized \\nanalytical behavior is true; otherwise, it is \\nfalse. Such analytical truth aids in \\nestablishing and documenting community \\nauthorization. Finally, the analytical \\nproblem result must be interpreted in \\nterms of the motivating scientific problem \\nresult, and vice versa. \\n3.2.1. S c i e n ti fi c p r o b l e m - s o l v i n g \\nworkflow \\nScientific problem-solving is conducted \\nfollowing the scientific method in the two \\nphase scientific problem-solving workflow \\n(Fig. 3) that uses the scientific and \\nanalytical scientific problem-solving \\nparadigm components (Fig. 2) as follows \\nand demonstrated in the next section with \\nthe Higgs boson experiment. \\n 9\\n \\nFigure 3: Scientific problem-solving \\nworkflow. \\nThe Discover phase consists of four steps. In \\nthe Define problem step, a scientific \\nproblem with its scientific problem \\nparameters is defined as a hypothetical \\nextension of the relevant scientific model. A \\ndomain-analysis map is defined to express \\n(interpret, explain) the scientific problem \\nand model in terms of an analytical \\nproblem with its analytical problem \\nparameters within its relevant analytical \\nmodel with its model parameters. Result \\nspecifications are defined on the scientific \\nand analytical sides to characterize the \\nhypothesized scientific and analytic \\nproblem results and to aid in all problem-\\nsolving steps. \\nIn the Design experiment step, an analytical \\nsolution is developed (derived) by \\naugmenting the relevant analytical model \\nwith the hypothesized analytical problem \\nparametrized by analytical problem \\nparameters. The instances to which the \\nanalytical solution is to be applied are to be \\ngiven as specific analytical problem \\nparameter values and analytical data. At \\nthis point, the experiment designed on the \\nanalytical side is used to guide the \\nconfiguration of the experiment on the \\nscientific side. Previously defined domain-\\nanalysis maps are used to guide (derive) \\nthe definition of another domain-analysis \\nmap with which to map the analytical \\nsolution to the scientific solution \\n(experiment) and its scientific problem \\nparameter values and the phenomenon \\nthat is the subject of the experiment. \\nSimilarly, previously defined derivation \\nrelationships can be used to derive and \\nverify the scientific solution. This step \\ninvolves developing and configuring \\nexperimental apparatus with which to \\nconduct the experiment designed on the \\nanalytical side, illustrated in §3.2.2 by the \\nLarge Hadron Collider (LHC) for the Higgs \\nboson experiment. In the Apply experiment \\nstep, the scientific solution (experiment) is \\napplied, i.e., the configured experimental \\napparatus is operated, under conditions \\ndefined with given scientific problem \\nparameter values against instances of the \\nphenomenon to produce the scientific \\nsolution result that is then analyzed in \\nterms of the hypotheses of the scientific \\nproblem to determine the scientific \\nproblem result. In practice, this step is \\ncompleted on the analytical side with the \\nprevious Design experiment step to verify \\nthe analytical solution prior to mapping it \\nto the scientific side. The previous domain-\\nanalysis maps are used to define the final \\ndomain-analysis map used to verify the \\nscientific solution (experimental) result and \\nscientific problem result by mapping them \\nto the analytical solution result and \\nanalytical problem result. In practice, this \\nstep is used to verify what was developed \\ndirectly on the scientific side. Finally, in the \\nAnalyze experiment & result step, \\ninformation is gathered from the Define, \\nDesign, and Apply experiment steps to be \\nanalyzed in the Interpret phase to interpret \\nand verify the experiment, and to validate \\nthat the scientific problem result are \\ncorrect both scientifically and analytically. \\nThe Interpret phase consists of four steps. \\nIn the Explain solution, interpret result step, \\nthe scientific and analytical solutions and \\ntheir domain-analysis map are used to \\nexplain the solutions, one in terms of the \\nother; and the analytical problem result, \\n \\n10\\nand the scientific problem result, and their \\ndomain-analysis map are used to interpret \\nthe results, one in terms of the other. In the \\nVerify explanation, validate interpretation \\nstep, the respective explanations are \\nverified, and the respective results are \\nvalidated using the respective domain-\\nanalysis maps and all relevant problem-\\nsolving components. In the Authorize \\nsolution & result step, the verified \\nexperiment and the validated experimental \\nresults are submitted to the relevant \\nscientific community, e.g., a journal or \\nconference, for authorization as proven \\nscientific results. In the Curate solution & \\nresult step, the authorized scientific \\nsolution and scientific problem result are \\ncurated into existing scientific knowledge. \\nIn practice, the scientific problem-solving \\nworkflow is applied rigorously following the \\nrules of the scientific method only on the \\nexperiment’s final, successful execution. \\nThe development of an experiment is truly \\nexperimental, seldom following the \\nworkflow steps as described above and \\nseldom strictly applying the scientific \\nmethod. Scientists incrementally explore all \\nscientific and analytical problem-solving \\ncomponents to better understand each \\nstep , as described next for the Higgs boson \\n6\\nexperiments. The CMS and ATLAS Higgs \\nboson experiments took 48 years of \\nincremental design, development, and \\nvalidation on both sides, each informing \\nthe other. \\n3.2.2. Scientific problem-solving example: \\nthe Higgs boson \\nConsider scientific problem-solving for the \\nscientific problem “does the Higgs boson \\nexist?” (left side of Fig. 4) as conducted in \\nthe Higgs boson experiments. The \\nexperiments were developed following the \\nscientific workflow by applying the \\nscientific problem-solving paradigm aided \\nby analytical problem-solving that were \\nverified to mirror each other. One aids the \\nother in expressing, deriving, analyzing, \\ndiscovering, and verifying corresponding \\nmodels, problems, solutions, and results. \\nConsider each side separately. The scientific \\nmodel was the conceptual model of the \\nstandard model of particle physics (SMPP) \\nwithout the Higgs boson. The scientific \\nproblem was expressed as an extension of \\nthe scientific model in terms of the \\nhypothesized mass of the Higgs boson  \\n7\\nhence the energy required to move, thus \\nsense, Higgs bosons, and the resulting \\nenergy cascades as bosons decay into other \\nelementary particles, e.g., leptons and \\nphotons. \\nThe scientific solution (experiment) was the \\nLHC  configured using the hypothesized \\n8\\nscientific problem parameters that were \\nvaried to cover all empirical conditions, i.e., \\nranges of Higgs boson weights and energy \\ncascades. Operating the configured LHC for \\nthe final runs produced the scientific \\nsolution (experimental) result, i.e., 2 years \\nof observed mass and energy cascade data. \\nThe scientific solution result was analyzed \\nto confirm the hypotheses thus interpreting \\n As Edison said, \"I didn\\'t fail 1,000 times. The light bulb was an invention with 1,000 steps.\"\\n6\\n 125.35 GeV with a precision of 0.15 GeV, an uncertainty of ~0.1%\\n7\\n The LHCis a sophisticated machine designed to directly manipulate and measure properties of individual \\n8\\nelementary particles under experimental conditions.\\n \\n11\\n(discovering) the scientific problem result, \\ni.e., the Higgs boson exists with a known \\nmass and energy cascade behavior. \\n \\nFigure 4: Scientific and analytical problem-\\nsolving for the Higgs boson experiments. \\nScientific-side discovery was conducted on \\nreal physical phenomena, e.g., elementary \\nparticles, using the LHC configured for the \\nSMPP, a conceptual scientific model. \\nAnalysis-side discovery was conducted \\nusing an SMPP simulation (analytical \\nmodel) with hypotheses (analytical \\nproblem) expressed as simulation \\nconfigurations that varied over conditions \\nanalogous to the empirical conditions. The \\nsimulation (analytical solution) was \\nconducted over the hypothesized \\nconditions (analytical problem parameter \\nvalues and data) producing observations \\n(analytical solution result) that were \\nevaluated to produce the simulation result \\n(analytical problem result). \\nThe Higgs boson experiments took 48 years \\nfrom Peter Higgs’ 1964 proposal to the \\n2012 CMS and Atlas completions. In that \\ntime, over 10,000 scientists worldwide \\nexplored potential scientific and analytical \\nsolutions. As in all scientific experiments, \\nthe steps of the Discover and Interpret \\nphases were not sequential but overlapped \\nsignificantly as did the scientific and \\nanalytical problem solutions. Progress was \\nmade initially on the analytical side as the \\nSMPP simulation existed and scientific \\nproblem-solving was possible only after the \\nLHC completion in 2008. Similar patterns \\narise in all problem-solving, especially in \\nthe formative stages of data science \\nproblem-solving, as explored next. \\n4.\\nData science problem-solving \\nTo better understand data science, the \\nconventional problem-solving paradigm is \\nused to compare the well-understood \\nscientific problem-solving paradigm and \\nworkflow with the emerging, inscrutable \\ndata science problem-solving paradigm and \\nworkflow. The comparison leads to \\npractical insights into the nature of data \\nscience and data science problem-solving. \\nFirst consider a fundamental difference. In \\nscientific problem-solving, analysis and \\nresulting discoveries are made on the \\ndomain side directly on phenomena, aided \\nby the analytical side on data representing \\nthe phenomena (Fig. 5). Data science is the \\nreverse. Analysis and discoveries are made \\non the analytical side directly on data  that \\n9\\nrepresents the phenomena. \\nThe complex data science problem-solving \\nparadigm is described below. First data \\nscience problem-solving terminology is \\nintroduced, followed by a high-level \\ndescription of the paradigm. Then the role \\nof each data science problem-solving \\ncomponent is explained, followed in §4.2, \\nby their use in the data science problem-\\nsolving workflow. \\n4.1.\\nData science problem-solving \\nparadigm \\nData science terminology The data science \\np r o b l e m - s o l v i n g p a r a d i g m i s a \\nExplanation - Interpretation\\nExplanation - Interpretation\\nExplanation\\nInterpretation\\nResult\\nProblem\\nSolution\\nModel\\nResult\\nProblem\\nSolution\\nModel\\nScientfic\\nAnalytical\\nKnowledge discovery \\nparadigm\\nProblem-solving  \\nparadigm\\nScientific side\\n· Real Elementary particle\\n· Dicovery means: LHC\\nLHC configured for \\nStandard model of \\nparticle physics\\nHiggs boson \\nhypotheses (energy)\\nfor LHC\\nLHC configured for Higgs\\nboson hyptheses\\nLHC detected hypothesized \\nenergies\\nInterpretation: \\nHiggs boson exists\\nAnalytical side\\n· Elementary particle math model\\n· Dicovery means: simulation\\nSimulation configured for \\nStandard model of particle \\nphysics\\nHiggs boson \\nhypotheses (energy)\\nfor simulation\\nSimulation + Higgs\\nboson hyptheses\\nSimulation detected \\nhypothesized energies\\nInterpretation: \\nHiggs boson exists\\n Data empiricism – learning from data – versus physical empiricism – learning from phenomena.\\n9\\n \\n12\\nspecialization of the conventional problem-\\nsolving paradigm (Fig. 1). Fig. 6 illustrates a \\ntypical application of the data science \\nproblem solving paradigm in which only the \\ndomain problem is known. Light blue \\nindicates problem-solving components that \\nare often unknown. Mapping analytical \\nterminology to data science terminology  \\n10\\nrequires explanation. \\n \\nFigure 5: Scientific problem-solving on \\nphenomena. \\nThe conventional problem-solving \\nanalytical model and problem correspond \\nto the untrained model and training \\nmethod, respectively, in data science \\nproblem-solving. A data science problem is \\ndefined by an untrained model, possibly \\nwith model parameters, e.g., neural net \\ndepth and width. An untrained model is a \\nparameterized AI-based data science \\nmethod that implements a class of \\nanalysis , i.e., a class of data science \\n11\\nproblems. The data science problem is \\nfurther defined by training the untrained \\nmodel using a training method, possibly \\nparameterized by its data science problem \\nparameters, applied to training data. A \\nresult specification guides the following \\nsteps by qualifying the desired data science \\nproblem result. \\nOften, an untrained model is trained using \\nthe training method and training data that \\nis derived from the domain problem and its \\nparameters, intuitively guided by the \\ndomain result specification. The resulting \\ntrained model with given data science \\nproblem parameter values corresponds to \\nthe conventional analytical solution for the \\nparameterized class of data science \\nproblems. The trained model can be \\napplied to a specific data science problem \\ninstance defined by its data science \\nproblem parameter values and inference \\ndata. This produces a trained model result \\nthat must be evaluated to determine the \\ndata science problem result that should \\nsatisfy the result specification. This \\nterminology emphasizes the unique nature \\nand philosophy of data science problem-\\nsolving, distinct from those of conventional \\nand scientific problem-solving, further \\ndescribed in §5. \\nData science problem-solving summary \\nThe purpose of data science problem-\\nsolving for an instance of a domain \\nproblem parameterized by its phenomenon \\nparameters is to produce a domain \\nproblem result that, to be safe to be \\napplied in practice, satisfies its result \\nspecification. The domain problem instance \\nis solved by expressing and solving it as a \\nExplanation - \\nInterpretation\\nExplanation - \\nInterpretation\\nExplanation\\nInterpretation\\nResult\\nProblem\\nSolution\\nModel\\nResult\\nProblem\\nSolution\\nModel\\nScientfic\\nproblem-solving\\nAnalytical\\nproblem-solving\\nKnowledge discovery \\nParadigm\\nProblem-solving\\n Paradigm\\ndomain-analysis maps\\nPhenomena\\nData\\n As data science is in its infancy, its concepts and terminology are emerging. To compare data science with \\n10\\nscience, we use the problem-solving paradigm terminology used in this research.\\nThe large and growing number of data science analysis types are expressed (programmed) by humans often \\n11\\nin neural networks including classification, regression, association, anomaly detection, sequence modeling, \\ntime series, recommendations, NLP, image recognition, speech recognition, reinforcement learning, etc.\\n \\n13\\ndata science problem instance on the \\nanalytical side, as follows. \\n \\nFigure 6: Data science problem-solving \\nparadigm. \\nAn untrained model with model \\nparameters is selected and trained using a \\ntraining method applied with data science \\nproblem parameters and training data to \\nproduce a trained model. The trained \\nmodel is applied with data science problem \\nparameter values and inference data that \\nrepresents the problem instance producing \\na trained model result that must be \\ninterpreted to produce a data science \\nproblem result that must satisfy its result \\nspecification. The data science problem \\nresult must be interpreted in terms of the \\ndesired domain problem result on the \\ndomain side by a human-created domain-\\nanalysis map (in red in Fig. 6). Since data \\nscience cannot be used to produce or verify \\neither the domain problem result nor the \\ndomain-analysis map, means outside data \\nscience must be used to ensure that it \\nsatisfies its result specification.  \\nData science problem-solving components \\nA domain problem expresses knowledge to \\nbe discovered or questions to be answered \\nabout instances of a phenomenon. Often, a \\ndomain problem is known, as are \\nphenomenon parameters that characterize \\nthe knowledge being sought. A domain \\nproblem can be used to gain insights into \\nits domain model, i.e., a model of the \\nphenomenon being analyzed, hence a \\ncontext for the domain problem. \\nAlternatively, the domain model of a \\nphenomenon is known of which one or \\nmore domain problems are to be analyzed. \\nA phenomenon can be analyzed from many \\nperspectives, each expressed as a distinct \\ndomain problem. A result specification is \\nused to characterize the domain problem \\nresult to aid in defining and verifying a \\ndomain problem result and the data \\nscience problem-solving components used \\nto produce it.  \\nThe domain model, domain problem, \\nphenomenon parameters, and the result \\nspecification are used to gain insights into \\ncreating or selecting an untrained model, \\nits training method using or resulting in \\nhuman created domain-analysis maps, i.e., \\nwhat type of analysis will solve the \\nproblem? An untrained model is selected \\nfrom a library or implemented using an \\nuntrained AI-based data science method, \\noften in a neural network, that conducts \\nthe intended analysis type to solve a data \\nscience problem corresponding to the \\ndomain problem. The training method and \\nother components – domain problem and \\nits phenomenon parameters, domain \\nmodel, untrained model, and any domain-\\nanalysis maps – are used to define training \\ndata  and inference data including their \\n12\\ndata quality requirements and acquisition \\nmethods. The untrained model is trained \\nusing the training method applied to \\ntraining data to produce a trained model \\nwith specifications for inference data and \\n This valuable insight leads to means for defining and acquiring the most critical element, high quality data \\n12\\nand its defining requirements and embeddings. Not all training methods require training data.\\n \\n14\\nfor data science problem parameter values, \\ni.e., parameters derived from the domain \\nproblem and data science model tuning \\nhyperparameters. The trained model is \\ntuned, using data science problem \\nparameter values, i.e., hyperparameter \\nvalues, and is tested over critical \\nrequirements using test inference data. As \\nmost trained AI-based models cannot be \\nvalidated to meet specific requirements, \\ne.g., accuracy and safety, guardrails  are \\n13\\noften required to limit its behavior, i.e., \\ntrained model result, within bounds \\ndefined by a result specification. The \\nresulting tested, trained model can be \\napplied to a data science problem instance \\ndefined by its data science problem \\nparameter values and inference data. This \\nproduces a trained model result that is \\ninterpreted as a data science problem \\nresult for the data science problem \\ninstance. The result specification is used to \\nensure that the data science problem result \\nis within bounds. The motivating domain \\nproblem instance is solved only when 1) \\nthe data science problem result has been \\ninterpreted in terms of the desired domain \\nproblem result, with a plausible domain-\\nanalysis map , and 2) means outside of \\n14\\ndata science have been used to verify that \\nthe domain problem result satisfies its \\nresult specification. A simple domain-\\nanalysis map example is in convolutional \\nneural network image recognition in which \\nsupervised learning establishes a map \\nbetween generic images, e.g., of bananas, \\nand the label “banana”. The trained model \\nknows nothing of such interpretations, e.g., \\nof bananas nor of English terms used to \\nlabel bananas; it programmatically maps \\nthe label “banana” to images that it \\nrecognized as bananas. \\nData science problem-solving requires that \\nall analytical problem-solving components \\n(Fig. 6 right side) be completed. Often, the \\ndomain problem is the only domain side \\nproblem-solving component that is known. \\nExploratory data science starts without a \\ndomain problem that is discovered in data \\nthat represents a phenomenon. A domain \\nproblem can be known without knowing its \\ndomain model since the purpose of a \\ndomain problem is to discover the nature \\nof the phenomenon being analyzed. While \\nit seems odd to solve a problem without \\nunderstanding its context, data science \\nenables such exploration due to its scope, \\nscale, complexity, and power to learn from \\ndata [3][4]. Data science enables such \\npreviously impossible knowledge discovery. \\nAll data science workflow phases and steps \\nare aided using domain-analysis maps and \\nderivation relationships. Due to the \\ninscrutability of AI-based data science, they \\ntoo are inscrutable. Attempting to define \\nthem can provide valuable insights into \\nunderstanding, developing, and verifying \\nrelated components. If the results are to be \\napplied in practice, two domain-analysis \\nmaps are critical and necessarily require \\nusing means outside data science to \\nestablish and verify them. First, to apply a \\ndomain problem result in practice, it must \\nsatisfy the domain problem result \\nspecification; insights for this can be gained \\nby ensuring that the corresponding data \\nscience problem result satisfies the data \\nscience result specification. Second, it is \\n Guardrails are empirically discovered and evaluated constraints on trained models intended to bound \\n13\\nbehavior, but without certainty. There is considerable research in developing these practical necessities. \\n The domain-analysis maps back to the domain side are called explanation for the trained model result and \\n14\\ninterpretation for the data science problem result. They map or de-embed data science data to domain data.\\n \\n15\\nessential to establish a correspondence \\nbetween the motivating domain problem \\nand the training method to verify or \\ndemonstrate, if only intuitively, that the \\ndata science problem being addressed \\ncorresponds to the motivating domain \\nproblem, i.e., was the analysis conducted of \\nthe intended type? Often, the domain \\nsolution is unknown. If a domain problem \\nresult can be demonstrated outside of data \\nscience to satisfy its result specification, a \\ndomain solution is not required, and may \\nbe beyond human understanding in scale \\nand complexity, as in AlphaGo and \\nAlphaFold [19]. \\n4.2.\\nThe data science problem-solving \\nworkflow \\nAs with all problem-solving, data science \\nproblem-solving has myriad workflows \\ndepending on what problem-solving \\ncomponents are known. The following is a \\ntypical data science problem-solving \\nworkflow (Fig. 7) corresponding to the \\ntypical data science problem in Fig 6. Many \\nmore can be deduced from insights in §5. \\nData science problem-solving is conducted \\nfollowing the data science method in the \\ntwo phase data problem-solving workflow \\n(Fig. 7) using data science problem-solving \\nparadigm components (Fig. 6) as follows \\nand demonstrated with AlphaFold in §4.3. \\nThe Discover phase consists of four steps. In \\nthe Define step, a motivating domain \\nproblem with its domain problem \\nparameters is defined, i.e., a specific \\nquestion or analysis of a phenomenon is \\ndefined, within a relevant domain model. \\nAs explained above, a domain model of the \\nphenomenon being analyzed may not be \\nadequately known as the domain problem \\nmay be learned from data. \\n \\nFigure 7: Data science problem-solving \\nworkflow \\nThe Define step provides insights into the \\nDesign step that is used to design or select \\nfrom a library an untrained model, i.e., a \\ndata science analytical method that \\nimplements the class of analysis required to \\nanswer the domain problem, together with \\nits training method. The training method \\nand other components – domain problem \\nand its phenomenon parameters, domain \\nmodel, untrained model, and any domain-\\nanalysis maps – are used to define training \\ndata and inference data including their data \\nquality requirements, acquisition method, \\nand embeddings. The untrained model is \\ntrained by applying the training method to \\nthe training data producing a trained model \\nwith specifications for its data science \\nproblem parameters and inference data \\nthat may be derived from knowledge of the \\nuntrained model, training method, and \\ntraining data and insights from the domain \\nside. The resulting trained model is tuned \\nby adjusting the data science parameter \\nvalues, i.e., hyperparameters, and testing \\nthe trained model with test inference data. \\nGuardrails are developed to attempt, \\nwithout guarantee, to limit the trained \\nmodel results, i.e., the behavior of the \\ntrained model, to satisfy the result \\nspecification. \\nIn the Apply step, the trained model is \\napplied to a specific data science problem \\ninstance defined by its inference data and \\ndata science parameter values producing a \\ntrained model result that must be \\n \\n16\\nevaluated to produce a data science \\nproblem result which should satisfy the \\nresult specification. No results – trained \\nmodel result, data science problem result, \\nhence domain problem result, can be relied \\nupon to satisfy result specification when \\napplied in practice even if they do so in \\ndeveloping the results. Result specifications \\nare metrics used to guide, without \\nguarantee, the development of a trained \\nmodel, i.e., data science solution, to be \\napplied to data science problem instances \\nthat are safe to apply in practice. Finally, in \\nthe Analyze step, all problem-solving \\ncomponents and their results are analyzed \\nin preparation for the Interpret phase. \\nDue to the inscrutability of AI-based data \\nscience, there is no theoretical or practical \\nbasis, as there is in science, for the \\nInterpret phase. The four step Interpret \\nphase must be fabricated by humans based \\non intuition, experience, and expertise with \\nempirically developed tools, i.e., guardrails, \\nand methods outside the field of data \\nscience. The Explain & interpret step \\nattempts to 1) explain the analysis \\nconducted by the trained model, ideally but \\nrarely confirming that it corresponded to \\nthe intended domain solution, and 2) \\ninterpret the trained model result in terms \\nof the data science problem result. In \\nsimple cases, the two results are identical. \\nFor complex trained models, such as the \\nAlphaFold [19] ensemble model, the \\ntrained model result must be interpreted in \\nterms of the data science problem result. \\nThe Explain & interpret step interprets the \\ndata science problem result in terms of the \\nmotivating domain problem result. The \\nValidate & verify step can be very complex \\nrequiring considerable human intuition and \\ndomain expertise. The Authorize and \\nCurate steps are like those in scientific \\nproblem-solving. The analysis and its \\nresults are documented and submitted for \\nauthorization, in research to a journal or \\nconference, and in industry to a relevant \\ninstitutional authority. While methods for \\nproving, hence authorizing, scientific \\nresults are well-known, means of \\nconsistently demonstrating, not even \\nproving, properties of data science results \\nare just emerging. \\nIn practice, data science problem-solving \\nranges from simple, developed and applied \\nin hours, to complex taking a decade to \\ndevelop, as in the case of AlphaFold, \\ndescribed next. Simple cases involve \\nexisting trained models in AI applications, \\ne.g., OpenAI’s ChatGPT, simply require \\ninput prompts and parameters without \\nmodifying the underlying trained model. \\nHowever, trained models reflect only the \\nknowledge on which they were trained, \\nhence do not reflect subsequently \\ndeveloped knowledge. Existing trained \\nmodels, especially Foundation models, can \\nbe updated or refined by repeating the \\nDiscover phase. \\n4.3.\\nData science problem solving \\nexample: AlphaFold \\nThe above data science problem-solving \\nparadigm and workflow descriptions are for \\nsimple data science problems with a single \\ndata science analytical method and model. \\nData science problems and solutions can be \\nconsiderably more complex, requiring many \\ntypes of expertise and models and \\nconsiderable time to develop and train. For \\nexample, consider DeepMind’s AlphaFold  \\n15\\n[19], one of the most successful and \\ncomplex AI-based data science solutions \\n Each of DeepMind’s series of Alpha models was an unprecedented milestone in the application of AI-based \\n15\\ndata science to address complex, previously unsolved problems, each surpassing the former.\\n \\n17\\n(illustrated in Fig. 8, light blue indicates \\ncomponents that are mostly unknown). \\n \\nFigure 8: Data science problem-solving for \\nAlphaFold \\nAlphaFold is a revolutionary technology and \\nbreakthrough success that accurately solves \\na 50 year old microbiology grand challenge \\n– predicting the 3D structure of a molecule \\ngiven its amino acid sequence. Domain side \\nproblem-solving components are at best \\npartially known, e.g., there may not be a \\nscientifically verified domain model, i.e., \\nthe conceptual model for the 3D structure \\nof all molecules. Prior to AlphaFold, over 30 \\nyears of experimentation produced costly, \\nimprecise computational analytical models \\nand solutions that could take up to ten \\nyears for one molecule. The partially \\nunderstood domain model and well-\\nunderstood domain problem may have \\nprovided insights to develop AlphaFold \\nmodel – a complex ensemble of 32 deep \\nlearning models. The domain solution and \\ndomain solution result are shaded light \\nblue in Fig. 8 to suggest that they are not \\nknown. But they are not required as \\nAlphaFold produces the desired domain \\nproblem result directly from the AlphaFold \\ndomain problem result, developed with \\nhuman knowledge and insight. The \\nAlphaFold solution applied with inputs, the \\nAlphaFold parameter values and Amino \\nacid sequence of a molecule, produces the \\nAlphaFold solution result that is then \\ninterpreted to produce the AlphaFold \\nproblem result that is further interpreted \\n(via the red domain-analysis map in Fig. 8) \\nto produce the desired domain problem \\nresult. The AlphaFold solution and result, \\nand interpretation domain-analysis map are \\ninscrutable within data science, i.e., lack \\nfull explanations and robust interpretations \\n– neither understood nor provable. Their \\nvalidity is established with means outside \\nof data science such as human knowledge, \\nexpertise, experience, and scientific \\nknowledge, e.g., cryo-electron microscopy. \\nAlphaFold illustrates the nature, \\ncomplexity, and scale of problems that can \\nbe successfully addressed with AI-based \\ndata science. While the exact size and \\ncomposition of the AlphaFold team and \\ntheir expertise is not publicly available, it \\nhas been estimated [39] to be 50-100 \\nscientists with world-class expertise in \\nprotein structure biology, machine learning, \\nco mp u tational b i ol ogy, software \\nengineering, structural bioinformatics, \\nbiophysics, and mathematics. \\n5.\\nScientific versus data science problem-\\nsolving \\nThis section summarizes critical differences \\nbetween science and data science, \\nintroduced above and in [3][4]. The \\nscientific and data science problem-solving \\nparadigms are specializations of the \\nconventional problem-solving paradigm; \\nyet they differ fundamentally in nature and \\ndetail. Hence, the conventional problem-\\nsolving paradigm is a framework with which \\nto compare the two paradigms, to \\nunderstand their similarities and \\ndifferences, and to gain insights into data \\nscience and data science problem-solving. \\nScientific and data science problem-solving \\ntake different paths to fundamentally \\ndifferent solutions. This and subsequent \\nDomain\\nsolution \\nresult\\nDomain\\nProblem\\nDomain\\nSolution\\nDomain\\nModel\\nPhenomenon   \\nParameters\\nResult \\nSpecification\\nDomain Problem \\nResult\\nAlphaFold\\nsolution\\nresult\\nTraining \\nmethod\\nTrained\\nmodel\\nAlphaFold\\nModel\\nAlphaFold\\nTraining Data\\nAlphaFold  \\nProblem Parameters\\nAmino acid\\nsequence of M\\nAlphaFold\\n Parameter Values\\nDomain-analysis map\\nDomain-analysis map\\nExplanation\\nInterpretation\\nAlphaFold\\n problem result\\nAlphaFold\\nResult Specification\\nEnsemble of 32 ML models used\\nto represent molecule (protein) M\\nPredict the 3-D structure of a \\nmolecule M given its amino acid \\nsequence\\nCharacteristics of the 3-D \\nstructure of molecule M given its \\namino acid sequence\\nBeyond human \\nunderstanding\\n3-D structure of \\nmolecule M\\nConceptual model of 3-D \\nstructure of a molecule based on \\nthe Central dogma of \\nmicrobiology\\nAlphaFold (molecule M parameters, \\namino acid sequence)\\nAlphaFold results\\n \\n18\\nitalicized speculations relate to Stephen \\nHawking’s final theory [13]. \\n5.1.\\nResults in science are universal, in \\ndata science local and specific \\nA fundamental difference between \\nscientific and data science problem-solving, \\nstated simply, is that while both are \\nevaluated on specific instances of a \\nphenomenon, scientific results are claimed \\nto be certain and apply universally to all \\nsuch phenomena under the same empirical \\nconditions; data science results are \\ninterpreted as uncertain insights into an \\ninstance or class of instances of a \\nphenomenon. \\nFollowing the scientific method, a scientific \\nsolution (experiment) is successful if it is \\nbased on verified scientific knowledge, the \\nexperiment and results are reproducible, all \\nof which has been documented, verified, \\nand authorized by the relevant scientific \\ncommunity. The resulting scientific \\nknowledge is fully understood and is \\nconsidered, by scientific induction , to be \\n16\\nuniversally true under the empirical \\nconditions, thus definitive. Such \\nexperiments are conducted twice – once to \\nestablish the scientific problem result (i.e., \\nscientific knowledge) and once to prove \\nrepeatability; after which the experiment \\nneed never be repeated. \\nFollowing the data science method, a \\nsuccessful data science solution (trained \\nmodel) is applied to a specific instance, or \\nclass of instances, of a data science \\nproblem defined by its inference data and \\ndata science problem parameter values. \\nData science is in its infancy and is evolving \\nrapidly, hence, the following observations \\nmay also evolve; just as science did when it \\nemerged centuries ago and does to this \\nday. First, AI-based data science solutions \\n(trained models) are inscrutable and \\nproduce results that are uncertain, i.e., not \\ndefinitive, and cannot be proven true \\nwithin data science. At best, data science \\nproblem results are local, specific to a \\ntrained model instance, its untrained \\nmodel, training method and training data, \\nand to the data science problem instance, \\ndefined by its data science problem \\nparameter values and inference data. \\nUnlike a universal scientific solution, a data \\nscience solution is intended to be applied \\nrepeatedly, each time to a different data \\nscience problem instance. As a data science \\nsolution and its data science problem \\nparameter values and inference data are \\ndeveloped using a limited training method \\nand limited training data, it is common \\npractice for each problem instance to \\nupdate the solution with additional training \\nand tuning. Also, in contrast with universal \\nscientific solutions, data science solutions \\nare not only local and specific to their \\ndevelopment, they are always imperfect, \\nhence subject to revision to improve the \\ndata science problem result for a \\nmotivating domain problem and domain \\nproblem instance. This leads to an \\ninteresting tradeoff. Like most inscrutable \\nproperties of data science solutions, their \\nlocality versus universality and their quality \\ncan often be improved. In contrast with \\nscientific results that are definitive; data \\nscience results need only be good enough, \\ni.e., satisfy the result specification for the \\nmotivating domain problem. At the same \\ntime, the unbounded potential of data \\nscience solutions for reuse and revision \\nreflect the corresponding unfathomed \\nscope and power of data science problem-\\nsolving. In contrast with the scientific \\nmethod, the data science method supports \\na range of solutions for the same \\n Scientific induction has been disputed since 1748 by Hume [16][17]\\n16\\n \\n19\\nmotivating domain problem that can vary \\ndepending on their development – purpose \\nof the motivating domain problem, training \\nmethod, training data, parameters, and \\ninference data. Local data science results \\nmay be more realistic hence more \\ninformative and valuable than alleged \\nuniversal scientific knowledge [4]. \\n5.2.\\nProblems and solutions in science \\nare human-scale, in data science \\nunfathomed \\nFor conceptual and practical reasons, \\nscientific models and results are at human \\nscale, e.g., fully understandable by a \\nscientist aided by models, e.g., natural \\nlanguage, mathematical, or simulation \\nmodels. Following the scientific method, an \\nexperiment must evaluate all combinations \\nof the factors that potentially cause a \\nhypothesized effect. To fully evaluate N \\npotentially casual factors requires that all \\ncombinations of those factors, i.e., 2N \\ncases, be evaluated. For practical reasons, \\nmost experiments explore less than 10 \\nfactors requiring 1,024 cases and more \\ntypically 4 factors requiring 16 cases. In \\ncontrast, data science problems and results \\nand their corresponding domain problem \\nresults can vastly exceed human scale, e.g., \\nNvidia’s LLM Megatron-Turing NLG 2.0 has \\n1 trillion parameters. The scale, i.e., \\ncomplexity, of problems and solutions that \\ncan be addressed by AI-based data science \\nis unfathomed. Data science offers insights \\ninto instances of complex problems not \\npreviously nor otherwise achievable. \\n5.3.\\nSolutions are complete in science, \\nincomplete in data science \\nScientific problem-solving is complete with \\nall scientific and analytical problem-solving \\ncomponents fully understood, as are \\ndomain-analysis maps used to prove \\nequivalence and derivation relationships \\nused to derive components. Similarly, all \\nsteps of the Discover and Interpret phases \\nof the scientific problem-solving workflow \\nare known with proven results. Hence, \\nscientific knowledge discovery is fully \\nunderstood with the successful application \\nof the scientific method producing proven, \\ni.e., certain, scientific knowledge. \\nIn contrast, data science problem-solving is \\nincomplete with problem solving \\ncomponents unknown to varying degrees \\n(light blue shading in Fig. 6-9). Consider the \\ndomain side. Often, the domain problem is \\nknown, but may have resulted from a \\nprevious data analysis. A domain solution is \\nrarely known and may be beyond human \\nunderstanding; however, it is not needed if \\nan adequate domain problem result has \\nbeen established. While domain-analysis \\nmaps and derivation relationships are \\ninscrutable, intuiting such maps aids \\ndefining analytical components. A domain \\nproblem can suggest the class of analysis to \\nbe conducted, hence an appropriate \\nuntrained model with its training method \\nand training data. With so little known on \\nthe domain side, analytical components \\nmust be developed with little guidance, \\nsave intuition, experience, and knowledge, \\ne.g., pre-trained models such as Foundation \\nmodels. In contrast with science’s tractable \\nInterpret phase, defined by the scientific \\nmethod, the data science method lacks a \\ntractable Interpret phase. It must be \\nfabricated based on intuition, knowledge, \\nand expertise with empirically developed \\ntools, i.e., guardrails, and methods outside \\ndata science as no theoretically-based \\nmethods exist. \\n5.4.\\nKnowledge discovered in science is \\ncertain, in data science is uncertain. \\nScientific results are authorized as true by \\nbeing accepted by the relevant expert \\ncommunity. Scientific approvals are based \\non centuries of applying the scientific \\n \\n20\\nmethod resulting in the certainty of \\nscientific knowledge, hence the certainty of \\nour knowledge of the natural world. This \\ncontrasts with the uncertainty of data \\nscience solutions and results. “Knowledge” \\ndiscovered by data science is inscrutable, \\nprobabilistic, possibly erroneous, lacking \\ntheoretical or practical means for \\nexplanations and interpretations. To be \\napplied in practice, means outside data \\nscience, especially empirically developed \\nguardrails, are used to limit resulting \\ndomain problem results within previously \\nspecified bounds. However, as quantum \\nmechanics demonstrated 100 years ago , \\n17\\nreality may not be as certain as assumed in \\nscience but probabilistically uncertain as in \\ndata science. The uncertainty and \\nincompleteness in the emerging philosophy \\nof data science may be more realistic than \\ncertainty and completeness in science, \\npossibly leading to transforming and \\nredefining science [4]. \\n5.5.\\nScience analyzes real things; data \\nscience real and abstract things \\nIn scientific problem-solving, discoveries \\nare made on the scientific (domain) side \\naided by analysis on the analytical side (Fig. \\n9, 10). Following the scientific method, \\nphenomena are central as knowledge is \\ndiscovered directly from phenomena \\ninstances by well-understood physical \\nempiricism and scientific theories, \\nproducing certain scientific results. In \\nscience, data on the analytical side, is \\nsecondary, used to develop an analytical \\nsolution and result to guide or to confirm \\nexperiments via domain-analysis maps. The \\nscope of science is limited to real, \\nmeasurable phenomena at human scale. \\nData science problem-solving is the \\nreverse. Discoveries are made (learned) \\nfrom data on the analytical side possibly \\naided by intuition for interpretations from \\nthe domain side. Data is central since for \\nknowledge to be discovered, it must be in \\nthe data. The scope of data science is any \\nreal, abstract, or imaginary, phenomenon \\nfor which there is adequate data, \\npotentially at scale beyond human \\nunderstanding. Hence, data science is data-\\ncentric  (data intensive, data-driven). \\n18\\nDiscoveries are made directly from data \\nfollowing the data science method by \\ninscrutable data empiricism with uncertain \\nresults. AI-based data science analytical \\nmethods are secondary as there are many \\nwith which to discover knowledge in data. \\nUnlike 20th C data that are assets, 21st C \\ndata science data is phenomenological – a \\nresource in which to discover phenomena \\nand their properties, previously and \\notherwise impossible [5]. \\n \\nFigure 9: Scientific versus data science \\nproblem-solving components. \\n5.6.\\nResults derived by humans in \\nscience, by computation in data \\nscience \\nScientific problem-solving is complete with \\nall problem solving components – models, \\n Einstein, by his own admission, was not comfortable with, educated or practiced in reasoning in uncertainty \\n17\\nwith uncertain, probabilistic, ambiguous outcomes, despite increasing evidence in modern physics.\\n Due to the power of AI-based data science methods, data science was initially considered model-centric.\\n18\\n \\n21\\nproblems, solutions, and results, both \\nscientific and analytic, domain-analysis \\nmaps, and derivations – developed, fully \\nunderstood, and proven by humans. In data \\nscience, only the untrained model problem-\\nsolving component, often expressed as a \\nneural network, is developed and largely \\nunderstood by humans. Even the domain \\nproblem may be initially unknown, \\ndiscovered computationally. All other \\nproblem-solving components – training \\nmethod, trained model, trained model \\nresult, data science problem result and \\neven the domain problem result are \\ndeveloped computationally and are \\ninscrutable. Consider the nature of what is \\ndeveloped computationally. An AI-based \\nanalytical model – an untrained model – is \\noften designed by humans in a neural \\nnetwork hence is largely understood. Yet, \\nthe “knowledge” that it contains, i.e., the \\nclass of analysis to be conducted such as \\nimage recognition, can be inscrutable. A \\ntraining method is used to train an \\nuntrained model with training data that \\nspecializes the initially human-defined data \\nscience analysis, i.e., untrained model, to \\nsolve the problem in specific instances, e.g., \\nwhat properties uniquely identify pizzas \\nand teapots, to produce a trained model. It \\ncan computationally establish a domain-\\nanalysis map, e.g., properties identifiable \\nby the trained model of all images of pizzas \\nin inference data with those properties as a \\n“pizza”. How a training method trains an \\nuntrained model, e.g., what properties \\ndefine a pizza or a teapot, or how an \\nuntrained model learns from the training is \\ninscrutable. Thus, as if by magic, a trained \\nmodel with data science problem \\nparameters values (e.g., hyperparameters) \\nis applied to previously unseen inference \\ndata, e.g., a pizza image, and solves that \\nproblem instance producing a trained \\nmodel result, i.e., it has the properties of a \\npizza that is (in this case trivially) \\ninterpreted as the data science problem \\nresult that it interprets by applying the \\ndomain-analysis map to produce the \\ndesired domain problem result, i.e., “It is an \\nimage of a pizza”. The training, learning, \\ninference is at the heart of the powerful \\nbut inscrutable data science knowledge \\ndiscovery paradigm. \\n \\nFigure 10: Scientific versus data science \\nproblem-solving nature. \\n5.7.\\nValue: understood, limited in \\nscience; inscrutable, unbounded in \\ndata science \\nConsider the above fascinating differences \\nfurther. The value of science, for centuries \\nour most powerful knowledge discovery \\nparadigm, is as a method of discovering \\nscientific knowledge of natural phenomena \\nof our universe. Science and the resulting \\nscientific knowledge are fully understood \\nbut limited to measurable real phenomena. \\nThe value of data science will emerge as an \\neven more powerful knowledge discovery \\nparadigm, despite its inscrutability. Data is \\nemerging as a source of knowledge never \\nbefore imagined, yet to be understood [5]. \\nConsider the inscrutability of data science \\nmethods and data as compelling mysteries \\nat the heart of AI-based data science. The \\nextent to which a trained model, i.e., a data \\n \\n22\\nscience solution, can discover valuable \\npatterns of values of properties of a \\nphenomenon, i.e., a desired data science \\nsolution result, in previously unseen \\ninference data depends on at least three \\nthings. First and foremost, the pattern must \\nbe in the inference data. This could be \\nverified outside data science by a costly, \\nexhaustive analysis. Second, the untrained \\nmodel, i.e., analytical method, underlying \\nthe trained model must be capable of \\ndiscovering the pattern. This cannot be \\nverified and is the subject of exhaustive \\nempirical testing at great expense. Third, \\nthe ability to discover the pattern must \\nhave been learned from the training data, \\ni.e., must have been present in the training \\ndata sufficiently for the pattern to be \\nincorporated in the trained model. This too \\ncannot be proven and is being explored \\nempirically and otherwise. While this \\ninscrutability poses significant challenges \\nfor validating data science problem-solving, \\nunderstanding them may lead to solution \\ninsights and opportunities summarized in \\n§6. Finally, the power of data science \\ntransforms the role, e.g., conception, of \\ndata from 17th C objects used to record \\nfacts and 20th C assets used to manage \\ninformation [21][22] to phenomenological \\nin the 21st C – a source for knowledge \\ndiscovery of any phenomena for which \\nthere is adequate data [5]. \\n6.\\nInsights from data science problem-\\nsolving scenarios \\nDespite the framework being graphically \\nsimple, it can be used to understand the \\ninherently complex data science problem-\\nsolving paradigm and solutions. Each \\ncomponent (bubble, rectangle) and arrow \\n(domain-analysis map, derivation) has a \\nsignificant meaning and role in data science \\nproblem-solving. These aid understanding \\nas each component can provide insights \\ninto data-science problem-solving \\nproblems, solutions, and results. Due to the \\nimmaturity and rapid development of the \\ndata science problem-solving paradigm, \\ninsights are required to understand, \\ndevelop, produce, and verify prospective \\nmodels, problems, solutions, results, \\ndomain-analysis maps, derivations, \\nworkflows, and the continuous process of \\ntesting and refining them and interpreting \\nthem in terms of motivating domain \\nproblems and desired domain problem \\nresults. \\nThis section summaries such insights \\nintroduced in earlier sections. Most such \\ninsights come from the subjects of analyses \\n– real or abstract phenomena on the \\ndomain side – into the inscrutable data \\nscience solutions and results on the \\nanalytical side, and vice versa. \\n6.1.\\nDomain Model \\nDomain problems can be understood as \\nhypotheses within a domain model or \\ncontext. For a given domain problem, the \\ndomain model may be known, or unknown \\nbut intuited. It may provide intuition for, or \\ninsights into, the domain problem, and, \\nwith similarly human-intuited domain-\\nanalysis maps, one or more untrained \\nmodels, i.e., analytical categories. For \\nexample, a domain problem concerning a \\ndomain model, i.e., of a phenomenon, \\ncould be analyzed in multiple ways, i.e., as \\nmultiple data science problems, each with \\nits own training method, parameters, \\ntraining data, and result specification. A \\ndomain model may be simple, representing \\na single, simple phenomenon, or could be \\ncomplex, composed of many phenomena, \\ne.g., an amino acid sequence of a protein. A \\ndomain model should provide intuitive \\ninsights into the domain problem, and \\ndomain problem result and vice versa. The \\nresult of such analyses may be a \\ncombination of the solutions as in an \\n \\n23\\nensemble model. A domain problem is \\noften solved using data science problem-\\nsolving without fully defining the domain \\nmodel or context. Due to the potentially \\ncomplex nature of data science trained \\nmodels and problem results, and the \\ncorresponding domain problem result, \\nthere may not have been an initial domain \\nmodel. A common use of data science \\nproblem solving is to learn from and \\ndiscover in data, properties of phenomena. \\nMultiple such discoveries can contribute to \\ndefining or elaborating a domain model. \\n6.2.\\nD o m a i n m o d e l , p r o b l e m , \\nphenomenon parameters, and result \\nspecification \\nData science problems often start with a \\ndomain problem to be solved with an \\nintuitive notion of a result specification and \\nof the phenomenon parameters that \\ncharacterize the knowledge to be \\ndiscovered, i.e., a domain problem result – \\nthe properties, or patterns of properties of \\nthe phenomenon to be analyzed. Data \\nscience problem-solving is often used when \\nthere is no domain solution  with which to \\n19\\ndirectly produce a domain problem result, \\nbut there is adequate data representing \\ninstances of the properties of the \\nphenomenon to be analyzed and one or \\nmore untrained models, i.e., AI-based data \\nscience analytical methods, to produce the \\ndesired domain problem result or provide \\ninsights into finding such a result.  \\nDomain problems can range from being \\nwell-defined, to ill-defined, to unknown. \\nThe extent to which a domain problem is \\ndefined provides intuition for or insights \\ninto the following, each aided by an \\nintuitive but inscrutable domain-analysis \\nmap or derivation relationship. All problem-\\nsolving components are selected by \\nhumans based on intuition  – possibly \\n20\\ngained from domain side components – \\nknowledge, experience, and previously \\ndeveloped solutions.  \\n•\\nIf the domain problem is unknown, as \\nin exploratory AI, a human can explore \\nan untrained or trained model to \\nanalyze candidate inference data to find \\npatterns to be used as hypotheses to \\ndefine a domain problem. \\n•\\nDomain model – a human-defined \\ngeneralization of the domain problem, \\ni.e., properties, or patterns of \\nproperties, of the phenomenon to be \\nanalyzed. \\n•\\nUntrained model – an untrained, AI-\\nbased method, selected by a human by \\nintuition, experience or knowledge to \\nconduct the intended category of \\nanalysis specialized by its training \\nmethod, data science problem \\nparameters, and training data that is \\nrequired to discover the desired domain \\nproblem result for a domain problem \\ninstance that satisfies its result \\nspecification. \\n•\\nTraining method – an AI-based method \\nfor using training data to train an \\n This is an example of data science thinking – producing a possibly uncertain domain problem result from \\n19\\ndata in the absence of a domain solution. In scientific thinking, proven scientific knowledge (results) are \\nestablished by means of a verified scientific solution.\\n AI-based data science has captured the world’s attention due to its problem-solving scope, scale, complexity, \\n20\\nand power previously impossible with human coded computations. Yet it is inscrutable. The Holy Grail of AI-\\nbased data science is to explain how it works and to interpret results in terms of the motivating domain \\nproblem. While this is the case, possibly forever, human intuition, guidance, and reasoning are critical for AI-\\nbased data science solutions to be safe and good enough in practice.\\n \\n24\\nuntrained model to discover an desired \\ndomain problem result in previously \\nunseen inference data. \\n•\\nTraining data – data that represents \\nknowledge of the properties, or of \\npatterns of properties of instances of \\nthe phenomenon to be discovered by \\nthe trained model in previously unseen \\ninference data. \\n•\\nResult specification – a characterization \\nof the intended data science problem \\nresult that must be interpreted in terms \\nof the domain problem result that must \\nsatisfy the domain problem result \\nspecification. \\n•\\nTrained model – a trained AI-based \\nmethod parameterized by data science \\nproblem parameter values that \\ntogether with the inference data define \\nthe instances of the phenomenon to be \\nanalyzed. In rare cases, human intuition \\nmight be used to develop a domain-\\nanalysis map to an intuited domain \\nsolution. \\n•\\nTrained model result – the result of \\napplying the trained model with specific \\ndata science problem parameter values \\nto inference data. It may be possible to \\nintuitively verify the correctness of the \\ncomputational result of the trained \\nmodel application. \\n•\\nData science problem result – the \\ninterpretation of the trained model \\nresult in terms of the data science \\nproblem. Considering the data science \\nproblem as a hypothesis, does the \\ntrained model result confirm or deny \\nthat hypothesis? \\n•\\nDomain problem result – the desired \\nresult for a specific instance of the \\ndomain problem. It should meet its \\ndomain problem result specifications. It \\nmust be interpreted entirely by human \\nintuition, experience, and knowledge as \\nthere are no means within data science \\nto do so. This critical interpretation is \\nmade primarily based on the data \\nscience problem result that in turn is \\nbased on the data science problem \\nsolving workflow steps and components \\nthat produced the data science problem \\nresult. \\nFinally, the quality of a data science \\nproblem result and of the desired domain \\nproblem result depends directly on data \\nquality [5], i.e., requirements of data – the \\nmost critical component in data science \\nanalyses. The corresponding most valuable \\ninsights are to be gained from the domain \\nproblem, phenomenon parameters, and \\nresult specification components that \\nconcern the training method, training data, \\nresult specification, and inference data. \\nTraining data determines what the \\nuntrained method will learn. Inference data \\ndetermines what the trained model will \\ninfer, to produce the data science problem \\nresult that leads to the domain problem \\nresult. All domain side components provide \\ninsights into identifying, acquiring, \\nevaluating, and refining the required data. \\n6.3.\\nDomain solution, domain problem \\np a r a m e t e r v a l u e s a n d t h e \\nphenomenon \\nFrequently, a domain solution is unknown \\nor is beyond human understanding in \\ncomplexity. This motivates the use of data \\nscience to find a data science solution, i.e., \\na trained model, to be applied to specific \\ninstances of the domain problem. Trained \\nmodels are inscrutable hence cannot be \\nproven to be correct but may be \\ndemonstrated by means outside of data \\nscience to produce data science and \\ndomain problem results that satisfy data \\nscience and domain result specifications. \\nSuch means are developed intuitively using \\nrelevant expertise, experience, and \\nempiricism as described for AlphaFold in \\n§4.3. Such demonstrations can be \\n \\n25\\ndeveloped only for specific domain \\nproblem instances and corresponding data \\nscience problem instances, not for all such \\ninstances. While a domain problem result \\ndemonstrated to satisfy its result \\nspecification obviates the need for a \\ndomain solution, the contributing problem-\\nsolving components may provide insights \\ninto the motivating domain problem \\ninstance, the domain problem, the domain \\nmodel, and ultimately into the domain \\nsolution. While many models contribute to \\nunderstanding protein folding (hydrophobic \\ncollapse; lattice, framework, and coarse-\\ngrained models), there is currently no \\ndomain model or solution. Some problems, \\nlike protein folding, may be too complex to \\nadmit of a domain solution for domain \\nproblem instances in contrast with the \\nuniversality of scientific solutions. \\n6.4.\\nTraining and inference data; \\nuntrained models and training \\nmethods \\nThe most important data science problem-\\nsolving components are first, training and \\ninference data, and second, an untrained \\nmodel and its training method. Data is most \\nimportant since for knowledge to be \\ndiscovered, it must be in the data. While \\nthere may be many insights into the nature \\nof the data, e.g., data typically used to \\ndescribe a phenomenon, such conventional \\ndata may exclude knowledge being sought. \\nSolving such problems required data \\nthinking [13]. Consider knowledge of \\nastrophysical phenomena that emerged \\nand possibly vanished since the origins of \\ntime. The James Web Space Telescope \\nrecords data from the beginning of time - \\n13.6 B (1011) years ago. Training and \\ninference data for data science analyses is \\nrepresented with conventional data \\nstructures . What training and inference \\n21\\ndata is required to discover previously \\nunknown astrophysical phenomena? As \\nwith the AlphaFold description in §4.3, \\nthere is no known theory or practice to \\ndefine that data a priori. Are conventional \\ndata structures adequate or do they \\npreclude that very knowledge being \\nsought? Insights must be gained by trial \\nand error based on intuition, experience, \\nand knowledge from many disciplines \\nexploiting the data science problem-solving \\ncomponents as suggested above. \\nAn untrained model and its training \\nmethod are the next most important \\nproblem-solving components. Just as \\ncritical data is discovered by insights \\nthrough trial and error, so too are untrained \\nmodels, i.e., categories of analysis required \\nto discover patterns of those phenomena, \\nand training method and training data \\nrequired to learn the patterns to produce a \\ntrained model to enable such discoveries \\nfor specific instances of those phenomena, \\nrepresented in the previously unseen \\ninference data. Insights will come from trial \\nand error evaluating many untrained \\nmodels, i.e., categories of analysis . Insights \\nmay come from proven data science \\nanalysis results for known phenomena, or \\nmay require novel AI-based analytical \\nmethods, i.e., untrained models. \\nAI-based data science is in its infancy, with \\nits greatest successes based on neural \\nnetworks – the solutions referenced in this \\npaper. Neural networks have limitations \\nand are not universal problem-solving \\narchitectures. Many new problem solving \\narchitectures are emerging thus expanding \\nAI-based data science. “The future of \\nmachine learning architectures promises \\n Infrared electromagnetic radiation (EMR) data stored as common extensible Markup Language (XML) in the \\n21\\ncommand and telemetry database [11].\\n \\n26\\nexciting developments that will lead to \\nmore powerful and versatile AI systems \\ncapable of tackling even more complex \\nchallenges.”  \\n22\\n7.\\nConclusion \\nThis paper provides a framework with \\nwhich to better understand the data \\nscience problem-solving paradigm and \\nworkflow and illustrates many resulting \\ninsights. \\nAs data science is better understood, it will \\nour most powerful source of insights into \\nour inherently uncertain, probabilistic \\nworld. Its dominant contribution may be its \\nscope, scale, complexity, and power for \\ngaining insights into our universe.  \\n8.\\nAppendices  \\n8.1.\\nMany useful definitions. \\nOver 200 years scientists have developed \\nmany useful definitions of the scientific \\nmethod and its expression in scientific \\nproblem-solving paradigms and scientific \\nworkflows each accepted by the relevant \\nscientific communities. No single definition \\nsatisfies all requirements. This paper uses a \\nsimple, incomplete definitions selected to \\nenable comparison with the emerging and \\ninherently complex data science. Similarly, \\nmany definitions of the data science \\nmethod and its expression in data science \\nproblem-solving paradigms and data \\nscience workflows are emerging reflecting \\nthe disciplines and problem classes that \\nthey serve. Due to the rapid development \\nof data science, they will continue to \\nevolve. Those offered here are intended to \\nexplore the inscrutable yet powerful data \\nscience problem-solving paradigm and \\nworkflow at this, the beginning of a \\ndecades-long discovery process. \\n8.2.\\nData science paradigm: learning \\nfrom data \\nThe framework presented here applies to \\nall of data science but addresses AI-based \\ndata science that, due to its inscrutability, \\nposes the greatest challenges. It does not \\naddress conventional data science, as \\ndefined below, excerpted from [3]. \\n“The philosophy of data science is the \\nworldview that provides the philosophical \\nunderpinnings (i.e., learning from data) for \\ndata science research for knowledge \\ndiscovery with which to reason about \\n(understand), discover, articulate, and \\nvalidate insights into the true nature of the \\nultimate questions about a phenomenon by \\ncomputational analyses of a dataset that \\nrepresents features of interest of some \\nsubset of the population of the \\nphenomenon. Data science results are \\nprobabilistic, correlational, possibly fragile \\nor specific to the analysis method or \\ndataset, cannot be proven complete or \\ncorrect, and lack explanations and \\ninterpretations for the motivating domain \\nproblem.” \\nWhile the term data science is new, the \\nfield of data science is as old as \\nmathematics, our most widely used \\nmethod for learning from data. Pre-AI data \\nscience, aka conventional data science, \\nincludes methods such as mathematics, \\nsimulation, databases, data mining, \\nstatistics, probability theory, approximation \\ntheory, and some AI techniques like \\ndecision trees and linear SVMs. While \\nconventional data science methods have \\n Conclusion of a response from Google’s Gemini (2.12.24) to the prompt “Please identify emerging AI-based \\n22\\ndata science problem-solving architectures.” Gemini identified ten classes of such architectures from verified \\npublished research papers.\\n \\n27\\nbeen used for centuries, only now are they \\nrecognized as the only transparent – \\nscrutable – methods and, now, the least \\npowerful. In this well understood category, \\nsolution explanations and results \\ninterpretations, while not inherent, are \\neasier to construct than for AI-based \\nmethods, e.g., weather prediction models \\nare designed, explained, and interpreted by \\nexperts using complex mathematics and \\nsimulations. Conventional methods and \\nmodels are designed by humans to meet \\nspecific requirements hence humans are \\nthe agents of learning. AI-based methods \\nemerged in the 1990s and now dominate \\ndata science. While designed by humans, \\nthey are developed computationally by AI \\nalgorithms such as machine learning (ML), \\nevolutionary, heuristic, and generative \\nalgorithms. AI-based methods are \\ninscrutable lacking solution explanations \\nand results interpretations. In conventional \\ndata science methods, humans are the \\nlearning agents. In AI-based data science \\nmethods, algorithms are the learning \\nagents. That difference alone – human \\nversus algorithmic learning – distinguishes \\nconventional versus AI-based data science. \\nIt also leads to the inscrutable scope, scale, \\ncomplexity and power of AI-based data \\nscience. \\nAcknowledgement \\nI am grateful to Daniel Fischer, Sr. Data \\nScientist in J.P. Morgan’s A.I. Research \\nDivision for valuable contributions to this \\nwork.  \\n9.\\nReferences \\n1. Bento M, Fantini I, Park J, Rittner L, \\nFrayne R. Deep Learning in Large and \\nMulti-Site Structural Brain MR Imaging \\nDatasets. Front Neuroinform. 2022 Jan \\n2 0 ; 1 5 : 8 0 5 6 6 9 . d o i : 1 0 . 3 3 8 9 /\\nfninf.2021.805669. PMID: 35126080; \\nPMCID: PMC8811356. \\n2. Britain’s NHS is trying once again to \\ncollate patients’ data: The project is \\nimperfect and controversial, but the \\ntechnology is needed, The Economist, \\nOct 18th, 2023. \\n3. Brodie, M.L., Defining data science: a \\nnew field of inquiry, arXiv preprint \\nh tt p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 /\\narXiv.2306.16177 Harvard University, \\nJuly 2023. \\n4. Brodie, M.L., A data science axiology: \\nthe nature, value, and risks of data \\nscience, arXiv preprint http://arxiv.org/\\nabs/2307.10460 Harvard University, \\nJuly 2023. \\n5. Brodie, M.L., Re-conceiving data in the \\n21st Century. Work in progress, Harvard \\nUniversity. \\n6. Casey BJ et al. The Adolescent Brain \\nCognitive Development (ABCD) study: \\nimaging acquisition across 21 sites. Dev. \\nCogn. Neurosci. 32, 43–54 (2018). \\n[PubMed: 29567376]  \\n7. Densen P. Challenges and opportunities \\nfacing medical education. Trans Am Clin \\nClimatol Assoc. 2011;122:48-58. PMID: \\n21686208; PMCID: PMC3116346. \\n8. Donoho, David (2017)\\xa050 Years of Data \\nScience,\\xa0 Journal of Computational and \\nGraphicalStatistics,\\xa0 26:4,\\xa0 745-766,\\xa0 DOI:\\xa0\\n10.1080/10618600.2017.1384734 \\n(republished with comments) \\n9. Eubanks, Virginia, 1972-, Automating \\nInequality: How High-tech Tools Profile, \\nPolice, and Punish the Poor. New York, \\nNY, St. Martin\\'s Press, 2018. \\n10. Franks,\\xa0 Bill.\\xa0 97 Things About Ethics \\nEveryone in Data Science Should \\nKnow.\\xa0 United States:\\xa0 O\\'Reilly \\nMedia,\\xa02020. \\n11. Gal-Edd, J. and Fatig, C.C., 2006, March. \\nJames Webb Space Telescope XML \\n \\n28\\ndatabase: from the beginning to today. \\nIn\\xa02006 IEEE Aerospace Conference\\xa0(pp. \\n7-pp). IEEE. \\n12. Hacking, I. (2006).\\xa0 The Emergence of \\nProbability: A Philosophical Study of \\nEarly Ideas about Probability, Induction \\nand Statistical Inference\\xa0 (2nd ed.). \\nCambridge: Cambridge University Press. \\ndoi:10.1017/CBO9780511817557 \\n13. Hazzan, O., Mike, K. (2023). Guide to \\nTe a c h i n g D a t a S c i e n c e : A n \\nInterdisciplinary Approach. Springer \\nInternational Publishing, 2023. https://\\ndoi.org/10.1007/978-3-031- 24758-3_1 \\n14. Hertog, Thomas.\\xa0On the Origin of Time: \\nStephen Hawking\\'s Final Theory. \\nBantam, 2023.  \\n15. Horien C, Noble S, Greene AS, Lee K, \\nBarron DS, Gao S, O\\'Connor D, Salehi M, \\nDadashkarimi J, Shen X, Lake EMR, \\nConstable RT, Scheinost D. A \\nhitchhiker\\'s guide to working with large, \\nopen-source neuroimaging datasets. \\nNat Hum Behav. 2021 Feb;5(2):185-193. \\ndoi: 10.1038/s41562-020-01005-4. \\nEpub 2020 Dec 7. PMID: 33288916; \\nPMCID: PMC7992920. \\n16. How to make Britain’s health service AI-\\nready: The NHS should clean up and \\nopen up its data. Patients will benefit. \\nOct 19th, 2023. \\n17. Hume,\\xa0 David.\\xa0 An Enquiry Concerning \\nHuman Understanding, Oxford: Oxford \\nUniversity Press, 1748 \\n18. Hume,\\xa0 David.\\xa0 An Enquiry Concerning \\nHuman Understanding.\\xa0 United \\nKingdom,\\xa0Oxford University Press,\\xa01999.  \\n19. Jumper, J., Evans, R., Pritzel, A. et al. \\nHighly accurate protein structure \\nprediction with AlphaFold. Nature 596, \\n583–589 (2021). https://doi.org/\\n10.1038/s41586-021-03819-2 Jul 15, \\n2021 \\n20. Kuhn, Thomas S.\\xa0 The Structure of \\nScientific Revolutions. Chicago: \\nUniversity of Chicago Press, 1962. \\n21. Leonelli, S. (2015). What counts as \\nscientific data? A relational framework. \\nPhilosophy of Science, 82(5), 810-821. \\nJanuary 1, 2022, Cambridge University \\nPress, https://doi.org/10.1086/684083 \\n22. Leonelli, S. (2019 a). \"Data — from \\nobjects to assets,\"\\xa0Nature, Nature, vol. \\n574(7778), pages 317-320, October. \\n23. Leonelli, S. (2019a). What distinguishes \\ndata from models? European Journal \\nfor the Philosophy of Science, 9(2), \\nArticle 22. https://doi.org/10.1007/\\ns13194-018-0246-0 \\n24. Leonelli, S. (2019b). “Data Governance \\ni s K e y t o I n t e r p r e t a ti o n : \\nReconceptualizing Data in Data Science” \\nHarvard Data Science Review, 1(1). \\nh tt p s : / / d o i . o r g /\\n10.1162/99608f92.17405bb6  \\n25. Leslie, David. ‘Tackling COVID-19 \\nThrough Responsible AI Innovation: Five \\nSteps in the Right Direction’. Harvard \\nData Science Review, no. Special Issue \\n26. Makmun, Abu Hassan. (2020). On the \\nquality of qualitative research: a simple \\ns e l f - r e m i n d e r . 1 0 . 1 3 1 4 0 /\\nRG.2.2.35384.98565.  \\n27. Makmun, Abu Hassan. (2020). Research \\nparadigm (presentation). 10.13140/\\nRG.2.2.10638.59202. \\n28. Mendling, J., Leopold, H., Meyerhenke, \\nH., & Depaire, B. (2023). Methodology \\nof Algorithm Engineering.\\xa0 ArXiv, abs/\\n2310.18979. \\n29. Miller KL et al. Multimodal population \\nbrain imaging in the UK Biobank \\nprospective epidemiological study. Nat. \\n \\n29\\nNeurosci. 19, 1523–1536 (2016). \\n[PubMed: 27643430]  \\n30. Paunović, K. (2008). Data, Information, \\nKnowledge. In: Kirch, W. (eds) \\nEncyclopedia of Public Health. Springer, \\nD o r d r e c h t . h tt p s : / / d o i . o r g /\\n10.1007/978-1-4020-5614-7_685  \\n31. Pearl, Judea, and Dana Mackenzie. \\n2019. The Book of Why. Harlow, \\nEngland: Penguin Books. \\n32. Spiegelhalter, David. ‘Should We Trust \\nAlgorithms?’ Harvard Data Science \\nReview 2 \\n33. Stodden, Victoria. The Data Science Life \\nCycle: A Disciplined Approach to \\nAdvancing Data Science as a Science. \\nCommun. ACM 63, no. 7 (2020): 58-66. \\n34. The world’s largest health-research \\nstudy is under way in Britain. It is aimed \\nat saving Britons—and the NHS, The \\nEconomist, October 18, 2023. \\n35. Tukey, John W, “The Future of Data \\nAnalysis,” The Annals of Mathematical \\nStatistics 33, no. 1 (1962): 6, \\n36. Tukey, John, W, Exploratory Data \\nAnalysis, Addison Wesley, 1977 \\n37. Van Essen DC et al. The WU-Minn \\nHuman Connectome Project: an \\noverview. Neuroimage 80, 62– 79 \\n(2013). [PubMed: 23684880]  \\n38. Welsh, Matt, Large Language Models \\nand The End of Programming, lecture at \\nHarvard, October 24, 2023. https://\\nwww.fixie.ai  \\n39. Varadi M, Anyango S, Deshpande M, \\nNair S, Natassia C, Yordanova G, Yuan D, \\nStroe O, Wood G, Laydon A, Žídek A, \\nGreen T, Tunyasuvunakool K, Petersen S, \\nJumper J, Clancy E, Green R, Vora A, \\nLutfi M, Figurnov M, Cowie A, Hobbs N, \\nKohli P, Kleywegt G, Birney E, Hassabis \\nD, Velankar S. AlphaFold Protein \\nStructure Database: massively \\nexpanding the structural coverage of \\nprotein-sequence space with high-\\naccuracy models. Nucleic Acids Res. \\n2022 Jan 7;50(D1):D439-D444. doi: \\n1 0 . 1 0 9 3 / n a r /g ka b 1 0 6 1 P M I D : \\n34791371; PMCID: PMC8728224. \\n40. Zhang, D., et. al., “The AI Index 2023 \\nAnnual Report,” AI Index Steering \\nCommittee, Institute for Human-\\nCentered AI, Stanford University, \\nStanford, CA, April 2023. \\n \\n30\\n',\n",
       " '2403.03387v2.pdf': 'A Systematic Literature Review of\\nUndergraduate Data Science Education\\nResearch\\nMine Dogucu∗\\nDepartment of Statistics, University of California, Irvine\\nand\\nSinem Demirci†\\nStatistics Department, California Polytechnic State University\\nand\\nHarry Bendekgey‡\\nDepartment of Computer Science, University of California, Irvine\\nand\\nFederica Zoe Ricci§\\nDepartment of Statistics, University of California, Irvine\\nand\\nCatalina M. Medina¶\\nDepartment of Statistics, University of California, Irvine\\nJanuary 3, 2025\\nAbstract\\nThe presence of data science has been profound in the scientific community in\\nalmost every discipline. An important part of the data science education expansion has\\nbeen at the undergraduate level. We conducted a systematic literature review to (1)\\nportray current evidence and knowledge gaps in self-proclaimed undergraduate data\\n∗Dogucu has been supported by NSF IIS award #2123366. Dogucu completed an earlier part of this\\nwork in Department of Statistical Science at University College London.\\n†Demirci has been supported by the Scientific and Technological Research Council of Türkiye. Demirci\\ncompleted an earlier part of this work in Department of Statistical Science at University College London.\\n‡Bendekgey has been supported by the HPI Research Center in Machine Learning and Data Science at\\nUC Irvine.\\n§Ricci has been supported by the HPI Research Center in Machine Learning and Data Science at UC\\nIrvine.\\n¶Medina has been supported by NSF IIS award #2123366.\\n1\\narXiv:2403.03387v2  [stat.OT]  3 Jan 2025\\nscience education research and (2) inform policymakers and the data science education\\ncommunity about what educators may encounter when searching for literature using\\nthe general keyword ‘data science education.’ While open-access publications that\\ntarget a broader audience of data science educators and include multiple examples of\\ndata science programs and courses are a strength, significant knowledge gaps remain.\\nThe undergraduate data science literature that we identified often lacks empirical\\ndata, research questions and reproducibility. Certain disciplines are less visible. We\\nrecommend that we should (1) cherish data science as an interdisciplinary field; (2)\\nadopt a consistent set of keywords/terminology to ensure data science education\\nliterature is easily identifiable; (3) prioritize investments in empirical studies.\\nKeywords: data science curriculum, data science programs, data science courses, educational\\ntechnology, open access\\n2\\n1\\nIntroduction\\nThe emergence of data science in the last few decades has given scientists a lot to talk and\\nwrite about, accumulating a wealth of scientific literature. The presence of data science has\\nbeen profound in the scientific community in almost every discipline. The National Science\\nFoundation (NSF) called “Harnessing the Data Revolution” one of its 10 big ideas which\\ncuts across all NSF Directorates (2017).\\nThe demand for data science skills in academia and industry resulted in many higher\\neducation institutions developing courses (e.g., Baumer 2015) as well as degree programs\\n(e.g., Glantz et al. 2023, Adhikari et al. 2021, Stern et al. 2021). With newer educational\\nopportunities, newer educational research questions have been formed and thus data science\\neducation emerged as a field. For instance, the American Statistical Association’s (ASA’s),\\nJournal of Statistics Education changed its name to Journal of Statistics and Data Science\\nEducation (JSDSE) (Witmer 2020).\\nOver the years, a major focus of the data science education community, unsurprisingly,\\nhas been at the undergraduate level. Different professional organizations and groups have\\ntried to describe the data science competencies including but not limited to the Park City\\nMath Institute report (De Veaux et al. 2017), the framework of the National Academies\\nof Sciences, Engineering, and Medicine (2018), the computing competencies guidelines of\\nthe Association for Computing Machinery’s (Danyluk et al. 2021), the accreditation for\\ndata science programs of the Accreditation Board for Engineering and Technology (ABET)\\n(2024), and the EDISON project of the European Union (Wiktorski et al. 2017). It is also\\nworth noting that the majority of data science programs are at the master’s level and there\\nis still a lot of room for growth at the undergraduate level (Li et al. 2021). Given the global\\n3\\nneed for data science education, it is important for us to understand undergraduate data\\nscience education as well as the scientific literature on this topic.\\nEven though there is not a consensus on what data science is and which disciplines are part\\nof it, a common view is that data science is interdisciplinary and statistics, computer science,\\nmathematics and other domains contribute to it (Donoho 2017). The interdisciplinary\\nstructure of data science naturally gets reflected in educational research as well, making\\ndata science education an interdisciplinary field with contributors from statistics education,\\ncomputer science education, and other educational research communities.\\nWhile contributions from different educational research communities can make data science\\neducation richer, these contributions may not easily circulate across disciplines in the absence\\nof a centralized community. For instance, Hazzan and Mike (2021) mention that, despite its\\nname, JSDSE mainly caters to the statistics community and state that “no journal exists\\ntoday that deals exclusively with data science education, let alone highlights data science\\neducation from an interdisciplinary perspective”.\\nThe aforementioned examples of courses, programs and curricular guidelines as well as the\\nexistence of conferences and journals with the title or keywords data science education show\\nthe growth of this field. In this manuscript, we detail a study conducted to understand\\nundergraduate data science education through in-depth readings of the existing literature.\\nAs undergraduate teacher-scholars, our focus was on data science education at the under-\\ngraduate level. In this study, we did not intend to define data science education research or\\njudge whether a study meets a specific definition of data science. Instead, we focused on\\npublications that claimed to be on “data science education”.\\nOur goals were to (1) specify current evidence and knowledge gaps in self-proclaimed\\n4\\nundergraduate data science education and (2) inform policymakers and the data science\\neducation community about what educators may encounter when searching for literature\\nusing the general keyword ‘data science education.’ We conducted a systematic literature\\nreview (Evans & Benefield 2001, Liberati et al. 2009) using criteria that we detail in Section 2.\\nIn Section 3 we share our findings and then we discuss their implications in Section 4.\\n2\\nData Collection and Analysis\\nThe target population of this literature review was publications on data science education\\nthat directly address the undergraduate level. Due to variations in terminology, keywords,\\nand language used by teacher-scholars across different data science fields, identifying the\\nentire target population was not feasible. Since there is no consensus on what data science is,\\nand consequently what data science education is, we did not evaluate whether publications\\nmet a certain definition of data science. Rather, we considered publications that self-\\nproclaim to focus on data science education. Therefore, we opted to identify the accessible\\npopulation for this study as publications that included “data science education” in quotes\\nin the title, abstract, or keywords.\\nFigure 1 summarizes the stages of our data collection and analysis processes which led to\\nthe sample of this study. As shown in the diagram, we extracted data from six databases\\nthat potentially include publications on data science education. These databases were: (1)\\nERIC ProQuest, (2) IEEE Xplore, (3) PubMed, (4) ScienceDirect, (5) Scopus, and (6) Web\\nof Science. These databases were selected because they cover a large number of publications\\nand they include multiple disciplines that are commonly linked to data science education.\\nFor instance, ERIC ProQuest is known to mainly include education-focused publications,\\nIEEE Xplore focuses on engineering and Web of Science is multidisciplinary-focused.\\n5\\nFigure 1: Flowchart of data collection and analysis. Publications were collected in December\\n2022.\\n6\\nFrom the selected databases, we collected publications including the term “data science\\neducation” (in quotes) in at least one of the following fields: title, abstract, keywords. We\\nacknowledge that there are many other combinations of search terms that could result\\nin publications related to data science education, including terms such as “data science\\ncourses”, “data science pedagogy”, “data science curriculum”. One can generate numerous\\nsimilar terms, with or without quotes. We specifically used the term “data science education”\\nfor its broadness and to set the scope of the research. We did not use “data science” and\\n“education” separately to avoid publications in education that employ data science methods\\nfor their analyses.\\nAcross the six databases, we found a total of 197 publications that met our search criteria.\\nWe extracted some variables including, but not limited to: author names; publication title;\\npublication venue (e.g., journal title or conference title). We conducted the initial database\\nsearch in December 2022, resulting in a pool of publications that were either published by\\nthat date or available online by that date but officially published in 2023.\\nThe data analysis was conducted in two stages: 1) preliminary data analysis and 2) in-depth\\ndata analysis. Each publication was randomly assigned to two authors of the present\\nmanuscript. At both data-analysis stages, the two assigned reviewers first examined each\\npublication independently and then discussed discrepancies between their analysis decisions,\\nto reach a consensus. In cases where conflicts persisted, the entire group of five authors\\ndeliberated on the final decision.\\nDuring preliminary data analysis, we manually opened and read the abstracts of all pub-\\nlications. At this stage, we sought to exclude publications that did not meet our format\\nand content criteria based on their abstracts.\\nSpecifically, we aimed to include jour-\\nnal/conference/magazine articles and book chapters (format) that focused on undergraduate\\n7\\ndata science education (content) and were written in English. During in-depth analysis we\\nexamined the full publications and, upon confirming that they met our inclusion criteria,\\nwe recorded variables of potential interest. The number of publications that were excluded\\ndue to format, content or other reason, at each stage of our analysis, is shown in Figure 1.\\nAcross both stages, we excluded 34 publications due to formatting (which included posters,\\npanels, letters to journal editors and meeting highlights). Of the remaining 163 publications,\\n77 were excluded due to their content: 12 of them were not about data science education\\n(including e.g. publications focused on data science methodology) and 65 of them focused\\nexclusively on a different level of education than undergraduate. Among the latter excluded\\ngroup, 18 publications focused on graduate level, 29 focused on K-12, middle school or high\\nschool and 18 publications focused on data science education for adults in non-academic\\nprograms (including, e.g. practitioners, citizen science and instructors).\\nPublications\\nthat focused on both undergraduate and non-undergraduate levels were included. Of the\\nremaining 86 publications, other 9 were excluded due to being: not written in English;\\nduplicated in our dataset; retracted by their authors.\\nAfter excluding publications for the reasons detailed above, we were left with 77 publications,\\nwhich we analyzed in-depth. In addition to the variables extracted from databases (e.g. title\\nof the publication, author names, etc.) for each publication we collected data on:\\n• affiliation country of researchers\\n• open access status (i.e. whether the full publication is accessible for free from a Google\\nScholar search)\\n• year when the publication was first published online\\n• document type (conference article, journal article, magazine article or book chapter)\\n8\\n• whether there were explicit research questions stated in the publication\\n• whether there was any reporting of data collection in the publication and, if data were\\ncollected, the type of data (quantitative, qualitative or mixed)\\n• publication focus, that is a categorization of the subject matter of the publication (for\\nexample “pedagogical approach”, “class activity” or “review of current state of data\\nscience education”)\\n• discipline of the publishing source, determined by examining the call for contributions\\nof the journal or conference, or the description of the book or magazine (“broad” when\\nthe publishing source called for contributions across all data science fields, otherwise\\na specific sub-field of data science, e.g. “computer science” or “statistics”)\\n• the discipline of the target audience, as expressed by the authors in the publication\\n(“broad” when the publication target were all data science educators, otherwise a\\nspecific sub-field of data science, e.g. “computer science” or “statistics”)\\n3\\nResults\\n0\\n5\\n10\\n15\\n20\\n2015\\n2017\\n2019\\n2021\\nFirst Available Online\\n# Publications\\nFigure 2: Undergraduate data science education publications (n = 77) by year they were\\nfirst made available online. Note that initial data retrieval was in December 2022.\\n9\\nPublication years. Figure 2 shows the distribution of the years when publications in our\\nreview were made available online. Some of these publications were available during our\\ndatabase search in December 2022, but would ultimately be published in a 2023 journal\\nedition (e.g., Pieterman-Bos & van Mil 2023, Schmitt et al. 2023). The body of literature\\nthat we identified is very recent, with the oldest paper available in 2015 and the volume of\\nwork increasing steadily in the following years.\\nOpen access status and document types. A majority of work on undergraduate data\\nscience education published over these 8 years has been freely available to the public: the\\nbreakdown of articles’ open access status is shown in Table 1 along with the document\\ntype. The vast majority of journal articles were open access, while conference publications\\nshowed a more even split. We reviewed only four publications which were neither journal\\nnor conference articles, of which two were magazine articles (Hazzan & Mike 2021, Bonnell\\net al. 2022) and two were book chapters (Manzella & Emery 2022, Ryan 2016).\\nTable 1: Document type stratified by open access.\\nDocument Type\\nOpen Access\\nNot Open Access\\nTotal\\nConference Article\\n24\\n19\\n43 (56%)\\nJournal Article\\n26\\n4\\n30 (39%)\\nBook Chapter\\n0\\n2\\n2 (3%)\\nMagazine Article\\n1\\n1\\n2 (3%)\\nTotal\\n51 (66%)\\n26 (34%)\\n77\\nAffiliation countries. We also investigated the geographic breakdown of institutions that\\nare contributing to literature on undergraduate data science education. A majority of the\\n10\\npublications analyzed included at least 1 author associated with an American institution (45\\nof 77), with European and Asian institutions providing the bulk of the remaining analyzed\\nliterature. Table 2 breaks down the analyzed publications by the institutional affiliations of\\ntheir authors. The higher number of scholars’ affiliations being in the United States may be\\nat least partly attributed to the large size of the US relative to many other countries, as\\nwell as selection bias due to our search being restricted to English-written publications and\\nto databases where US scholars might be more represented.\\nTable 2: For each country below, we report the number of publications which included\\nat least one affiliated author. Affiliation country of authors was determined based on\\ntheir home institutions as reported in the publication, and does not represent authors’\\nnationalities.\\nCountry\\n# Publications\\nCountry\\n# Publications\\nUnited States of America\\n45\\nAustralia\\n2\\nNetherlands\\n8\\nEgypt\\n2\\nCanada\\n5\\nGermany\\n2\\nNorway\\n5\\nGreece\\n1\\nUnited Kingdom\\n5\\nHong Kong S.A.R.\\n1\\nSpain\\n4\\nIreland\\n1\\nChina\\n3\\nNew Zealand\\n1\\nIndia\\n3\\nPortugal\\n1\\nIsrael\\n3\\nRomania\\n1\\nItaly\\n3\\nSlovakia\\n1\\nJapan\\n3\\nThailand\\n1\\n11\\nSwitzerland\\n3\\nUnited Arab Emirates\\n1\\nSource and audience disciplines. Figure 3 shows the breakdown of articles by discipline.\\nWe report both the discipline of the publishing venue (such as the journal’s or conference’s\\ndiscipline), and the audience that the publication stated (or implied) to target. About half\\nof the articles (40/77) were written for the broad community of data science educators.\\nThe remaining articles were written for educators in particular disciplines, most commonly\\ncomputer science (12 publications), statistics (5 publications), and information sciences\\n(5 publications). We found that publication venues in all of these disciplines, as well as\\nengineering, contributed articles aimed at their specific audience as well as articles aimed at\\nthe broad community of data science educators.\\nResearch Question and Data Collection. Most publications (69 of 77 reviewed) either\\nposed research questions and collected data or did neither, although there were exceptions.\\nFour publications had research questions but no data collected (Vance et al. 2022, Pieterman-\\nBos & van Mil 2023, Hagen 2020, Robeva et al. 2020). On the other hand, four publications\\ncollected data but did not pose a specific research question or study goal (Hicks & Irizarry\\n2018, Rao et al. 2018, Liu & Wei 2020, Cuadrado-Gallego et al. 2021). Of the 40 publications\\nthat included collected data, 6, 9, and 25 publications had qualitative, quantitative, and\\nmixed data respectively.\\nPublication focus. We also classified publications based on their focus. We found publi-\\ncations that reviewed the current state of data science education and provided guidelines.\\nWe also encountered publications that provided examples of programs, courses, class activi-\\nties, and extracurricular activities. Some publications specifically focused on educational\\ntechnology or discussed a pedagogical approach. Some publications included more than one\\n12\\nScience\\nMedical Sciences\\nSTEM\\nBiological Sciences\\nLibrary Science\\nEducation\\nMathematics\\nStatistics\\nEngineering\\nInformation Sciences\\nComputer Science\\nBroad\\nBusiness\\nMedical Sciences\\nSTEM\\nBiological Sciences\\nMathematics\\nStatistics\\nEngineering\\nInformation Sciences\\nComputer Science\\nBroad\\nSource\\nAudience\\n1\\n1\\n2\\n1\\n1\\n2\\n1\\n5\\n1\\n2\\n1\\n4\\n1\\n2\\n3\\n7\\n2\\n1\\n1\\n5\\n5\\n7\\n12\\n7\\nFigure 3: Composition of the discipline of targeted audiences of publications on data science\\neducation (right) by the discipline of the publishing source (left). A total of 2 publications\\nwere not included in the plot, one because it stated to target both statistics and computer\\nscience, the other one because it was not possible to determine its audience discipline.\\n13\\n        Guidelines            \\n Review of Current State      \\nof Data Science Education     \\n     Course Example           \\n     Program Example          \\n Class Activity Example       \\n   Education Technology       \\n   Pedagogical Approach       \\n      Call to Action               \\nNA\\n14\\n2\\n1\\n4\\n9\\n9\\n5\\n4\\n2\\n6\\n13\\n12\\n1\\n1\\n5\\n1\\n1\\n8\\n5\\n2\\n1\\n2\\n1\\n10\\n7\\n7\\n1\\n1\\n2\\n2\\n2\\n6\\n1\\n10\\n8\\n3\\n2\\n6\\n1\\n0\\n5\\n10\\n15\\n# Publications\\nHad Research Question\\nNo\\nYes\\n0\\n5\\n10\\n15\\n# Publications\\nData Collection Type\\nNone\\nQualitative\\nMixed\\nQuantitative\\nFigure 4: Number of publications with communicated research questions and with collected\\ndata stratified by publication focus.\\nfocus: in such cases, we noted down all the categories that the publication falls into, with\\nthe exception of call to action. Almost all publications we read were calling the scientific\\ncommunity to action and this is unsurprising for scientific publications. For instance, an\\neducational technology paper might call to action to use a specific tool and a pedagogy paper\\nmight call to action to use a specific pedagogical approach. However, some publications\\nsolely made a call to action, without content falling into other publication focuses (e.g.,\\neducation technology or course example). For these publications, we reserved the publication\\nfocus “call to action”, that is, papers that we labeled in the “call to action” category did\\nnot fall in any other category for publication focus.\\nFor each category of publication focus, Figure 4 shows how many publications featured that\\npublication focus, had a research question and included qualitative, quantitative or mixed\\ntype of data. Note that, in this figure, publication focuses are arranged and grouped to\\n14\\nmatch the order in which we are going to illustrate them below, as we highlight contributions\\nfrom each publication focus.\\nSimilar to what we are doing in our study, many scholars wanted to understand the current\\nstate of data science education. Many of the studies conducted wanted to understand\\nand evaluate data science programs (Wiktorski et al. 2016, Oliver & McNeil 2021, Song &\\nZhu 2016, Shao et al. 2021, Li et al. 2021, Raj et al. 2019) or curricula (Schmitt et al. 2023).\\nDavis (2020) specifically looked at the current state of ethics in undergraduate data science\\neducation and Ceccucci et al. (2015) considered data science education from a scientific\\nliteracy perspective. Among these reviews, there were also comparative studies: for instance,\\nBile Hassan & Liu (2020)’s comparison of informatics and data science programs. Some\\nstudies also compared differences of data science approaches at country or regional levels\\nsuch as in Japan (Takemura 2018), or in Middle Eastern countries (Zakaria 2023). In\\nunderstanding the current state of data science education, scholars also wanted to understand\\nthe data science practice. For example, Belloum et al. (2019) developed semi-automated\\nmethods to determine the competencies needed in the job-market and Kross & Guo (2019)\\nfocused on understanding the skills that data science practitioners who teach data science\\nin various settings pass onto their students. In addition to reviewing data science education,\\nscholars also provided guidelines for data science degrees (Blair et al. 2021).\\nWe found multiple data science program examples, including but not limited to general\\ndata science programs (e.g., Demchenko et al. 2017, 2019, Kakeshita et al. 2022). Some\\npublications were about data science education in specific programs such as computer\\nscience education (Bile Hassan & Liu 2020), microbiology (Dill-McFarland et al. 2021),\\ninformation schools (Song & Zhu 2017, Hagen 2020) and business (Miah et al. 2020). Less\\ntraditional programs were also featured, such as the Data Mine which takes on data science\\n15\\neducation in a residential community of students (Betz et al. 2020).\\nThe literature we read also included many course examples with some clever ways of\\nincluding data science concepts in different courses. For instance, Fisler (2022) suggested\\nto include data science and data structures in an introductory computing course, Rao\\net al. (2019) teach data science through the use of education data in their engineering\\ncourse on modern technologies, and Haynes et al. (2019) teach data science in a general\\neducation IT course. Some institutions developed data science courses for specific disciplines\\nincluding information schools (Hagen 2020) and medicine (Doudesis & Manataki 2022).\\nSome scholars also described courses that merge data science with philosophy of science\\n(Pieterman-Bos & van Mil 2023) or with humanities perspectives (Vance et al. 2022). Last\\nbut not least, data science educators also tried to provide real-life data science experiences\\nthrough work-integrated learning (Bilgin et al. 2022), capstone projects (Allen 2021) and\\nentirely case-study based courses (Hicks & Irizarry 2018).\\nSimilar to program examples and course examples provided in the literature, some publica-\\ntions focused on activity examples. Among class activity examples, Yamamoto et al.\\n(2021) developed a programming exercise to bring higher-order tensor content to undergrad-\\nuate level by using a 3-D puzzle. Another example is by Banadaki (2022) who developed\\ndifferent activities that include applications of data science in Mechanical Engineering,\\nBiomedical science, Quantum Physics, and Cybersecurity. In data science education, there\\nis also room for learning outside the classroom as an extra-curricular activity. This\\nincluded data hackathons (Anslow et al. 2016).\\nIn terms of education technology, as much of data science education tackles with issues\\nsuch as computing power, storage of large datasets, and sometimes automation, some of\\nthese publications focused on cloud-based data science platforms for teaching purposes (e.g.,\\n16\\nDemchenko et al. 2017). Many others focused on tools such as online platforms, learning\\nenvironments, and apps that support learning data science (e.g., Hoyt & Wangia-Anderson\\n2018, Bornschlegl 2016, Liu & Wei 2020, Nair et al. 2020).\\nStudies that focused on pedagogical approaches were also frequently encountered. These\\nstudies focused on various subtopics including team-based learning (e.g., Vance et al. 2022),\\nproject-based learning (e.g., Mike et al. 2020) and large-scale product development which\\nhas both project- and team-based learning aspects (e.g., Bhavya et al. 2020). Scholars also\\nstudied social topics such as ethics and equity in the data science classroom (Alexander\\net al. 2022) and student self-regulation (Zhang & Wu 2020).\\nAlmost all publications we read were calling the scientific community to action. For instance,\\nthrough a systematic review, Davis (2020) called the community to action to include\\nethics in undergraduate data science education. We also encountered publications with an\\nimportant message about data science education but without a review of programs, courses,\\nactivities etc. For instance, Robeva et al. (2020) argued for the inclusion of data science in\\nquantitative biology education. Engel (2017) drew attention to the importance of statistical\\nliteracy in data science education. We have also seen a to-do list (i.e., action list) for the\\ncommunity including starting a multidisciplinary data science education journal (Hazzan\\n& Mike 2021), building a consensus on data science education and curricula (Dinov 2019),\\ndeveloping and deploying degree programs for university students, having basic data science\\ntraining for university students, and training instructors to teach data science (Bonnell et al.\\n2022).\\n17\\n4\\nDiscussion\\nTurning our attention to the knowledge that was gathered and summarized in our study,\\nthis section examines the strengths and gaps in the literature authored by researchers\\nwho self-proclaim to focus on undergraduate data science education. Building upon the\\nfindings detailed in Section 3, we also extend comprehensive recommendations to both\\npolicymakers and the data science education community, aiming to enhance capacity building\\nin undergraduate data science education studies.\\nBefore proceeding, we believe that it is essential to address the limitations (1) inherent in\\nthe field of data science and (2) related to the scope of this study. Acknowledging these\\nconstraints provides crucial context and helps avoid overgeneralization, by enabling readers\\nto more effectively contextualize the strengths, gaps, and recommendations presented in\\nthis section.\\n4.1\\nInherent Limitations in the Field of Data Science (Education)\\nData science is an interdisciplinary (Cao 2017) and relatively young field.\\nWhile its\\nemergent nature brings numerous opportunities, it also introduces inherent limitations that\\ncan influence the accumulation of knowledge in this field and, consequently, the findings of\\nthis systematic literature review.\\nPerhaps the most critical limitation is the lack of consensus on the definition of data science\\n(Hazzan & Mike 2023). Although we acknowledge that the absence of a standard definition\\nis not unique to the field of data science, this lack of agreement at such an early stage,\\nespecially in a field that is so interdisciplinary, may result in a deeper issue: the challenge\\nof generating an identifiable cumulative body of knowledge in this field.\\n18\\nIndeed, a corollary of this definitional ambiguity is the difficulty in labeling research as\\ndata science education. Some studies may be considered data science education by certain\\nscholars, but lack explicit labeling as such, while others that we included might not be\\ncategorized as data science education under different interpretations. Therefore, capturing\\nall undergraduate data science education research papers is extremely challenging at this\\nstage.\\nArguably, the lack of clear identifiability and consistent self-labeling creates at least two\\nadditional challenges. First, it restricts practitioners’ access to findings that could inform\\nand enhance their data science teaching practices. Second, it may hinder researchers from\\nidentifying recurring patterns and comprehensively understanding the current state of data\\nscience education research.\\nIn this study, we deliberately refrained from defining data science and data science education.\\nInstead, we used “data science education” as a keyword to capture publications that self-\\nproclaim as undergraduate data science education research, across all disciplines. We opted\\nfor this approach because we did not want to take the role of gate-keepers by assessing\\nwhether publications adhered to any specific definition of data science. However, this choice\\nmay lead to an incomplete picture of the undergraduate “data science education” literature,\\nas we discuss in the subsequent section.\\n4.2\\nLimitations of the Scope of This Study\\nWe acknowledge that, in this study, we did not reach the entire target population of\\nresearch publications on undergraduate data science education. In addition to the inherent\\nlimitations of the field outlined above, methodological limitations may also contribute to\\nthis incomplete picture of undergraduate data science education literature.\\n19\\nOne key methodological limitation is the use of a single term, “data science education”,\\nfor our search. While the choice of this term was aimed at capturing a broad range of\\nself-proclaimed undergraduate data science education research, it may have restricted the\\nresults. For instance, an article that includes terms such as “data science class”, “data\\nscience activity”, or “data science curriculum” but does not use the keyword “data science\\neducation” would be excluded from this study (e.g., Baumer et al. 2022). Definitional\\nambiguities and self-labeling challenges in the field may exacerbate this issue.\\nOur review may have missed relevant publications due to several other factors, including\\npublications written in languages other than English and publications that are not included\\nin the databases that we searched (e.g., Finzer 2013).\\nDespite these challenges, we believe our study offers valuable insights into the strengths and\\nknowledge gaps within self-proclaimed undergraduate data science education research. It\\nprovides a realistic portrayal of what data science educators might encounter when searching\\nfor literature using the general keyword “data science education.” Furthermore, it offers\\ninsights that can benefit the broader data science education community, including those\\nwhose work does not explicitly label itself as data science education.\\n4.3\\nStrengths in Undergraduate Data Science Education\\nThe majority of published studies on undergraduate data science education are open access,\\nmarking a substantial strength in the field. The freely available information and scholars’\\ninsights regarding the status of undergraduate data science education not only contribute\\nto overcoming barriers, but also facilitate the dissemination and application of knowledge.\\nOver the eight years included in our analysis, a higher percentage of conference articles\\n(56%) have been published compared to journal articles (39%). Education technology is\\n20\\nthe publication focus that is studied most by scholars. Among educational technologies\\nto support learning data science, learning environments (e.g., Bornschlegl 2016, Hoyt &\\nWangia-Anderson 2018) are one of the popular ones. Ethics is also one of the recurring\\nthemes among the studies that we reviewed (e.g., Davis 2020, Shapiro et al. 2020, Alexander\\net al. 2022).\\nAs shown in Figure 3, the majority of self-proclaimed undergraduate data science publications\\nare directed to the broad audience of data science educators generally. Specifically, 40\\nout of 77 publications target the undergraduate data science education community as\\na whole, rather than appealing to a specific discipline in data science (e.g., computer\\nscience) or a subset of disciplines in data science (e.g., STEM). This inclusivity can be seen\\nas a potential strength, as the insights from these broader publications provide valuable\\nperspectives that scholars and researchers can adapt and apply across diverse contexts in data\\nscience to enhance teaching practices, develop curricula, and foster a more comprehensive\\nunderstanding of data science education.\\nStudies including multiple examples of data science programs and courses across various\\nfields add another layer of strength to the undergraduate data science education literature.\\nIn addition to overall data science programs (e.g., Demchenko et al. 2019, Kakeshita et al.\\n2022), we have also seen data science education practices in different programs such as\\ncomputer science education (Bile Hassan & Liu 2020), microbiology (Dill-McFarland et al.\\n2021), and business (Miah et al. 2020).\\nThe course examples showcase a diverse array of strategies for incorporating data science\\nconcepts, ranging from introductory computing (Fisler 2022), modern technologies for\\ncomputer science and engineering students (Rao et al. 2019) and general education IT\\n(Information Technology) (Haynes et al. 2019), to medicine (Doudesis & Manataki 2022) and\\n21\\nintroduction to psychological statistics (Tucker et al. 2023). This broad spectrum in both\\nprogram and course examples serves as compelling evidence, illustrating that the intrinsic\\ninterdisciplinary nature of data science attracts the attention of scholars from diverse fields.\\n4.4\\nKnowledge Gaps in Undergraduate Data Science Education\\nWhile the strengths of the current data science education literature, as outlined above, are\\nevident, our study also reveals some knowledge gaps. Knowledge gaps are areas or topics\\nderived from the synthesis of an existing body of literature (Cooper 1998). Understanding\\nthese gaps is crucial because it adds a more structured and evidence-supported layer to our\\nknowledge. In this section, we discuss some knowledge gaps in self-proclaimed undergraduate\\ndata science education.\\nKnowledge gap 1: Certain disciplines in data science are less visible in the\\ncurrent body of literature.\\nDespite the lack of consensus on defining what exactly data science is, there is agreement that\\ndata science encompasses various disciplines, including statistics, mathematics, computer\\nscience, and other relevant domains, as defined by many interpretations (e.g., Cao 2017,\\nHazzan & Mike 2023). However, these disciplines are not equally represented among the\\nself-proclaimed data science education publications that we examined.\\nIn terms of source discipline (i.e., the main discipline of a journal, conference, book chapter\\netc.) we have seen many publications published in venues related to computer science,\\ninformation science, engineering, and statistics. The difference in quantity of publications\\npublished in sources related to computer science and statistics is worth remembering from\\nFigure 3. This result partly aligns with the study of Wiktorski et al. (2017), who reported\\nthat mathematics and statistics departments are not at the forefront of data science. This\\n22\\nresult can also potentially be explained by the fact that different disciplines have different\\npublication rates in general. For instance, in the Science and Technology Indicators report\\npublished by NSF it is stated that 2.3% of the articles published in 2016 is in the field of\\nmathematics and 8.3% are in computer sciences (2018).\\nWe have seen even fewer publications in venues related to domain sciences. These were\\nmainly related to education, biological sciences, library science, and medical sciences. The\\nfindings may indicate a need to have journals and conferences in other domain sciences\\n(e.g., astronomy, economics, psychology) that provide more opportunities for disseminating\\nworks of data science education researchers to a broader data science community. It is\\nalso possible that such journals and conferences for domain sciences might exist, but the\\nkeyword “data science education” is not used in these venues. For instance, economists may\\ncontinue to use the term “econometrics education” rather than “data science education”.\\nAmong source disciplines, we have also identified publication venues for the broader data\\nscience community. This, however, mainly consisted of conferences. The two journals that\\nwere identified as broad were International Journal of Data Science and Analytics and\\nFoundations of Data Science. Both of these journals focus on data science while allowing\\nfor education-related publications, but neither of these journals focuses specifically on data\\nscience education. These findings reiterate the importance of the call made by Hazzan and\\nMike (2021) on having an interdisciplinary journal on data science education.\\nIn terms of audience disciplines, scholars write for the broader data science education\\ncommunity as well as specific disciplines.\\nRepresentation of both broad and specific\\ndisciplines is important and can enrich the data science education research. There are\\nfewer publications written for the audiences in domain sciences. This might indicate further\\nneed for data science education research targeting these audiences, or this might be due to\\n23\\nunder-utilization of the keyword “data science education” in these disciplines.\\nStatistics is underrepresented both as source and audience discipline in the body of literature\\nthat we reviewed. Arguably, statisticians are contributing to data science education research\\nmore than this, but their work was not prominently captured in this study.\\nSeveral\\nexplanations may account for this finding. Statisticians might avoid using the term “data\\nscience” in their work, perceiving their research as strictly related to statistics and targeting\\ntheir primary audience within that domain. Another possibility is that they consider\\ndata science as a subset or natural extension of statistics, and thus do not see the need\\nto explicitly label their work as data science education research. Lastly, they might be\\nusing some other keywords in their title, abstract, and keywords instead of “data science\\neducation”, making it less visible to the broader data science education community.\\nThis limited visibility of statisticians in the current literature underscores the insufficiency of\\nthe existing body of data science education research. Most importantly, it highlights a critical\\ngap: the lack of visible perspectives from statisticians in this identified body of literature.\\nAs statistics is one of the foundational disciplines in data science, its under-representation\\nnot only diminishes the diversity of insights but may also impede the development of\\ncomprehensive, interdisciplinary approaches to data science education.\\nAlthough statistics is underrepresented both as source and audience discipline in comparison\\nto computer science, it is worth noting that both computer science and statistics sources\\nwrite for broad data science education audiences as well as their corresponding specific\\ndiscipline audiences.\\nFor foundational sciences of data science, such as statistics and\\ncomputer science, writing for the broader data science community as well as the specific\\ndiscipline is important and can continue to enrich data science education research in the\\nyears to come.\\n24\\nKnowledge gap 2: Within the identified body of data science education literature,\\nthere is lack of empirical data and identifiable research questions.\\nResearch questions and data are two common elements in empirical research. Data for\\nempirical research include both qualitative and quantitative approaches where the researchers\\ncollect ‘observable information about or direct experience of the world’. And perhaps most\\nimportantly, empirical data are not just stored as in numbers but also words and categories\\n(Punch & Oancea 2014).\\nOne of the important functions of systematic literature reviews is to gain a deeper un-\\nderstanding and inform possible further research avenues that can be conducted in the\\nfield (Evans & Benefield 2001, Liberati et al. 2009). To facilitate this, we categorized the\\nstudies based on their publication focus, the existence of research questions, and the types\\nof data collection. As stated earlier, 37 studies out of 77 did not collect data. Given the\\nscopes of publication focus such as calls to action, educational technology, and program\\nexamples coupled with the emergent nature of the field, the lack of empirical data is not\\na surprising finding. However, it also suggests that undergraduate data science education\\nresearchers have not yet begun to systematically collect empirical data to assess, for example,\\nthe effectiveness of educational technologies, programs, learning outcomes and/or other\\npedagogical approaches.\\nA lack of empirical data could impede the development of systematic literature reviews\\nor meta-analyses on a specific publication focus (e.g., educational technology), which are\\nessential for identifying trends, studied variables, and recurring patterns in undergraduate\\ndata science education through qualitative and/or quantitative approaches. It is essential\\nto clarify that our emphasis on lack of empirical data is not a promotion of empiricism over\\nall other ‘ways of knowing’. We acknowledge and appreciate alternative forms of knowledge,\\n25\\nsuch as expert opinions (Fraenkel et al. 2012), for their valuable contributions to the data\\nscience education community’s know-how. These forms of knowledge are important catalysts\\nin guiding researchers towards areas that require systematic data collection. What we are\\nhighlighting is the disproportionately high percentage of studies lacking empirical data and\\nidentifiable research questions, which complicates the literature’s potential for gaining a\\ndeeper understanding and identifying recurring patterns.\\nKnowledge gap 3: Reproducibility is one of the potential challenges in under-\\ngraduate data science education research.\\nThe corollary of a lack of empirical data and identifiable research questions may introduce\\nanother challenge: the reproducibility of certain studies. We speculate that the absence\\nof critical information about research designs, such as the lack of research questions and\\nnon-collection of data, may contribute to the reduced reproducibility of available studies.\\nThis makes it challenging to replicate or modify research, impeding the identification of\\nrecurring patterns.\\nLack of reproducibility is not unique to data science education research. Its importance\\nand lack thereof have been discussed in many disciplines including physics (Junk & Lyons\\n2020), economics (Chang & Li 2015), and psychology (Open Science Collaboration 2015).\\nIn recent years, this has even been referred to as the reproducibility crisis.\\nPotential explanations for the lack of reproducibility in data science education research might\\nbe similar to reasons seen in the broader science community. Namely, word limitations can\\nresult in lack of detailed information on research design and data collection in publications\\n(Bausell 2021).\\nAnother explanation might simply be the “publish or perish” culture in academic settings.\\n26\\nThere are even academics who publish a paper every five days (Ioannidis et al. 2018). Even\\nif not at this rate, many academics might feel under pressure to get publications out, without\\nhaving much time to focus on the reproducibility of their work.\\nReproducibility is an important skill for teacher-scholars of data science both in their\\nteaching and research (Dogucu & Çetinkaya-Rundel 2022, Dogucu 2024). Considering that\\nmuch of the published research in the literature are written by those who also teach data\\nscience, closing the reproducibility gap both in research and teaching is extremely important.\\nOne potential reason for this gap may also be the minimal training that most instructors\\nreceive in reproducibility (Horton et al. 2022).\\n4.5\\nRecommendations\\nConsidering the findings and our arguments, we present three recommendations for the\\nfuture of undergraduate data science education studies: one for policymakers and funding\\nagencies, and two for institutions and scholars whose research focus includes data science\\neducation.\\nRecommendation 1: Cherish data science as an interdisciplinary field.\\nDespite the existence of other key fields offering data science courses at the undergraduate\\nlevel, there is a noticeable gap in studies reflecting their perspectives in data science\\neducation. To address this gap, it is imperative for undergraduate data science education\\nstudies to incorporate the viewpoints of scholars from foundational disciplines such as\\nstatistics, mathematics, and other application domains intersecting with data science\\neducation practices. We need to encourage scholars in all data science fields to maintain\\ntheir visibility and contribute more to publications. We posit that future endeavors in\\nthis direction will significantly enhance our understanding of the strengths and needs in\\n27\\nundergraduate data science education.\\nAs statistics community, we must take an active role in expanding the data science education\\nresearch and other opportunities. For instance, despite having Guidelines for Assessment\\nand Instruction in Statistics Education (GAISE) (2016) and Curriculum Guidelines for\\nUndergraduate Programs in Statistical Science (CGUPSS) (2014), ASA has not yet written\\nany guidelines specific to data science education. However, ASA Board of Directors endorsed\\nthe Park City Math Institute report (De Veaux et al. 2017) and have provided input on\\nthe criteria for the data science program ABET accreditation (Liu 2022). At the time of\\nwriting of this manuscript, ASA is working to update GAISE. Further efforts by ASA similar\\nto GAISE and CGUPSS or newer versions of these documents can also help distinguish\\nstatistics and data science programs and courses.\\nRecommendation 2: Adopt a consistent set of keywords/terminology to ensure\\ndata science education literature is easily identifiable. While there is no doubt\\nthat all data science fields are contributing to data science education research, some of\\ntheir work was not prominently captured in this study. Speculatively, it appears that the\\nkeyword “data science education” is more commonly embraced by certain fields compared\\nto others. As a result, a researcher broadly interested in data science education may not\\nsee the interdisciplinary diversity of insights within the field, if there is a lack of consistent\\nuse of keywords and terminology.\\nWe believe that using both broad, cross-disciplinary terms and specialized terminology\\nunique to each field/domain is essential for facilitating communication within individual\\ndisciplines/domains and across the wider data science community. Adopting a consistent\\nset of keywords does not require a shared definition of what is and is not data science. If a\\nset of keywords is consistently used by data science education researchers, it may facilitate\\n28\\nthe accumulation of collective knowledge and help identify what works or does not in the\\nfield. Therefore, it is important to standardize terminology to enhance the accessibility and\\ncomprehensiveness of the data science education literature.\\nRecommendation 3: Prioritize investments in empirical studies. Not having\\nsufficient empirical data is one of the knowledge gaps in undergraduate data science\\neducation research. Accumulating empirical data is essential to be able to gain a sound\\nunderstanding of data science education studies at a large scale. Hence, we recommend that\\npolicymakers and funding agencies prioritize investments in undergraduate data science\\nstudies dedicated to systematic data collection. Additionally, directing investments toward\\nempirical studies in application domains of data science that were underrepresented in\\nour study, such as astronomy, psychology, and economics could also help provide a more\\ncomplete picture of data science education in the long run.\\nThese strategic approaches will enable a comprehensive assessment of foundational dis-\\nciplines and various application domains of data science, including the effectiveness of\\neducational technologies, program impact, learning outcomes, and other pedagogical ap-\\nproaches, ultimately contributing to a more informed and robust understanding of data\\nscience education.\\n4.6\\nClosing Remarks\\nIn summary, the results of this study show that data science education is an emerging\\nfield with much more room for growth. Scientific studies are an integral part of reviewing\\nexisting practices as well as of improving higher education institutions’ data science practices.\\nTherefore, we should diversify our research efforts by investing in more empirical studies\\nand fostering scholars from key fields in data science, especially in statistics and domain\\n29\\nsciences.\\nFurther research studies may improve or try to replicate the findings of this study in multiple\\nways by utilizing different keywords, databases, and languages. An even larger endeavor\\ncan be taken if investigators want to work off on a definition of data science utilizing a\\nsystematic literature review.\\nLastly, we also believe that the data collected as part of this study can help novice researchers\\nin data science education find inspiration and examples from the existing literature. We\\npresented findings of the study at the Electronic Conference on Teaching Statistics 2024\\nwhere we had participants design their own research study by grouping them according\\nto their publication focus (e.g., educational technology) of interest. Readers interested in\\npursuing research in data science education may utilize our dataset to create a reading list\\nfor their research agenda.\\nData Availability Statement\\nThe data on all the publications and the associated codebook for the variables are publicly\\navailable in a GitHub repository at https://github.com/mdogucu/comp-data-sci and an\\nOSF project at https://osf.io/b3u7y/.\\nReferences\\nABET (2024), ‘Criteria for Accrediting Applied and Natural Science Programs, 2024-2025’.\\nURL: https:// www.abet.org/accreditation/accreditation-criteria/criteria-for-\\naccrediting-applied-and-natural-science-programs-2024-2025/\\nAdhikari, A., DeNero, J. & Jordan, M. (2021), ‘Interleaving Computational and Inferential\\n30\\nThinking in an Undergraduate Data Science Curriculum’, Harvard Data Science Review\\n3(2).\\nAlexander, N., Eaton, C. D., Shrout, A., Tsinnajinnie, B. & Tsosie, K. (2022), ‘Beyond\\nEthics: Considerations for Centering Equity-Minded Data Science’, Journal of Humanistic\\nMathematics 12(2), 254–300.\\nAllen, G. I. (2021), Experiential Learning in Data Science: Developing an Interdisciplinary,\\nClient-Sponsored Capstone Program, in ‘SIGCSE ’21’, p. 516–522.\\nAmerican Statistical Association (2014), ‘Curriculum Guidelines for Undergraduate Pro-\\ngrams in Statistical Science’.\\nURL: https:// www.amstat.org/docs/ default-source/amstat-documents/ edu-\\nguidelines2014-11-15.pdf\\nAnslow, C., Brosz, J., Maurer, F. & Boyes, M. (2016), Datathons: An Experience Report\\nof Data Hackathons for Data Science Education, in ‘SIGCSE ’16’, p. 615–620.\\nBanadaki, Y. (2022), Enabling Data Science Education in STEM Disciplines through\\nSupervised Undergraduate Research Experiences, in ‘2022 ASEE Annual Conference and\\nExposition’.\\nBaumer, B. (2015), ‘A Data Science Course for Undergraduates: Thinking with Data’, The\\nAmerican Statistician 69(4), 334–342.\\nBaumer, B. S., Garcia, R. L., Kim, A. Y., Kinnaird, K. M. & Ott, M. Q. (2022), ‘Integrating\\ndata science ethics into an undergraduate major: A case study’, Journal of Statistics and\\nData Science Education 30(1), 15–28.\\nBausell, R. B. (2021), Publishing Issues and Their Impact on Reproducibility, Oxford\\n31\\nUniversity Press, p. 0. DOI: 10.1093/oso/9780197536537.003.0010.\\nURL: https://doi.org/10.1093/oso/9780197536537.003.0010\\nBelloum, A. S., Koulouzis, S., Wiktorski, T. & Manieri, A. (2019), ‘Bridging the Demand\\nand the Offer in Data Science’, Concurrency and Computation: Practice and Experience\\n31(17), e5200.\\nBetz, M., Gundlach, E., Hillery, E., Rickus, J. & Ward, M. D. (2020), ‘The Next Wave:\\nWe Will All Be Data Scientists’, Statistical Analysis and Data Mining: The ASA Data\\nScience Journal 13(6), 544–547.\\nBhavya, B., Boughoula, A., Green, A. & Zhai, C. (2020), Collective Development of Large\\nScale Data Science Products via Modularized Assignments: An Experience Report, in\\n‘SIGCSE ’20’, Association for Computing Machinery, p. 1200–1206.\\nBile Hassan, I. & Liu, J. (2020), A Comparative Study of the Academic Programs between\\nInformatics BioInformatics and Data Science in the U.S, in ‘2020 IEEE 44th Annual\\nComputers, Software, and Applications Conference (COMPSAC)’, pp. 165–171.\\nBilgin, A. A. B., Powell, A. & Richards, D. (2022), ‘Work Integrated Learning in Data\\nScience and a Proposed Assessment Framework’, Statistics Education Research Journal\\n21(2), 12–12.\\nBlair, J. R. S., Jones, L., Leidig, P., Murray, S., Raj, R. K. & Romanowski, C. J. (2021),\\nEstablishing ABET Accreditation Criteria for Data Science, in ‘SIGCSE ’21’, p. 535–540.\\nBonnell, J., Ogihara, M. & Yesha, Y. (2022), ‘Challenges and Issues in Data Science\\nEducation’, Computer 55(2), 63–66. Publisher: IEEE.\\nBornschlegl, M. X. (2016), IVIS4BigData: Qualitative Evaluation of an Information Visual-\\n32\\nization Reference Model Supporting Big Data Analysis in Virtual Research Environments,\\nin M. X. Bornschlegl, F. C. Engel, R. Bond & M. L. Hemmje, eds, ‘Advanced Visual\\nInterfaces. Supporting Big Data Applications: AVI 2016 Workshop’, Lecture Notes in\\nComputer Science, Cham, pp. 127–142.\\nCao, L. (2017), ‘Data Science: a Comprehensive Overview’, ACM Computing Surveys\\n(CSUR) 50(3), 1–42.\\nCeccucci, W., Tamarkin, D. & Jones, K. (2015), ‘The Effectiveness of Data Science as\\na means to achieve Proficiency in Scientific Literacy’, Information Systems Education\\nJournal 13(4), 64.\\nChang, A. C. & Li, P. (2015), ‘Is Economics Research Replicable? Sixty Published Papers\\nfrom Thirteen Journals Say ’Usually Not’’, FEDS Working Paper No. 2015-083 .\\nCooper, H. M. (1998), Synthesizing Research: A Guide for Literature Reviews, Vol. 2, Sage.\\nCuadrado-Gallego, J. J., Demchenko, Y., Losada, M. A. & Ormandjieva, O. (2021), Classifi-\\ncation and Analysis of Techniques and Tools for Data Visualization Teaching, in ‘2021\\nIEEE Global Engineering Education Conference (EDUCON)’, pp. 1593–1599. ISSN:\\n2165-9567.\\nURL: https://ieeexplore.ieee.org/abstract/document/9453917\\nDanyluk, A., Leidig, P., Bari, A., Buck, S., Cassel, L., Doyle, M., Hines, K., Ho, T. K.,\\nMcGettrick, A., Qian, W., Schmitt, K., , Servin, C., Stefik, A. & Wang, H. (2021),\\n‘Computing competencies for undergraduate data science curricula’.\\nDavis, K. C. (2020), Ethics in Data Science Education, in ‘2020 ASEE Virtual Annual\\nConference Content Access’.\\n33\\nDe Veaux, R. D., Agarwal, M., Averett, M., Baumer, B. S., Bray, A., Bressoud, T. C.,\\nBryant, L., Cheng, L. Z., Francis, A., Gould, R. et al. (2017), ‘Curriculum Guidelines for\\nUndergraduate Programs in Data Science’, Annual Review of Statistics and Its Application\\n4, 15–30.\\nDemchenko, Y., Belloum, A., de Laat, C., Loomis, C., Wiktorski, T. & Spekschoor, E.\\n(2017), Customisable Data Science Educational Environment: From Competences Man-\\nagement and Curriculum Design to Virtual Labs On-Demand, in ‘2017 IEEE International\\nConference on Cloud Computing Technology and Science (CloudCom)’, pp. 363–368.\\nDemchenko, Y., Comminiello, L. & Reali, G. (2019), Designing Customisable Data Science\\nCurriculum Using Ontology for Data Science Competences and Body of Knowledge, in\\n‘ICBDE ’19’, p. 124–128.\\nDill-McFarland, K. A., König, S. G., Mazel, F., Oliver, D. C., McEwen, L. M., Hong, K. Y.\\n& Hallam, S. J. (2021), ‘An Integrated, Modular Approach to Data Science Education in\\nMicrobiology’, PLOS Computational Biology 17(2), e1008661.\\nDinov, I. D. (2019), ‘Quant Data Science meets Dexterous Artistry’, International Journal\\nof Data Science and Analytics 7(2), 81–86. PMID: 30923735 PMCID: PMC6433171.\\nDogucu, M. (2024), ‘Reproducibility in the Classroom’. Publisher: Annual Reviews.\\nURL:\\nhttps://www.annualreviews.org/content/journals/10.1146/annurev-statistics-\\n112723-034436\\nDogucu, M. & Çetinkaya-Rundel, M. (2022), ‘Tools and Recommendations for Reproducible\\nTeaching’, Journal of Statistics and Data Science Education 30(3), 251–260.\\nDonoho, D. (2017), ‘50 Years of Data Science’, Journal of Computational and Graphical\\nStatistics 26(4), 745–766.\\n34\\nDoudesis, D. & Manataki, A. (2022), ‘Data Science in Undergraduate Medicine: Course\\nOverview and Student Perspectives’, International Journal of Medical Informatics\\n159, 104668. PMID: 35033982.\\nEngel, J. (2017), ‘Statistical Literacy for Active Citizenship: A Call for Data Science\\nEducation’, Statistics Education Research Journal 16(1), 44–49.\\nEvans, J. & Benefield, P. (2001), ‘Systematic Reviews of Educational Research: Does the\\nMedical Model Fit?’, British Educational Research Journal 27(5), 527–541.\\nFinzer, W. (2013), ‘The data science education dilemma’, Technology Innovations in\\nStatistics Education 7(2).\\nFisler, K. (2022), Data-Centricity: Rethinking Introductory Computing to Support Data Sci-\\nence, in ‘1st International Workshop on Data Systems Education’, DataEd ’22, Association\\nfor Computing Machinery, p. 1–3.\\nFraenkel, J., Wallen, N. & Hyun, H. (2012), How to Design and Evaluate Research in\\nEducation (8th) ed.), McGraw-Hill.\\nGAISE (2016), ‘Guidelines for Assessment and Instruction in Statistics Education (GAISE):\\nCollege report’.\\nURL: http://www.amstat.org/education/gaise\\nGlantz, M., Johnson, J., Macy, M., Nunez, J., Saidi, R. & Velez Ramirez, C. (2023),\\n‘Students’ Experience and Perspective of a Data Science Program in a Two-Year College’,\\nJournal of Statistics and Data Science Education 31(3), 248–257.\\nHagen, L. (2020), ‘Teaching Undergraduate Data Science for Information Schools’, Education\\nfor Information 36(2), 109–117.\\n35\\nHaynes, M., Groen, J., Sturzinger, E., Zhu, D., Shafer, J. & McGee, T. (2019), Integrating\\nData Science into a General Education Information Technology Course: An Approach to\\nDeveloping Data Savvy Undergraduates, in ‘SIGITE ’19’, p. 183–188.\\nHazzan, O. & Mike, K. (2021), ‘A Journal for Interdisciplinary Data Science Education’,\\nCommunications of the ACM 64(8), 10–11. Publisher: ACM New York, NY, USA.\\nHazzan, O. & Mike, K. (2023), What is Data Science?, in ‘Guide to Teaching Data Science:\\nAn Interdisciplinary Approach’, Springer, pp. 19–34.\\nHicks, S. C. & Irizarry, R. A. (2018), ‘A Guide to Teaching Data Science’, The American\\nStatistician 72(4), 382–391.\\nHorton, N. J., Alexander, R., Parker, M., Piekut, A. & Rundel, C. (2022), ‘The Growing\\nImportance of Reproducibility and Responsible Workflow in the Data Science and Statistics\\nCurriculum’.\\nHoyt, R. & Wangia-Anderson, V. (2018), ‘An Overview of Two Open Interactive Computing\\nEnvironments Useful for Data Science Education’, JAMIA Open 1(2), 159–165.\\nIoannidis, J. P. A., Klavans, R. & Boyack, K. W. (2018), ‘Thousands of Scientists Publish\\na Paper Every Five Days’, Nature 561(7722), 167–169. Bandiera_abtest: a Cg_type:\\nComment Publisher: Nature Publishing Group Subject_term: Authorship, Publishing.\\nURL: https://www.nature.com/articles/d41586-018-06185-8\\nJunk, T. R. & Lyons, L. (2020), ‘Reproducibility and Replication of Experimental Particle\\nPhysics Results’, Harvard Data Science Review 2(4). arXiv:2009.06864 [physics].\\nURL: http://arxiv.org/abs/2009.06864\\nKakeshita, T., Ishii, K., Ishikawa, Y., Matsubara, H., Matsuo, Y., Murata, T., Nakano,\\n36\\nM., Nakatani, T., Okumura, H., Takahashi, N., Takahashi, N., Uchida, G., Uematsu, E.,\\nSaeki, S. & Kato, H. (2022), Development of IPSJ Data Science Curriculum Standard, in\\nD. Passey, D. Leahy, L. Williams, J. Holvikivi & M. Ruohonen, eds, ‘IFIP Advances in\\nInformation and Communication Technology’, pp. 156–167.\\nKross, S. & Guo, P. J. (2019), Practitioners Teaching Data Science in Industry and Academia:\\nExpectations, Workflows, and Challenges, in ‘CHI ’19’, p. 1–14.\\nLi, F., Xiao, Z., Ng, J. T. D. & Hu, X. (2021), Exploring Interdisciplinary Data Science\\nEducation for Undergraduates: Preliminary Results, in K. Toeppe, H. Yan & S. K. W.\\nChu, eds, ‘International Conference on Information’, Lecture Notes in Computer Science,\\nSpringer International Publishing, Cham, pp. 551–561.\\nLiberati, A., Altman, D. G., Tetzlaff, J., Mulrow, C., Gøtzsche, P. C., Ioannidis, J. P., Clarke,\\nM., Devereaux, P. J., Kleijnen, J. & Moher, D. (2009), ‘The PRISMA Statement for\\nReporting Systematic Reviews and Meta-Analyses of Studies that Evaluate Health Care\\nInterventions: Explanation and Elaboration’, Annals of Internal Medicine 151(4), W–65.\\nLiu, D. (2022), Prepare Data Science Program Student Outcomes and Curricula for ABET\\nAccreditation, in ‘2022 ASEE Annual Conference & Exposition’.\\nLiu, Y. & Wei, X. (2020), How to use stock data for data science education: A simulated\\ntrading platform in classroom, in ‘2020 IEEE 2nd International Conference on Computer\\nScience and Educational Informatization (CSEI)’, pp. 5–8.\\nManzella, G. M. & Emery, W. (2022), How Can Ocean Science Observations Contribute to\\nHumanity?, in ‘Ocean Science Data’, Elsevier, pp. 319–335.\\nMiah, S. J., Solomonides, I. & Gammack, J. G. (2020), ‘A Design-Based Research Ap-\\n37\\nproach for Developing Data-Focussed Business Curricula’, Education and Information\\nTechnologies 25(1), 553–581.\\nMike, K., Nemirovsky-Rotman, S. & Hazzan, O. (2020), Interdisciplinary Education - The\\nCase of Biomedical Signal Processing , in ‘2020 IEEE Global Engineering Education\\nConference (EDUCON)’, pp. 339–343.\\nNair, R., Chugani, M. N. & Thangavel, S. K. (2020), MetaData: A Tool to Supplement\\nData Science Education for the First Year Undergraduates, in ‘ICIET 2020’, New York,\\nNY, USA, p. 153–160.\\nNational Academies of Sciences, Engineering and Medicine (2018), Data Science for Under-\\ngraduates: Opportunities and Options, National Academies Press.\\nNational Science Foundation (2017), ‘NSF’s 10 Big Ideas’.\\nURL: https://www.nsf.gov/news/special_reports/big_ideas/\\nNational Science Foundation (2018), ‘Science and Engineering Indicators’.\\nURL: https://www.nsf.gov/statistics/2018/nsb20181/assets/nsb20181.pdf\\nOliver, J. C. & McNeil, T. (2021), ‘Undergraduate Data Science Degrees Emphasize\\nComputer Science and Statistics but Fall Short in Ethics Training and Domain-Specific\\nContext’, PeerJ Computer Science 7, e441.\\nOpen Science Collaboration (2015), ‘Estimating the Reproducibility of Psychological Science’,\\nScience 349(6251), aac4716. Publisher: American Association for the Advancement of\\nScience.\\nURL: https://www.science.org/doi/10.1126/science.aac4716\\nPieterman-Bos, A. & van Mil, M. H. W. (2023), ‘Integration of Philosophy of Science in\\n38\\nBiomedical Data Science Education to Foster Better Scientific Practice’, Science and\\nEducation 32(6), 1709–1738.\\nPunch, K. F. & Oancea, A. E. (2014), ‘Introduction to Research Methods in Education’.\\nRaj, R. K., Parrish, A., Impagliazzo, J., Romanowski, C. J., Aly, S. G., Bennett, C. C.,\\nDavis, K. C., McGettrick, A., Pereira, T. S. M. & Sundin, L. (2019), An Empirical\\nApproach to Understanding Data Science and Engineering Education, in ‘ITiCSE-WGR\\n’19’, p. 73–87.\\nRao, A., Bihani, A. & Nair, M. (2018), Milo: A Visual Programming Environment for Data\\nScience Education , in ‘2018 IEEE Symposium on Visual Languages and Human-Centric\\nComputing (VL/HCC)’, pp. 211–215.\\nRao, A. R., Desai, Y. & Mishra, K. (2019), Data Science Education Through Education\\nData: an End-to-End Perspective , in ‘2019 IEEE Integrated STEM Education Conference\\n(ISEC)’, pp. 300–307.\\nRobeva, R. S., Jungck, J. R. & Gross, L. J. (2020), ‘Changing the Nature of Quantita-\\ntive Biology Education: Data Science as a Driver’, Bulletin of Mathematical Biology\\n82(10), 127.\\nRyan, L. (2016), From Self-Service to Self-Sufficiency, in L. Ryan, ed., ‘The Visual Impera-\\ntive’, Morgan Kaufmann, Boston, pp. 45–60.\\nSchmitt, K. R. B., Clark, L., Kinnaird, K. M., Wertz, R. E. H. & Sandstede, B. (2023),\\n‘Evaluation of EDISON’s Data Science Competency Framework Through a Comparative\\nLiterature Analysis’, Foundations of Data Science 5(2), 177–198.\\nShao, G., Quintana, J. P., Zakharov, W., Purzer, S. & Kim, E. (2021), ‘Exploring Potential\\n39\\nRoles of Academic Libraries in Undergraduate Data Science Education Curriculum\\nDevelopment’, The Journal of Academic Librarianship 47(2), 102320.\\nShapiro, B. R., Meng, A., O’Donnell, C., Lou, C., Zhao, E., Dankwa, B. & Hostetler, A.\\n(2020), Re-shape: A method to teach data ethics for data science education, in ‘CHI ’20’,\\np. 1–13.\\nSong, I.-Y. & Zhu, Y. (2016), ‘Big Data and Data Science: What Should We Teach?’, Expert\\nSystems 33(4), 364–373.\\nSong, I.-Y. & Zhu, Y. (2017), ‘Big Data and Data Science: Opportunities and Challenges\\nof iSchools’, Journal of Data and Information Science 2(3), 1–18.\\nStern, H. S., Richardson, D. J. & Papaefthymiou, M. (2021), ‘Data science and computing:\\nThe view from a sister campus’, Harvard Data Science Review 3(2).\\nTakemura, A. (2018), ‘A New Era of Statistics and Data Science Education in Japanese\\nUniversities’, Japanese Journal of Statistics and Data Science 1(1), 109–116.\\nTucker, M. C., Shaw, S. T., Son, J. Y. & Stigler, J. W. (2023), ‘Teaching Statistics and\\nData Analysis with R’, Journal of Statistics and Data Science Education 31(1), 18–32.\\nVance, E. A., Glimp, D. R., Pieplow, N. D., Garrity, J. M. & Melbourne, B. A. (2022),\\n‘Integrating the Humanities into Data Science Education’, Statistics Education Research\\nJournal 21(2), 9–9.\\nWiktorski, T., Demchenko, Y. & Belloum, A. (2017), Model Curricula for Data Science\\nEDISON Data Science Framework, in ‘2017 IEEE International Conference on Cloud\\nComputing Technology and Science (CloudCom)’, IEEE, pp. 369–374.\\nWiktorski, T., Demchenko, Y., Belloum, A. & Shirazi, A. (2016), Quantitative and Qual-\\n40\\nitative Analysis of Current Data Science Programs from Perspective of Data Science\\nCompetence Groups and Framework, in ‘2016 IEEE International Conference on Cloud\\nComputing Technology and Science (CloudCom)’, pp. 633–638. ISSN: 2330-2186.\\nWitmer, J. (2020), ‘ASA Journal Gets New Name, Mission’, Amstat News .\\nURL: https://magazine.amstat.org/blog/2020/12/01/journal-gets-new-mission/\\nYamamoto, N., Ishida, A., Ogitsuka, K., Oishi, N. & Murakami, J. (2021), ‘Development of\\nOnline Learning Material for Data Science Programming Using 3D Puzzle’, International\\nJournal of Information and Education Technology 11(4), 154–163.\\nZakaria, M. S. (2023), ‘Data Science Education Programmes in Middle Eastern Institutions:\\nA Survey Study’, IFLA Journal 49(1), 157–179.\\nZhang, J. & Wu, B. (2020), Self and Socially Shared Regulation of Learning in Data Science\\nEducation: A Case Study of “Quantified Self” Project, in ‘ICLS 2020 Proceedings’.\\n41\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove non-ASCII characters \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Remove page numbers or lines that look like \"Page X of Y\"\n",
    "    text = re.sub(r'Page\\s+\\d+\\s+(of\\s+\\d+)?', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    # Remove excessive spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove lines with very few characters (e.g., headers/footers)\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [line for line in lines if len(line.strip()) > 10]\n",
    "    \n",
    "    # Join the cleaned lines back together\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_texts(pdf_texts):\n",
    "    \"\"\"\n",
    "    Clean the extracted text for all PDFs in a dictionary.\n",
    "    \"\"\"\n",
    "    cleaned_texts = {}\n",
    "    for pdf_name, text in pdf_texts.items():\n",
    "        print(f\"Cleaning text for: {pdf_name}\")\n",
    "        cleaned_texts[pdf_name] = clean_text(text)\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text for: 1501.05039v1.pdf\n",
      "Cleaning text for: 1602.00203v1.pdf\n",
      "Cleaning text for: 1607.00858v1.pdf\n",
      "Cleaning text for: 1705.03921v1.pdf\n",
      "Cleaning text for: 1711.03577v1.pdf\n",
      "Cleaning text for: 1805.03551v2.pdf\n",
      "Cleaning text for: 1805.04825v1.pdf\n",
      "Cleaning text for: 1805.08355v1.pdf\n",
      "Cleaning text for: 1806.01756v1.pdf\n",
      "Cleaning text for: 1812.05448v4.pdf\n",
      "Cleaning text for: 1901.02354v2.pdf\n",
      "Cleaning text for: 1901.04195v1.pdf\n",
      "Cleaning text for: 1901.09388v2.pdf\n",
      "Cleaning text for: 1908.02130v1.pdf\n",
      "Cleaning text for: 2002.05658v1.pdf\n",
      "Cleaning text for: 2007.03606v1.pdf\n",
      "Cleaning text for: 2010.05125v2.pdf\n",
      "Cleaning text for: 2106.00120v3.pdf\n",
      "Cleaning text for: 2108.01468v1.pdf\n",
      "Cleaning text for: 2108.11510v1.pdf\n",
      "Cleaning text for: 2112.01590v3.pdf\n",
      "Cleaning text for: 2201.05852v1.pdf\n",
      "Cleaning text for: 2201.05867v1.pdf\n",
      "Cleaning text for: 2303.01980v1.pdf\n",
      "Cleaning text for: 2303.02715v1.pdf\n",
      "Cleaning text for: 2306.13586v1.pdf\n",
      "Cleaning text for: 2306.16177v3.pdf\n",
      "Cleaning text for: 2308.04896v1.pdf\n",
      "Cleaning text for: 2403.00776v1.pdf\n",
      "Cleaning text for: 2403.03387v2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Clean the extracted text for all PDFs\n",
    "cleaned_pdf_texts = clean_all_texts(pdf_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=512):\n",
    "    \"\"\"\n",
    "    Split the text into chunks of maximum `max_tokens` length.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_token + len(word) + 1 > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_token = 0\n",
    "        current_chunk.append(word)\n",
    "        current_token += len(word) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_all_texts(cleaned_texts, max_length=500):\n",
    "    \"\"\"\n",
    "    Chunk all cleaned texts into smaller sections.\n",
    "    \"\"\"\n",
    "    chunked_texts = {}\n",
    "    for pdf_name, text in cleaned_texts.items():\n",
    "        print(f\"Chunking text for: {pdf_name}\")\n",
    "        chunked_texts[pdf_name] = chunk_text(text, max_length)\n",
    "    return chunked_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text for: 1501.05039v1.pdf\n",
      "Chunking text for: 1602.00203v1.pdf\n",
      "Chunking text for: 1607.00858v1.pdf\n",
      "Chunking text for: 1705.03921v1.pdf\n",
      "Chunking text for: 1711.03577v1.pdf\n",
      "Chunking text for: 1805.03551v2.pdf\n",
      "Chunking text for: 1805.04825v1.pdf\n",
      "Chunking text for: 1805.08355v1.pdf\n",
      "Chunking text for: 1806.01756v1.pdf\n",
      "Chunking text for: 1812.05448v4.pdf\n",
      "Chunking text for: 1901.02354v2.pdf\n",
      "Chunking text for: 1901.04195v1.pdf\n",
      "Chunking text for: 1901.09388v2.pdf\n",
      "Chunking text for: 1908.02130v1.pdf\n",
      "Chunking text for: 2002.05658v1.pdf\n",
      "Chunking text for: 2007.03606v1.pdf\n",
      "Chunking text for: 2010.05125v2.pdf\n",
      "Chunking text for: 2106.00120v3.pdf\n",
      "Chunking text for: 2108.01468v1.pdf\n",
      "Chunking text for: 2108.11510v1.pdf\n",
      "Chunking text for: 2112.01590v3.pdf\n",
      "Chunking text for: 2201.05852v1.pdf\n",
      "Chunking text for: 2201.05867v1.pdf\n",
      "Chunking text for: 2303.01980v1.pdf\n",
      "Chunking text for: 2303.02715v1.pdf\n",
      "Chunking text for: 2306.13586v1.pdf\n",
      "Chunking text for: 2306.16177v3.pdf\n",
      "Chunking text for: 2308.04896v1.pdf\n",
      "Chunking text for: 2403.00776v1.pdf\n",
      "Chunking text for: 2403.03387v2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Chunk the cleaned text\n",
    "chunked_texts = chunk_all_texts(cleaned_pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3703\n"
     ]
    }
   ],
   "source": [
    "# Flatten the chunked texts for embedding\n",
    "flat_chunks = [chunk for pdf_chunks in chunked_texts.values() for chunk in pdf_chunks]\n",
    "print(f\"Total chunks created: {len(flat_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 116/116 [00:57<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 3703 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = model.encode(flat_chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"Generated embeddings for {len(flat_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06688278,  0.00978954,  0.02699181, ..., -0.06405541,\n",
       "        -0.00827712, -0.03206822],\n",
       "       [-0.01601519, -0.06354769,  0.00032897, ..., -0.09346824,\n",
       "         0.04706018, -0.02880853],\n",
       "       [-0.01615418,  0.01419142,  0.00591523, ..., -0.03564106,\n",
       "        -0.01791636, -0.03747967],\n",
       "       ...,\n",
       "       [-0.02278017,  0.02977859, -0.01137762, ..., -0.00439937,\n",
       "        -0.00036648, -0.00719787],\n",
       "       [-0.08764973,  0.01078109, -0.0598324 , ..., -0.02534723,\n",
       "         0.00824314,  0.05721295],\n",
       "       [ 0.00350639,  0.04448356, -0.01410194, ...,  0.00064699,\n",
       "        -0.04002129, -0.01773401]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and populated with embeddings.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Define the dimension of the embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(embeddings))\n",
    "print(\"FAISS index created and populated with embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, model, index, chunks, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant text chunks for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k=top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatGroq' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example Query\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the key components of a transformer model?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m relevant_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_relevant_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevant Chunks:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m relevant_chunks:\n",
      "Cell \u001b[1;32mIn[41], line 5\u001b[0m, in \u001b[0;36mretrieve_relevant_chunks\u001b[1;34m(query, model, index, chunks, top_k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve_relevant_chunks\u001b[39m(query, model, index, chunks, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Retrieve the most relevant text chunks for a given query.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m([query])\n\u001b[0;32m      6\u001b[0m     distances, indices \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(np\u001b[38;5;241m.\u001b[39marray(query_embedding), k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [chunks[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\soura\\anaconda3\\envs\\llmpy\\Lib\\site-packages\\pydantic\\main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatGroq' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# Example Query\n",
    "query = \"What are the key components of a transformer model?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, model, index, flat_chunks)\n",
    "\n",
    "print(\"Relevant Chunks:\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the GROQ chat model\n",
    "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(relevant_chunks, max_length=3000):\n",
    "    \"\"\"\n",
    "    Combine text chunks into a single context string within a token limit.\n",
    "    \"\"\"\n",
    "    combined_text = \"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if len(combined_text) + len(chunk) <= max_length:\n",
    "            combined_text += chunk + \"\\n\"\n",
    "        else:\n",
    "            break\n",
    "    return combined_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context):\n",
    "    \"\"\"\n",
    "    Generate a response using LangChain and the retrieved context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Use the context provided to answer the question accurately. \n",
    "    If you do not have information to answer the question, you can say 'I don't have enough information to answer this question'.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    # Invoke the language model\n",
    "    response = llm.invoke(prompt=prompt, max_tokens=300)\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_system(query,model, max_context_length=3000):\n",
    "    \"\"\"\n",
    "    Full RAG system pipeline: retrieves context and generates a response.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, model, index, top_k=5)  # Your retrieval logic here\n",
    "    \n",
    "    # Step 2: Combine retrieved chunks into context\n",
    "    context = combine_chunks(relevant_chunks, max_length=max_context_length)\n",
    "    \n",
    "    # Step 3: Generate a response\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the concept of self-attention in transformers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_context_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m, in \u001b[0;36mquery_rag_system\u001b[1;34m(query, model, max_context_length)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mFull RAG system pipeline: retrieves context and generates a response.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: Retrieve relevant chunks\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m relevant_chunks \u001b[38;5;241m=\u001b[39m retrieve_relevant_chunks(query, model, index, \u001b[43mchunks\u001b[49m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Your retrieval logic here\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Combine retrieved chunks into context\u001b[39;00m\n\u001b[0;32m      9\u001b[0m context \u001b[38;5;241m=\u001b[39m combine_chunks(relevant_chunks, max_length\u001b[38;5;241m=\u001b[39mmax_context_length)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Explain the concept of self-attention in transformers.\"\n",
    "response = query_rag_system(query, model=llm, max_context_length=3000)\n",
    "print(\"Generated Response:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
